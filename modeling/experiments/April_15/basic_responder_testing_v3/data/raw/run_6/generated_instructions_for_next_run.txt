<think>
Despite the interpretability and solid parameter recovery for the loss aversion parameter in the previous run, the model underperformed on both BIC and accuracy versus more concise log-ratio models (see the "best run" in the history with significantly better recovery and fit). The linear piecewise/prospect format, while familiar, isn't optimized for smooth parametric variation—especially in the region just around the fairness reference (rel_offer ≈ rel_deserved). This regime produces abrupt kinks and flat/ambiguous gradients, which can lead to estimation noise, particularly when participant data clusters around this norm.

The history of best models shows the log-ratio family offers: (a) direct, reference-relative valuation; (b) clear parametric control (scaling via a single β or shape/loss aversion parameter); (c) monotonicity and smoothness, ensuring every trial—no matter where the proposal lands—provides information for parameter estimation. Models with nonlinear power/exponential transforms deliver similar benefits, provided the shape parameter is active everywhere (not confined to "loss" or "gain" regions only). Further, the best-recovered models avoided strictly additive bias/intercept parameters.

To boost both fit and parameter recovery for the next round, instructions must:

- Explicitly recommend a *log-ratio* or *generalized ratio-based* model as the base form, highlighting its empirical superiority in parameter recovery and BIC.
- Encourage generalization by incorporating a *single, smoothly-acting, globally-active nonlinearity parameter*—for example, modifying the exponent or sensitivity of the log-ratio, or smoothly tuning reference-dependence.
- Require *no abrupt kinks*, *piecewise switches*, or *max*/*indicator* operations. Transformations should maintain differentiability and parameter sensitivity across the offer reference boundary.
- Emphasize normalization of all amounts (e.g., by “deserved” or “pot size”) to prevent collinearity, increase comparability, and support psychological interpretability.
- Suggest that if loss aversion/asymmetry is modeled, it must smoothly modulate sensitivity below the norm (not switch regimes), such as through a continuous exponent, or by multiplying the log-ratio by a learnable “loss aversion” parameter everywhere (rather than only below the norm).
- Stress the *removal or strong justification* of intercept/bias parameters, as past evidence suggests these add noise and hurt recovery.
- Push towards fully monotonic and interpretable transforms, making calibration of each parameter separable in synthetic and real data.
- Invite creative, out-of-the-box variants on these forms—such as applying a smooth sigmoid or power to the log-ratio, or scaling via a single learnable parameter that impacts unfairness nonlinearly, without causing non-identifiable overlaps.
</think>

---
**Instructions for Run 7 Computational Modeling**

When formulating your formal responder choice model, use the following principles to maximize fit, parameter recovery, and interpretability, intentionally moving beyond standard piecewise or thresholded models:

1. **Adopt a Log-Ratio or Generalized Reference-Dependent Base**
   - Model utility as a (possibly transformed) function of the *ratio* between the actual offer and a fairness norm (e.g., offer/deserved).
   - Encourage using log or log-like functions (including exponentiated or power-law forms) for direct, reference-relative sensitivity; these have empirically yielded the highest recoverability.

2. **Single, Globally-Active, Nonlinear Parameter**
   - Incorporate only one main learnable parameter that smoothly modulates decision sensitivity or loss aversion *across the entire offer space* (not just above or below the reference point).
   - This parameter might directly scale the log-ratio (e.g., β × log(offer/deserved)), apply a power (e.g., log(offer/deserved)^α), or enter as an exponent (e.g., (offer/deserved)^α – 1).
   - Ensure changes in this parameter always correspond to monotonic changes in model predictions for every input.

3. **Strictly No Piecewise, Max, or Discontinuous Terms**
   - Disallow “if-else”, “max(0, …)”, indicator, or stepwise structures that partition the offer space.
   - Any asymmetry or loss aversion should be implemented via a continuous parameter, such as an exponent or reweighting coefficient, that is active globally and produces a smooth response curve.

4. **Psychological Clarity and Normalization**
   - All computation should be made using normalized/relative variables (e.g., percent of pot offered, contribution shares, or offer/deserved ratios), never absolute values unless specifically justified.
   - Give each parameter a clear, isolable psychological interpretation (e.g., “fairness sensitivity,” “nonlinear loss aversion”).

5. **Parameter Limitation and Identifiability**
   - Do not include additive bias/intercept terms or multiple sensitivity parameters unless it is mathematically clear that each controls a fully independent aspect of model output.
   - Every parameter must be demonstrably recoverable through simulation: provide a brief verbal account of how variation in the parameter distinctly shifts predicted acceptance behavior throughout the data range.

6. **Format and Output**
   - Express your mathematical model precisely between <MODEL>…</MODEL> tags, containing *only* the equation.
   - Provide variable descriptions as JSON in <VARIABLES>…</VARIABLES> tags, specifying variable roles (learnable/data/calculated), ranges, and sources.
   - State your prediction target in <target_variable> tags.
   - In <SUMMARY> tags, concisely describe model structure and psychological basis, avoiding generalities or repetition of task details.

7. **Encourage Bold, Yet Justified Innovations**
   - You may propose innovative extensions—like scaling log-ratio by a parameterized sigmoid, applying a global exponent to the reference-dependent deviation, or otherwise achieving interpretable, non-redundant nonlinearity.
   - The model must remain succinct, structurally identifiable, and monotonic.

**Special Emphasis:**
- *No regime switches or thresholded penalties:* The effect of every parameter must be visible and learnable across the entire dataset.
- *Favor log-ratio or smoothly transformed ratio-based models above all.* If proposing a novel nonlinearity, explain how it enhances recoverability and interpretability versus standard forms.
- *Reject intercept or bias terms unless their contribution to fit cannot be absorbed by a sensitivity/shape parameter.*

Be thorough and creative, but guarantee that all parameters are live, independent, interpretable, and yield improved recoverability across the observed offer range.