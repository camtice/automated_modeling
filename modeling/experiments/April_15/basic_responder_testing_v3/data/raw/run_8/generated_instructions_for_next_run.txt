<think>
The previous model achieved a strong balance of fit (BIC, accuracy) and parameter recovery, with two highly recoverable parameters (bias and fairness_sensitivity). However, the learnable epsilon (the log denominator offset) was essentially unrecoverable (r ≈ 0), indicating it functions mainly as a technical stabilizer rather than a meaningful psychological parameter. The overall structure—a bias plus scaled log-ratio of actual/deserved percent—is highly effective, but further gains may require:

1. Addressing the redundancy/non-informativeness of epsilon, either by reimagining its role (as a learnable slope/nonlinearity instead, or as a robust but fixed value), or reallocating complexity to a more globally-influential but still monotonic and separable parameter.
2. Introducing an additional nonlinearity or smooth asymmetry parameter, so long as all components remain globally differentiable, monotonic, and interpretable. For example, a power/exponent shaping of the (splitperc_self / deserved_perc) ratio, or a smooth, learnable slope difference above/below norm—implemented via a hyperbolic tangent or smooth exponential function, not via step/if/max.
3. Ensuring all parameters have non-overlapping psychological interpretations, live gradients across all data, and produce separable effects observable in parameter recovery.

Other possibilities include:
- A model where the main effect is through a power-law (e.g., (offer/deserved)^rho or its log variant), providing flexible psychophysical scaling—rho can control “compression/expansion” around fairness.
- Use of a soft, differentiable global nonlinearity such as tanh, sigmoid, or softplus applied to the bias/scaled log-ratio sum, or transformation of the ratio itself (e.g., log(1 + gamma*[x - 1])).
- Introduction of a single smooth "loss aversion" or "asymmetry" parameter, but in a way that does not rely on piecewise/if/else logic (e.g., via tanh or a parametrized scalar multiplying log-ratio for negative values).

It is essential to avoid adding parameters unless they improve BIC/accuracy and are demonstrably and theoretically recoverable—do not introduce technical terms whose estimation is neither robust nor interpretable. The goal is to surpass previous performance by increasing either fit or identifiability—or, ideally, both—using creative, smooth, nonlinear transforms or asymmetry, but never at the expense of modeling clarity or inference soundness.

Finally, rigorous parameterization and careful explanation of how each parameter can be independently recovered is crucial. If bias is retained, make sure it is not redundant with any new nonlinearity or scaling term.

Encourage out-of-the-box innovations such as generalized ratio exponents, soft nonlinear "loss aversion," activation-based models, or flexible, reference-dependent sigmoids, always within the constraints of monotonicity, smoothness, and interpretability.
</think>

---

**Instructions for Run 9 Computational Modeling**

To design the next-generation computational model for responder behavior in this task, adhere to the following principles to maximize predictive accuracy, model fit (BIC), and parameter interpretability/recovery:

1. **Globally Smooth, Reference-Dependent Utility**
   - Construct your model as a smooth, monotonic function of the *relative* offer—the ratio of the participant's proposed share to their deserved share (e.g., splitperc_self / deserved_perc).
   - All nonlinearities or transforms must be differentiable everywhere and globally monotonic; no discontinuities or regime changes.

2. **Parameterization: Parsimony AND Power**
   - Include up to two or three learnable parameters max, but only if all are demonstrably separable, have clear psychological interpretations, and impact the utility globally across all data.
   - You MUST provide a succinct rationale for how each parameter affects predictions and for why their effects are strictly nonredundant.
   - Strongly consider creative, globally-applied nonlinearity—e.g., a learnable exponent, softplus, or other activation applied either to the ratio itself or to the output of log-ratio/linear sum.
   - If a bias/intercept is included, make clear its unique role and show it does not overlap functionally with the main fairness/shape parameters.

3. **Innovation: Flexible Nonlinearity or Smooth Asymmetry**
   - Encourage use of non-standard but monotonic transforms, such as:
     - Raising the (splitperc_self / deserved_perc) to a learnable exponent (U = bias + beta * ((splitperc_self / deserved_perc)^rho - 1)), to flexibly shape sensitivity above/below fairness.
     - A smooth, differentiable asymmetry parameter controlling loss/gain sensitivity (e.g., U = bias + beta * asinh(alpha * [splitperc_self / deserved_perc - 1])).
     - A global transform applied to the output (e.g., tanh, softplus, or sigmoid of a scaled log-ratio + bias).
     - Explicitly avoid technical parameters that are unrecoverable; any offset/epsilon/floor-type parameter should be included only if empirically justified and recoverable.
   - All proposed transforms must be strictly monotonic and smooth—NO use of max, min, abs, or piecewise logic.

4. **Normalization and Interpretability**
   - All central calculations MUST be in normalized or percent units for offers/deserved shares.
   - Specify, in the variable descriptions, the range, type, (learnable/status), and a brief note on interpretation.
   - For each parameter, describe how varying it shifts the utility curve, and how its impact is kept independent from the others.

5. **Parameter Recoverability**
   - For every learnable parameter included, offer a brief justification for its expected identifiability given the model structure and observed data.

6. **Formatting and Output**
   - The model formula (and ONLY the formula) goes between <MODEL>...</MODEL> tags.
   - Variables, including ranges, descriptions, learnable/data status, and (for learnable) prior distribution, must be formatted as specified in JSON between <VARIABLES>...</VARIABLES> tags.
   - The predicted variable, one from the dataset, goes in <target_variable> tags.
   - Concise model summary (structure, psychological logic, not task details) belongs in <SUMMARY>...</SUMMARY> tags.

7. **Caution on Parameter Addition**
   - Avoid technical or redundant parameters (e.g., a "stabilization" epsilon) unless it is empirically shown to improve model fit and is readily recoverable.
   - Favor adding a global shape/nonlinearity parameter or asymmetry IF and ONLY IF you can explain and demonstrate how it increases both fit (expected BIC/accuracy) and parameter recovery.
   - Do not introduce parameters that act only in pathological data regimes or can be compensated for by other parameters.

8. **Creativity Encouraged**
   - Go beyond the linear and log-ratio staples: propose a novel, smooth, monotonic, and interpretable global nonlinearity or asymmetry, ensuring compatibility with robust parameter recovery and fit.
   - Examples (to inspire, not limit): learnable exponents, arctan/tanh-scaled log-ratio, softplus applied to normalized offer gaps, or gain/loss smoothness via sigmoid-multiplicative slopes.
   - If you use a new nonlinearity, briefly explain its theoretical motivation and its practical advantage over standard log/linear specifications.

Be concise, rigorous, and inventive. Your model must remain implementable with the given dataset and comply with the model and variable formatting standards above. Prioritize genuine improvements in BIC/accuracy or parameter recovery, and explain clearly how each parameter uniquely contributes.