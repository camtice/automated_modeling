

---

**INSTRUCTIONS – RUN 11**

Your mission remains to design a computational model of responder (accept/reject) choices using the provided dataset. Your primary goals are:

- (1) **Maximize parameter recoverability** – Each learnable parameter must independently and robustly affect the model output in a way that is *readily* measurable via empirical data simulation and fitting.
- (2) **Optimize model fit (BIC/accuracy)** – Continue to target the lowest possible BIC and highest predictive accuracy.
- (3) **Enforce psychological plausibility and interpretability** – Mechanisms must correspond to distinct, realistic psychological constructs (fairness threshold, unfairness aversion, generosity/reward sensitivity, and context sensitivity).

**Updated Guidelines and Requirements:**

1. **Absolute Nonoverlap of Parameter Effects:**
   - **Three learnable parameters maximum.** Each must uniquely govern a single, well-separated computational mechanism:
     - One (and only one) parameter sets the fairness threshold, as a bounded scaler of entitlement.
     - One *specifically and only* controls penalty/shape/sensitivity below threshold (the “loss” region).
     - One *specifically and only* controls bonus/shape/sensitivity above threshold (the “gain” region).
   - **Strictly no overlap**: A parameter used in one region/function must not appear anywhere else in the equation.
   - **Context sensitivity, if present, MUST be mediated by its own, separately learnable parameter.** For the sake of recoverability, do not use fixed context multipliers; instead, introduce a third parameter for context amplification in the generosity region, or allow "generosity_weight" to specifically capture *contextual* amplification.
   - Each parameter’s effect on the acceptance curve must be clearly, uniquely visible in simulated toy data when all other parameters are held constant.

2. **Explicit Loss–Gain Asymmetry and Strong Nonlinearities:**
   - You must use *different* nonlinear functions (not just mirrored) for below- and above-threshold regions.
   - Avoid saturating functions whose output range is too limited, unless the steepness can be reliably controlled by a parameter.
   - Consider creative nonlinearities: softplus, powered functions, (exponential minus one), piecewise polynomial, or other *non-tanh* approaches for either/both regions.
   - **Loss/gain regions must not only differ; they must each have a clearly parameterized shape or magnitude.**

3. **Recoverability Emphasis:**
   - In your step-by-step reasoning, explicitly outline—for each parameter—how simulated changes (with all else fixed) produce unique, sizable, and readily distinguishable patterns in the predicted acceptance function.
   - Provide, in your reasoning, at least one toy scenario illustrating how low/medium/high setting for each parameter traces different acceptance curves.
   - Consider especially the above-threshold (“generosity”) region: ensure that the bonus parameter has ample opportunity to uniquely affect acceptances over the trial space.

4. **Contextual Sensitivity as a Learnable Parameter:**
   - If context (e.g., combined_earning) is incorporated, its impact must be handled via a *dedicated*, bounded, and learnable parameter (not a fixed multiplier). This parameter should *only* modulate the generosity region or bonus function.
   - Alternatively, structure the generosity region so that “relative” generosity (e.g., percent above threshold) is used—this can improve parameter identifiability, especially for context.

5. **Parameter Bounds and Variable Description:**
   - Specify wide but plausible finite bounds for all learnable parameters in the <VARIABLES> JSON.
   - Use unambiguous, Python-safe parameter and variable names; no parameter should be named ambiguously or used in more than one computational role.

6. **Strict Prohibitions:**
   - No purely additive (linear) models.
   - No parameter overlap in mathematical regions.
   - Do not allow the context parameter or any other parameter to have effects outside its strictly designated region.
   - Do not use nonlinearities that are so flat or saturating as to erase real data variance (e.g., limit-bounded tanh unless steepness is very tunable).

7. **Output and Format:**
   - Start with a detailed, parameter-by-parameter reasoning—including simulation thought experiments—on identifiability.
   - Mathematical model: Provide only the equation(s) between <MODEL> and </MODEL> tags.
   - All variables and parameters fully described, with bounds, in <VARIABLES> JSON.
   - Explicit target variable within <target_variable> tags.
   - Concise, interpretation-focused model summary in <SUMMARY> tags.

**MANDATORY CREATIVE CHALLENGE:**  
This run, your model MUST include BOTH:
- (A) A *distinct, learnable, and strongly nonlinear penalty function* for sub-threshold (unfair) offers—try a powered or exponentially parameterized penalty, not just log.
- (B) A *distinct, learnable, and strongly nonlinear bonus function* for supra-threshold (generous) offers—consider integrating an explicit context parameter, or parameterizing the steepness (e.g., softplus or power), or using relative generosity as a core argument.

Push for out-of-the-box nonlinearities and mechanisms. The best model will be the one that most clearly dissociates parameter influences—and produces highly recoverable, well-fitted, and interpretable factors. Stimulate creative and mathematically innovative solutions.

---