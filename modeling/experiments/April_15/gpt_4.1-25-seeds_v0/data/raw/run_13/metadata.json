{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "\n\n---\n\n**INSTRUCTIONS \u2013 RUN 13**\n\nYour task is to design a high-performance computational model to predict responder (accept/reject) decisions, focusing on maximal parameter recovery, interpretability, and data fit. Your priorities are:\n\n- **(1) Maximize parameter recoverability and influence:** Every learnable parameter must robustly and *exclusively* control one region of model output, with strong, visually recoverable effects\u2014especially in both sub-threshold (\"unfair\") and supra-threshold (\"generous\") zones. Parameter tuning should always meaningfully and independently alter the acceptance curve, even for edge cases and rare trial types.\n- **(2) Directly optimize for low BIC and high accuracy:** Employ creative nonlinearities and context normalizations to capture the breadth of behavioral patterns seen in the data, not just mean or typical offers.\n- **(3) Retain strict psychological interpretability:** Map all computations and parameters to clear social/psychological constructs (fairness, unfairness aversion, generosity, or context-sensitivity).\n\n**Updated and Expanded Guidelines:**\n\n1. **Parameter Exclusivity and Region Separation:**\n   - Strictly limit to exactly three learnable parameters, each acting *only* in one region: fairness threshold, unfairness penalty, generosity bonus.\n   - Absolutely no parameter overlap between model regions.\n   - Each region's functional form and impact must extend broadly across the actual trial variable range\u2014do not confine effects to a narrow margin or rarely-visited corner.\n\n2. **Powerfully Nonlinear, Distinct Functions (with Explicit Context and Steepness Parameters):**\n   - **Penalty region** (below threshold): Must employ an *exponential-minus-one* or *steeply parameterized* nonlinearity, robustly tunable via a *learnable shape parameter* (not just a scale/strength). Example: -A * [exp(B * relative deficit) \u2013 1], where both A (scale) and B (steepness/power) are possible but only one is learnable; tune B when possible.\n   - **Generosity/bonus region** (above threshold): \n     - Must use a *parameterized*, sharply nonlinear function\u2014such as exponential, steep power, or context-weighted sigmoid.\n     - Must include a *learnable parameter* that governs either the bonus's slope/steepness, its context-dependence, or its relative amplification above threshold. For example: bonus = C * sigmoid(D * context-normed surplus).\n     - The *bonus* function MUST be normalized (by context, threshold, or both) so that its effects generalize across pots and conditions, and parameter estimation is robust.\n     - You are strongly encouraged to introduce a \"context_weight\" or \"bonus_steepness\" parameter that operates *only* in the bonus region.\n   - Functions that saturate quickly, or produce vanishingly small outputs over most of the domain, are **not allowed** unless their \"window\" is fully tunable by a learnable parameter.\n\n3. **Fairness Threshold Construction:**\n   - Implement fairness threshold as a direct function of participant-specific entitlement (e.g., via tokens) if possible. Use a multiplicative scaling parameter (e.g., fairness_multiplier) to anchor recoverability, rather than a raw splitperc cutoff.\n   - If you retain a percentage-based threshold, justify the choice in simulation.\n\n4. **Mandatory, Detailed Simulation-Based Parameter Reasoning:**\n   - For *each* parameter, **explicitly and quantitatively** describe\u2014using at least two realistic trial scenarios for each (one high and one low combined_earning or split)\u2014how changing **only** that parameter alters model utility predictions. For the *bonus parameter*, focus especially on demonstrating parameter impact and recoverability for above-threshold offers of different magnitudes/context.\n   - For the generosity bonus, you must show that as you sweep the bonus parameter, U strongly and smoothly varies for all offers above threshold across small and large pot sizes, *holding all other parameters fixed*.\n   - Explicitly compare and contrast the discriminability (recoverability) between parameters\u2014show there is no redundancy.\n   - Highlight in your reasoning any edge cases (e.g., high generosity offers, large combined_earning) where parameter effects might be strongest or weakest.\n\n5. **Contextual Amplification:**\n   - Where possible, include a *separate, strictly-bounded, learnable parameter* for context sensitivity that acts only in the bonus region (e.g., context_weight for large pots).\n   - In the penalty region, you may normalize or scale by the threshold or by combined_earning for stability and identifiability, but do NOT share scaling parameters across regions.\n\n6. **Formatting and Output Requirements:**\n   - Start with simulation-based parameter reasoning with full quantitative examples and clear cross-comparisons.\n   - Present model equations strictly between <MODEL> and </MODEL> tags, with no descriptive text inside.\n   - Provide all parameter and variable descriptions (with precise bounds and types) in JSON within <VARIABLES> tags, clearly indicating which are learnable.\n   - Include the model\u2019s target variable in <target_variable> tags.\n   - Write a concise, 1\u20132 line mechanism-oriented summary in <SUMMARY> tags focusing on the innovation and recoverability axes of the model design.\n\n7. **Additional Creative Challenge:**  \n   - Combine standard nonlinearities with at least one context-weighted or context-parameterized element in the generosity region, such as: context_weight * exp(surplus), or bonus = A * sigmoid(context_weight * (surplus)).  \n   - Hybrid approaches (e.g. a power-of-exponential bonus, or context-weighted hinge) are actively encouraged!\n\n8. **Explicit Prohibitions:**  \n   - Do NOT use identical or mirrored mathematical forms for both penalty and bonus.\n   - No unparameterized (fixed) nonlinearities or non-contextual bonuses.\n   - No parameter interactions or \"soft overlap\" between regions.\n\nThe goal of this round is for all parameters\u2014threshold, penalty, and generosity/context bonus\u2014to show strong, non-redundant, and highly recoverable effects in fits to both real and simulated data. \n\nPush for innovation, but do not sacrifice parameter identifiability or interpretability!\n\n---",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: \n\n---\n\n**INSTRUCTIONS \u2013 RUN 13**\n\nYour task is to design a high-performance computational model to predict responder (accept/reject) decisions, focusing on maximal parameter recovery, interpretability, and data fit. Your priorities are:\n\n- **(1) Maximize parameter recoverability and influence:** Every learnable parameter must robustly and *exclusively* control one region of model output, with strong, visually recoverable effects\u2014especially in both sub-threshold (\"unfair\") and supra-threshold (\"generous\") zones. Parameter tuning should always meaningfully and independently alter the acceptance curve, even for edge cases and rare trial types.\n- **(2) Directly optimize for low BIC and high accuracy:** Employ creative nonlinearities and context normalizations to capture the breadth of behavioral patterns seen in the data, not just mean or typical offers.\n- **(3) Retain strict psychological interpretability:** Map all computations and parameters to clear social/psychological constructs (fairness, unfairness aversion, generosity, or context-sensitivity).\n\n**Updated and Expanded Guidelines:**\n\n1. **Parameter Exclusivity and Region Separation:**\n   - Strictly limit to exactly three learnable parameters, each acting *only* in one region: fairness threshold, unfairness penalty, generosity bonus.\n   - Absolutely no parameter overlap between model regions.\n   - Each region's functional form and impact must extend broadly across the actual trial variable range\u2014do not confine effects to a narrow margin or rarely-visited corner.\n\n2. **Powerfully Nonlinear, Distinct Functions (with Explicit Context and Steepness Parameters):**\n   - **Penalty region** (below threshold): Must employ an *exponential-minus-one* or *steeply parameterized* nonlinearity, robustly tunable via a *learnable shape parameter* (not just a scale/strength). Example: -A * [exp(B * relative deficit) \u2013 1], where both A (scale) and B (steepness/power) are possible but only one is learnable; tune B when possible.\n   - **Generosity/bonus region** (above threshold): \n     - Must use a *parameterized*, sharply nonlinear function\u2014such as exponential, steep power, or context-weighted sigmoid.\n     - Must include a *learnable parameter* that governs either the bonus's slope/steepness, its context-dependence, or its relative amplification above threshold. For example: bonus = C * sigmoid(D * context-normed surplus).\n     - The *bonus* function MUST be normalized (by context, threshold, or both) so that its effects generalize across pots and conditions, and parameter estimation is robust.\n     - You are strongly encouraged to introduce a \"context_weight\" or \"bonus_steepness\" parameter that operates *only* in the bonus region.\n   - Functions that saturate quickly, or produce vanishingly small outputs over most of the domain, are **not allowed** unless their \"window\" is fully tunable by a learnable parameter.\n\n3. **Fairness Threshold Construction:**\n   - Implement fairness threshold as a direct function of participant-specific entitlement (e.g., via tokens) if possible. Use a multiplicative scaling parameter (e.g., fairness_multiplier) to anchor recoverability, rather than a raw splitperc cutoff.\n   - If you retain a percentage-based threshold, justify the choice in simulation.\n\n4. **Mandatory, Detailed Simulation-Based Parameter Reasoning:**\n   - For *each* parameter, **explicitly and quantitatively** describe\u2014using at least two realistic trial scenarios for each (one high and one low combined_earning or split)\u2014how changing **only** that parameter alters model utility predictions. For the *bonus parameter*, focus especially on demonstrating parameter impact and recoverability for above-threshold offers of different magnitudes/context.\n   - For the generosity bonus, you must show that as you sweep the bonus parameter, U strongly and smoothly varies for all offers above threshold across small and large pot sizes, *holding all other parameters fixed*.\n   - Explicitly compare and contrast the discriminability (recoverability) between parameters\u2014show there is no redundancy.\n   - Highlight in your reasoning any edge cases (e.g., high generosity offers, large combined_earning) where parameter effects might be strongest or weakest.\n\n5. **Contextual Amplification:**\n   - Where possible, include a *separate, strictly-bounded, learnable parameter* for context sensitivity that acts only in the bonus region (e.g., context_weight for large pots).\n   - In the penalty region, you may normalize or scale by the threshold or by combined_earning for stability and identifiability, but do NOT share scaling parameters across regions.\n\n6. **Formatting and Output Requirements:**\n   - Start with simulation-based parameter reasoning with full quantitative examples and clear cross-comparisons.\n   - Present model equations strictly between <MODEL> and </MODEL> tags, with no descriptive text inside.\n   - Provide all parameter and variable descriptions (with precise bounds and types) in JSON within <VARIABLES> tags, clearly indicating which are learnable.\n   - Include the model\u2019s target variable in <target_variable> tags.\n   - Write a concise, 1\u20132 line mechanism-oriented summary in <SUMMARY> tags focusing on the innovation and recoverability axes of the model design.\n\n7. **Additional Creative Challenge:**  \n   - Combine standard nonlinearities with at least one context-weighted or context-parameterized element in the generosity region, such as: context_weight * exp(surplus), or bonus = A * sigmoid(context_weight * (surplus)).  \n   - Hybrid approaches (e.g. a power-of-exponential bonus, or context-weighted hinge) are actively encouraged!\n\n8. **Explicit Prohibitions:**  \n   - Do NOT use identical or mirrored mathematical forms for both penalty and bonus.\n   - No unparameterized (fixed) nonlinearities or non-contextual bonuses.\n   - No parameter interactions or \"soft overlap\" between regions.\n\nThe goal of this round is for all parameters\u2014threshold, penalty, and generosity/context bonus\u2014to show strong, non-redundant, and highly recoverable effects in fits to both real and simulated data. \n\nPush for innovation, but do not sacrifice parameter identifiability or interpretability!\n\n---\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "Let me think step by step to meet all requirements:\n\n---\n\n## Step 1: Core design and region partition\n\n- **Three regions:** (1) Below fairness threshold = penalty (strong, nonlinear, exponential-minus-one), (2) At threshold = zero, (3) Above threshold = context-sensitive generosity bonus (steep, non-saturating, nonlinear, context-amplified).\n- **No parameter overlaps:** Each parameter (threshold, penalty strength/shape, context/bonus weight) controls only one region and effect.\n- **Threshold as entitlement:** Use tokens to determine fairness point.\n- **Context normalization:** All above-threshold (generosity) effects must scale with context (pot size, etc.).\n- **Distinct nonlinearities:** Exponential-minus-one for penalty; context-weighted sigmoid or exponential for bonus.\n\n---\n\n## Step 2: Mathematical Fundamentals\n\n- Let participant\u2019s \"deserved\" share (threshold) be a function of their token count and total tokens:\n  - threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp)) \n      - So fairness_multiplier = learnable, capturing the strictness with which the participant expects their \u2018entitled\u2019 share.\n      - cutoff_point = threshold for fairness as percentage, adaptively tracked per participant and trial.\n- Deficit (D) = threshold_perc - splitperc_self (proposer\u2019s offer is below what responder expects): negative region.\n- Surplus (S) = splitperc_self - threshold_perc (proposer\u2019s offer is above what responder expects): positive region. \n\n- Penalty: For D > 0 (below threshold): Exponential-minus-one (\u2212 penalty_strength \u00d7 [exp(penalty_shape \u00d7 D_norm) \u2212 1]), where D_norm = D / threshold_perc (context normalization), and penalty_shape is a **learnable** sharpness/steepness parameter.\n- Bonus: For S > 0 (above threshold): context_bonus_weight \u00d7 sigmoid(context_bonus_steepness \u00d7 S_norm), where S_norm = S / (100 \u2212 threshold_perc) (context normalization). context_bonus_steepness is a **learnable** parameter\u2014controls how sharply the bonus saturates as offers get more \u2018generous.\u2019 The *context_bonus_weight* is **fixed for this model** (so only one learnable in bonus region with parameter controlling slope).\n\n**Parameter exclusivity:**  \n- fairness_multiplier: Only the *location* of threshold.  \n- penalty_shape: Only negative region\u2019s nonlinearity, not strength (strength is fixed for identifiability).  \n- context_bonus_steepness: Only the steepness of bonus amplification, affecting bonus region.\n\n---\n\n## Step 3: Quantitative Simulation-based Parameter Reasoning\n\n### Parameter 1: fairness_multiplier (controls \u201cwhere\u201d threshold falls)\n- **Low multiplier (e.g., 0.7):** threshold_perc = 0.7 \u00d7 share, so e.g. with tokens [6, 4] (self, opp), threshold_perc = 0.7 \u00d7 (6/10) = 0.42, i.e., 42%; respondent accepts splits above 42%.\n- **High multiplier (e.g., 1.2):** threshold_perc = 1.2 \u00d7 (6/10) = 0.72, so threshold is 72%. Offers need to be much more generous to avoid penalty.\n- **Effect:** Direct shift of the \u201cstep\u201d point demarcating penalty/bonus; moving this multiplier cleanly slides all decision boundaries, without altering penalty or bonus slopes.\n\n### Parameter 2: penalty_shape (controls **penalty function curvature**)\n- Trial: Combined earning = \u00a360 (20 tokens), self: 8, opp: 12,\n  - threshold_perc = .8 \u00d7 (8/20) = 0.32\n  - Offer: splitperc_self = 0.2 (i.e., \u00a312 for self)\n  - D = 0.32 \u2212 0.2 = 0.12, D_norm = 0.12/0.32 = 0.375\n- **Low penalty_shape=1:** Penalty term \u2248 \u2212penalty_strength \u00d7 [exp(1 \u00d7 0.375) \u2212 1] \u2248 \u2212penalty_strength \u00d7 [1.455 \u2212 1] \u2248 \u22120.455 \u00d7 penalty_strength  \n- **High penalty_shape=4:** Penalty \u2248 \u2212penalty_strength \u00d7 [exp(4 \u00d7 0.375) \u2212 1] \u2248 \u2212penalty_strength \u00d7 [exp(1.5) \u2212 1] \u2248 \u2212penalty_strength \u00d7 [4.48 \u2212 1] \u2248 \u22123.48 \u00d7 penalty_strength  \n- **Result:** As penalty_shape increases, penalties for small deficits get much harsher, but bonus/threshold unchanged.\n\n### Parameter 3: context_bonus_steepness (only shapes above-threshold bonus)\n- Participant: tokens [6, 12], threshold_perc = .9 \u00d7 (6/18) \u2248 0.3\n- Offers: splitperc_self = 0.31, S = 0.01, S_norm = 0.01/0.7 \u2248 0.014\n  - **Low context_bonus_steepness (0.5):** sigmoid(0.5 \u00d7 0.014) \u2248 0.507, negligible bonus.\n  - Now: generous offer: splitperc_self = 0.8 (S_norm \u2248 0.7)\n      - context_bonus_steepness = 0.5: sigmoid(0.5 \u00d7 0.7) \u2248 0.586\n      - context_bonus_steepness = 8: sigmoid(8 \u00d7 0.7) \u2248 0.982  \n- **Effect:** Strong, exclusive control of bonus region (only above threshold) steepness and scale. In bigger pots (smaller S_norm step for same \u00a3 surplus), bonus goes up only if context_bonus_steepness is large, directly promoting recoverability.\n\n**Discriminability:**  \nSwapping two of these parameters (e.g., raising bonus_steepness while lowering penalty_shape) will *not* produce similar effect: only penalty region or bonus region respond, never both, and threshold always stays fixed by fairness_multiplier.\n\n---\n\n## Step 4: Edge Cases\n\n- **High combined_earning, high generosity:** As the pot increases, S_norm shrinks for same \u00a3 bonus. So context_bonus_steepness directly governs if rare, large surpluses in big payouts produce maximal bonus (steepness amplifies sensitivity).\n- **Threshold near 100%, super-stringent:** If fairness_multiplier \u2248 1.3, responder never sees a \u201cgenerosity\u201d region, and learns to never accept.\n- **Penalty function at low/high tokens:** Penalty scaling always acts only on normalized deficit, so parameter is robustly recoverable even if participant has <50% tokens.\n\n---\n\n## Step 5: Model Formula\n\nLet\u2019s formalize:\n\nIf splitperc_self < threshold_perc:\n\\[\nU = -penalty_strength \\cdot \\left(e^{penalty_shape \\cdot D_{norm}} - 1\\right)\n\\]\nIf splitperc_self == threshold_perc: \n\\[\nU = 0\n\\]\nIf splitperc_self > threshold_perc:\n\\[\nU = context_bonus_weight \\cdot \\sigma(context_bonus_steepness \\cdot S_{norm})\n\\]\n\nwhere:\n- \\(threshold\\_perc = fairness\\_multiplier \\cdot (token\\_self / (token\\_self + token\\_opp))\\)\n- \\(D_{norm} = \\frac{threshold\\_perc - splitperc\\_self}{threshold\\_perc}\\)\n- \\(S_{norm} = \\frac{splitperc\\_self - threshold\\_perc}{1 - threshold\\_perc}\\)\n- \\(\\sigma(x) = 1/(1+e^{-x})\\) (sigmoid)\n- \\(penalty\\_strength\\) and \\(context\\_bonus\\_weight\\) fixed for identifiability (e.g., penalty_strength = 5, context_bonus_weight = 5)\n\n*Note: Boundaries will be handled so that only one region is active for each trial/offer.*\n\n---\n\n## Step 6: Variables and Parameter Bounds\n\n- fairness_multiplier: [0.5, 1.5] (covers \"modest to demanding\")\n- penalty_shape: [0.5, 10] (from gentle to extremely sharp\u00a0nonlinearity)\n- context_bonus_steepness: [0.5, 20] (gentle to maximally greedy/explosive bonus effect)\n\n---\n\n## Now, final output\u2014\n\n---\n\n### Simulation-based parameter impact recap:\n**fairness_multiplier:** Sliding from 0.5 to 1.5 shifts where the fairness threshold (so penalty region) begins, impacting which offers trigger penalty vs. bonus\u2014no effect on curve shape in penalty/bonus region.\n**penalty_shape:** Controls only steepness of penalty drop-off for unfair offers\u2014at high values, even small deficits lead to sharp drop in utility.\n**context_bonus_steepness:** Controls only the rate at which generous offers (above threshold) translate to acceptance utility\u2014parameter sweep directly and exclusively unfolds the utility in generous region, is robust for tiny to very large surpluses, and is context-normalized for big pots.\n\n---\n\n<MODEL>\nthreshold_perc = fairness_multiplier * (token_self / (token_self + token_opp))\nD_norm = (threshold_perc - splitperc_self) / threshold_perc\nS_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc)\nif splitperc_self < threshold_perc:\n    U = -penalty_strength * (exp(penalty_shape * D_norm) - 1)\nelif splitperc_self == threshold_perc:\n    U = 0\nelse:\n    U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"threshold_perc\": {\n      \"description\": \"Entitlement-based fairness threshold as a percent, set by tokens and fairness_multiplier\",\n      \"range\": {\"min\": 0.0, \"max\": 1.5, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant in pre-task\",\n      \"range\": {\"min\": 0, \"max\": 36, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\"min\": 0, \"max\": 36, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed share for the participant as a proportion (0\u20131)\",\n      \"range\": {\"min\": 0.0, \"max\": 1.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"fairness_multiplier\": {\n      \"description\": \"Learned scaling factor for participant's fairness expectation (threshold); controls where penalty/bonus regions start\",\n      \"range\": {\"min\": 0.5, \"max\": 1.5, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"penalty_strength\": {\n      \"description\": \"(Fixed) Scalar for penalty region amplitude; set for identifiability\",\n      \"range\": {\"min\": 5.0, \"max\": 5.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"fixed\"\n    },\n    \"penalty_shape\": {\n      \"description\": \"Learnable steepness parameter for penalty exponential; exclusively controls penalty nonlinearity\",\n      \"range\": {\"min\": 0.5, \"max\": 10.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"context_bonus_weight\": {\n      \"description\": \"(Fixed) Scalar magnitude for above-threshold bonus; set for identifiability\",\n      \"range\": {\"min\": 5.0, \"max\": 5.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"fixed\"\n    },\n    \"context_bonus_steepness\": {\n      \"description\": \"Learnable slope/steepness of bonus sigmoid for generosity region; governs context scaling and smooth bonus impact\",\n      \"range\": {\"min\": 0.5, \"max\": 20.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"D_norm\": {\n      \"description\": \"Deficit below fairness threshold, normalized by threshold\",\n      \"range\": {\"min\": 0.0, \"max\": 1.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"S_norm\": {\n      \"description\": \"Surplus above fairness threshold, normalized by possible maximum\",\n      \"range\": {\"min\": 0.0, \"max\": 1.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA three-region, context-adaptive model with exclusive, robustly recoverable parameters for fairness threshold, penalty nonlinearity, and generosity bonus steepness; combines exponential-minus-one penalty and context-weighted sigmoid bonus for maximal separation and identifiability across all trial contexts.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['fairness_multiplier', 'penalty_shape', 'context_bonus_steepness']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp))\nD_norm = (threshold_perc - splitperc_self) / threshold_perc\nS_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc)\nif splitperc_self < threshold_perc:\n    U = -penalty_strength * (exp(penalty_shape * D_norm) - 1)\nelif splitperc_self == threshold_perc:\n    U = 0\nelse:\n    U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"threshold_perc\": {\n      \"description\": \"Entitlement-based fairness threshold as a percent, set by tokens and fairness_multiplier\",\n      \"range\": {\n        \"min\": 0.0,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant in pre-task\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed share for the participant as a proportion (0\\u20131)\",\n      \"range\": {\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"fairness_multiplier\": {\n      \"description\": \"Learned scaling factor for participant's fairness expectation (threshold); controls where penalty/bonus regions start\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"penalty_strength\": {\n      \"description\": \"(Fixed) Scalar for penalty region amplitude; set for identifiability\",\n      \"range\": {\n        \"min\": 5.0,\n        \"max\": 5.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"fixed\"\n    },\n    \"penalty_shape\": {\n      \"description\": \"Learnable steepness parameter for penalty exponential; exclusively controls penalty nonlinearity\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 10.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"context_bonus_weight\": {\n      \"description\": \"(Fixed) Scalar magnitude for above-threshold bonus; set for identifiability\",\n      \"range\": {\n        \"min\": 5.0,\n        \"max\": 5.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"fixed\"\n    },\n    \"context_bonus_steepness\": {\n      \"description\": \"Learnable slope/steepness of bonus sigmoid for generosity region; governs context scaling and smooth bonus impact\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 20.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"D_norm\": {\n      \"description\": \"Deficit below fairness threshold, normalized by threshold\",\n      \"range\": {\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"S_norm\": {\n      \"description\": \"Surplus above fairness threshold, normalized by possible maximum\",\n      \"range\": {\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\ndef simulate_model(trial_data, fairness_multiplier=1.0, penalty_shape=1.0, context_bonus_steepness=1.0):\n    model_predictions = []\n    penalty_strength = 5.0\n    context_bonus_weight = 5.0\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        splitperc_self = trial.get(\"splitperc_self\", 0.5)  # Default value to prevent errors\n\n        # Calculate threshold_perc\n        if (token_self + token_opp) == 0:\n            threshold_perc = 0.5 # handle cases where both are zero (avoid division by zero, assign neutral fairness)\n        else:\n            threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp))\n\n        # Handle boundary cases for D_norm and S_norm\n        if threshold_perc == 0:\n            D_norm = 0.0 # avoid division by zero\n        else:\n            D_norm = (threshold_perc - splitperc_self) / threshold_perc\n\n        if threshold_perc == 1:\n            S_norm = 0.0 # avoid division by zero when the threshold is 1.\n        else:\n            S_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc)\n\n        # Calculate Utility based on regions\n        if splitperc_self < threshold_perc:\n            U = -penalty_strength * (math.exp(penalty_shape * D_norm) - 1)\n        elif splitperc_self == threshold_perc:\n            U = 0.0\n        else:\n            U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\n\n        model_predictions.append(U)\n    return model_predictions\n```",
      "timestamp": 1744726626.111535
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['fairness_multiplier', 'penalty_shape', 'context_bonus_steepness']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp))\nD_norm = (threshold_perc - splitperc_self) / threshold_perc\nS_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc)\nif splitperc_self < threshold_perc:\n    U = -penalty_strength * (exp(penalty_shape * D_norm) - 1)\nelif splitperc_self == threshold_perc:\n    U = 0\nelse:\n    U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"threshold_perc\": {\n      \"description\": \"Entitlement-based fairness threshold as a percent, set by tokens and fairness_multiplier\",\n      \"range\": {\n        \"min\": 0.0,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant in pre-task\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed share for the participant as a proportion (0\\u20131)\",\n      \"range\": {\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"fairness_multiplier\": {\n      \"description\": \"Learned scaling factor for participant's fairness expectation (threshold); controls where penalty/bonus regions start\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"penalty_strength\": {\n      \"description\": \"(Fixed) Scalar for penalty region amplitude; set for identifiability\",\n      \"range\": {\n        \"min\": 5.0,\n        \"max\": 5.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"fixed\"\n    },\n    \"penalty_shape\": {\n      \"description\": \"Learnable steepness parameter for penalty exponential; exclusively controls penalty nonlinearity\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 10.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"context_bonus_weight\": {\n      \"description\": \"(Fixed) Scalar magnitude for above-threshold bonus; set for identifiability\",\n      \"range\": {\n        \"min\": 5.0,\n        \"max\": 5.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"fixed\"\n    },\n    \"context_bonus_steepness\": {\n      \"description\": \"Learnable slope/steepness of bonus sigmoid for generosity region; governs context scaling and smooth bonus impact\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 20.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"D_norm\": {\n      \"description\": \"Deficit below fairness threshold, normalized by threshold\",\n      \"range\": {\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"S_norm\": {\n      \"description\": \"Surplus above fairness threshold, normalized by possible maximum\",\n      \"range\": {\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "```python\nimport json\nimport math\n\ndef simulate_model(trial_data, fairness_multiplier, penalty_shape, context_bonus_steepness):\n    model_predictions = []\n    penalty_strength = 5.0\n    context_bonus_weight = 5.0\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        splitperc_self = trial.get(\"splitperc_self\", 0.0)\n\n        threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp)) if (token_self + token_opp) > 0 else fairness_multiplier * 0.5 # handling edge case of zero tokens total. Assumes 50% split if no tokens.\n\n        if splitperc_self < threshold_perc:\n            D_norm = (threshold_perc - splitperc_self) / threshold_perc if threshold_perc > 0 else 0 # prevent division by zero\n            U = -penalty_strength * (math.exp(penalty_shape * D_norm) - 1)\n        elif abs(splitperc_self - threshold_perc) < 1e-6:  # Use a small tolerance for equality check\n            U = 0\n        else:\n            S_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc) if (1 - threshold_perc) > 0 else 0 # prevent division by zero\n            U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\n\n        model_predictions.append(U)\n\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 1, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 11, \"token_self\": 25, \"combined_earning\": 72, \"split_opp\": 33.12, \"split_self\": 38.88, \"splitperc_opp\": 46, \"splitperc_self\": 54, \"accept\": 1.0, \"accepted_amount\": 33.12, \"accepted_perc\": 46.0, \"proposed_perc\": 54.0, \"proposed_amount\": 38.88}, {\"ID\": 2, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 11, \"token_self\": 25, \"combined_earning\": 60, \"split_opp\": 33.0, \"split_self\": 27.0, \"splitperc_opp\": 55, \"splitperc_self\": 45, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 45.0, \"proposed_amount\": 27.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data, fairness_multiplier=0.5, penalty_shape=0.5, context_bonus_steepness=0.5)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n",
      "timestamp": 1744726630.249762
    }
  ],
  "learnable_parameters": {
    "fairness_multiplier": {
      "description": "Learned scaling factor for participant's fairness expectation (threshold); controls where penalty/bonus regions start",
      "range": {
        "min": 0.5,
        "max": 1.5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "penalty_shape": {
      "description": "Learnable steepness parameter for penalty exponential; exclusively controls penalty nonlinearity",
      "range": {
        "min": 0.5,
        "max": 10.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "context_bonus_steepness": {
      "description": "Learnable slope/steepness of bonus sigmoid for generosity region; governs context scaling and smooth bonus impact",
      "range": {
        "min": 0.5,
        "max": 20.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp))\nD_norm = (threshold_perc - splitperc_self) / threshold_perc\nS_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc)\nif splitperc_self < threshold_perc:\n    U = -penalty_strength * (exp(penalty_shape * D_norm) - 1)\nelif splitperc_self == threshold_perc:\n    U = 0\nelse:\n    U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)",
  "variable_descriptions": {
    "threshold_perc": {
      "description": "Entitlement-based fairness threshold as a percent, set by tokens and fairness_multiplier",
      "range": {
        "min": 0.0,
        "max": 1.5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "token_self": {
      "description": "Number of tokens found by participant in pre-task",
      "range": {
        "min": 0,
        "max": 36,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent",
      "range": {
        "min": 0,
        "max": 36,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "splitperc_self": {
      "description": "Proposed share for the participant as a proportion (0\u20131)",
      "range": {
        "min": 0.0,
        "max": 1.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "fairness_multiplier": {
      "description": "Learned scaling factor for participant's fairness expectation (threshold); controls where penalty/bonus regions start",
      "range": {
        "min": 0.5,
        "max": 1.5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "penalty_strength": {
      "description": "(Fixed) Scalar for penalty region amplitude; set for identifiability",
      "range": {
        "min": 5.0,
        "max": 5.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "fixed"
    },
    "penalty_shape": {
      "description": "Learnable steepness parameter for penalty exponential; exclusively controls penalty nonlinearity",
      "range": {
        "min": 0.5,
        "max": 10.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "context_bonus_weight": {
      "description": "(Fixed) Scalar magnitude for above-threshold bonus; set for identifiability",
      "range": {
        "min": 5.0,
        "max": 5.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "fixed"
    },
    "context_bonus_steepness": {
      "description": "Learnable slope/steepness of bonus sigmoid for generosity region; governs context scaling and smooth bonus impact",
      "range": {
        "min": 0.5,
        "max": 20.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "D_norm": {
      "description": "Deficit below fairness threshold, normalized by threshold",
      "range": {
        "min": 0.0,
        "max": 1.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "S_norm": {
      "description": "Surplus above fairness threshold, normalized by possible maximum",
      "range": {
        "min": 0.0,
        "max": 1.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    }
  },
  "model_summary": "A three-region, context-adaptive model with exclusive, robustly recoverable parameters for fairness threshold, penalty nonlinearity, and generosity bonus steepness; combines exponential-minus-one penalty and context-weighted sigmoid bonus for maximal separation and identifiability across all trial contexts.",
  "target_variable": "accept",
  "full_reasoning": "Let me think step by step to meet all requirements:\n\n---\n\n## Step 1: Core design and region partition\n\n- **Three regions:** (1) Below fairness threshold = penalty (strong, nonlinear, exponential-minus-one), (2) At threshold = zero, (3) Above threshold = context-sensitive generosity bonus (steep, non-saturating, nonlinear, context-amplified).\n- **No parameter overlaps:** Each parameter (threshold, penalty strength/shape, context/bonus weight) controls only one region and effect.\n- **Threshold as entitlement:** Use tokens to determine fairness point.\n- **Context normalization:** All above-threshold (generosity) effects must scale with context (pot size, etc.).\n- **Distinct nonlinearities:** Exponential-minus-one for penalty; context-weighted sigmoid or exponential for bonus.\n\n---\n\n## Step 2: Mathematical Fundamentals\n\n- Let participant\u2019s \"deserved\" share (threshold) be a function of their token count and total tokens:\n  - threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp)) \n      - So fairness_multiplier = learnable, capturing the strictness with which the participant expects their \u2018entitled\u2019 share.\n      - cutoff_point = threshold for fairness as percentage, adaptively tracked per participant and trial.\n- Deficit (D) = threshold_perc - splitperc_self (proposer\u2019s offer is below what responder expects): negative region.\n- Surplus (S) = splitperc_self - threshold_perc (proposer\u2019s offer is above what responder expects): positive region. \n\n- Penalty: For D > 0 (below threshold): Exponential-minus-one (\u2212 penalty_strength \u00d7 [exp(penalty_shape \u00d7 D_norm) \u2212 1]), where D_norm = D / threshold_perc (context normalization), and penalty_shape is a **learnable** sharpness/steepness parameter.\n- Bonus: For S > 0 (above threshold): context_bonus_weight \u00d7 sigmoid(context_bonus_steepness \u00d7 S_norm), where S_norm = S / (100 \u2212 threshold_perc) (context normalization). context_bonus_steepness is a **learnable** parameter\u2014controls how sharply the bonus saturates as offers get more \u2018generous.\u2019 The *context_bonus_weight* is **fixed for this model** (so only one learnable in bonus region with parameter controlling slope).\n\n**Parameter exclusivity:**  \n- fairness_multiplier: Only the *location* of threshold.  \n- penalty_shape: Only negative region\u2019s nonlinearity, not strength (strength is fixed for identifiability).  \n- context_bonus_steepness: Only the steepness of bonus amplification, affecting bonus region.\n\n---\n\n## Step 3: Quantitative Simulation-based Parameter Reasoning\n\n### Parameter 1: fairness_multiplier (controls \u201cwhere\u201d threshold falls)\n- **Low multiplier (e.g., 0.7):** threshold_perc = 0.7 \u00d7 share, so e.g. with tokens [6, 4] (self, opp), threshold_perc = 0.7 \u00d7 (6/10) = 0.42, i.e., 42%; respondent accepts splits above 42%.\n- **High multiplier (e.g., 1.2):** threshold_perc = 1.2 \u00d7 (6/10) = 0.72, so threshold is 72%. Offers need to be much more generous to avoid penalty.\n- **Effect:** Direct shift of the \u201cstep\u201d point demarcating penalty/bonus; moving this multiplier cleanly slides all decision boundaries, without altering penalty or bonus slopes.\n\n### Parameter 2: penalty_shape (controls **penalty function curvature**)\n- Trial: Combined earning = \u00a360 (20 tokens), self: 8, opp: 12,\n  - threshold_perc = .8 \u00d7 (8/20) = 0.32\n  - Offer: splitperc_self = 0.2 (i.e., \u00a312 for self)\n  - D = 0.32 \u2212 0.2 = 0.12, D_norm = 0.12/0.32 = 0.375\n- **Low penalty_shape=1:** Penalty term \u2248 \u2212penalty_strength \u00d7 [exp(1 \u00d7 0.375) \u2212 1] \u2248 \u2212penalty_strength \u00d7 [1.455 \u2212 1] \u2248 \u22120.455 \u00d7 penalty_strength  \n- **High penalty_shape=4:** Penalty \u2248 \u2212penalty_strength \u00d7 [exp(4 \u00d7 0.375) \u2212 1] \u2248 \u2212penalty_strength \u00d7 [exp(1.5) \u2212 1] \u2248 \u2212penalty_strength \u00d7 [4.48 \u2212 1] \u2248 \u22123.48 \u00d7 penalty_strength  \n- **Result:** As penalty_shape increases, penalties for small deficits get much harsher, but bonus/threshold unchanged.\n\n### Parameter 3: context_bonus_steepness (only shapes above-threshold bonus)\n- Participant: tokens [6, 12], threshold_perc = .9 \u00d7 (6/18) \u2248 0.3\n- Offers: splitperc_self = 0.31, S = 0.01, S_norm = 0.01/0.7 \u2248 0.014\n  - **Low context_bonus_steepness (0.5):** sigmoid(0.5 \u00d7 0.014) \u2248 0.507, negligible bonus.\n  - Now: generous offer: splitperc_self = 0.8 (S_norm \u2248 0.7)\n      - context_bonus_steepness = 0.5: sigmoid(0.5 \u00d7 0.7) \u2248 0.586\n      - context_bonus_steepness = 8: sigmoid(8 \u00d7 0.7) \u2248 0.982  \n- **Effect:** Strong, exclusive control of bonus region (only above threshold) steepness and scale. In bigger pots (smaller S_norm step for same \u00a3 surplus), bonus goes up only if context_bonus_steepness is large, directly promoting recoverability.\n\n**Discriminability:**  \nSwapping two of these parameters (e.g., raising bonus_steepness while lowering penalty_shape) will *not* produce similar effect: only penalty region or bonus region respond, never both, and threshold always stays fixed by fairness_multiplier.\n\n---\n\n## Step 4: Edge Cases\n\n- **High combined_earning, high generosity:** As the pot increases, S_norm shrinks for same \u00a3 bonus. So context_bonus_steepness directly governs if rare, large surpluses in big payouts produce maximal bonus (steepness amplifies sensitivity).\n- **Threshold near 100%, super-stringent:** If fairness_multiplier \u2248 1.3, responder never sees a \u201cgenerosity\u201d region, and learns to never accept.\n- **Penalty function at low/high tokens:** Penalty scaling always acts only on normalized deficit, so parameter is robustly recoverable even if participant has <50% tokens.\n\n---\n\n## Step 5: Model Formula\n\nLet\u2019s formalize:\n\nIf splitperc_self < threshold_perc:\n\\[\nU = -penalty_strength \\cdot \\left(e^{penalty_shape \\cdot D_{norm}} - 1\\right)\n\\]\nIf splitperc_self == threshold_perc: \n\\[\nU = 0\n\\]\nIf splitperc_self > threshold_perc:\n\\[\nU = context_bonus_weight \\cdot \\sigma(context_bonus_steepness \\cdot S_{norm})\n\\]\n\nwhere:\n- \\(threshold\\_perc = fairness\\_multiplier \\cdot (token\\_self / (token\\_self + token\\_opp))\\)\n- \\(D_{norm} = \\frac{threshold\\_perc - splitperc\\_self}{threshold\\_perc}\\)\n- \\(S_{norm} = \\frac{splitperc\\_self - threshold\\_perc}{1 - threshold\\_perc}\\)\n- \\(\\sigma(x) = 1/(1+e^{-x})\\) (sigmoid)\n- \\(penalty\\_strength\\) and \\(context\\_bonus\\_weight\\) fixed for identifiability (e.g., penalty_strength = 5, context_bonus_weight = 5)\n\n*Note: Boundaries will be handled so that only one region is active for each trial/offer.*\n\n---\n\n## Step 6: Variables and Parameter Bounds\n\n- fairness_multiplier: [0.5, 1.5] (covers \"modest to demanding\")\n- penalty_shape: [0.5, 10] (from gentle to extremely sharp\u00a0nonlinearity)\n- context_bonus_steepness: [0.5, 20] (gentle to maximally greedy/explosive bonus effect)\n\n---\n\n## Now, final output\u2014\n\n---\n\n### Simulation-based parameter impact recap:\n**fairness_multiplier:** Sliding from 0.5 to 1.5 shifts where the fairness threshold (so penalty region) begins, impacting which offers trigger penalty vs. bonus\u2014no effect on curve shape in penalty/bonus region.\n**penalty_shape:** Controls only steepness of penalty drop-off for unfair offers\u2014at high values, even small deficits lead to sharp drop in utility.\n**context_bonus_steepness:** Controls only the rate at which generous offers (above threshold) translate to acceptance utility\u2014parameter sweep directly and exclusively unfolds the utility in generous region, is robust for tiny to very large surpluses, and is context-normalized for big pots.\n\n---\n\n<MODEL>\nthreshold_perc = fairness_multiplier * (token_self / (token_self + token_opp))\nD_norm = (threshold_perc - splitperc_self) / threshold_perc\nS_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc)\nif splitperc_self < threshold_perc:\n    U = -penalty_strength * (exp(penalty_shape * D_norm) - 1)\nelif splitperc_self == threshold_perc:\n    U = 0\nelse:\n    U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"threshold_perc\": {\n      \"description\": \"Entitlement-based fairness threshold as a percent, set by tokens and fairness_multiplier\",\n      \"range\": {\"min\": 0.0, \"max\": 1.5, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant in pre-task\",\n      \"range\": {\"min\": 0, \"max\": 36, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\"min\": 0, \"max\": 36, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed share for the participant as a proportion (0\u20131)\",\n      \"range\": {\"min\": 0.0, \"max\": 1.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"fairness_multiplier\": {\n      \"description\": \"Learned scaling factor for participant's fairness expectation (threshold); controls where penalty/bonus regions start\",\n      \"range\": {\"min\": 0.5, \"max\": 1.5, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"penalty_strength\": {\n      \"description\": \"(Fixed) Scalar for penalty region amplitude; set for identifiability\",\n      \"range\": {\"min\": 5.0, \"max\": 5.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"fixed\"\n    },\n    \"penalty_shape\": {\n      \"description\": \"Learnable steepness parameter for penalty exponential; exclusively controls penalty nonlinearity\",\n      \"range\": {\"min\": 0.5, \"max\": 10.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"context_bonus_weight\": {\n      \"description\": \"(Fixed) Scalar magnitude for above-threshold bonus; set for identifiability\",\n      \"range\": {\"min\": 5.0, \"max\": 5.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"fixed\"\n    },\n    \"context_bonus_steepness\": {\n      \"description\": \"Learnable slope/steepness of bonus sigmoid for generosity region; governs context scaling and smooth bonus impact\",\n      \"range\": {\"min\": 0.5, \"max\": 20.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"D_norm\": {\n      \"description\": \"Deficit below fairness threshold, normalized by threshold\",\n      \"range\": {\"min\": 0.0, \"max\": 1.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"S_norm\": {\n      \"description\": \"Surplus above fairness threshold, normalized by possible maximum\",\n      \"range\": {\"min\": 0.0, \"max\": 1.0, \"inclusive_min\": true, \"inclusive_max\": true},\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA three-region, context-adaptive model with exclusive, robustly recoverable parameters for fairness threshold, penalty nonlinearity, and generosity bonus steepness; combines exponential-minus-one penalty and context-weighted sigmoid bonus for maximal separation and identifiability across all trial contexts.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp))\nD_norm = (threshold_perc - splitperc_self) / threshold_perc\nS_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc)\nif splitperc_self < threshold_perc:\n    U = -penalty_strength * (exp(penalty_shape * D_norm) - 1)\nelif splitperc_self == threshold_perc:\n    U = 0\nelse:\n    U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\nSummary: A three-region, context-adaptive model with exclusive, robustly recoverable parameters for fairness threshold, penalty nonlinearity, and generosity bonus steepness; combines exponential-minus-one penalty and context-weighted sigmoid bonus for maximal separation and identifiability across all trial contexts.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [
    {
      "retry_number": 0,
      "error": "Traceback (most recent call last):\n  File \"<string>\", line 47, in <module>\n    results = simulate_model(data, fairness_multiplier=0.5, penalty_shape=0.5, context_bonus_steepness=0.5)\n  File \"<string>\", line 38, in simulate_model\n    U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\n                               ^^^^^^^\nNameError: name 'sigmoid' is not defined\n",
      "timestamp": 1744726626.1740391
    }
  ],
  "current_retry": 1,
  "simulation_error": "Traceback (most recent call last):\n  File \"<string>\", line 47, in <module>\n    results = simulate_model(data, fairness_multiplier=0.5, penalty_shape=0.5, context_bonus_steepness=0.5)\n  File \"<string>\", line 38, in simulate_model\n    U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\n                               ^^^^^^^\nNameError: name 'sigmoid' is not defined\n",
  "simulation_code": "def simulate_model(trial_data, fairness_multiplier, penalty_shape, context_bonus_steepness):\n    model_predictions = []\n    penalty_strength = 5.0\n    context_bonus_weight = 5.0\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        splitperc_self = trial.get(\"splitperc_self\", 0.0)\n\n        threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp)) if (token_self + token_opp) > 0 else fairness_multiplier * 0.5 # handling edge case of zero tokens total. Assumes 50% split if no tokens.\n\n        if splitperc_self < threshold_perc:\n            D_norm = (threshold_perc - splitperc_self) / threshold_perc if threshold_perc > 0 else 0 # prevent division by zero\n            U = -penalty_strength * (math.exp(penalty_shape * D_norm) - 1)\n        elif abs(splitperc_self - threshold_perc) < 1e-6:  # Use a small tolerance for equality check\n            U = 0\n        else:\n            S_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc) if (1 - threshold_perc) > 0 else 0 # prevent division by zero\n            U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\n\n        model_predictions.append(U)\n\n    return model_predictions",
  "total_retries": 1,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.568070818070818,
  "group_accuracies": {
    "Control": 0.6375968992248062,
    "Cocaine": 0.505787037037037
  },
  "group_parameter_averages": {
    "Control": {
      "fairness_multiplier": 0.9005714481404069,
      "penalty_shape": 4.84066961310628,
      "context_bonus_steepness": 8.013111044798682
    },
    "Cocaine": {
      "fairness_multiplier": 0.9764736325216263,
      "penalty_shape": 4.264497239592062,
      "context_bonus_steepness": 10.393544806594702
    }
  },
  "fitting_results": [
    {
      "fairness_multiplier": 0.6941303045934932,
      "penalty_shape": 1.9226083880013851,
      "context_bonus_steepness": 13.451079539640624,
      "success": true,
      "log_likelihood": -165.24175205224336,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.05555555555555555,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.746037500723368,
      "penalty_shape": 3.970384778205799,
      "context_bonus_steepness": 7.134571768255686,
      "success": true,
      "log_likelihood": -90.24175227485276,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7488429230158385,
      "penalty_shape": 2.3471189002865938,
      "context_bonus_steepness": 1.2574617761226892,
      "success": true,
      "log_likelihood": -85.24175213248024,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8390083571008025,
      "penalty_shape": 6.3445557621993425,
      "context_bonus_steepness": 9.474313251892303,
      "success": true,
      "log_likelihood": -105.24175223033083,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.1794557376510655,
      "penalty_shape": 7.570052294084515,
      "context_bonus_steepness": 15.084233627062192,
      "success": true,
      "log_likelihood": -100.24175224517148,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5165279979792452,
      "penalty_shape": 1.4556668436270355,
      "context_bonus_steepness": 4.8547185159616655,
      "success": true,
      "log_likelihood": -65.2417523490559,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8718348421881842,
      "penalty_shape": 6.0949473344893415,
      "context_bonus_steepness": 4.195357913189724,
      "success": true,
      "log_likelihood": -0.24175254198398927,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7511037207604092,
      "penalty_shape": 4.124878565026147,
      "context_bonus_steepness": 11.909744289239546,
      "success": true,
      "log_likelihood": -60.24175236389656,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7778844861219867,
      "penalty_shape": 1.8531862947050288,
      "context_bonus_steepness": 7.859087974926907,
      "success": true,
      "log_likelihood": -0.24175254198398927,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7917977604084665,
      "penalty_shape": 1.1817681625198924,
      "context_bonus_steepness": 14.649420675665777,
      "success": true,
      "log_likelihood": -110.24175221549022,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5868397714257768,
      "penalty_shape": 2.0728707615022977,
      "context_bonus_steepness": 17.393052057883107,
      "success": true,
      "log_likelihood": -100.2417522451715,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.3239318856566857,
      "penalty_shape": 2.745480212082041,
      "context_bonus_steepness": 5.608139386270339,
      "success": true,
      "log_likelihood": -160.24175206708398,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.1111111111111111,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.2725538008964878,
      "penalty_shape": 1.9675613602619553,
      "context_bonus_steepness": 10.988671569839662,
      "success": true,
      "log_likelihood": -105.24175223033086,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7529335423929074,
      "penalty_shape": 9.631984777183522,
      "context_bonus_steepness": 8.263787122070674,
      "success": true,
      "log_likelihood": -0.24175254198398927,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8160294686058237,
      "penalty_shape": 0.8451250538684345,
      "context_bonus_steepness": 8.560662532606695,
      "success": true,
      "log_likelihood": -0.24175254198398927,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.6001342196581656,
      "penalty_shape": 6.514889276540161,
      "context_bonus_steepness": 10.90119347134185,
      "success": true,
      "log_likelihood": -0.24175254198398927,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.323556514429213,
      "penalty_shape": 6.807857000084336,
      "context_bonus_steepness": 12.667958115720275,
      "success": true,
      "log_likelihood": -0.24175254198398927,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5210071641767744,
      "penalty_shape": 1.1374868330267638,
      "context_bonus_steepness": 16.242628484553585,
      "success": true,
      "log_likelihood": -25.24175246778088,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5732071093233959,
      "penalty_shape": 3.27386497421692,
      "context_bonus_steepness": 9.918449692507778,
      "success": true,
      "log_likelihood": -20.241752482621468,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.2432515852589674,
      "penalty_shape": 6.607210909545631,
      "context_bonus_steepness": 11.717159116859454,
      "success": true,
      "log_likelihood": -0.24175254198398927,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8795716870299607,
      "penalty_shape": 3.6651925765464206,
      "context_bonus_steepness": 2.2126199586446655,
      "success": true,
      "log_likelihood": -80.24175230453399,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7024485024037889,
      "penalty_shape": 9.793724021195732,
      "context_bonus_steepness": 12.704875525307425,
      "success": true,
      "log_likelihood": -25.241752467780877,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.1082613949017603,
      "penalty_shape": 1.6582167085968755,
      "context_bonus_steepness": 0.9643946944919386,
      "success": true,
      "log_likelihood": -115.24175187332318,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3611111111111111,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.4536358444483861,
      "penalty_shape": 7.2436116604434915,
      "context_bonus_steepness": 6.392798596540215,
      "success": true,
      "log_likelihood": -85.24175228969337,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.42157557727425,
      "penalty_shape": 9.121940831078582,
      "context_bonus_steepness": 1.467786489828351,
      "success": true,
      "log_likelihood": -90.2417522748527,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.9583836709365204,
      "penalty_shape": 1.4323135468647648,
      "context_bonus_steepness": 2.7278870023023445,
      "success": true,
      "log_likelihood": -75.24175231937461,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.2770718204034956,
      "penalty_shape": 5.620738892690626,
      "context_bonus_steepness": 16.719334616893825,
      "success": true,
      "log_likelihood": -80.24175230453402,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8217163197425669,
      "penalty_shape": 7.338092357444639,
      "context_bonus_steepness": 1.0826873702327824,
      "success": true,
      "log_likelihood": -130.2417512025776,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.2777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5,
      "penalty_shape": 2.227735633351782,
      "context_bonus_steepness": 0.5,
      "success": true,
      "log_likelihood": -20.23700408692703,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5008416800436127,
      "penalty_shape": 8.739110628843468,
      "context_bonus_steepness": 10.456269478450347,
      "success": true,
      "log_likelihood": -40.24175242325902,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.450671503537989,
      "penalty_shape": 8.248200880511583,
      "context_bonus_steepness": 6.456759507117729,
      "success": true,
      "log_likelihood": -0.24175254198398927,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8870705004251017,
      "penalty_shape": 2.1693491977963877,
      "context_bonus_steepness": 1.0983890078242347,
      "success": true,
      "log_likelihood": -165.241751652254,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.1111111111111111,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.3022132753442335,
      "penalty_shape": 2.096583569085551,
      "context_bonus_steepness": 1.3731173012964528,
      "success": true,
      "log_likelihood": -105.24175223032294,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.507182770936722,
      "penalty_shape": 8.821913346478034,
      "context_bonus_steepness": 10.240901413527068,
      "success": true,
      "log_likelihood": -110.24175221549022,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.6293134286642034,
      "penalty_shape": 8.551595172106223,
      "context_bonus_steepness": 13.253714806024833,
      "success": true,
      "log_likelihood": -0.24175254198398927,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.6212873044387194,
      "penalty_shape": 7.772691429494559,
      "context_bonus_steepness": 10.749021467281556,
      "success": true,
      "log_likelihood": -65.24175234905591,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.532238048775543,
      "penalty_shape": 2.9434260344223677,
      "context_bonus_steepness": 5.596306631388362,
      "success": true,
      "log_likelihood": -55.241752378737175,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.274465824311998,
      "penalty_shape": 8.28499768508102,
      "context_bonus_steepness": 13.198970835378601,
      "success": true,
      "log_likelihood": -90.24175227485274,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5086739651105444,
      "penalty_shape": 6.382663198393659,
      "context_bonus_steepness": 12.13854700758513,
      "success": true,
      "log_likelihood": -60.241752363896566,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.1654835390484286,
      "penalty_shape": 1.7653108420723993,
      "context_bonus_steepness": 0.7700983212496285,
      "success": true,
      "log_likelihood": -15.241747216723155,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.3953138241492875,
      "penalty_shape": 3.6786577750604357,
      "context_bonus_steepness": 6.089251095612091,
      "success": true,
      "log_likelihood": -70.24175233421528,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.061070591245352,
      "penalty_shape": 6.608146297965868,
      "context_bonus_steepness": 1.283620499376475,
      "success": true,
      "log_likelihood": -140.2417521237779,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.2222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8360125044979734,
      "penalty_shape": 5.515082596588419,
      "context_bonus_steepness": 4.954732418378757,
      "success": true,
      "log_likelihood": -5.241752527143365,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.9013072084223757,
      "penalty_shape": 5.463164472938894,
      "context_bonus_steepness": 4.71924340746482,
      "success": true,
      "log_likelihood": -80.24175230453405,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.6050916523593679,
      "penalty_shape": 3.8540853858485926,
      "context_bonus_steepness": 6.977241855180378,
      "success": true,
      "log_likelihood": -70.2417523342153,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5991369175341219,
      "penalty_shape": 5.464608821306326,
      "context_bonus_steepness": 19.611392952799836,
      "success": true,
      "log_likelihood": -50.24175239357779,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7598279082926085,
      "penalty_shape": 0.9532032571045584,
      "context_bonus_steepness": 2.8203340304337905,
      "success": true,
      "log_likelihood": -85.24175228969337,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.2294095714781257,
      "penalty_shape": 2.6169211952326115,
      "context_bonus_steepness": 1.5416315398348746,
      "success": true,
      "log_likelihood": -110.24175221548873,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3611111111111111,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.6326360874633751,
      "penalty_shape": 6.6464382368646175,
      "context_bonus_steepness": 1.9617457670310199,
      "success": true,
      "log_likelihood": -105.2417522302884,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.6956106103188907,
      "penalty_shape": 2.202382942611257,
      "context_bonus_steepness": 18.781402880809896,
      "success": true,
      "log_likelihood": -85.24175228969337,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.1407960497810503,
      "penalty_shape": 2.9264626588150646,
      "context_bonus_steepness": 5.318813623053342,
      "success": true,
      "log_likelihood": -55.241752378737196,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.3273454908260218,
      "penalty_shape": 6.1305234832259785,
      "context_bonus_steepness": 15.189265457859472,
      "success": true,
      "log_likelihood": -155.2417520819246,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.16666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.1176001255565748,
      "penalty_shape": 1.9075096057461178,
      "context_bonus_steepness": 7.107967343247429,
      "success": true,
      "log_likelihood": -85.24175228969338,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8554902298581805,
      "penalty_shape": 8.783125849924518,
      "context_bonus_steepness": 1.0643565848715768,
      "success": true,
      "log_likelihood": -75.24175141070424,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.3466258188690416,
      "penalty_shape": 7.815207157871443,
      "context_bonus_steepness": 0.5329383199597867,
      "success": true,
      "log_likelihood": -110.24175159008084,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.37855218202352,
      "penalty_shape": 3.031236736757893,
      "context_bonus_steepness": 1.375961101578128,
      "success": true,
      "log_likelihood": -120.24175218580764,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3333333333333333,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.0322817213655657,
      "penalty_shape": 3.452961208550908,
      "context_bonus_steepness": 2.475690493213464,
      "success": true,
      "log_likelihood": -130.24175215612772,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.6665938750872988,
      "penalty_shape": 6.427306221283084,
      "context_bonus_steepness": 18.751299235702263,
      "success": true,
      "log_likelihood": -20.241752482621486,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5516661066843858,
      "penalty_shape": 2.261348215864295,
      "context_bonus_steepness": 0.7234543635513077,
      "success": true,
      "log_likelihood": -160.2407422398013,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.1388888888888889,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.2867984376811203,
      "penalty_shape": 5.087231889239355,
      "context_bonus_steepness": 1.5079438156068856,
      "success": true,
      "log_likelihood": -35.241752438098835,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.0772531527842109,
      "penalty_shape": 2.0125144479309953,
      "context_bonus_steepness": 11.246194253476661,
      "success": true,
      "log_likelihood": -70.24175233421528,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.6489292804374945,
      "penalty_shape": 2.211759219178737,
      "context_bonus_steepness": 14.295678380972664,
      "success": true,
      "log_likelihood": -90.24175227485276,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.2298058950950896,
      "penalty_shape": 5.464179023769274,
      "context_bonus_steepness": 2.9105252855725254,
      "success": true,
      "log_likelihood": -165.24175205224336,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.08333333333333333,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8558010811847041,
      "penalty_shape": 7.1124722484379195,
      "context_bonus_steepness": 19.580024122713667,
      "success": true,
      "log_likelihood": -100.2417522451715,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.6299068879806282,
      "penalty_shape": 1.2553129333980773,
      "context_bonus_steepness": 1.3682791618514112,
      "success": true,
      "log_likelihood": -140.24175201394138,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.2222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.3209015945527964,
      "penalty_shape": 1.30485290255475,
      "context_bonus_steepness": 14.115998307608951,
      "success": true,
      "log_likelihood": -80.24175230453402,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.1778673667667907,
      "penalty_shape": 6.237709601565175,
      "context_bonus_steepness": 9.40125148947127,
      "success": true,
      "log_likelihood": -15.241752497462148,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.9325225950424728,
      "penalty_shape": 1.6129033928400602,
      "context_bonus_steepness": 18.729890256330908,
      "success": true,
      "log_likelihood": -25.241752467780877,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.9292797179577355,
      "penalty_shape": 1.51612588962124,
      "context_bonus_steepness": 13.840252082489734,
      "success": true,
      "log_likelihood": -80.24175230453402,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.0405966966932505,
      "penalty_shape": 3.0806674852585987,
      "context_bonus_steepness": 15.382269252013511,
      "success": true,
      "log_likelihood": -15.241752497462134,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.948972708491901,
      "penalty_shape": 0.8053966368645642,
      "context_bonus_steepness": 9.095315729822829,
      "success": true,
      "log_likelihood": -160.24175206708398,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.1111111111111111,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8322103261727668,
      "penalty_shape": 5.3456646231324685,
      "context_bonus_steepness": 2.2416632634488503,
      "success": true,
      "log_likelihood": -155.24175208192455,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.1388888888888889,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.4798265422973746,
      "penalty_shape": 8.73197566627046,
      "context_bonus_steepness": 11.860614984189212,
      "success": true,
      "log_likelihood": -85.24175228969341,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.4311415874498334,
      "penalty_shape": 9.380139814479376,
      "context_bonus_steepness": 13.602199179176171,
      "success": true,
      "log_likelihood": -40.24175242325904,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.2641129862925031,
      "penalty_shape": 5.439313342946254,
      "context_bonus_steepness": 11.991356998386639,
      "success": true,
      "log_likelihood": -90.24175227485274,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.1629200792642433,
      "penalty_shape": 1.9933326517028218,
      "context_bonus_steepness": 15.219144556137092,
      "success": true,
      "log_likelihood": -55.241752378737196,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8758369719906827,
      "penalty_shape": 7.9354031294687655,
      "context_bonus_steepness": 19.321156410819377,
      "success": true,
      "log_likelihood": -10.241752512302766,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7287428605051179,
      "penalty_shape": 7.568808793685178,
      "context_bonus_steepness": 18.645550969697336,
      "success": true,
      "log_likelihood": -170.24175203740273,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.05555555555555555,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5906105775755531,
      "penalty_shape": 3.445491937187948,
      "context_bonus_steepness": 9.340818364457272,
      "success": true,
      "log_likelihood": -70.24175233421532,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.5338022668097888,
      "penalty_shape": 0.907150333749881,
      "context_bonus_steepness": 9.765257538420805,
      "success": true,
      "log_likelihood": -165.24175205224336,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.08333333333333333,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.9040196033764687,
      "penalty_shape": 8.173385828749243,
      "context_bonus_steepness": 19.882962164924006,
      "success": true,
      "log_likelihood": -85.24175228969337,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.125567278044478,
      "penalty_shape": 8.487041644160971,
      "context_bonus_steepness": 18.519702702230607,
      "success": true,
      "log_likelihood": -15.24175249746215,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.9946401350296401,
      "penalty_shape": 1.0681461238634682,
      "context_bonus_steepness": 1.1361619917022687,
      "success": true,
      "log_likelihood": -160.24175199750653,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.1111111111111111,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.9084535531634625,
      "penalty_shape": 4.822683475357861,
      "context_bonus_steepness": 15.333148279627254,
      "success": true,
      "log_likelihood": -90.24175227485274,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7740009268647589,
      "penalty_shape": 2.010122586068544,
      "context_bonus_steepness": 18.5950581754691,
      "success": true,
      "log_likelihood": -70.24175233421528,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.8231455920322912,
      "penalty_shape": 1.1133324808456653,
      "context_bonus_steepness": 8.211988236880845,
      "success": true,
      "log_likelihood": -160.24175206708398,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.1111111111111111,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.7721725666451804,
      "penalty_shape": 6.545271553815245,
      "context_bonus_steepness": 14.06482999416188,
      "success": true,
      "log_likelihood": -55.241752378737175,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.0058938819424086,
      "penalty_shape": 2.320699588785152,
      "context_bonus_steepness": 14.191234293911638,
      "success": true,
      "log_likelihood": -140.24175212644647,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.2222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 0.961955840252221,
      "penalty_shape": 4.260000295330436,
      "context_bonus_steepness": 3.1630843070246577,
      "success": true,
      "log_likelihood": -10.241752512302766,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.3255129913950277,
      "penalty_shape": 6.536829769104566,
      "context_bonus_steepness": 13.563980943596125,
      "success": true,
      "log_likelihood": -95.2417522600121,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "fairness_multiplier": 1.4575608233455608,
      "penalty_shape": 0.9734627411097556,
      "context_bonus_steepness": 19.81387626776221,
      "success": true,
      "log_likelihood": -55.2417523787372,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "fairness_multiplier": {
        "r": -0.1439713243043738,
        "p": 0.15298433120395374
      },
      "penalty_shape": {
        "r": 0.045802898268709404,
        "p": 0.6509030349241413
      },
      "context_bonus_steepness": {
        "r": 0.003952003216852256,
        "p": 0.9688718008029806
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 166.28887974547254,
  "bic_Control": 141.93151474316795,
  "bic_Cocaine": 188.10901922670377,
  "bic_results": {
    "average_bic": 166.28887974547254,
    "individual_bics": [
      341.23406091985504,
      191.23406136507384,
      181.23406108032881,
      221.23406127603,
      211.2340613057113,
      141.23406151348013,
      11.234061899336309,
      131.23406154316146,
      11.234061899336309,
      231.23406124634877,
      211.23406130571132,
      331.2340609495363,
      221.23406127603005,
      11.234061899336309,
      11.234061899336309,
      11.234061899336309,
      11.234061899336309,
      61.234061750930096,
      51.23406178061127,
      11.234061899336309,
      171.2340614244363,
      61.23406175093008,
      241.2340605620147,
      181.23406139475506,
      191.23406136507373,
      161.23406145411755,
      171.23406142443636,
      271.23405922052353,
      51.22456498922239,
      91.23406166188637,
      11.234061899336309,
      341.23406011987635,
      221.23406127601422,
      231.23406124634877,
      11.234061899336309,
      141.23406151348016,
      121.23406157284268,
      191.2340613650738,
      131.23406154316146,
      41.23405124881464,
      151.23406148379888,
      291.2340610629241,
      21.23406186965506,
      171.23406142443642,
      151.23406148379894,
      111.23406160252391,
      181.23406139475506,
      231.2340612463458,
      221.23406127594512,
      181.23406139475506,
      121.23406157284272,
      321.23406097921753,
      181.2340613947551,
      161.2340596367768,
      231.23405999553,
      251.2340611869836,
      271.23406112762376,
      51.2340617806113,
      331.23204129497094,
      81.234061691566,
      151.23406148379888,
      191.23406136507384,
      341.23406091985504,
      211.23406130571132,
      291.2340608432511,
      171.23406142443636,
      41.23406181029263,
      61.23406175093008,
      171.23406142443636,
      41.2340618102926,
      331.2340609495363,
      321.2340609792174,
      181.23406139475514,
      91.2340616618864,
      191.2340613650738,
      121.23406157284272,
      31.23406183997386,
      351.2340608901738,
      151.23406148379897,
      341.23406091985504,
      181.23406139475506,
      41.23406181029263,
      331.2340608103814,
      191.2340613650738,
      151.23406148379888,
      331.2340609495363,
      121.23406157284268,
      291.23406106826127,
      31.23406183997386,
      201.23406133539254,
      121.23406157284273
    ],
    "group_enabled": true,
    "group_bics": {
      "Control": [
        341.23406091985504,
        191.23406136507384,
        181.23406108032881,
        221.23406127603,
        211.2340613057113,
        141.23406151348013,
        11.234061899336309,
        131.23406154316146,
        11.234061899336309,
        231.23406124634877,
        211.23406130571132,
        331.2340609495363,
        221.23406127603005,
        11.234061899336309,
        11.234061899336309,
        11.234061899336309,
        11.234061899336309,
        61.234061750930096,
        51.23406178061127,
        11.234061899336309,
        171.2340614244363,
        61.23406175093008,
        241.2340605620147,
        181.23406139475506,
        191.23406136507373,
        161.23406145411755,
        171.23406142443636,
        271.23405922052353,
        51.22456498922239,
        91.23406166188637,
        11.234061899336309,
        341.23406011987635,
        221.23406127601422,
        231.23406124634877,
        11.234061899336309,
        141.23406151348016,
        121.23406157284268,
        191.2340613650738,
        131.23406154316146,
        41.23405124881464,
        151.23406148379888,
        291.2340610629241,
        21.23406186965506
      ],
      "Cocaine": [
        171.23406142443642,
        151.23406148379894,
        111.23406160252391,
        181.23406139475506,
        231.2340612463458,
        221.23406127594512,
        181.23406139475506,
        121.23406157284272,
        321.23406097921753,
        181.2340613947551,
        161.2340596367768,
        231.23405999553,
        251.2340611869836,
        271.23406112762376,
        51.2340617806113,
        331.23204129497094,
        81.234061691566,
        151.23406148379888,
        191.23406136507384,
        341.23406091985504,
        211.23406130571132,
        291.2340608432511,
        171.23406142443636,
        41.23406181029263,
        61.23406175093008,
        171.23406142443636,
        41.2340618102926,
        331.2340609495363,
        321.2340609792174,
        181.23406139475514,
        91.2340616618864,
        191.2340613650738,
        121.23406157284272,
        31.23406183997386,
        351.2340608901738,
        151.23406148379897,
        341.23406091985504,
        181.23406139475506,
        41.23406181029263,
        331.2340608103814,
        191.2340613650738,
        151.23406148379888,
        331.2340609495363,
        121.23406157284268,
        291.23406106826127,
        31.23406183997386,
        201.23406133539254,
        121.23406157284273
      ]
    },
    "num_parameters": 3,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- fairness_multiplier: r = -0.144\n- penalty_shape: r = 0.046\n- context_bonus_steepness: r = 0.004",
  "previous_models": [
    "Specification: threshold_perc = fairness_multiplier * (token_self / (token_self + token_opp))\nD_norm = (threshold_perc - splitperc_self) / threshold_perc\nS_norm = (splitperc_self - threshold_perc) / (1 - threshold_perc)\nif splitperc_self < threshold_perc:\n    U = -penalty_strength * (exp(penalty_shape * D_norm) - 1)\nelif splitperc_self == threshold_perc:\n    U = 0\nelse:\n    U = context_bonus_weight * sigmoid(context_bonus_steepness * S_norm)\nSummary: A three-region, context-adaptive model with exclusive, robustly recoverable parameters for fairness threshold, penalty nonlinearity, and generosity bonus steepness; combines exponential-minus-one penalty and context-weighted sigmoid bonus for maximal separation and identifiability across all trial contexts.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 166.28887974547254\nOverall Accuracy: 0.5681\nGroup Accuracies:\n- Control: 0.6376\n- Cocaine: 0.5058\n\nParameter Recovery:\n- fairness_multiplier: r = -0.144\n- penalty_shape: r = 0.046\n- context_bonus_steepness: r = 0.004"
  ]
}