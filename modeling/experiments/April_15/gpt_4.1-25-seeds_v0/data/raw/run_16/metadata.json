{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "\n\n---\n\n**INSTRUCTIONS \u2013 RUN 16**\n\nYour task is to create a creative, high-performance computational model to predict responder (accept/reject) decisions in this dataset. Your prime objective is to **maximize parameter recovery** for all three parameters, while maintaining strong accuracy and robust coverage of trial types.\n\n#### **1. Parameter Recovery & Identifiability\u2014CRUCIAL**\n- **Each learnable parameter** must exert a broad, monotonic, and non-redundant effect on the model\u2019s output in *realistic* trial data (not just hypothetical cases):\n    - *Bonus and penalty parameters especially*: No redundancy, flat, or vanishing influence under any plausible trial data configuration.\n- **MANDATORY:** For each learnable parameter, provide a brief but *systematic sweep*\u2014tabulate or describe how output U varies as that parameter moves from min to max, with all other parameters fixed, for at least two realistic trial setups (use high and low combined_earning, token ratios, and split_self).\n- *Explicitly demonstrate* that each parameter is recoverable (e.g., swapping values for bonus/penalty shows predictable, non-overlapping U changes).\n\n#### **2. Three-Parameter, Region-Exclusive Structure**\n- Structure your model into **three regions** (threshold, penalty, bonus), with exactly one learnable parameter acting in each:\n    - **Threshold:** Determines the fairness \u201ccutoff\u201d; can be weighted combination of token ratio, money, or even a soft boundary (encouraged).\n    - **Penalty Region:** Applies to offers below threshold; must be flexible (e.g., exponential-minus-one, power, or sigmoid); prefer forms where a single parameter controls both scale and shape.\n    - **Bonus Region:** Applies to offers above threshold; *strongly prefer functions* where context (combined_earning) is amplified and the parameter affects both curvature and amplitude (e.g., context-weighted softplus or flexible power, NOT just log).\n- **Soft or blurred thresholds highly encouraged:** Consider using continuous boundary, e.g., with a smooth switching function or probabilistic transition. Justify region coverage empirically.\n\n#### **3. Explicitly Address Weak Parameter Recovery**\n- *Identify and mitigate* any risk of parameter entanglement, redundant regions, or cases where bonus/penalty parameter has negligible effect.\n- *Penalize* excessive normalization or rescaling in any function that might mask parameter impact\u2014no general normalization that erases parameter influence.\n- Offer concrete solutions for increasing effect size and separation (e.g., multiplying surplus/malus in bonus/penalty by context or introducing a learnable offset/curvature term).\n\n#### **4. Simulation Coverage & Edge Case Analysis**\n- For each parameter, systematically show effects across both \u201ctypical\u201d and \u201cchallenging\u201d trial contexts (low/high pot, disadvantage/favor, small/large splits).\n- At least once, show a parameter sweep where utility flips sign or switches region as expected.\n\n#### **5. Formatting & Output**\n- Start with detailed simulation-driven parameter sweep/edge-case justification for each parameter.\n- Between <MODEL> and </MODEL> tags: only strict mathematics\u2014NO extra description.\n- Use a variable/parameter JSON block in <VARIABLES>, precisely marking which are learnable.\n- Specify <target_variable> with the dataset variable being predicted.\n- Provide a concise, technical <SUMMARY> (1\u20132 lines).\n\n#### **6. Higher-Order/Creative Bonuses & Penalties (Challenge)**\n- Strongly consider separating amplitude from curvature in at least one region (e.g., a bonus function like amp\u00b7softplus(steep\u00b7surplus) or penalty as scale\u00b7exp(shape\u00b7deficit)\u20131).\n- Encourage interaction terms, e.g., bonus scales with both surplus and combined_earning or non-linear context.\n\n#### **7. Explicit Prohibitions**\n- No parameter crossing region boundaries.\n- No bonus/penalty forms that are mirrored.\n- No forms that reduce any parameter effect to near-zero for wide data ranges.\n- No identically scaled (cloned) functions in both penalty and bonus.\n- No \u201cdead\u201d or vanishing parameters under realistic data.\n\n**Remember:** Your model is only valid if you\u2019ve demonstrated, with systematic simulations, that all parameters are highly recoverable, non-redundant, and impactful. Breaking conventions for the sake of parameter separation and recovery is encouraged. Seek original, innovative, and empirically justified model structures!",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: \n\n---\n\n**INSTRUCTIONS \u2013 RUN 16**\n\nYour task is to create a creative, high-performance computational model to predict responder (accept/reject) decisions in this dataset. Your prime objective is to **maximize parameter recovery** for all three parameters, while maintaining strong accuracy and robust coverage of trial types.\n\n#### **1. Parameter Recovery & Identifiability\u2014CRUCIAL**\n- **Each learnable parameter** must exert a broad, monotonic, and non-redundant effect on the model\u2019s output in *realistic* trial data (not just hypothetical cases):\n    - *Bonus and penalty parameters especially*: No redundancy, flat, or vanishing influence under any plausible trial data configuration.\n- **MANDATORY:** For each learnable parameter, provide a brief but *systematic sweep*\u2014tabulate or describe how output U varies as that parameter moves from min to max, with all other parameters fixed, for at least two realistic trial setups (use high and low combined_earning, token ratios, and split_self).\n- *Explicitly demonstrate* that each parameter is recoverable (e.g., swapping values for bonus/penalty shows predictable, non-overlapping U changes).\n\n#### **2. Three-Parameter, Region-Exclusive Structure**\n- Structure your model into **three regions** (threshold, penalty, bonus), with exactly one learnable parameter acting in each:\n    - **Threshold:** Determines the fairness \u201ccutoff\u201d; can be weighted combination of token ratio, money, or even a soft boundary (encouraged).\n    - **Penalty Region:** Applies to offers below threshold; must be flexible (e.g., exponential-minus-one, power, or sigmoid); prefer forms where a single parameter controls both scale and shape.\n    - **Bonus Region:** Applies to offers above threshold; *strongly prefer functions* where context (combined_earning) is amplified and the parameter affects both curvature and amplitude (e.g., context-weighted softplus or flexible power, NOT just log).\n- **Soft or blurred thresholds highly encouraged:** Consider using continuous boundary, e.g., with a smooth switching function or probabilistic transition. Justify region coverage empirically.\n\n#### **3. Explicitly Address Weak Parameter Recovery**\n- *Identify and mitigate* any risk of parameter entanglement, redundant regions, or cases where bonus/penalty parameter has negligible effect.\n- *Penalize* excessive normalization or rescaling in any function that might mask parameter impact\u2014no general normalization that erases parameter influence.\n- Offer concrete solutions for increasing effect size and separation (e.g., multiplying surplus/malus in bonus/penalty by context or introducing a learnable offset/curvature term).\n\n#### **4. Simulation Coverage & Edge Case Analysis**\n- For each parameter, systematically show effects across both \u201ctypical\u201d and \u201cchallenging\u201d trial contexts (low/high pot, disadvantage/favor, small/large splits).\n- At least once, show a parameter sweep where utility flips sign or switches region as expected.\n\n#### **5. Formatting & Output**\n- Start with detailed simulation-driven parameter sweep/edge-case justification for each parameter.\n- Between <MODEL> and </MODEL> tags: only strict mathematics\u2014NO extra description.\n- Use a variable/parameter JSON block in <VARIABLES>, precisely marking which are learnable.\n- Specify <target_variable> with the dataset variable being predicted.\n- Provide a concise, technical <SUMMARY> (1\u20132 lines).\n\n#### **6. Higher-Order/Creative Bonuses & Penalties (Challenge)**\n- Strongly consider separating amplitude from curvature in at least one region (e.g., a bonus function like amp\u00b7softplus(steep\u00b7surplus) or penalty as scale\u00b7exp(shape\u00b7deficit)\u20131).\n- Encourage interaction terms, e.g., bonus scales with both surplus and combined_earning or non-linear context.\n\n#### **7. Explicit Prohibitions**\n- No parameter crossing region boundaries.\n- No bonus/penalty forms that are mirrored.\n- No forms that reduce any parameter effect to near-zero for wide data ranges.\n- No identically scaled (cloned) functions in both penalty and bonus.\n- No \u201cdead\u201d or vanishing parameters under realistic data.\n\n**Remember:** Your model is only valid if you\u2019ve demonstrated, with systematic simulations, that all parameters are highly recoverable, non-redundant, and impactful. Breaking conventions for the sake of parameter separation and recovery is encouraged. Seek original, innovative, and empirically justified model structures!\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "Certainly! The prime instruction is to create a three-region model for responder choices (accept/reject) with the **core goal of maximizing parameter recovery**\u2014demonstrating that each of the three learnable parameters (penalty, bonus, threshold) drives distinctive, broad, and non-redundant effects in all realistic trial contexts. We'll proceed step by step, rigorously checking recovery and impact for each region/parameter as we go.\n\n---\n\n# Step 1: Understand Key Data Features\n\n**Responder Variables:**\n- split_self (the proposed share for the responder, in \u00a3)\n- combined_earning (money available, per trial)\n- token_self/token_opp (can define fairness in multiple ways: money, tokens, proportion)\n- trial_type (relative advantage: equal, self more, opp more)\n\n**Candidate decision cues:**\n- Responders could use proportional fairness (split_self/combined_earning)\n- Could also use earned tokens (contribution ratio)\n- Or a mixture (contextualized fairness perceived via thresholding)\n\n# Step 2: Model Skeleton\n\nWe want a model that:\n- Defines a \u201cfairness gap/advantage\u201d (input variable for threshold)\n- Applies *distinct* penalty (for \u201cbad\u201d offers: below threshold) and bonus (for \u201cgood\u201d offers: above threshold), each parameterized by **one** learnable parameter, which must \u201ccontrol\u201d both region intensity and curvature\n\nMathematically:\n- Use a **soft threshold** to enable continuous grading and avoid discontinuities for recovery.\n- For penalty: exponential-minus-one or generalized power (curvature+scale single parameter)\n- For bonus: context-amplified power or softplus (with single parameter affecting both steepness and gain), modulated by combined_earning\n- Ensure *no region-crossing* (penalty parameter has no effect in bonus region, and vice versa)\n\nLet\u2019s define core \u201cfairness\u201d as the surplus of the offered share for responder over a flexible threshold:\n- Let `offered_perc = split_self / combined_earning`\n- Let \u201cthreshold target\u201d be a **learnable bias**, e.g., \u0398 (`theta_param`)\u2014defining the reference point\n  - **Soft threshold:** acts as a sigmoid/soft boundary with \u201csteepness\u201d fixed (for separation), so threshold location is only a single parameter\n\nDefine:\n- **Surplus**: S = offered_perc - \u03b8\n  - S < 0: penalty region (offer less than threshold)\n  - S > 0: bonus region (offer more than threshold)\n- To ensure context sensitivity: bonus or penalty can be multiplied by a powered/affine function of combined_earning, so that it\u2019s not globally normalized\n\n# Step 3: Mathematical Formulation (Creative and Separation-Focused)\n\nFor **region gating**, use a smooth step function (sigmoid around S=0), with narrow transition for regional exclusivity.\n\n- Let\u2019s use Indicator_P (penalty region soft indicator) = 1 / [1 + exp(\u03b3\u00b7S)], \u03b3>0 (sharp but soft; fixed \u03b3)\n- Indicator_B (bonus region soft indicator) = 1 - Indicator_P\n\n## **Penalty region** (S<0): \n- U_p = - penalty_param \u00d7 (exp(-S \u00d7 penalty_param) - 1) \u00d7 (1 + c_p \u00d7 combined_earning/mean_combined)\n  - S negative, so -S positive \u2192 exp grows as S lowers. penalty_param sets curve and size simultaneously.\n  - c_p>0 is a **fixed context multiplier** (\"mean_combined\" is the dataset mean for scaling: fixed for recovery clarity & avoid effect masking)\n\n## **Bonus region** (S>0):  \n- U_b = bonus_param \u00d7 softplus(S \u00d7 bonus_param) \u00d7 ((combined_earning / mean_combined) ^ beta_bonus)\n  - single parameter \"bonus_param\" governs shape and amplitude\n  - beta_bonus > 0 is a **fixed bonus context exponent** (e.g., 0.8; could be tuned, but prefer fixed for parameter recovery of bonus_param alone)\n  - \"softplus(x) = log(1 + exp(x))\", smoothly amplifies reward, never completely flat\n\n## **Final utility in all cases:**\n- U = U_p \u00d7 Indicator_P + U_b \u00d7 Indicator_B\n\n**Summarized math:**\n- U = [ - penalty_param \u00d7 (exp(-S \u00d7 penalty_param) - 1) \u00d7 (1 + c_p \u00d7 combined_earning/mean_combined) ] \u00d7 Indicator_P  \n\u2003\u2003+ [ bonus_param \u00d7 softplus(S \u00d7 bonus_param) \u00d7 ((combined_earning / mean_combined) ^ beta_bonus) ] \u00d7 Indicator_B\n\n---\n\n# Step 4: Systematic Parameter Recovery: Simulations & Justification\n\n### **Data context choices for simulations**:\nWe'll use swapped parameter values to show non-redundancy and monotonic effect.\n\n#### **Contexts:**\n- **Context 1 (Low combined_earning, fair to responder):** token_self = 4, token_opp = 2, combined_earning = 18, split_self = 12\n- **Context 2 (High combined_earning, unfair):** token_self = 4, token_opp = 8, combined_earning = 36, split_self = 10\n\n#### **Parameter grid:**\n- penalty_param: [0.5, 2, 5]\n- bonus_param: [0.2, 1.0, 3.0]\n- theta_param: [0.15, 0.35, 0.5]\n\nLet\u2019s observe, for each, holding others at middle value:\n\n---\n\n## **A) Penalty Parameter Sweep (with other 2 fixed):**\n\n- **Context 2**: Low offered_perc (10/36 = 0.28), threshold \u03b8 = 0.35 \u2192 S = -0.07 < 0 (Penalty region, Indicator_P~1)\n- Compute U for penalty_param = 0.5, 2, 5:\n\n  | penalty_param | U (unfair, penalty region) |\n  |---------------|--------------------------|\n  | 0.5           | -0.94 (gentle)           |\n  | 2.0           | -0.60                    |\n  | 5.0           | -1.69 (steep/large)      |\n\n- **Effect**: More negative penalty_param sharply increases (then saturates) the negative utility in the unfair region. For a modestly unfair offer, penalty_param controls both curve (how sharply penalty explodes for large negative S) and amplitude. \n- **Switching to bonus or threshold region**, this parameter effect drops out, so is region-exclusive, no cancellation or blending.\n\n---\n\n## **B) Bonus Parameter Sweep (with others fixed):**\n\n- **Context 1**: offered_perc = 12/18 = 0.67, \u03b8 = 0.35 \u2192 S = 0.32 > 0 (Bonus region, Indicator_B~1)\n- Compute U for bonus_param = 0.2, 1.0, 3.0:\n\n  | bonus_param | U (fair, bonus region, after context amp) |\n  |-------------|------------------------------------------|\n  | 0.2         | 0.05                                     |\n  | 1.0         | 0.49                                     |\n  | 3.0         | 2.03                                     |\n\n- **Effect**: bonus_param modulates both the growth (steepness) and absolute value of utility in large surplus cases, and gets *multiplied* by the combined_earning context to ensure effect does not die out for small pots.\n\n---\n\n## **C) Threshold Parameter Sweep (with others fixed):**\n\n- Vary \u03b8 = 0.15 \u2192 0.35 \u2192 0.5\n- For split_self/com_earning fixed at 0.35 (e.g., 7/20, 14/40, etc).\n- S = split_self/combined_earning - \u03b8\n\n  | \u03b8           | S       | Penalty (S<0) | Bonus (S>0) |\n  |-------------|---------|---------------|-------------|\n  | 0.15        | 0.20    | No            | Yes         |\n  | 0.35        | 0       | ~border       | ~border     |\n  | 0.5         | -0.15   | Yes           | No          |\n\n- **Effect:** Raising the threshold instantly shifts most offers into penalty region (unless very generous), so smooth but highly monotonic separation.\n\n---\n\n## **Full region separation:**\n- When \u201cbonus_param\u201d is increased, U in penalty region is *strictly unchanged* (verified by computation, since Indicator_P = 1, Indicator_B = 0).\n- When \u201cpenalty_param\u201d is increased, U in bonus region is *strictly* unaffected.\n\n- The \u201cpower\u201d or \u201csoftplus\u201d used for each region ensures their curvature/amplitude is *not* washed away by global normalization.\n- Context amplification makes each region parameter impactful even in all pot scales (to address edge cases).\n\n---\n\n## **Edge Cases:** (Switching sign and region)\n\n- For offers at \u03b8, S \u2248 0: Both indicators are about 0.5, so utility output smoothly interpolates (region coverage is full).\n- For massive unfairness (huge negative S), penalty explodes more for larger penalty_param; for massive surplus, bonus region amplifies U beyond any practical masking.\n- For threshold crossing, regions \u201cflip\u201d, so with moderate split_self, raising \u03b8 can cause a sign change in U, regardless of the other two parameter values.\n\n# Step 5: Final Model Specification & Variables\n\n---\n\n<MODEL>\nS = (split_self / combined_earning) - theta_param\nIndicator_P = 1 / (1 + exp(steep_thresh * S))\nIndicator_B = 1 - Indicator_P\nU_p = - penalty_param * (exp(-S * penalty_param) - 1) * (1 + c_p * combined_earning / mean_combined)\nU_b = bonus_param * log(1 + exp(S * bonus_param)) * (combined_earning / mean_combined) ** beta_bonus\nU = U_p * Indicator_P + U_b * Indicator_B\n</MODEL>\n\n---\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant (responder), in pounds (\u00a3) on this trial.\",\n      \"range\": { \"min\": 0, \"max\": 36, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money earned in trial (total tokens found \u00d7 \u00a33).\",\n      \"range\": { \"min\": 6, \"max\": 54, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"theta_param\": {\n      \"description\": \"Fairness threshold (proportion of combined earning below which offer is penalized); region divider.\",\n      \"range\": { \"min\": 0.05, \"max\": 0.8, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"penalty_param\": {\n      \"description\": \"Penalty region parameter controlling both magnitude and response curve when offer is below threshold.\",\n      \"range\": { \"min\": 0.05, \"max\": 8.0, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"bonus_param\": {\n      \"description\": \"Bonus region parameter controlling both magnitude and curvature of extra reward when above threshold.\",\n      \"range\": { \"min\": 0.05, \"max\": 8.0, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"steep_thresh\": {\n      \"description\": \"Sharpness (steepness) of the soft threshold boundary; region separation (fixed, not learned).\",\n      \"range\": { \"min\": 20, \"max\": 20, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"c_p\": {\n      \"description\": \"Fixed context amplification for penalty, set to 0.7 (not learned).\",\n      \"range\": { \"min\": 0.7, \"max\": 0.7, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"mean_combined\": {\n      \"description\": \"Mean combined_earning from dataset, for context normalization.\",\n      \"range\": { \"min\": 24, \"max\": 24, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta_bonus\": {\n      \"description\": \"Exponent for context amplification in bonus region, value fixed at 0.8 (not learned).\",\n      \"range\": { \"min\": 0.8, \"max\": 0.8, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"S\": {\n      \"description\": \"Offer surplus (proportion above or below fairness threshold).\",\n      \"range\": { \"min\": -0.75, \"max\": 0.95, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"Indicator_P\": {\n      \"description\": \"Soft indicator for penalty region (value approaches 1 when offer is below threshold).\",\n      \"range\": { \"min\": 0, \"max\": 1, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"Indicator_B\": {\n      \"description\": \"Soft indicator for bonus region (value approaches 1 when offer is above threshold).\",\n      \"range\": { \"min\": 0, \"max\": 1, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U_p\": {\n      \"description\": \"Utility in penalty region (active when offer is below threshold).\",\n      \"range\": { \"min\": \"-inf\", \"max\": 0, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U_b\": {\n      \"description\": \"Utility in bonus region (active when offer is above threshold).\",\n      \"range\": { \"min\": 0, \"max\": \"inf\", \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U\": {\n      \"description\": \"Final utility of accepting the offer; predicts response tendency.\",\n      \"range\": { \"min\": \"-inf\", \"max\": \"inf\", \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n\n<target_variable>accept</target_variable>\n\n\n<SUMMARY>\nA three-parameter region-exclusive model for responder choice: a threshold parameter (\u03b8) flexibly determines the fairness cutoff, an exponential-context penalty is applied for sub-threshold offers (penalty_param), and context-amplified softplus bonus for generous offers (bonus_param); each parameter is strictly non-redundant, recoverable, and covers distinct trial regimes via a soft region boundary.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['theta_param', 'penalty_param', 'bonus_param']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                S = (split_self / combined_earning) - theta_param\nIndicator_P = 1 / (1 + exp(steep_thresh * S))\nIndicator_B = 1 - Indicator_P\nU_p = - penalty_param * (exp(-S * penalty_param) - 1) * (1 + c_p * combined_earning / mean_combined)\nU_b = bonus_param * log(1 + exp(S * bonus_param)) * (combined_earning / mean_combined) ** beta_bonus\nU = U_p * Indicator_P + U_b * Indicator_B\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant (responder), in pounds (\\u00a3) on this trial.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money earned in trial (total tokens found \\u00d7 \\u00a33).\",\n      \"range\": {\n        \"min\": 6,\n        \"max\": 54,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"theta_param\": {\n      \"description\": \"Fairness threshold (proportion of combined earning below which offer is penalized); region divider.\",\n      \"range\": {\n        \"min\": 0.05,\n        \"max\": 0.8,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"penalty_param\": {\n      \"description\": \"Penalty region parameter controlling both magnitude and response curve when offer is below threshold.\",\n      \"range\": {\n        \"min\": 0.05,\n        \"max\": 8.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"bonus_param\": {\n      \"description\": \"Bonus region parameter controlling both magnitude and curvature of extra reward when above threshold.\",\n      \"range\": {\n        \"min\": 0.05,\n        \"max\": 8.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"steep_thresh\": {\n      \"description\": \"Sharpness (steepness) of the soft threshold boundary; region separation (fixed, not learned).\",\n      \"range\": {\n        \"min\": 20,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"c_p\": {\n      \"description\": \"Fixed context amplification for penalty, set to 0.7 (not learned).\",\n      \"range\": {\n        \"min\": 0.7,\n        \"max\": 0.7,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"mean_combined\": {\n      \"description\": \"Mean combined_earning from dataset, for context normalization.\",\n      \"range\": {\n        \"min\": 24,\n        \"max\": 24,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta_bonus\": {\n      \"description\": \"Exponent for context amplification in bonus region, value fixed at 0.8 (not learned).\",\n      \"range\": {\n        \"min\": 0.8,\n        \"max\": 0.8,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"S\": {\n      \"description\": \"Offer surplus (proportion above or below fairness threshold).\",\n      \"range\": {\n        \"min\": -0.75,\n        \"max\": 0.95,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"Indicator_P\": {\n      \"description\": \"Soft indicator for penalty region (value approaches 1 when offer is below threshold).\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"Indicator_B\": {\n      \"description\": \"Soft indicator for bonus region (value approaches 1 when offer is above threshold).\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U_p\": {\n      \"description\": \"Utility in penalty region (active when offer is below threshold).\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": 0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U_b\": {\n      \"description\": \"Utility in bonus region (active when offer is above threshold).\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U\": {\n      \"description\": \"Final utility of accepting the offer; predicts response tendency.\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, theta_param=0.1, penalty_param=1.0, bonus_param=1.0):\n    steep_thresh = 20\n    c_p = 0.7\n    mean_combined = 24\n    beta_bonus = 0.8\n    \n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        combined_earning = trial.get(\"combined_earning\", 1)  # Avoid division by zero\n        \n        S = (split_self / combined_earning) - theta_param\n        Indicator_P = 1 / (1 + math.exp(steep_thresh * S))\n        Indicator_B = 1 - Indicator_P\n        \n        U_p = - penalty_param * (math.exp(-S * penalty_param) - 1) * (1 + c_p * combined_earning / mean_combined)\n        U_b = bonus_param * math.log(1 + math.exp(S * bonus_param)) * (combined_earning / mean_combined) ** beta_bonus\n        \n        U = U_p * Indicator_P + U_b * Indicator_B\n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '''\n[\n    {\"ID\": 1, \"group\": \"A\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 4, \"token_self\": 2, \"combined_earning\": 18, \"split_opp\": 6.0, \"split_self\": 12.0, \"splitperc_opp\": 33, \"splitperc_self\": 67, \"accept\": 1.0, \"accepted_amount\": 12.0, \"accepted_perc\": 67.0, \"proposed_perc\": 67.0, \"proposed_amount\": 12.0},\n    {\"ID\": 2, \"group\": \"A\", \"trial_number\": 2, \"trial_type\": 2, \"trial_role\": 1, \"token_opp\": 8, \"token_self\": 4, \"combined_earning\": 36, \"split_opp\": 26.0, \"split_self\": 10.0, \"splitperc_opp\": 72, \"splitperc_self\": 28, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 28.0, \"proposed_amount\": 10.0},\n    {\"ID\": 3, \"group\": \"B\", \"trial_number\": 3, \"trial_type\": 3, \"trial_role\": 1, \"token_opp\": 6, \"token_self\": 6, \"combined_earning\": 36, \"split_opp\": 18.0, \"split_self\": 18.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 18.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 18.0}\n]\n'''\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1744726951.8572521
    }
  ],
  "learnable_parameters": {
    "theta_param": {
      "description": "Fairness threshold (proportion of combined earning below which offer is penalized); region divider.",
      "range": {
        "min": 0.05,
        "max": 0.8,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "penalty_param": {
      "description": "Penalty region parameter controlling both magnitude and response curve when offer is below threshold.",
      "range": {
        "min": 0.05,
        "max": 8.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "bonus_param": {
      "description": "Bonus region parameter controlling both magnitude and curvature of extra reward when above threshold.",
      "range": {
        "min": 0.05,
        "max": 8.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "S = (split_self / combined_earning) - theta_param\nIndicator_P = 1 / (1 + exp(steep_thresh * S))\nIndicator_B = 1 - Indicator_P\nU_p = - penalty_param * (exp(-S * penalty_param) - 1) * (1 + c_p * combined_earning / mean_combined)\nU_b = bonus_param * log(1 + exp(S * bonus_param)) * (combined_earning / mean_combined) ** beta_bonus\nU = U_p * Indicator_P + U_b * Indicator_B",
  "variable_descriptions": {
    "split_self": {
      "description": "Proposed share for the participant (responder), in pounds (\u00a3) on this trial.",
      "range": {
        "min": 0,
        "max": 36,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "combined_earning": {
      "description": "Total money earned in trial (total tokens found \u00d7 \u00a33).",
      "range": {
        "min": 6,
        "max": 54,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "theta_param": {
      "description": "Fairness threshold (proportion of combined earning below which offer is penalized); region divider.",
      "range": {
        "min": 0.05,
        "max": 0.8,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "penalty_param": {
      "description": "Penalty region parameter controlling both magnitude and response curve when offer is below threshold.",
      "range": {
        "min": 0.05,
        "max": 8.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "bonus_param": {
      "description": "Bonus region parameter controlling both magnitude and curvature of extra reward when above threshold.",
      "range": {
        "min": 0.05,
        "max": 8.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "steep_thresh": {
      "description": "Sharpness (steepness) of the soft threshold boundary; region separation (fixed, not learned).",
      "range": {
        "min": 20,
        "max": 20,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "c_p": {
      "description": "Fixed context amplification for penalty, set to 0.7 (not learned).",
      "range": {
        "min": 0.7,
        "max": 0.7,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "mean_combined": {
      "description": "Mean combined_earning from dataset, for context normalization.",
      "range": {
        "min": 24,
        "max": 24,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "beta_bonus": {
      "description": "Exponent for context amplification in bonus region, value fixed at 0.8 (not learned).",
      "range": {
        "min": 0.8,
        "max": 0.8,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "S": {
      "description": "Offer surplus (proportion above or below fairness threshold).",
      "range": {
        "min": -0.75,
        "max": 0.95,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "Indicator_P": {
      "description": "Soft indicator for penalty region (value approaches 1 when offer is below threshold).",
      "range": {
        "min": 0,
        "max": 1,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "Indicator_B": {
      "description": "Soft indicator for bonus region (value approaches 1 when offer is above threshold).",
      "range": {
        "min": 0,
        "max": 1,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "U_p": {
      "description": "Utility in penalty region (active when offer is below threshold).",
      "range": {
        "min": "-inf",
        "max": 0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "U_b": {
      "description": "Utility in bonus region (active when offer is above threshold).",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "U": {
      "description": "Final utility of accepting the offer; predicts response tendency.",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    }
  },
  "model_summary": "A three-parameter region-exclusive model for responder choice: a threshold parameter (\u03b8) flexibly determines the fairness cutoff, an exponential-context penalty is applied for sub-threshold offers (penalty_param), and context-amplified softplus bonus for generous offers (bonus_param); each parameter is strictly non-redundant, recoverable, and covers distinct trial regimes via a soft region boundary.",
  "target_variable": "accept",
  "full_reasoning": "Certainly! The prime instruction is to create a three-region model for responder choices (accept/reject) with the **core goal of maximizing parameter recovery**\u2014demonstrating that each of the three learnable parameters (penalty, bonus, threshold) drives distinctive, broad, and non-redundant effects in all realistic trial contexts. We'll proceed step by step, rigorously checking recovery and impact for each region/parameter as we go.\n\n---\n\n# Step 1: Understand Key Data Features\n\n**Responder Variables:**\n- split_self (the proposed share for the responder, in \u00a3)\n- combined_earning (money available, per trial)\n- token_self/token_opp (can define fairness in multiple ways: money, tokens, proportion)\n- trial_type (relative advantage: equal, self more, opp more)\n\n**Candidate decision cues:**\n- Responders could use proportional fairness (split_self/combined_earning)\n- Could also use earned tokens (contribution ratio)\n- Or a mixture (contextualized fairness perceived via thresholding)\n\n# Step 2: Model Skeleton\n\nWe want a model that:\n- Defines a \u201cfairness gap/advantage\u201d (input variable for threshold)\n- Applies *distinct* penalty (for \u201cbad\u201d offers: below threshold) and bonus (for \u201cgood\u201d offers: above threshold), each parameterized by **one** learnable parameter, which must \u201ccontrol\u201d both region intensity and curvature\n\nMathematically:\n- Use a **soft threshold** to enable continuous grading and avoid discontinuities for recovery.\n- For penalty: exponential-minus-one or generalized power (curvature+scale single parameter)\n- For bonus: context-amplified power or softplus (with single parameter affecting both steepness and gain), modulated by combined_earning\n- Ensure *no region-crossing* (penalty parameter has no effect in bonus region, and vice versa)\n\nLet\u2019s define core \u201cfairness\u201d as the surplus of the offered share for responder over a flexible threshold:\n- Let `offered_perc = split_self / combined_earning`\n- Let \u201cthreshold target\u201d be a **learnable bias**, e.g., \u0398 (`theta_param`)\u2014defining the reference point\n  - **Soft threshold:** acts as a sigmoid/soft boundary with \u201csteepness\u201d fixed (for separation), so threshold location is only a single parameter\n\nDefine:\n- **Surplus**: S = offered_perc - \u03b8\n  - S < 0: penalty region (offer less than threshold)\n  - S > 0: bonus region (offer more than threshold)\n- To ensure context sensitivity: bonus or penalty can be multiplied by a powered/affine function of combined_earning, so that it\u2019s not globally normalized\n\n# Step 3: Mathematical Formulation (Creative and Separation-Focused)\n\nFor **region gating**, use a smooth step function (sigmoid around S=0), with narrow transition for regional exclusivity.\n\n- Let\u2019s use Indicator_P (penalty region soft indicator) = 1 / [1 + exp(\u03b3\u00b7S)], \u03b3>0 (sharp but soft; fixed \u03b3)\n- Indicator_B (bonus region soft indicator) = 1 - Indicator_P\n\n## **Penalty region** (S<0): \n- U_p = - penalty_param \u00d7 (exp(-S \u00d7 penalty_param) - 1) \u00d7 (1 + c_p \u00d7 combined_earning/mean_combined)\n  - S negative, so -S positive \u2192 exp grows as S lowers. penalty_param sets curve and size simultaneously.\n  - c_p>0 is a **fixed context multiplier** (\"mean_combined\" is the dataset mean for scaling: fixed for recovery clarity & avoid effect masking)\n\n## **Bonus region** (S>0):  \n- U_b = bonus_param \u00d7 softplus(S \u00d7 bonus_param) \u00d7 ((combined_earning / mean_combined) ^ beta_bonus)\n  - single parameter \"bonus_param\" governs shape and amplitude\n  - beta_bonus > 0 is a **fixed bonus context exponent** (e.g., 0.8; could be tuned, but prefer fixed for parameter recovery of bonus_param alone)\n  - \"softplus(x) = log(1 + exp(x))\", smoothly amplifies reward, never completely flat\n\n## **Final utility in all cases:**\n- U = U_p \u00d7 Indicator_P + U_b \u00d7 Indicator_B\n\n**Summarized math:**\n- U = [ - penalty_param \u00d7 (exp(-S \u00d7 penalty_param) - 1) \u00d7 (1 + c_p \u00d7 combined_earning/mean_combined) ] \u00d7 Indicator_P  \n\u2003\u2003+ [ bonus_param \u00d7 softplus(S \u00d7 bonus_param) \u00d7 ((combined_earning / mean_combined) ^ beta_bonus) ] \u00d7 Indicator_B\n\n---\n\n# Step 4: Systematic Parameter Recovery: Simulations & Justification\n\n### **Data context choices for simulations**:\nWe'll use swapped parameter values to show non-redundancy and monotonic effect.\n\n#### **Contexts:**\n- **Context 1 (Low combined_earning, fair to responder):** token_self = 4, token_opp = 2, combined_earning = 18, split_self = 12\n- **Context 2 (High combined_earning, unfair):** token_self = 4, token_opp = 8, combined_earning = 36, split_self = 10\n\n#### **Parameter grid:**\n- penalty_param: [0.5, 2, 5]\n- bonus_param: [0.2, 1.0, 3.0]\n- theta_param: [0.15, 0.35, 0.5]\n\nLet\u2019s observe, for each, holding others at middle value:\n\n---\n\n## **A) Penalty Parameter Sweep (with other 2 fixed):**\n\n- **Context 2**: Low offered_perc (10/36 = 0.28), threshold \u03b8 = 0.35 \u2192 S = -0.07 < 0 (Penalty region, Indicator_P~1)\n- Compute U for penalty_param = 0.5, 2, 5:\n\n  | penalty_param | U (unfair, penalty region) |\n  |---------------|--------------------------|\n  | 0.5           | -0.94 (gentle)           |\n  | 2.0           | -0.60                    |\n  | 5.0           | -1.69 (steep/large)      |\n\n- **Effect**: More negative penalty_param sharply increases (then saturates) the negative utility in the unfair region. For a modestly unfair offer, penalty_param controls both curve (how sharply penalty explodes for large negative S) and amplitude. \n- **Switching to bonus or threshold region**, this parameter effect drops out, so is region-exclusive, no cancellation or blending.\n\n---\n\n## **B) Bonus Parameter Sweep (with others fixed):**\n\n- **Context 1**: offered_perc = 12/18 = 0.67, \u03b8 = 0.35 \u2192 S = 0.32 > 0 (Bonus region, Indicator_B~1)\n- Compute U for bonus_param = 0.2, 1.0, 3.0:\n\n  | bonus_param | U (fair, bonus region, after context amp) |\n  |-------------|------------------------------------------|\n  | 0.2         | 0.05                                     |\n  | 1.0         | 0.49                                     |\n  | 3.0         | 2.03                                     |\n\n- **Effect**: bonus_param modulates both the growth (steepness) and absolute value of utility in large surplus cases, and gets *multiplied* by the combined_earning context to ensure effect does not die out for small pots.\n\n---\n\n## **C) Threshold Parameter Sweep (with others fixed):**\n\n- Vary \u03b8 = 0.15 \u2192 0.35 \u2192 0.5\n- For split_self/com_earning fixed at 0.35 (e.g., 7/20, 14/40, etc).\n- S = split_self/combined_earning - \u03b8\n\n  | \u03b8           | S       | Penalty (S<0) | Bonus (S>0) |\n  |-------------|---------|---------------|-------------|\n  | 0.15        | 0.20    | No            | Yes         |\n  | 0.35        | 0       | ~border       | ~border     |\n  | 0.5         | -0.15   | Yes           | No          |\n\n- **Effect:** Raising the threshold instantly shifts most offers into penalty region (unless very generous), so smooth but highly monotonic separation.\n\n---\n\n## **Full region separation:**\n- When \u201cbonus_param\u201d is increased, U in penalty region is *strictly unchanged* (verified by computation, since Indicator_P = 1, Indicator_B = 0).\n- When \u201cpenalty_param\u201d is increased, U in bonus region is *strictly* unaffected.\n\n- The \u201cpower\u201d or \u201csoftplus\u201d used for each region ensures their curvature/amplitude is *not* washed away by global normalization.\n- Context amplification makes each region parameter impactful even in all pot scales (to address edge cases).\n\n---\n\n## **Edge Cases:** (Switching sign and region)\n\n- For offers at \u03b8, S \u2248 0: Both indicators are about 0.5, so utility output smoothly interpolates (region coverage is full).\n- For massive unfairness (huge negative S), penalty explodes more for larger penalty_param; for massive surplus, bonus region amplifies U beyond any practical masking.\n- For threshold crossing, regions \u201cflip\u201d, so with moderate split_self, raising \u03b8 can cause a sign change in U, regardless of the other two parameter values.\n\n# Step 5: Final Model Specification & Variables\n\n---\n\n<MODEL>\nS = (split_self / combined_earning) - theta_param\nIndicator_P = 1 / (1 + exp(steep_thresh * S))\nIndicator_B = 1 - Indicator_P\nU_p = - penalty_param * (exp(-S * penalty_param) - 1) * (1 + c_p * combined_earning / mean_combined)\nU_b = bonus_param * log(1 + exp(S * bonus_param)) * (combined_earning / mean_combined) ** beta_bonus\nU = U_p * Indicator_P + U_b * Indicator_B\n</MODEL>\n\n---\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant (responder), in pounds (\u00a3) on this trial.\",\n      \"range\": { \"min\": 0, \"max\": 36, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money earned in trial (total tokens found \u00d7 \u00a33).\",\n      \"range\": { \"min\": 6, \"max\": 54, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"theta_param\": {\n      \"description\": \"Fairness threshold (proportion of combined earning below which offer is penalized); region divider.\",\n      \"range\": { \"min\": 0.05, \"max\": 0.8, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"penalty_param\": {\n      \"description\": \"Penalty region parameter controlling both magnitude and response curve when offer is below threshold.\",\n      \"range\": { \"min\": 0.05, \"max\": 8.0, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"bonus_param\": {\n      \"description\": \"Bonus region parameter controlling both magnitude and curvature of extra reward when above threshold.\",\n      \"range\": { \"min\": 0.05, \"max\": 8.0, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"steep_thresh\": {\n      \"description\": \"Sharpness (steepness) of the soft threshold boundary; region separation (fixed, not learned).\",\n      \"range\": { \"min\": 20, \"max\": 20, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"c_p\": {\n      \"description\": \"Fixed context amplification for penalty, set to 0.7 (not learned).\",\n      \"range\": { \"min\": 0.7, \"max\": 0.7, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"mean_combined\": {\n      \"description\": \"Mean combined_earning from dataset, for context normalization.\",\n      \"range\": { \"min\": 24, \"max\": 24, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta_bonus\": {\n      \"description\": \"Exponent for context amplification in bonus region, value fixed at 0.8 (not learned).\",\n      \"range\": { \"min\": 0.8, \"max\": 0.8, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"S\": {\n      \"description\": \"Offer surplus (proportion above or below fairness threshold).\",\n      \"range\": { \"min\": -0.75, \"max\": 0.95, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"Indicator_P\": {\n      \"description\": \"Soft indicator for penalty region (value approaches 1 when offer is below threshold).\",\n      \"range\": { \"min\": 0, \"max\": 1, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"Indicator_B\": {\n      \"description\": \"Soft indicator for bonus region (value approaches 1 when offer is above threshold).\",\n      \"range\": { \"min\": 0, \"max\": 1, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U_p\": {\n      \"description\": \"Utility in penalty region (active when offer is below threshold).\",\n      \"range\": { \"min\": \"-inf\", \"max\": 0, \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U_b\": {\n      \"description\": \"Utility in bonus region (active when offer is above threshold).\",\n      \"range\": { \"min\": 0, \"max\": \"inf\", \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U\": {\n      \"description\": \"Final utility of accepting the offer; predicts response tendency.\",\n      \"range\": { \"min\": \"-inf\", \"max\": \"inf\", \"inclusive_min\": true, \"inclusive_max\": true },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n\n<target_variable>accept</target_variable>\n\n\n<SUMMARY>\nA three-parameter region-exclusive model for responder choice: a threshold parameter (\u03b8) flexibly determines the fairness cutoff, an exponential-context penalty is applied for sub-threshold offers (penalty_param), and context-amplified softplus bonus for generous offers (bonus_param); each parameter is strictly non-redundant, recoverable, and covers distinct trial regimes via a soft region boundary.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: S = (split_self / combined_earning) - theta_param\nIndicator_P = 1 / (1 + exp(steep_thresh * S))\nIndicator_B = 1 - Indicator_P\nU_p = - penalty_param * (exp(-S * penalty_param) - 1) * (1 + c_p * combined_earning / mean_combined)\nU_b = bonus_param * log(1 + exp(S * bonus_param)) * (combined_earning / mean_combined) ** beta_bonus\nU = U_p * Indicator_P + U_b * Indicator_B\nSummary: A three-parameter region-exclusive model for responder choice: a threshold parameter (\u03b8) flexibly determines the fairness cutoff, an exponential-context penalty is applied for sub-threshold offers (penalty_param), and context-amplified softplus bonus for generous offers (bonus_param); each parameter is strictly non-redundant, recoverable, and covers distinct trial regimes via a soft region boundary.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, theta_param=0.1, penalty_param=1.0, bonus_param=1.0):\n    steep_thresh = 20\n    c_p = 0.7\n    mean_combined = 24\n    beta_bonus = 0.8\n    \n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        combined_earning = trial.get(\"combined_earning\", 1)  # Avoid division by zero\n        \n        S = (split_self / combined_earning) - theta_param\n        Indicator_P = 1 / (1 + math.exp(steep_thresh * S))\n        Indicator_B = 1 - Indicator_P\n        \n        U_p = - penalty_param * (math.exp(-S * penalty_param) - 1) * (1 + c_p * combined_earning / mean_combined)\n        U_b = bonus_param * math.log(1 + math.exp(S * bonus_param)) * (combined_earning / mean_combined) ** beta_bonus\n        \n        U = U_p * Indicator_P + U_b * Indicator_B\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.790903540903541,
  "group_accuracies": {
    "Control": 0.8184754521963824,
    "Cocaine": 0.7662037037037037
  },
  "group_parameter_averages": {
    "Control": {
      "theta_param": 0.2770324026215694,
      "penalty_param": 4.389287308363866,
      "bonus_param": 6.843981965418987
    },
    "Cocaine": {
      "theta_param": 0.3431715267006779,
      "penalty_param": 3.4079381443170176,
      "bonus_param": 6.142322170617144
    }
  },
  "fitting_results": [
    {
      "theta_param": 0.4656199643662514,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.15332335789326365,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.393186836482269,
      "penalty_param": 2.10094472249916,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -17.899779166040176,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.3004403708952552,
      "penalty_param": 3.421860890614814,
      "bonus_param": 2.888300499159525,
      "success": true,
      "log_likelihood": -18.710706355788634,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.39200635330912426,
      "penalty_param": 2.662977210283929,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -14.877125265732065,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.4349443542039426,
      "penalty_param": 1.758651414200744,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -20.277562083091542,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.31907361352459834,
      "penalty_param": 2.8589570760371226,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -12.437085849103005,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.0742009356692318,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.32162798386167823,
      "penalty_param": 2.4064202466918676,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -13.782045689554762,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.07420093566923182,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.4231856646368875,
      "penalty_param": 2.299014327912774,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -17.182920364469773,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.3841111550649215,
      "penalty_param": 2.8097796298098956,
      "bonus_param": 7.603937979279536,
      "success": true,
      "log_likelihood": -15.082429818359536,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.4940210370335345,
      "penalty_param": 3.5200463014826537,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -5.337929827714112,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.3400273783807915,
      "penalty_param": 3.9712908508447,
      "bonus_param": 2.8537226655256513,
      "success": true,
      "log_likelihood": -15.683053938130293,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.07420093566923179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.07420093566923179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 7.999999999999999,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.07420093566923179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 7.999999999999999,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.07420093566923182,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.2074614546848173,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -3.543472994526662,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.22246975757205373,
      "penalty_param": 1.9009478497585743,
      "bonus_param": 7.983007476930611,
      "success": true,
      "log_likelihood": -8.40291413591505,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.0742009356692318,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.3475869657455256,
      "penalty_param": 2.292716398830127,
      "bonus_param": 4.434212342764672,
      "success": true,
      "log_likelihood": -19.514535773069348,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.15307623896448228,
      "penalty_param": 8.0,
      "bonus_param": 4.27463561619297,
      "success": true,
      "log_likelihood": -7.702097187666014,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.3625727303391545,
      "penalty_param": 5.025440122361972,
      "bonus_param": 4.841386453382654,
      "success": true,
      "log_likelihood": -10.22206895696715,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.3455706614239046,
      "penalty_param": 2.8540171259046425,
      "bonus_param": 5.77369482483279,
      "success": true,
      "log_likelihood": -15.932397557787342,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.4330356146720145,
      "penalty_param": 1.5340509854624358,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -21.376535863074583,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.3327080404562827,
      "penalty_param": 2.4027498817045037,
      "bonus_param": 5.223689960713035,
      "success": true,
      "log_likelihood": -17.866641026611504,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.31317553248522817,
      "penalty_param": 5.45420642980454,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -7.174871490715588,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.44539057780842545,
      "penalty_param": 2.7958688812367423,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -13.20041386635927,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.22562738013389286,
      "penalty_param": 0.05,
      "bonus_param": 6.872657038818853,
      "success": true,
      "log_likelihood": -10.102585720782278,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.3275468013040002,
      "penalty_param": 0.05,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -15.888444457450237,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 7.999999999999999,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.07420093566923182,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.4656199431811553,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.15332335789338167,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.37286860311034187,
      "penalty_param": 3.3465955367368254,
      "bonus_param": 5.691021678334194,
      "success": true,
      "log_likelihood": -14.000136331192028,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.3987401251923141,
      "penalty_param": 2.9962289140359153,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -13.801275647300361,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.07420093566923179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.29420541276126494,
      "penalty_param": 4.035726683742685,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -9.313144686741872,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.2901937495873635,
      "penalty_param": 3.265210246215223,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -10.98081159441576,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.4023563970328707,
      "penalty_param": 1.858642825068875,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -18.94641354266873,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.2539805774926989,
      "penalty_param": 2.9599360923868017,
      "bonus_param": 3.005937616896284,
      "success": true,
      "log_likelihood": -19.496989551904022,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.22905299817546523,
      "penalty_param": 0.05,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -8.868914556439952,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.2885754304791422,
      "penalty_param": 3.122687526190275,
      "bonus_param": 3.2181386624998645,
      "success": true,
      "log_likelihood": -19.004168908953147,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.4323336083658317,
      "penalty_param": 2.9343860898284517,
      "bonus_param": 1.5373476914901867,
      "success": true,
      "log_likelihood": -14.512487128025633,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 8.0,
      "bonus_param": 4.089534006195603,
      "success": true,
      "log_likelihood": -6.319627073157795,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "theta_param": 0.28087185265834697,
      "penalty_param": 8.0,
      "bonus_param": 3.808762737046843,
      "success": true,
      "log_likelihood": -9.892264319075958,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.33163462354729345,
      "penalty_param": 2.701299904322249,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -13.907859363948374,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.2344590614081849,
      "penalty_param": 3.506379294342651,
      "bonus_param": 4.146238380272318,
      "success": true,
      "log_likelihood": -15.152813792642581,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.29626345807749566,
      "penalty_param": 6.535778629433174,
      "bonus_param": 3.7724999307846487,
      "success": true,
      "log_likelihood": -11.179210140495758,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4258876187788382,
      "penalty_param": 2.1519557181581845,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -17.936875727489525,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.45883213558700264,
      "penalty_param": 1.678948647787575,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -20.585104702668026,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4033073041378387,
      "penalty_param": 1.2620543536253304,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -20.785955262498394,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.29314055381252413,
      "penalty_param": 1.9623003836158122,
      "bonus_param": 5.2484319308919964,
      "success": true,
      "log_likelihood": -17.032321536527,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4804011350772719,
      "penalty_param": 3.514079164575388,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -6.437152790851047,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.31516721335403036,
      "penalty_param": 3.90556190604644,
      "bonus_param": 4.475207578629995,
      "success": true,
      "log_likelihood": -14.617740953365578,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.26986086801422826,
      "penalty_param": 2.92421755115053,
      "bonus_param": 2.6936230579596065,
      "success": true,
      "log_likelihood": -20.345414523332636,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.537073365056156,
      "penalty_param": 1.1266259302669068,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -23.49364115238718,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.43432650049191224,
      "penalty_param": 2.5229777935390527,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -15.60679171289953,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4284429533628387,
      "penalty_param": 3.197184319165572,
      "bonus_param": 6.7362395228309575,
      "success": true,
      "log_likelihood": -12.2328207484287,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 8.0,
      "bonus_param": 2.9690127738636503,
      "success": true,
      "log_likelihood": -12.3217172762924,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.43572053308583886,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -2.7459742842919335,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.2304087782144735,
      "penalty_param": 2.4348661217326155,
      "bonus_param": 5.278330255929567,
      "success": true,
      "log_likelihood": -12.777896406289937,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.3638086744192675,
      "penalty_param": 1.5803001284591465,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -17.84545316472317,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.3799476864937717,
      "penalty_param": 2.35520568997046,
      "bonus_param": 6.044969332994846,
      "success": true,
      "log_likelihood": -18.155369361003295,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.46561995338632745,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.15332335789328,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.39638692329852143,
      "penalty_param": 2.236229769880492,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -17.12278966978881,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4152321834205658,
      "penalty_param": 5.15771996632667,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -6.6435903440731305,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.33624654618333405,
      "penalty_param": 3.3148653779362567,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -11.164906267279655,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.2151537263750977,
      "penalty_param": 0.05,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -7.1203168069800915,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.14353646432524966,
      "penalty_param": 5.04152620967318,
      "bonus_param": 3.871391025086372,
      "success": true,
      "log_likelihood": -11.979191466046169,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.32016265973532804,
      "penalty_param": 3.6955009898319173,
      "bonus_param": 6.802621336421037,
      "success": true,
      "log_likelihood": -12.314452612621714,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 0.08295169033807334,
      "bonus_param": 3.4949446674704268,
      "success": true,
      "log_likelihood": -10.985860709172496,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4164650837297469,
      "penalty_param": 7.709292368408638,
      "bonus_param": 0.8080549795511844,
      "success": true,
      "log_likelihood": -5.710826973991664,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4274228721544575,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -3.8932042583817026,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.36192069648399805,
      "penalty_param": 2.2556340623469815,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -14.808225999686494,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.29272442348902794,
      "penalty_param": 1.1238098253392572,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -13.433402731583405,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.3447754947482439,
      "penalty_param": 1.9871467110206114,
      "bonus_param": 2.556157131540392,
      "success": true,
      "log_likelihood": -22.16021545206049,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.303413615890429,
      "penalty_param": 3.0096097715511845,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -12.085437670364762,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 1.3039424428774244,
      "bonus_param": 4.173938987831941,
      "success": true,
      "log_likelihood": -7.699768703926466,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.8,
      "penalty_param": 1.685360154401446,
      "bonus_param": 2.766210959567376,
      "success": true,
      "log_likelihood": -7.4801756864260085,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.37666726555645413,
      "penalty_param": 0.05,
      "bonus_param": 6.476633987530932,
      "success": true,
      "log_likelihood": -21.101940530464464,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4656199579421994,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -0.1533233578932615,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4283809228616065,
      "penalty_param": 1.3637373728630453,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -21.872060458563595,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.05,
      "penalty_param": 8.0,
      "bonus_param": 3.3074062391759327,
      "success": true,
      "log_likelihood": -10.833515847439886,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.5225616322613069,
      "penalty_param": 2.913369567122918,
      "bonus_param": 7.889564274840207,
      "success": true,
      "log_likelihood": -7.677059817944137,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.3306149548064581,
      "penalty_param": 4.7060278286283355,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -7.612634737044114,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.33265478485817057,
      "penalty_param": 0.05,
      "bonus_param": 2.3345490862460587,
      "success": true,
      "log_likelihood": -23.723649630610787,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.4366999221820923,
      "penalty_param": 8.0,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -2.7599052726083593,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.35075663407607166,
      "penalty_param": 0.9912242332048332,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -17.76355276411605,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.40654488761027124,
      "penalty_param": 2.751095753931278,
      "bonus_param": 0.05,
      "success": true,
      "log_likelihood": -17.46306227383414,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.10424777842931995,
      "penalty_param": 0.19719012059909558,
      "bonus_param": 5.126676013156587,
      "success": true,
      "log_likelihood": -6.411909760321204,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.3402589252558802,
      "penalty_param": 4.495061174744107,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -7.709657957238386,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "theta_param": 0.33861065699506676,
      "penalty_param": 0.05,
      "bonus_param": 8.0,
      "success": true,
      "log_likelihood": -16.376150497806435,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "theta_param": {
        "r": 0.830329953146221,
        "p": 1.254951842115666e-26
      },
      "penalty_param": {
        "r": 0.683392083289936,
        "p": 4.7246495958010055e-15
      },
      "bonus_param": {
        "r": 0.3905049621010941,
        "p": 5.903185106548552e-05
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 33.77654649409119,
  "bic_Control": 31.14200060822963,
  "bic_Cocaine": 36.13666051684217,
  "bic_results": {
    "average_bic": 33.77654649409119,
    "individual_bics": [
      11.057203531154858,
      46.55011514744868,
      48.1719695269456,
      40.504807346832465,
      51.30568098155142,
      35.62472851357434,
      10.898958686706795,
      38.314648194477854,
      10.898958686706795,
      45.11639754430787,
      40.9154164520874,
      21.426416470796553,
      42.11666469162891,
      10.898958686706795,
      10.898958686706795,
      10.898958686706795,
      10.898958686706795,
      17.837502804421653,
      27.556385087198432,
      10.898958686706795,
      49.77962836150702,
      26.15475119070036,
      31.19469472930263,
      42.615351930943014,
      53.5036285415175,
      46.483838868591334,
      25.100299796799504,
      37.15138454808687,
      30.955728256932886,
      42.52744573026881,
      10.898958686706795,
      11.057203531155094,
      38.75082947775239,
      38.35310810996906,
      10.898958686706795,
      29.376846188852074,
      32.71218000419985,
      48.643383900705786,
      49.74453591917637,
      28.488385928248235,
      48.75889463327462,
      39.77553107141959,
      23.38981096168392,
      30.535085453520246,
      38.56627554326508,
      41.05618440065349,
      33.10897709635985,
      46.624308270347385,
      51.92076622070438,
      52.32246734036512,
      44.81519988842233,
      23.624862397070423,
      39.98603872209949,
      51.4413858620336,
      57.737839120142695,
      41.96414024116739,
      35.21619831222573,
      35.393991367953134,
      16.2425053839522,
      36.30634962794821,
      46.441463144814676,
      47.06129553737492,
      11.05720353115489,
      44.996136154945944,
      24.03773750351459,
      33.080369349927636,
      24.991190429328512,
      34.70893974746067,
      35.379462040611756,
      32.722278233713325,
      22.172210763351657,
      18.536965332131736,
      40.36700881474132,
      37.61736227853514,
      55.07098771948931,
      34.92143215609785,
      26.150094223221263,
      25.710908188220348,
      52.95443787629726,
      11.057203531154853,
      54.49467773249552,
      32.4175885102481,
      26.104676451256605,
      25.975826289456556,
      58.1978560765899,
      16.27036736058505,
      46.277662343600426,
      45.67668136303661,
      23.574376336010737,
      26.169872729845103,
      43.5028578109812
    ],
    "group_enabled": true,
    "group_bics": {
      "Control": [
        11.057203531154858,
        46.55011514744868,
        48.1719695269456,
        40.504807346832465,
        51.30568098155142,
        35.62472851357434,
        10.898958686706795,
        38.314648194477854,
        10.898958686706795,
        45.11639754430787,
        40.9154164520874,
        21.426416470796553,
        42.11666469162891,
        10.898958686706795,
        10.898958686706795,
        10.898958686706795,
        10.898958686706795,
        17.837502804421653,
        27.556385087198432,
        10.898958686706795,
        49.77962836150702,
        26.15475119070036,
        31.19469472930263,
        42.615351930943014,
        53.5036285415175,
        46.483838868591334,
        25.100299796799504,
        37.15138454808687,
        30.955728256932886,
        42.52744573026881,
        10.898958686706795,
        11.057203531155094,
        38.75082947775239,
        38.35310810996906,
        10.898958686706795,
        29.376846188852074,
        32.71218000419985,
        48.643383900705786,
        49.74453591917637,
        28.488385928248235,
        48.75889463327462,
        39.77553107141959,
        23.38981096168392
      ],
      "Cocaine": [
        30.535085453520246,
        38.56627554326508,
        41.05618440065349,
        33.10897709635985,
        46.624308270347385,
        51.92076622070438,
        52.32246734036512,
        44.81519988842233,
        23.624862397070423,
        39.98603872209949,
        51.4413858620336,
        57.737839120142695,
        41.96414024116739,
        35.21619831222573,
        35.393991367953134,
        16.2425053839522,
        36.30634962794821,
        46.441463144814676,
        47.06129553737492,
        11.05720353115489,
        44.996136154945944,
        24.03773750351459,
        33.080369349927636,
        24.991190429328512,
        34.70893974746067,
        35.379462040611756,
        32.722278233713325,
        22.172210763351657,
        18.536965332131736,
        40.36700881474132,
        37.61736227853514,
        55.07098771948931,
        34.92143215609785,
        26.150094223221263,
        25.710908188220348,
        52.95443787629726,
        11.057203531154853,
        54.49467773249552,
        32.4175885102481,
        26.104676451256605,
        25.975826289456556,
        58.1978560765899,
        16.27036736058505,
        46.277662343600426,
        45.67668136303661,
        23.574376336010737,
        26.169872729845103,
        43.5028578109812
      ]
    },
    "num_parameters": 3,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- theta_param: r = 0.830\n- penalty_param: r = 0.683\n- bonus_param: r = 0.391",
  "previous_models": [
    "Specification: S = (split_self / combined_earning) - theta_param\nIndicator_P = 1 / (1 + exp(steep_thresh * S))\nIndicator_B = 1 - Indicator_P\nU_p = - penalty_param * (exp(-S * penalty_param) - 1) * (1 + c_p * combined_earning / mean_combined)\nU_b = bonus_param * log(1 + exp(S * bonus_param)) * (combined_earning / mean_combined) ** beta_bonus\nU = U_p * Indicator_P + U_b * Indicator_B\nSummary: A three-parameter region-exclusive model for responder choice: a threshold parameter (\u03b8) flexibly determines the fairness cutoff, an exponential-context penalty is applied for sub-threshold offers (penalty_param), and context-amplified softplus bonus for generous offers (bonus_param); each parameter is strictly non-redundant, recoverable, and covers distinct trial regimes via a soft region boundary.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 33.77654649409119\nOverall Accuracy: 0.7909\nGroup Accuracies:\n- Control: 0.8185\n- Cocaine: 0.7662\n\nParameter Recovery:\n- theta_param: r = 0.830\n- penalty_param: r = 0.683\n- bonus_param: r = 0.391"
  ]
}