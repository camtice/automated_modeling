{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "\n\n---\n\n**INSTRUCTIONS \u2013 RUN 7**\n\nYour task is to design a computational model of responder (accept/reject) choice using the dataset described. Your principal goals are: (1) maximize recoverability of all learnable parameters, (2) ensure strong overall fit (e.g., BIC/accuracy), and (3) capture psychologically plausible, nonlinear, context- and fairness-sensitive mechanisms that go beyond simple additive models.\n\nPlease rigorously adhere to the following expanded process:\n\n1. **Nonlinear and Contextual Mechanisms:**\n   - Identify points in the offer/entitlement space where sharp, nonlinear, or stepwise changes in choice likelihood are expected.\n   - Explicitly map and justify, for each model region (e.g., offer < threshold, offer \u2265 threshold), what type of penalty/reward is applied and why.\n   - Use at least one of: a saturating penalty/bonus (e.g., log or hyperbolic), loss/gain asymmetry (distinct treatment for unfair vs generous), or an explicit indicator/step/sigmoid function. Avoid simple power-law penalty exponents unless you can demonstrate that this term is strictly uncorrelated with penalty sensitivity.\n   - Strongly consider a model structure where the psychological fairness threshold itself is a learnable but bounded parameter (not always a literal proportional entitlement).\n\n2. **Parameter Separation, Recovery, and Identifiability:**\n   - Your three learnable parameters must have clearly defined, *non-overlapping functional zones*: for example, one for placement of threshold, one for penalty strength below threshold, one for bonus magnitude above.\n   - Do **not** allow any two parameters to both affect the same region (e.g., avoid penalty sensitivity *and* penalty exponent acting together).\n   - In your **reasoning** step, perform a simulated \u201ctoy recovery\u201d thought experiment: suppose each parameter is changed in isolation (holding others fixed at mid-range)\u2014how does the accept/reject curve over split_self vs entitlement change? Are these changes visually separable and uniquely attributable?\n   - Clearly predict, in your reasoning, how the model would distinguish a fairness-stickler from a greedy-acceptor or generous thresholder (give at least two parameter profiles and expected patterns).\n\n3. **Move Beyond Additive Sweeps:**\n   - At least one core interaction, ratio, or piecewise region must be used. *Mandatory*: Avoid purely additive linear models unless you can robustly defend recoverability.\n   - If you use soft-threshold or sigmoid-like logic, be explicit: \u201cThreshold placement parameter T controls where the S-curve bends; penalty/bonus steepness parameter S affects how rapidly acceptance moves around threshold.\u201d\n\n4. **Exploit Absolute and Proportional Context:**\n   - Combine both absolute offer amount (split_self) and contextual fairness (e.g., proportional entitlement, combined_earning, token_self/token_opp).\n   - When modeling threshold, *do not assume participants' threshold is always at their objective entitlement*; let this be inferred (learnable), ideally as a proportion.\n\n5. **Parameterization, Bounds, and Interpretability:**\n   - Strict maximum of three learnable parameters, all with wide, explicit, inclusive numerical bounds.\n   - For each, state in reasoning which region it governs and how it affects choices and fits.\n   - Document variables/parameters with python-safe names and ranges in the required JSON schema.\n   - All variables in the model formula must be listed in the JSON spec.\n   - Always specify which dataset variable is the prediction target.\n\n6. **Model Output Requirements and Format:**\n   - Think step by step (show all reasoning before formalizing the model).\n   - Output the full mathematical model strictly between <MODEL> and </MODEL> tags (math only).\n   - Output all variable and parameter documentation between <VARIABLES> and </VARIABLES> tags.\n   - Output the prediction target within <target_variable> tags.\n   - Output a purely descriptive <SUMMARY> (do not mention the task name).\n   - Use only Python-safe parameter names.\n\n**MANDATORY CREATIVE CHALLENGE:**  \nAttempt at least one of the following:\n- (A) Implement a smooth or soft threshold via sigmoidal or logistic or hyperbolic tangent function, with learnable threshold location and slope.\n- (B) Introduce an explicit, parameterized \"fairness threshold\" scaling (e.g., tau_fairness: threshold = tau_fairness \u00d7 entitlement), learnable from data.\n- (C) Model an interaction term such that effect of unfairness is larger for bigger pots (or another contextual variable).\n- (D) Any highly interpretable, innovative structure that breaks linear or exponent-only trade-off and uniquely partitions parameter impacts.\n*State which you choose and why in your reasoning step.*\n\nThink boldly and ground every parameter in a clear, testable psychological hypothesis.\n\n---",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: \n\n---\n\n**INSTRUCTIONS \u2013 RUN 7**\n\nYour task is to design a computational model of responder (accept/reject) choice using the dataset described. Your principal goals are: (1) maximize recoverability of all learnable parameters, (2) ensure strong overall fit (e.g., BIC/accuracy), and (3) capture psychologically plausible, nonlinear, context- and fairness-sensitive mechanisms that go beyond simple additive models.\n\nPlease rigorously adhere to the following expanded process:\n\n1. **Nonlinear and Contextual Mechanisms:**\n   - Identify points in the offer/entitlement space where sharp, nonlinear, or stepwise changes in choice likelihood are expected.\n   - Explicitly map and justify, for each model region (e.g., offer < threshold, offer \u2265 threshold), what type of penalty/reward is applied and why.\n   - Use at least one of: a saturating penalty/bonus (e.g., log or hyperbolic), loss/gain asymmetry (distinct treatment for unfair vs generous), or an explicit indicator/step/sigmoid function. Avoid simple power-law penalty exponents unless you can demonstrate that this term is strictly uncorrelated with penalty sensitivity.\n   - Strongly consider a model structure where the psychological fairness threshold itself is a learnable but bounded parameter (not always a literal proportional entitlement).\n\n2. **Parameter Separation, Recovery, and Identifiability:**\n   - Your three learnable parameters must have clearly defined, *non-overlapping functional zones*: for example, one for placement of threshold, one for penalty strength below threshold, one for bonus magnitude above.\n   - Do **not** allow any two parameters to both affect the same region (e.g., avoid penalty sensitivity *and* penalty exponent acting together).\n   - In your **reasoning** step, perform a simulated \u201ctoy recovery\u201d thought experiment: suppose each parameter is changed in isolation (holding others fixed at mid-range)\u2014how does the accept/reject curve over split_self vs entitlement change? Are these changes visually separable and uniquely attributable?\n   - Clearly predict, in your reasoning, how the model would distinguish a fairness-stickler from a greedy-acceptor or generous thresholder (give at least two parameter profiles and expected patterns).\n\n3. **Move Beyond Additive Sweeps:**\n   - At least one core interaction, ratio, or piecewise region must be used. *Mandatory*: Avoid purely additive linear models unless you can robustly defend recoverability.\n   - If you use soft-threshold or sigmoid-like logic, be explicit: \u201cThreshold placement parameter T controls where the S-curve bends; penalty/bonus steepness parameter S affects how rapidly acceptance moves around threshold.\u201d\n\n4. **Exploit Absolute and Proportional Context:**\n   - Combine both absolute offer amount (split_self) and contextual fairness (e.g., proportional entitlement, combined_earning, token_self/token_opp).\n   - When modeling threshold, *do not assume participants' threshold is always at their objective entitlement*; let this be inferred (learnable), ideally as a proportion.\n\n5. **Parameterization, Bounds, and Interpretability:**\n   - Strict maximum of three learnable parameters, all with wide, explicit, inclusive numerical bounds.\n   - For each, state in reasoning which region it governs and how it affects choices and fits.\n   - Document variables/parameters with python-safe names and ranges in the required JSON schema.\n   - All variables in the model formula must be listed in the JSON spec.\n   - Always specify which dataset variable is the prediction target.\n\n6. **Model Output Requirements and Format:**\n   - Think step by step (show all reasoning before formalizing the model).\n   - Output the full mathematical model strictly between <MODEL> and </MODEL> tags (math only).\n   - Output all variable and parameter documentation between <VARIABLES> and </VARIABLES> tags.\n   - Output the prediction target within <target_variable> tags.\n   - Output a purely descriptive <SUMMARY> (do not mention the task name).\n   - Use only Python-safe parameter names.\n\n**MANDATORY CREATIVE CHALLENGE:**  \nAttempt at least one of the following:\n- (A) Implement a smooth or soft threshold via sigmoidal or logistic or hyperbolic tangent function, with learnable threshold location and slope.\n- (B) Introduce an explicit, parameterized \"fairness threshold\" scaling (e.g., tau_fairness: threshold = tau_fairness \u00d7 entitlement), learnable from data.\n- (C) Model an interaction term such that effect of unfairness is larger for bigger pots (or another contextual variable).\n- (D) Any highly interpretable, innovative structure that breaks linear or exponent-only trade-off and uniquely partitions parameter impacts.\n*State which you choose and why in your reasoning step.*\n\nThink boldly and ground every parameter in a clear, testable psychological hypothesis.\n\n---\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "Let\u2019s design the computational model following all instructions, especially on nonlinearity, parameter separation, and psychological interpretability.\n\n---\n\n**STEP BY STEP REASONING**\n\n### Key Goals\n\n1. **Maximize parameter recoverability**: Each parameter must uniquely control a separable, identifiable feature of the accept/reject curve.\n2. **Strong fit**: Model must capture empirical effects including nonlinear thresholding and context dependence.\n3. **Psychological plausibility with nonlinearity and context**: Context and reference dependence (fairness/entitlement), soft thresholding, and variable weighting of inequity.\n4. **Mandatory creative challenge**: *Choose at least one: A/Sigmoid threshold; B/Learnable scaling of fairness threshold; C/Interaction with pot size; D/Other interpretable, unique structure.*\n\n---\n\n### Step 1: What matters to responders?\n\nResponder receives an offer (split_self), compares to expectation or entitlement. Often, acceptance probability is low for \u201cunfair\u201d offers and rises sharply around a fairness reference/threshold.\n\n- **Threshold is personalized**\u2014not always at exactly proportional entitlement.\n- **Sensitivity/stickiness**: How steeply choices flip around a threshold.\n- **Bonus/penalty for above/below threshold**: Some accept slightly less, others demand much more.\n- **Context**: In larger-pots, unfairness may feel worse; or the salience of fairness may change.\n\n---\n\n### Step 2: Model Structure and Parameterization\n\n#### **Region Mapping and Creative Features**\n\n- **Region 1**: Offers below threshold \u2192 penalty or steep drop in acceptance probability.\n- **Region 2**: Offers near/above threshold \u2192 rapid rise in acceptance, may include a generous bonus.\n- **Threshold placement is NOT hard-coded at entitlement, but rather is a *proportional scaling* of entitlement** (CREATIVE CHALLENGE B).\n- **Transition is soft (logistic/sigmoid), not hard, with *learnable slope parameter* (CREATIVE CHALLENGE A).**\n- Contextual interaction: *Penalty for unfairness can be magnified for bigger pots* (CREATIVE CHALLENGE C).\n    - Let\u2019s achieve this by multiplying (split_self - threshold) by a scaling function of combined_earning.\n\n**We pick a model with a soft threshold at a participant-specific \u201cfairness expectation,\u201d itself a scaling of entitlement, and with contextual scaling via combined_earning.**\n\n---\n\n#### **LMentary Model Outline**\n\n**Utility before logistic transformation:**\n- u = s \u00d7 (split_self - threshold), where:\n    - s: softmax slope parameter (higher = more abrupt flip).\n    - threshold: tau_fairness \u00d7 (token_self / (token_self + token_opp)) \u00d7 combined_earning.\n        - tau_fairness: personalized, learnable proportional threshold (0.6 = highly demanding, 1.2 = surprisingly generous).\n    - Context dependence: s itself is scaled if desired by combined_earning, or penalty is modulated by big/small pots.\n\n**Three learnable parameters (example):**\n1. tau_fairness: controls where threshold sits relative to entitlement (threshold placement).\n2. slope: sharpness/steepness of sigmoid.\n3. context_weight: magnitude by which penalty for \u201cunfair\u201d offers is magnified for larger pots.\n\n**Contextual scaling**: For simplicity and identifiability, context_weight can modulate the *impact* of (split_self - threshold) as a linear function of combined_earning (normalized).\n\n**PARAMETER RECOVERABILITY:**\n- Vary tau_fairness alone: *Threshold moves left or right*\u2014acceptance curve shifts along x-axis.\n- Vary slope alone: *Sharpness increases or decreases*\u2014 S-curve more/less step-like.\n- Vary context_weight alone: *Penalty for unfair offers gets steeper or shallower with bigger pots; for small pots, effect is dampened*.\n\nThis separation is strict; each parameter alters a unique zone or quality of behavior.\n\n---\n\n#### **Toy Simulations: Visually Distinguishable Effects**\n\n- **Fairness-stickler**: tau_fairness = high, slope = high, context_weight = moderate \u2192 needs much more than proportional, flips abruptly.\n- **Greedy-acceptor**: tau_fairness = low, slope = low, context_weight = low \u2192 accepts much less than entitlement, gradual increase, not affected much by large pots.\n- **Generous thresholder**: tau_fairness = below 1, accepts sub-entitlement, moderate slope, possibly context_weight high if they react to big pots.\n\n---\n\n**Variable mapping:**\n- split_self: actual amount offered to responder.\n- Token and combined_earning: compute responder's proportional entitlement; threshold = tau_fairness \u00d7 share_proportional \u00d7 combined_earning.\n- share_proportional = token_self / (token_self + token_opp)\n- Modulate penalty for below-threshold offers more for big pots: context_weight \u00d7 (combined_earning / mean_combined_earning)\n\nOptional: To ensure identifiability and interpretability, keep only three parameters: tau_fairness [threshold location], slope [S-curve sharpness], context_weight [fairness salience as a function of pot size].\n\n---\n\n### Step 3: Modeling Formula\n\n**Mathematical expression:**\n\nLet:\n- share_proportional = token_self / (token_self + token_opp)\n- fairness_threshold = tau_fairness \u00d7 share_proportional \u00d7 combined_earning\n- context_scaling = 1 + context_weight \u00d7 (combined_earning / mean_combined_earning - 1)  \n    - (So context_scaling = 1 if average pot, increases/decreases for larger/smaller pots)\n\nLet utility (before logistic) be:\n- U = slope \u00d7 context_scaling \u00d7 (split_self - fairness_threshold)\n\nFinal choice prediction: accept = 1 if logistic(U) > 0.5\n\n---\n\n### Step 4: Complete model definition\n\n**Model summarizes as:**\n\n---\n<MODEL>\nshare_proportional = token_self / (token_self + token_opp)\nfairness_threshold = tau_fairness * share_proportional * combined_earning\ncontext_scaling = 1 + context_weight * ((combined_earning / mean_combined_earning) - 1)\nU = slope * context_scaling * (split_self - fairness_threshold)\n</MODEL>\n---\n\n**Variable descriptions:**\n\n(Notice all variables appear in the model formula; mean_combined_earning is constant, precomputed for whole dataset.)\n\n---\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"share_proportional\": {\n      \"description\": \"Participant's proportional entitlement; their token share out of total tokens in trial\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in pretask on this trial\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent in pretask on this trial\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"tau_fairness\": {\n      \"description\": \"Personal fairness threshold scaling; where 1.0 means matching exact entitlement, <1 means generous, >1 means demanding\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": 0.5,\n          \"high\": 1.5\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"fairness_threshold\": {\n      \"description\": \"Participant's fairness expectation for this trial, in \u00a3\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total amount of money for this trial (in \u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"mean_combined_earning\": {\n      \"description\": \"Mean value of combined_earning across all trials in the dataset; acts as normalization reference\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"context_weight\": {\n      \"description\": \"Sensitivity to pot/context; positive values make the penalty for unfairness stronger when pots are larger, zero disables this\",\n      \"range\": {\n        \"min\": -1.0,\n        \"max\": 2.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": -1.0,\n          \"high\": 2.0\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"context_scaling\": {\n      \"description\": \"Trialwise scaling factor for impact of (un)fairness, increases above 1 for big pots if context_weight>0\",\n      \"range\": {\n        \"min\": -1,\n        \"max\": 3,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed \u00a3 share for participant (self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"slope\": {\n      \"description\": \"Soft-threshold (logistic) steepness parameter; higher means more abrupt switch. Controls sensitivity around threshold.\",\n      \"range\": {\n        \"min\": 0.1,\n        \"max\": 20.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": 0.1,\n          \"high\": 20.0\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"U\": {\n      \"description\": \"Utility value for accepting offer (before logistical transformation; >0 means >50% accept)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<target_variable>accept</target_variable>\n<SUMMARY>\nThe model predicts acceptance with a soft-threshold S-curve centered at each participant\u2019s personalized fairness expectation, which is a learnable scaling of their objective entitlement. It further allows for context-dependent modulation, such that the impact of unfairness increases or decreases systematically with total stakes. The three learnable parameters govern threshold placement, sharpness of responding, and context sensitivity, each altering distinct, identifiable aspects of responder behavior.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['tau_fairness', 'context_weight', 'slope']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                share_proportional = token_self / (token_self + token_opp)\nfairness_threshold = tau_fairness * share_proportional * combined_earning\ncontext_scaling = 1 + context_weight * ((combined_earning / mean_combined_earning) - 1)\nU = slope * context_scaling * (split_self - fairness_threshold)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"share_proportional\": {\n      \"description\": \"Participant's proportional entitlement; their token share out of total tokens in trial\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in pretask on this trial\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent in pretask on this trial\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"tau_fairness\": {\n      \"description\": \"Personal fairness threshold scaling; where 1.0 means matching exact entitlement, <1 means generous, >1 means demanding\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": 0.5,\n          \"high\": 1.5\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"fairness_threshold\": {\n      \"description\": \"Participant's fairness expectation for this trial, in \\u00a3\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total amount of money for this trial (in \\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"mean_combined_earning\": {\n      \"description\": \"Mean value of combined_earning across all trials in the dataset; acts as normalization reference\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"context_weight\": {\n      \"description\": \"Sensitivity to pot/context; positive values make the penalty for unfairness stronger when pots are larger, zero disables this\",\n      \"range\": {\n        \"min\": -1.0,\n        \"max\": 2.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": -1.0,\n          \"high\": 2.0\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"context_scaling\": {\n      \"description\": \"Trialwise scaling factor for impact of (un)fairness, increases above 1 for big pots if context_weight>0\",\n      \"range\": {\n        \"min\": -1,\n        \"max\": 3,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed \\u00a3 share for participant (self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"slope\": {\n      \"description\": \"Soft-threshold (logistic) steepness parameter; higher means more abrupt switch. Controls sensitivity around threshold.\",\n      \"range\": {\n        \"min\": 0.1,\n        \"max\": 20.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": 0.1,\n          \"high\": 20.0\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"U\": {\n      \"description\": \"Utility value for accepting offer (before logistical transformation; >0 means >50% accept)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, tau_fairness=1.0, context_weight=0.0, slope=1.0):\n    model_predictions = []\n    mean_combined_earning = sum(trial.get(\"combined_earning\", 0) for trial in trial_data) / len(trial_data) if trial_data else 0\n\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n        split_self = trial.get(\"split_self\", 0)\n\n        share_proportional = token_self / (token_self + token_opp) if (token_self + token_opp) > 0 else 0\n        fairness_threshold = tau_fairness * share_proportional * combined_earning\n        context_scaling = 1 + context_weight * ((combined_earning / mean_combined_earning) - 1) if mean_combined_earning > 0 else 1\n        U = slope * context_scaling * (split_self - fairness_threshold)\n\n        # Logistic transform (using standard library, avoid overflow)\n        try:\n            probability_accept = 1 / (1 + math.exp(-U))\n        except OverflowError:\n            probability_accept = 0.0 if U < 0 else 1.0\n\n        model_predictions.append(probability_accept)\n\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 0, \"token_opp\": 17, \"token_self\": 19, \"combined_earning\": 72, \"split_opp\": 37.0, \"split_self\": 35.0, \"splitperc_opp\": 51, \"splitperc_self\": 49, \"accept\": 1.0, \"accepted_amount\": 35.0, \"accepted_perc\": 49.0, \"proposed_perc\": 49.0, \"proposed_amount\": 35.0}, {\"ID\": 1, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 0, \"token_opp\": 17, \"token_self\": 19, \"combined_earning\": 72, \"split_opp\": 22.0, \"split_self\": 50.0, \"splitperc_opp\": 31, \"splitperc_self\": 69, \"accept\": 1.0, \"accepted_amount\": 50.0, \"accepted_perc\": 69.0, \"proposed_perc\": 69.0, \"proposed_amount\": 50.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1744726034.4818938
    }
  ],
  "learnable_parameters": {
    "tau_fairness": {
      "description": "Personal fairness threshold scaling; where 1.0 means matching exact entitlement, <1 means generous, >1 means demanding",
      "range": {
        "min": 0.5,
        "max": 1.5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {
          "low": 0.5,
          "high": 1.5
        }
      },
      "learnable": true,
      "source": "learnable"
    },
    "context_weight": {
      "description": "Sensitivity to pot/context; positive values make the penalty for unfairness stronger when pots are larger, zero disables this",
      "range": {
        "min": -1.0,
        "max": 2.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {
          "low": -1.0,
          "high": 2.0
        }
      },
      "learnable": true,
      "source": "learnable"
    },
    "slope": {
      "description": "Soft-threshold (logistic) steepness parameter; higher means more abrupt switch. Controls sensitivity around threshold.",
      "range": {
        "min": 0.1,
        "max": 20.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {
          "low": 0.1,
          "high": 20.0
        }
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "share_proportional = token_self / (token_self + token_opp)\nfairness_threshold = tau_fairness * share_proportional * combined_earning\ncontext_scaling = 1 + context_weight * ((combined_earning / mean_combined_earning) - 1)\nU = slope * context_scaling * (split_self - fairness_threshold)",
  "variable_descriptions": {
    "share_proportional": {
      "description": "Participant's proportional entitlement; their token share out of total tokens in trial",
      "range": {
        "min": 0,
        "max": 1,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "token_self": {
      "description": "Number of tokens found by the participant in pretask on this trial",
      "range": {
        "min": 0,
        "max": 36,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent in pretask on this trial",
      "range": {
        "min": 0,
        "max": 36,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "tau_fairness": {
      "description": "Personal fairness threshold scaling; where 1.0 means matching exact entitlement, <1 means generous, >1 means demanding",
      "range": {
        "min": 0.5,
        "max": 1.5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {
          "low": 0.5,
          "high": 1.5
        }
      },
      "learnable": true,
      "source": "learnable"
    },
    "fairness_threshold": {
      "description": "Participant's fairness expectation for this trial, in \u00a3",
      "range": {
        "min": 0,
        "max": 108,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "combined_earning": {
      "description": "Total amount of money for this trial (in \u00a3)",
      "range": {
        "min": 0,
        "max": 108,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "mean_combined_earning": {
      "description": "Mean value of combined_earning across all trials in the dataset; acts as normalization reference",
      "range": {
        "min": 0,
        "max": 108,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "context_weight": {
      "description": "Sensitivity to pot/context; positive values make the penalty for unfairness stronger when pots are larger, zero disables this",
      "range": {
        "min": -1.0,
        "max": 2.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {
          "low": -1.0,
          "high": 2.0
        }
      },
      "learnable": true,
      "source": "learnable"
    },
    "context_scaling": {
      "description": "Trialwise scaling factor for impact of (un)fairness, increases above 1 for big pots if context_weight>0",
      "range": {
        "min": -1,
        "max": 3,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "split_self": {
      "description": "Proposed \u00a3 share for participant (self)",
      "range": {
        "min": 0,
        "max": 108,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "slope": {
      "description": "Soft-threshold (logistic) steepness parameter; higher means more abrupt switch. Controls sensitivity around threshold.",
      "range": {
        "min": 0.1,
        "max": 20.0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {
          "low": 0.1,
          "high": 20.0
        }
      },
      "learnable": true,
      "source": "learnable"
    },
    "U": {
      "description": "Utility value for accepting offer (before logistical transformation; >0 means >50% accept)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    }
  },
  "model_summary": "The model predicts acceptance with a soft-threshold S-curve centered at each participant\u2019s personalized fairness expectation, which is a learnable scaling of their objective entitlement. It further allows for context-dependent modulation, such that the impact of unfairness increases or decreases systematically with total stakes. The three learnable parameters govern threshold placement, sharpness of responding, and context sensitivity, each altering distinct, identifiable aspects of responder behavior.",
  "target_variable": "accept",
  "full_reasoning": "Let\u2019s design the computational model following all instructions, especially on nonlinearity, parameter separation, and psychological interpretability.\n\n---\n\n**STEP BY STEP REASONING**\n\n### Key Goals\n\n1. **Maximize parameter recoverability**: Each parameter must uniquely control a separable, identifiable feature of the accept/reject curve.\n2. **Strong fit**: Model must capture empirical effects including nonlinear thresholding and context dependence.\n3. **Psychological plausibility with nonlinearity and context**: Context and reference dependence (fairness/entitlement), soft thresholding, and variable weighting of inequity.\n4. **Mandatory creative challenge**: *Choose at least one: A/Sigmoid threshold; B/Learnable scaling of fairness threshold; C/Interaction with pot size; D/Other interpretable, unique structure.*\n\n---\n\n### Step 1: What matters to responders?\n\nResponder receives an offer (split_self), compares to expectation or entitlement. Often, acceptance probability is low for \u201cunfair\u201d offers and rises sharply around a fairness reference/threshold.\n\n- **Threshold is personalized**\u2014not always at exactly proportional entitlement.\n- **Sensitivity/stickiness**: How steeply choices flip around a threshold.\n- **Bonus/penalty for above/below threshold**: Some accept slightly less, others demand much more.\n- **Context**: In larger-pots, unfairness may feel worse; or the salience of fairness may change.\n\n---\n\n### Step 2: Model Structure and Parameterization\n\n#### **Region Mapping and Creative Features**\n\n- **Region 1**: Offers below threshold \u2192 penalty or steep drop in acceptance probability.\n- **Region 2**: Offers near/above threshold \u2192 rapid rise in acceptance, may include a generous bonus.\n- **Threshold placement is NOT hard-coded at entitlement, but rather is a *proportional scaling* of entitlement** (CREATIVE CHALLENGE B).\n- **Transition is soft (logistic/sigmoid), not hard, with *learnable slope parameter* (CREATIVE CHALLENGE A).**\n- Contextual interaction: *Penalty for unfairness can be magnified for bigger pots* (CREATIVE CHALLENGE C).\n    - Let\u2019s achieve this by multiplying (split_self - threshold) by a scaling function of combined_earning.\n\n**We pick a model with a soft threshold at a participant-specific \u201cfairness expectation,\u201d itself a scaling of entitlement, and with contextual scaling via combined_earning.**\n\n---\n\n#### **LMentary Model Outline**\n\n**Utility before logistic transformation:**\n- u = s \u00d7 (split_self - threshold), where:\n    - s: softmax slope parameter (higher = more abrupt flip).\n    - threshold: tau_fairness \u00d7 (token_self / (token_self + token_opp)) \u00d7 combined_earning.\n        - tau_fairness: personalized, learnable proportional threshold (0.6 = highly demanding, 1.2 = surprisingly generous).\n    - Context dependence: s itself is scaled if desired by combined_earning, or penalty is modulated by big/small pots.\n\n**Three learnable parameters (example):**\n1. tau_fairness: controls where threshold sits relative to entitlement (threshold placement).\n2. slope: sharpness/steepness of sigmoid.\n3. context_weight: magnitude by which penalty for \u201cunfair\u201d offers is magnified for larger pots.\n\n**Contextual scaling**: For simplicity and identifiability, context_weight can modulate the *impact* of (split_self - threshold) as a linear function of combined_earning (normalized).\n\n**PARAMETER RECOVERABILITY:**\n- Vary tau_fairness alone: *Threshold moves left or right*\u2014acceptance curve shifts along x-axis.\n- Vary slope alone: *Sharpness increases or decreases*\u2014 S-curve more/less step-like.\n- Vary context_weight alone: *Penalty for unfair offers gets steeper or shallower with bigger pots; for small pots, effect is dampened*.\n\nThis separation is strict; each parameter alters a unique zone or quality of behavior.\n\n---\n\n#### **Toy Simulations: Visually Distinguishable Effects**\n\n- **Fairness-stickler**: tau_fairness = high, slope = high, context_weight = moderate \u2192 needs much more than proportional, flips abruptly.\n- **Greedy-acceptor**: tau_fairness = low, slope = low, context_weight = low \u2192 accepts much less than entitlement, gradual increase, not affected much by large pots.\n- **Generous thresholder**: tau_fairness = below 1, accepts sub-entitlement, moderate slope, possibly context_weight high if they react to big pots.\n\n---\n\n**Variable mapping:**\n- split_self: actual amount offered to responder.\n- Token and combined_earning: compute responder's proportional entitlement; threshold = tau_fairness \u00d7 share_proportional \u00d7 combined_earning.\n- share_proportional = token_self / (token_self + token_opp)\n- Modulate penalty for below-threshold offers more for big pots: context_weight \u00d7 (combined_earning / mean_combined_earning)\n\nOptional: To ensure identifiability and interpretability, keep only three parameters: tau_fairness [threshold location], slope [S-curve sharpness], context_weight [fairness salience as a function of pot size].\n\n---\n\n### Step 3: Modeling Formula\n\n**Mathematical expression:**\n\nLet:\n- share_proportional = token_self / (token_self + token_opp)\n- fairness_threshold = tau_fairness \u00d7 share_proportional \u00d7 combined_earning\n- context_scaling = 1 + context_weight \u00d7 (combined_earning / mean_combined_earning - 1)  \n    - (So context_scaling = 1 if average pot, increases/decreases for larger/smaller pots)\n\nLet utility (before logistic) be:\n- U = slope \u00d7 context_scaling \u00d7 (split_self - fairness_threshold)\n\nFinal choice prediction: accept = 1 if logistic(U) > 0.5\n\n---\n\n### Step 4: Complete model definition\n\n**Model summarizes as:**\n\n---\n<MODEL>\nshare_proportional = token_self / (token_self + token_opp)\nfairness_threshold = tau_fairness * share_proportional * combined_earning\ncontext_scaling = 1 + context_weight * ((combined_earning / mean_combined_earning) - 1)\nU = slope * context_scaling * (split_self - fairness_threshold)\n</MODEL>\n---\n\n**Variable descriptions:**\n\n(Notice all variables appear in the model formula; mean_combined_earning is constant, precomputed for whole dataset.)\n\n---\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"share_proportional\": {\n      \"description\": \"Participant's proportional entitlement; their token share out of total tokens in trial\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in pretask on this trial\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent in pretask on this trial\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 36,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"tau_fairness\": {\n      \"description\": \"Personal fairness threshold scaling; where 1.0 means matching exact entitlement, <1 means generous, >1 means demanding\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": 0.5,\n          \"high\": 1.5\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"fairness_threshold\": {\n      \"description\": \"Participant's fairness expectation for this trial, in \u00a3\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total amount of money for this trial (in \u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"mean_combined_earning\": {\n      \"description\": \"Mean value of combined_earning across all trials in the dataset; acts as normalization reference\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"context_weight\": {\n      \"description\": \"Sensitivity to pot/context; positive values make the penalty for unfairness stronger when pots are larger, zero disables this\",\n      \"range\": {\n        \"min\": -1.0,\n        \"max\": 2.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": -1.0,\n          \"high\": 2.0\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"context_scaling\": {\n      \"description\": \"Trialwise scaling factor for impact of (un)fairness, increases above 1 for big pots if context_weight>0\",\n      \"range\": {\n        \"min\": -1,\n        \"max\": 3,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed \u00a3 share for participant (self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 108,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"slope\": {\n      \"description\": \"Soft-threshold (logistic) steepness parameter; higher means more abrupt switch. Controls sensitivity around threshold.\",\n      \"range\": {\n        \"min\": 0.1,\n        \"max\": 20.0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": 0.1,\n          \"high\": 20.0\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"U\": {\n      \"description\": \"Utility value for accepting offer (before logistical transformation; >0 means >50% accept)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<target_variable>accept</target_variable>\n<SUMMARY>\nThe model predicts acceptance with a soft-threshold S-curve centered at each participant\u2019s personalized fairness expectation, which is a learnable scaling of their objective entitlement. It further allows for context-dependent modulation, such that the impact of unfairness increases or decreases systematically with total stakes. The three learnable parameters govern threshold placement, sharpness of responding, and context sensitivity, each altering distinct, identifiable aspects of responder behavior.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: share_proportional = token_self / (token_self + token_opp)\nfairness_threshold = tau_fairness * share_proportional * combined_earning\ncontext_scaling = 1 + context_weight * ((combined_earning / mean_combined_earning) - 1)\nU = slope * context_scaling * (split_self - fairness_threshold)\nSummary: The model predicts acceptance with a soft-threshold S-curve centered at each participant\u2019s personalized fairness expectation, which is a learnable scaling of their objective entitlement. It further allows for context-dependent modulation, such that the impact of unfairness increases or decreases systematically with total stakes. The three learnable parameters govern threshold placement, sharpness of responding, and context sensitivity, each altering distinct, identifiable aspects of responder behavior.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, tau_fairness=1.0, context_weight=0.0, slope=1.0):\n    model_predictions = []\n    mean_combined_earning = sum(trial.get(\"combined_earning\", 0) for trial in trial_data) / len(trial_data) if trial_data else 0\n\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n        split_self = trial.get(\"split_self\", 0)\n\n        share_proportional = token_self / (token_self + token_opp) if (token_self + token_opp) > 0 else 0\n        fairness_threshold = tau_fairness * share_proportional * combined_earning\n        context_scaling = 1 + context_weight * ((combined_earning / mean_combined_earning) - 1) if mean_combined_earning > 0 else 1\n        U = slope * context_scaling * (split_self - fairness_threshold)\n\n        # Logistic transform (using standard library, avoid overflow)\n        try:\n            probability_accept = 1 / (1 + math.exp(-U))\n        except OverflowError:\n            probability_accept = 0.0 if U < 0 else 1.0\n\n        model_predictions.append(probability_accept)\n\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.5763125763125763,
  "group_accuracies": {
    "Control": 0.6001291989664083,
    "Cocaine": 0.5549768518518519
  },
  "group_parameter_averages": {
    "Control": {
      "tau_fairness": 0.6718085664239667,
      "context_weight": 0.36517093714146753,
      "slope": 13.89200556254764
    },
    "Cocaine": {
      "tau_fairness": 0.7812945586496519,
      "context_weight": 0.33771233335856743,
      "slope": 15.133305301568186
    }
  },
  "fitting_results": [
    {
      "tau_fairness": 1.3593111808527867,
      "context_weight": -1.0,
      "slope": 19.699016776833048,
      "success": true,
      "log_likelihood": -24.573425115190652,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7273091746156268,
      "context_weight": -1.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -20.484305928540792,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6806760634422557,
      "context_weight": 0.25502831942166243,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -19.72482526341916,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7177386544252766,
      "context_weight": 1.4309320562175682,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -22.411740149498733,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7137215804251247,
      "context_weight": -1.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -20.420100203894545,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5599090662461922,
      "context_weight": 1.7455164062107236,
      "slope": 15.836025742632154,
      "success": true,
      "log_likelihood": -18.52062673700918,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.5245795797976677,
      "slope": 3.343025540358651,
      "success": true,
      "log_likelihood": -15.72995040850304,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5270204597882514,
      "context_weight": 1.9483243953337226,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -18.233134894951334,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.5248684323374759,
      "slope": 3.343612747254745,
      "success": true,
      "log_likelihood": -15.729950415305334,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7273323868666575,
      "context_weight": -1.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -21.484153458848827,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6309387378191406,
      "context_weight": -0.9220505499877405,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -19.200023304747305,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.3528319997097102,
      "context_weight": -1.0,
      "slope": 18.254454434027682,
      "success": true,
      "log_likelihood": -24.573438636922432,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6812870129639568,
      "context_weight": 0.06035851141390762,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -19.735042899773916,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.5245105397926488,
      "slope": 3.34283162112209,
      "success": true,
      "log_likelihood": -15.729950409167044,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.5446075671820273,
      "slope": 3.3894918420659628,
      "success": true,
      "log_likelihood": -15.729986696097681,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.5242386677208712,
      "slope": 3.3437859914246633,
      "success": true,
      "log_likelihood": -15.729950424003118,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.6187382609514832,
      "slope": 3.407066378101167,
      "success": true,
      "log_likelihood": -15.730270603159,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": -0.7073220991819562,
      "slope": 7.768307130774691,
      "success": true,
      "log_likelihood": -15.92923866388472,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.8506915768182296,
      "slope": 4.90321080289592,
      "success": true,
      "log_likelihood": -15.771965667775628,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.5243384674992551,
      "slope": 3.3422804870736074,
      "success": true,
      "log_likelihood": -15.72995041783021,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.677654486810498,
      "context_weight": 1.5237771775430236,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -19.627673091307006,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 1.5845771649879352,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -15.911983702085513,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.980353990351955,
      "context_weight": 1.724889183991579,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -23.442194857255245,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6812214443667821,
      "context_weight": 0.029717035862869084,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -20.733892083491916,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7137145562963924,
      "context_weight": -1.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -20.420064303808683,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5547533640500887,
      "context_weight": 0.6739466407785022,
      "slope": 10.357796328007316,
      "success": true,
      "log_likelihood": -19.56673275909448,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.652843315356511,
      "context_weight": -0.2853611748112578,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -19.333220955353966,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8267750267305584,
      "context_weight": 1.2860381916811499,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -21.820631974463094,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 1.5853296478104042,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -15.911989937449912,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5344413509871434,
      "context_weight": -0.6181878030910085,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -17.60049908006106,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.4420864275028434,
      "slope": 3.281074076606384,
      "success": true,
      "log_likelihood": -15.730191329621807,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.3535209335126481,
      "context_weight": -1.0,
      "slope": 19.547361964221096,
      "success": true,
      "log_likelihood": -24.573425322149237,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.042927095906671,
      "context_weight": 1.6847192965615465,
      "slope": 14.121379856812233,
      "success": true,
      "log_likelihood": -24.155277509010794,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7276689573893468,
      "context_weight": 0.1552996530800318,
      "slope": 16.64918379570216,
      "success": true,
      "log_likelihood": -22.570824880214563,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.5268285439503216,
      "slope": 3.339872632687448,
      "success": true,
      "log_likelihood": -15.729950990569797,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.6408140528311546,
      "slope": 3.568639435077781,
      "success": true,
      "log_likelihood": -20.03618714768696,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 1.1357517061883595,
      "slope": 7.74794183816456,
      "success": true,
      "log_likelihood": -17.326534440851855,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7289506928673956,
      "context_weight": -1.0,
      "slope": 14.749080176467876,
      "success": true,
      "log_likelihood": -20.56323822884169,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6811773258491878,
      "context_weight": 0.10506154675496823,
      "slope": 15.801491609117942,
      "success": true,
      "log_likelihood": -19.787376442699873,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5044446630442855,
      "context_weight": 1.5226002310715683,
      "slope": 16.46268987999606,
      "success": true,
      "log_likelihood": -16.33829383378301,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6549053459932535,
      "context_weight": 2.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -19.111558341892977,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8943394895628698,
      "context_weight": 0.5371026428615665,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -23.018112861958315,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": -1.0,
      "slope": 1.7566181021232896,
      "success": true,
      "log_likelihood": -16.52208260463879,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6508566182620904,
      "context_weight": 1.4331907685299894,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -21.197856694611833,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5514898338291997,
      "context_weight": 0.027798915597338582,
      "slope": 12.533927475857928,
      "success": true,
      "log_likelihood": -19.558594626816497,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5087425449904772,
      "context_weight": 2.0,
      "slope": 10.821086741570685,
      "success": true,
      "log_likelihood": -19.361108458372666,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6799260714961767,
      "context_weight": 1.5341891351297259,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -20.612442313564056,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8548521861689885,
      "context_weight": 2.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -22.705666170783715,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7137430553062751,
      "context_weight": -1.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -21.420068307354914,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6530369626238363,
      "context_weight": -0.23520352651795137,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -23.331211283070143,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 2.0,
      "slope": 5.22963885690776,
      "success": true,
      "log_likelihood": -19.628732183297014,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.354473570945634,
      "context_weight": -1.0,
      "slope": 17.309877566949478,
      "success": true,
      "log_likelihood": -24.573456327982136,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6530933277383149,
      "context_weight": -0.21398342974236212,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -21.33121842870617,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5383117932730305,
      "context_weight": 0.46552014216049986,
      "slope": 1.2717302326686597,
      "success": true,
      "log_likelihood": -23.50632455759911,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8757883174539582,
      "context_weight": -1.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -22.811342445374944,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7176352235141545,
      "context_weight": 1.4237122339671133,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -21.412737629327438,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.9477335895786608,
      "context_weight": 1.716576933391433,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -23.36609768072215,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 2.0,
      "slope": 2.036033100729594,
      "success": true,
      "log_likelihood": -18.927843136848324,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.3553700177520958,
      "context_weight": -1.0,
      "slope": 17.15806936859667,
      "success": true,
      "log_likelihood": -24.573460210733085,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": -0.816950166121292,
      "slope": 2.7758576878749883,
      "success": true,
      "log_likelihood": -18.153724614418824,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6503931450348549,
      "context_weight": 0.16110181691316916,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -20.29257260797968,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6786076363107265,
      "context_weight": 1.4369106573637875,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -19.677500275083318,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.3613558840297544,
      "context_weight": -1.0,
      "slope": 16.51379569064321,
      "success": true,
      "log_likelihood": -24.57348609290611,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5639223106387119,
      "context_weight": 1.1429727559754683,
      "slope": 11.270417207391999,
      "success": true,
      "log_likelihood": -23.27895998985906,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.9803425742261912,
      "context_weight": 1.7250894326522241,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -23.442194941156203,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6528388815617434,
      "context_weight": -0.2857583609422082,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -19.333220942358004,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 1.5730138161469827,
      "slope": 15.4408827831653,
      "success": true,
      "log_likelihood": -15.86649103445318,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": -1.0,
      "slope": 3.6569420463719884,
      "success": true,
      "log_likelihood": -16.879958578662123,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6532931191585777,
      "context_weight": -0.3371234315278182,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -21.329422081539672,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 1.0309499228919738,
      "slope": 6.100802320014341,
      "success": true,
      "log_likelihood": -16.802333519417232,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.9642992142771255,
      "context_weight": -0.4634024867639732,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -23.45512776409062,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.3490593290185517,
      "context_weight": -1.0,
      "slope": 19.56820633794819,
      "success": true,
      "log_likelihood": -24.57342604647824,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6528395626730285,
      "context_weight": -0.28579168283531664,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -20.333220786996517,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": -0.7306695878129676,
      "slope": 8.171184039824471,
      "success": true,
      "log_likelihood": -17.930101769229235,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7138294344883913,
      "context_weight": -1.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -22.420563364111658,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5125229563136028,
      "context_weight": 1.6735540463466545,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -18.559286564358008,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 0.8479408898772323,
      "slope": 4.889245000101619,
      "success": true,
      "log_likelihood": -15.77181679807686,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.5,
      "context_weight": -0.7123419968838618,
      "slope": 10.9938303435501,
      "success": true,
      "log_likelihood": -25.234232627821115,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8317127954158579,
      "context_weight": -1.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -21.94607338296843,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.3492032514564887,
      "context_weight": -0.7624702302515702,
      "slope": 18.18720193547823,
      "success": true,
      "log_likelihood": -24.57345884125128,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.969569921905542,
      "context_weight": 2.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -23.281677434608962,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 1.0087967601799974,
      "slope": 4.891761293091231,
      "success": true,
      "log_likelihood": -17.78823596255306,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.5,
      "context_weight": 1.7066326270836882,
      "slope": 9.37309696402215,
      "success": true,
      "log_likelihood": -25.2294939140085,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.57079391441091,
      "context_weight": 1.4467655641796173,
      "slope": 19.8703473816246,
      "success": true,
      "log_likelihood": -20.2430502674442,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7662422662248267,
      "context_weight": -0.3223459082169156,
      "slope": 16.12677756646824,
      "success": true,
      "log_likelihood": -22.145123264440368,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.0627159526740708,
      "context_weight": -1.0,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -24.193544749415306,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3611111111111111,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5575688044333378,
      "context_weight": -0.25779072563029465,
      "slope": 13.802700113427163,
      "success": true,
      "log_likelihood": -19.569282174732123,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.5,
      "context_weight": -0.9812130809024212,
      "slope": 0.9365131300607551,
      "success": true,
      "log_likelihood": -25.045893720366593,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "context_weight": 1.5847136958681025,
      "slope": 20.0,
      "success": true,
      "log_likelihood": -15.91198369608653,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5696161757623471,
      "context_weight": 1.3974586587184634,
      "slope": 17.91695515315236,
      "success": true,
      "log_likelihood": -22.24449975093485,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.25,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5363585722357561,
      "context_weight": -0.7216521576132731,
      "slope": 19.55177413778121,
      "success": true,
      "log_likelihood": -20.601295776890066,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "tau_fairness": {
        "r": 0.4564134352235589,
        "p": 1.8187864612904467e-06
      },
      "context_weight": {
        "r": 0.0814337627061819,
        "p": 0.42056679334935565
      },
      "slope": {
        "r": -0.032258653338989536,
        "p": 0.7500194188381815
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 51.190191557493016,
  "bic_Control": 48.93678783754569,
  "bic_Cocaine": 53.20886572327917,
  "bic_results": {
    "average_bic": 51.190191557493016,
    "individual_bics": [
      59.89740704574963,
      51.71916867244991,
      50.20020734220665,
      55.574037114365794,
      51.590757223157425,
      47.79181028938669,
      42.210457632374414,
      47.216826605270995,
      42.210457645979,
      53.71886373306599,
      49.150603424862936,
      59.89743408921319,
      50.22064261491616,
      42.21045763370242,
      42.21053020756369,
      42.21045766337457,
      42.211098021686325,
      42.60903414313777,
      42.29448815091959,
      42.21045765102875,
      50.005902997982346,
      42.574524219539356,
      57.63494652987882,
      52.21834098235216,
      51.5906854229857,
      49.8840223335573,
      49.41699872607626,
      54.39182076429452,
      42.574536690268154,
      45.95155497549045,
      42.21093947461195,
      59.89740745966681,
      59.061111833389916,
      55.89220657579746,
      42.21045879650792,
      50.822931110742246,
      45.403625697072044,
      51.87703327305171,
      50.325309700768074,
      43.42714448293435,
      48.97367349915429,
      56.78678253928496,
      43.79472202464591,
      53.146270204592,
      49.86774606900133,
      49.472773732113666,
      51.97544144249645,
      56.16188915693576,
      53.59069343007816,
      57.41297938150862,
      50.00802118196236,
      59.8974694713326,
      53.41299367278067,
      57.763205930566556,
      56.37324170611822,
      53.5760320740232,
      57.482752176812625,
      48.606243089064975,
      59.897477236834504,
      47.05800604420598,
      51.335702031327685,
      50.10555736553496,
      59.897529001180544,
      57.30847679508645,
      57.63494669768073,
      49.416998700084335,
      42.483538884274694,
      44.51047397269258,
      53.40940097844768,
      44.35522385420279,
      57.660812343549566,
      59.897408908324806,
      51.41699838936137,
      46.610760353826805,
      55.59168354359164,
      47.86912994408435,
      42.29419041152205,
      61.219022071010556,
      54.642703581305184,
      59.89747449787089,
      57.31391168458626,
      46.32702874047445,
      61.20954464338533,
      51.236657350256735,
      55.04080334424907,
      59.137646314198946,
      49.889121164832574,
      60.84234425610151,
      42.57452420754139,
      55.239556317238026,
      51.953148369148465
    ],
    "group_enabled": true,
    "group_bics": {
      "Control": [
        59.89740704574963,
        51.71916867244991,
        50.20020734220665,
        55.574037114365794,
        51.590757223157425,
        47.79181028938669,
        42.210457632374414,
        47.216826605270995,
        42.210457645979,
        53.71886373306599,
        49.150603424862936,
        59.89743408921319,
        50.22064261491616,
        42.21045763370242,
        42.21053020756369,
        42.21045766337457,
        42.211098021686325,
        42.60903414313777,
        42.29448815091959,
        42.21045765102875,
        50.005902997982346,
        42.574524219539356,
        57.63494652987882,
        52.21834098235216,
        51.5906854229857,
        49.8840223335573,
        49.41699872607626,
        54.39182076429452,
        42.574536690268154,
        45.95155497549045,
        42.21093947461195,
        59.89740745966681,
        59.061111833389916,
        55.89220657579746,
        42.21045879650792,
        50.822931110742246,
        45.403625697072044,
        51.87703327305171,
        50.325309700768074,
        43.42714448293435,
        48.97367349915429,
        56.78678253928496,
        43.79472202464591
      ],
      "Cocaine": [
        53.146270204592,
        49.86774606900133,
        49.472773732113666,
        51.97544144249645,
        56.16188915693576,
        53.59069343007816,
        57.41297938150862,
        50.00802118196236,
        59.8974694713326,
        53.41299367278067,
        57.763205930566556,
        56.37324170611822,
        53.5760320740232,
        57.482752176812625,
        48.606243089064975,
        59.897477236834504,
        47.05800604420598,
        51.335702031327685,
        50.10555736553496,
        59.897529001180544,
        57.30847679508645,
        57.63494669768073,
        49.416998700084335,
        42.483538884274694,
        44.51047397269258,
        53.40940097844768,
        44.35522385420279,
        57.660812343549566,
        59.897408908324806,
        51.41699838936137,
        46.610760353826805,
        55.59168354359164,
        47.86912994408435,
        42.29419041152205,
        61.219022071010556,
        54.642703581305184,
        59.89747449787089,
        57.31391168458626,
        46.32702874047445,
        61.20954464338533,
        51.236657350256735,
        55.04080334424907,
        59.137646314198946,
        49.889121164832574,
        60.84234425610151,
        42.57452420754139,
        55.239556317238026,
        51.953148369148465
      ]
    },
    "num_parameters": 3,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- tau_fairness: r = 0.456\n- context_weight: r = 0.081\n- slope: r = -0.032",
  "previous_models": [
    "Specification: share_proportional = token_self / (token_self + token_opp)\nfairness_threshold = tau_fairness * share_proportional * combined_earning\ncontext_scaling = 1 + context_weight * ((combined_earning / mean_combined_earning) - 1)\nU = slope * context_scaling * (split_self - fairness_threshold)\nSummary: The model predicts acceptance with a soft-threshold S-curve centered at each participant\u2019s personalized fairness expectation, which is a learnable scaling of their objective entitlement. It further allows for context-dependent modulation, such that the impact of unfairness increases or decreases systematically with total stakes. The three learnable parameters govern threshold placement, sharpness of responding, and context sensitivity, each altering distinct, identifiable aspects of responder behavior.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 51.190191557493016\nOverall Accuracy: 0.5763\nGroup Accuracies:\n- Control: 0.6001\n- Cocaine: 0.5550\n\nParameter Recovery:\n- tau_fairness: r = 0.456\n- context_weight: r = 0.081\n- slope: r = -0.032"
  ]
}