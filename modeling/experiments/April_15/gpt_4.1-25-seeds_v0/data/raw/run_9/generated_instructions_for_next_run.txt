

---

**INSTRUCTIONS – RUN 10**

Your task is to design a computational model of responder (accept/reject) choices using the provided dataset. Your overriding goals are: (1) maximize the *unique recoverability* and *interpretability* of all learnable parameters, (2) further improve model fit (BIC/accuracy), and (3) ensure your model embodies psychologically plausible, highly *nonlinear*, and context/fairness-sensitive mechanisms—moving beyond any model previously used.

Please strictly adhere to the following guidelines:

1. **Demand Full Nonoverlap of Parameter Effects:**
   - Each of the model’s up to three learnable parameters must govern a *wholly distinct* mathematical and psychological process:
     - One (and *only* one) parameter may determine the threshold for fairness (e.g., personalized fairness point).
     - One parameter should govern *penalty strength or shape* for unfair (below-threshold) offers.
     - One parameter must govern *bonus strength, steepness, or sensitivity* for generous (above-threshold) offers (distinct from threshold and penalty parameters).
   - No two learnable parameters may co-modulate any region of the equation (e.g., bonus, penalty, or threshold)—each must have its *own* region/function.
   - The *contextual effect* must be implemented only by a *separate, mathematically and psychologically interpretable* parameter—e.g., as a continuous bonus for higher combined_earning, or by modulating the generosity bonus only.

2. **Mandate Explicit Loss/Gain Asymmetry and Nonlinearities:**
   - The model *must* use *distinct* (not just sign-switched) nonlinear functions for below- versus above-threshold offers (e.g., log penalty for loss, softplus or tanh bonus for gain).
   - Include at least one nonlinear or saturating function (log, tanh, softplus, softmax, etc.) with a learnable magnitude or steepness.
   - Loss/gain regions *must* be handled explicitly and asymmetrically.

3. **Parameter Recovery and Theoretical Interpretation:**
   - In your reasoning, explicitly theorize how changes in *each* parameter—holding the others fixed—would uniquely and visibly alter the predicted acceptance curve (e.g., horizontal shift, sharpness of drop, boost for generosity).
   - For at least two hypothetical responder profiles (e.g., "strict egalitarian," "pragmatic maximizer," "generosity-rewarding altruist"), explain how the *fit* values would differ for each parameter.

4. **Threshold as a Flexible, Learnable Scaling:**
   - Threshold must be a *bounded, learnable* proportional scaling of entitlement, NOT simply the literal proportional split.
   - Entitlement must be directly computed from data; threshold must be explicit in the model.

5. **Parameter Count and Ranges:**
   - You may use *at most three* learnable parameters.
   - Each must map to only one region or mechanism, with *broad, non-overlapping* bounds (expand range if justified for psychological realism).
   - Describe *all* mathematical variables in <VARIABLES> JSON. Select *clear, Python-safe* parameter names.

6. **Strict Prohibitions:**
   - Purely linear additive models are NOT permissible.
   - NO parameter may appear in more than one equation part (e.g., cannot both set threshold and scale penalty).
   - Do NOT allow context to scale the penalty region directly; it may influence only its own additive or multiplicative bonus, or a separate reward process.

7. **Model Output and Format:**
   - First, provide clear step-by-step reasoning, especially regarding identifiability and psychological mapping.
   - The mathematical model should appear *only* between <MODEL> and </MODEL> tags—no interleaved prose.
   - All variables and parameters must be described in <VARIABLES> JSON.
   - Explicitly indicate the prediction target (<target_variable> tags).
   - Give a concise, interpretation-focused summary inside <SUMMARY> tags only.

**MANDATORY CREATIVE CHALLENGE:**  
This run, your model MUST incorporate BOTH:
- (A) A *distinct, parameterized nonlinear penalty* for sub-threshold (unfair) offers.
- (B) A *distinct, parameterized nonlinear bonus* for supra-threshold (generous) offers.

You should use *different* nonlinear functions for these two regions. Bonus points for contextual sensitivity that acts through the generosity/reward region ONLY, not through threshold or penalty.

Be creative, rigorous, and innovative. Consider less common nonlinearities and ensure every parameter’s functional role can be disentangled in simulations and toy data.

---