{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "\n\n---\n\n**INSTRUCTIONS \u2013 RUN 9**\n\nYour task is to design a computational model of responder (accept/reject) choice using the provided dataset. Your principal goals are: (1) maximize the recoverability and interpretability of ALL learnable parameters, (2) improve model fit (BIC/accuracy), and (3) ensure the model captures psychologically plausible, nonlinear, and context/fairness-sensitive mechanisms that go beyond na\u00efve additive forms.\n\nPlease rigorously adhere to this revised process:\n\n1. **Enforce Strict Nonoverlap of Parameter Effects:**\n   - Each learnable parameter must govern a conceptually and mathematically *distinct* region of the model:\n     - One parameter for threshold placement (e.g., personalized fairness threshold).\n     - One parameter for penalty below threshold (strength/shape of unfairness aversion).\n     - One parameter for bonus or acceptance steepness above threshold, or a distinct slope\u2014but NO TWO parameters may jointly scale the same penalty or region.\n   - Additive or multiplicative combinations of learnable parameters on the same process are **NOT permitted**.\n   - All \u201ccontext sensitivity\u201d features must act through a *separately parameterized distinct* effect, not by directly modulating another parameter's impact.\n\n2. **Loss/Gain Asymmetry & Nonlinear Transitions:**\n   - The model **must** include explicit, separate treatment of offers *below* versus *above* the fairness threshold, with different penalties/bonuses or functionals (e.g., step, log, indicator, softplus, saturating effect).\n   - Use at least one *nonlinear* penalty or reward (log, indicator, softplus, etc.) in a way that allows easy visual separation in toy curves.\n\n3. **Parameter Recovery and Toy Curve Diagnostics:**\n   - In your reasoning, explicitly theorize how changing each parameter\u2014holding others at midrange\u2014would uniquely shift the acceptance curve (e.g., left/right movement, change in steepness, penalty strength).\n   - For at least two hypothetical profiles (e.g., fairness fundamentalist, greedy egalitarian), explain qualitatively how model fits would differ.\n\n4. **Threshold as Learnable, Not Fixed:**\n   - Threshold should be a bounded, learnable proportional scaling of the entitlement amount\u2014not forced to literal proportional split.\n   - Explicitly tie threshold to contextual variables (e.g., tau_fairness \u00d7 entitlement).\n\n5. **Strict Parameter Limit and Variable Description:**\n   - Maximum three (3) learnable parameters, each with wide, inclusive, **non-overlapping** bounds.\n   - Each parameter must be mapped to a single psychological mechanism or region.\n   - All mathematical variables used in <MODEL> must be described in the <VARIABLES> JSON schema.\n   - Always specify the prediction target.\n\n6. **Prohibited Practices:**\n   - Purely additive linear models are forbidden.\n   - No more than one learnable parameter can affect any single region of the model (e.g., penalty strength, exponent, and context scaling cannot all jointly govern below-threshold utility).\n   - Do NOT have a parameter that both sets threshold and also scales penalty/bonus.\n\n7. **Model Output & Format:**\n   - Walk through your reasoning before presenting the model.\n   - Embed your mathematical model only between <MODEL> and </MODEL> tags.\n   - List variables/parameter descriptions only between <VARIABLES> and </VARIABLES> (Python-safe names).\n   - Show the target variable in <target_variable> tags.\n   - Give a concise, descriptive summary ONLY between <SUMMARY> tags.\n\n**MANDATORY CREATIVE CHALLENGE:**  \nImplement at least one of the following (specify your choice in reasoning):\n\n- (A) Explicit, parameterized loss/gain asymmetry, e.g., penalty for unfair offers distinct from bonus for generous offers.\n- (B) Use of a nonlinear or saturating function (log, softplus, tanh, or step) for sub- and/or suprathreshold utility, with learnable magnitude or steepness.\n- (C) Contextual effect as a separate, parameterized bonus/penalty (not by scaling penalty regions), such as a reward for generosity in large pots.\n- (D) Any highly interpretable model structure enabling truly separable parameter recovery.\n\nThink creatively but rigorously, and ground each parameter in a clear psychological hypothesis\u2014especially regarding fairness, strictness, generosity, or contextual awareness.\n\n---",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: \n\n---\n\n**INSTRUCTIONS \u2013 RUN 9**\n\nYour task is to design a computational model of responder (accept/reject) choice using the provided dataset. Your principal goals are: (1) maximize the recoverability and interpretability of ALL learnable parameters, (2) improve model fit (BIC/accuracy), and (3) ensure the model captures psychologically plausible, nonlinear, and context/fairness-sensitive mechanisms that go beyond na\u00efve additive forms.\n\nPlease rigorously adhere to this revised process:\n\n1. **Enforce Strict Nonoverlap of Parameter Effects:**\n   - Each learnable parameter must govern a conceptually and mathematically *distinct* region of the model:\n     - One parameter for threshold placement (e.g., personalized fairness threshold).\n     - One parameter for penalty below threshold (strength/shape of unfairness aversion).\n     - One parameter for bonus or acceptance steepness above threshold, or a distinct slope\u2014but NO TWO parameters may jointly scale the same penalty or region.\n   - Additive or multiplicative combinations of learnable parameters on the same process are **NOT permitted**.\n   - All \u201ccontext sensitivity\u201d features must act through a *separately parameterized distinct* effect, not by directly modulating another parameter's impact.\n\n2. **Loss/Gain Asymmetry & Nonlinear Transitions:**\n   - The model **must** include explicit, separate treatment of offers *below* versus *above* the fairness threshold, with different penalties/bonuses or functionals (e.g., step, log, indicator, softplus, saturating effect).\n   - Use at least one *nonlinear* penalty or reward (log, indicator, softplus, etc.) in a way that allows easy visual separation in toy curves.\n\n3. **Parameter Recovery and Toy Curve Diagnostics:**\n   - In your reasoning, explicitly theorize how changing each parameter\u2014holding others at midrange\u2014would uniquely shift the acceptance curve (e.g., left/right movement, change in steepness, penalty strength).\n   - For at least two hypothetical profiles (e.g., fairness fundamentalist, greedy egalitarian), explain qualitatively how model fits would differ.\n\n4. **Threshold as Learnable, Not Fixed:**\n   - Threshold should be a bounded, learnable proportional scaling of the entitlement amount\u2014not forced to literal proportional split.\n   - Explicitly tie threshold to contextual variables (e.g., tau_fairness \u00d7 entitlement).\n\n5. **Strict Parameter Limit and Variable Description:**\n   - Maximum three (3) learnable parameters, each with wide, inclusive, **non-overlapping** bounds.\n   - Each parameter must be mapped to a single psychological mechanism or region.\n   - All mathematical variables used in <MODEL> must be described in the <VARIABLES> JSON schema.\n   - Always specify the prediction target.\n\n6. **Prohibited Practices:**\n   - Purely additive linear models are forbidden.\n   - No more than one learnable parameter can affect any single region of the model (e.g., penalty strength, exponent, and context scaling cannot all jointly govern below-threshold utility).\n   - Do NOT have a parameter that both sets threshold and also scales penalty/bonus.\n\n7. **Model Output & Format:**\n   - Walk through your reasoning before presenting the model.\n   - Embed your mathematical model only between <MODEL> and </MODEL> tags.\n   - List variables/parameter descriptions only between <VARIABLES> and </VARIABLES> (Python-safe names).\n   - Show the target variable in <target_variable> tags.\n   - Give a concise, descriptive summary ONLY between <SUMMARY> tags.\n\n**MANDATORY CREATIVE CHALLENGE:**  \nImplement at least one of the following (specify your choice in reasoning):\n\n- (A) Explicit, parameterized loss/gain asymmetry, e.g., penalty for unfair offers distinct from bonus for generous offers.\n- (B) Use of a nonlinear or saturating function (log, softplus, tanh, or step) for sub- and/or suprathreshold utility, with learnable magnitude or steepness.\n- (C) Contextual effect as a separate, parameterized bonus/penalty (not by scaling penalty regions), such as a reward for generosity in large pots.\n- (D) Any highly interpretable model structure enabling truly separable parameter recovery.\n\nThink creatively but rigorously, and ground each parameter in a clear psychological hypothesis\u2014especially regarding fairness, strictness, generosity, or contextual awareness.\n\n---\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "Let me proceed step by step as instructed, following the constraints.\n\n---\n\n**Step 1: Identify critical psychological processes and dataset variables**\n\n- Participants as responders judge proposals using fairness and context (stake size, proportional and absolute offer).\n- They compare the offer to a 'fairness threshold' based on their entitlement, i.e., relative contribution: **entitlement = (token_self / (token_self + token_opp)) \u00d7 combined_earning**.\n- Acceptance is asymmetric: accepting below-threshold offers is costly (penalty/aversion), whereas generosity above threshold may have a different effect (reward or steepness, possibly less than penalty).\n- Stakes matter: people may be more/less likely to accept unfair offers if the stakes are low/high (context bonus), worth potentially encoding as a gain context effect, but (per challenge C), this must NOT be implemented by scaling penalty/bonus, but as a separate additive effect.\n\n**Step 2: Mechanistic mapping onto parameters**\n\n**Strict nonoverlap:**  \n- \u03c4_fairness: Proportional fairness threshold, controls left-right placement of the utility curve wrt entitlement (NOT scaling penalty or bonus).\n- alpha_unfair: Nonlinearly penalizes offers below threshold (only for unfair/offer < threshold; e.g., logarithmic penalty)\u2014does NOT scale threshold or generosity.\n- gamma_context: Additive context bonus, ONLY applicable to trials with large combined_earning (not scaling or modulating penalty/bonus), e.g., a step or indicator adding gamma_context if combined_earning exceeds a datum-specific limit. This would be a strictly separable effect.\n\n**Nonlinear transition (Challenge B):**  \n- Below threshold: Penalty is log(threshold - offer + 1), so as offer sinks below threshold, penalty sharply increases.\n- Above threshold: Accept baseline utility (no proportional bonus), but could add a mild boost but that's not required here.\n\n**Explicit asymmetry (Challenge A):**  \n- Loss/gain sides have structurally different forms: log penalty for unfair, but no bonus for generous offers (or could add a symmetric bonus with a different parameter, but capping at 3 parameters per instruction).\n\n**Contextual bonus/penalty (Challenge C):**  \n- Add a fixed bonus if the combined earnings are high (e.g., above median combined_earning), representing increased willingness to accept large-stake offers.\n\n---\n\n**Step 3: Constructing the mathematical model**\n\nLet:\n- offer = split_self (the amount offered to participant)\n- entitlement = (token_self / (token_self + token_opp)) * combined_earning\n- fairness_threshold = tau_fairness \u00d7 entitlement\n\nDefine:\n- penalty = alpha_unfair \u00d7 log(1 + max(0, fairness_threshold - offer))\n- context_bonus = gamma_context \u00d7 indicator[combined_earning > median_combined_earning]\n\nSo:\n\nU = - penalty + context_bonus\n\nWhere\n- If offer >= fairness_threshold: penalty = 0\n- If offer < fairness_threshold: penalty = alpha_unfair \u00d7 log(1 + [fairness_threshold - offer])\n\nAll parameters appear in distinct, nonoverlapping regions by construction.\n\n---\n\n**Step 4: Parameter recoverability / interpretability**\n\n- **tau_fairness**: shifts the acceptance curve left (more generous threshold) or right (stricter threshold). A fairness fundamentalist will fit tau_fairness ~=1; a greedy egalitarian may have tau_fairness <1.\n- **alpha_unfair**: determines how steeply utility drops for unfair offers\u2014higher for \"angry responders\", low for tolerant or self-interested ones.\n- **gamma_context**: induces an upward shift in utility *only* for large-stake trials, without affecting shape or threshold.\n\nProfiles:\n\n- **Fairness Fundamentalist**: tau_fairness\u22481, alpha_unfair high, gamma_context zero or low (rejects all unfair deals, even at high stakes).\n- **Greedy Egalitarian**: tau_fairness low (accepting little), alpha_unfair low (not much penalty for unfairness), gamma_context high (accepts more when pot is big).\n\n---\n\n**Step 5: Choosing context threshold**\n\n- Let\u2019s set **median_combined_earning** as the data-driven threshold (computed over all observations). This ensures implementation flexibility and interpretability.\n\n- All variables in formula are either from data, computed per-trial, or learnable parameters.\n\n---\n\n**Step 6: Variable Descriptions & Ranges**\n\n- tau_fairness: \u03c4_fairness in [0.5, 1.5] (since fairness personal thresholds may be lower or higher than entitlement, but must avoid degenerate cases).\n- alpha_unfair: \u03b1_unfair in [0.05, 10] (prevents zero penalty and wild sensitivity).\n- gamma_context: \u03b3_context in [-10, 10] (allows positive/negative context bonus: could be more or less accepting for big pots).\n\n---\n\n**Step 7: Model output & tags**\n\n- Model outputs per-trial utility, targeting 'accept', where P(accept) = sigmoid(U).\n\n---\n\n### Final Specification\n\n<MODEL>\noffer = split_self  \nentitlement = (token_self / (token_self + token_opp)) * combined_earning  \nfairness_threshold = tau_fairness * entitlement  \npenalty = alpha_unfair * log(1 + max(0, fairness_threshold - offer))  \ncontext_bonus = gamma_context * indicator(combined_earning > median_combined_earning)  \nU = -penalty + context_bonus\n</MODEL>\n\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"offer\": {\n      \"description\": \"Amount offered to the participant in pounds (\u00a3), same as split_self.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 500,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant in pre-task.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by opponent in pre-task.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money available for split (in pounds), 3 times sum of tokens.\",\n      \"range\": {\n        \"min\": 6,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed amount for participant (in pounds).\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"entitlement\": {\n      \"description\": \"Participant's data-derived entitlement, calculated as (token_self / (token_self + token_opp)) \u00d7 combined_earning.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"fairness_threshold\": {\n      \"description\": \"Fairness threshold, proportional to entitlement: tau_fairness \u00d7 entitlement.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 180,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"tau_fairness\": {\n      \"description\": \"Personal proportional fairness threshold multiplier. Shifts the cutoff for what offer is considered 'fair' for acceptance.\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha_unfair\": {\n      \"description\": \"Penalty strength for unfair (sub-threshold) offers. Governs how sharply utility decreases as offer drops below threshold.\",\n      \"range\": {\n        \"min\": 0.05,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"gamma_context\": {\n      \"description\": \"Additive utility bonus or penalty for trials with combined_earning greater than the dataset median.\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"median_combined_earning\": {\n      \"description\": \"Median combined_earning across all dataset trials, used for context split.\",\n      \"range\": {\n        \"min\": 6,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"penalty\": {\n      \"description\": \"Nonlinear penalty applied to offers below fairness_threshold; zero if offer equals or exceeds threshold.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"+inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"context_bonus\": {\n      \"description\": \"Additive utility shift if combined_earning exceeds median, zero otherwise.\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U\": {\n      \"description\": \"Latent utility of accepting the offer, prior to conversion to choice probability via logistic function.\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nThis model predicts responder acceptance by comparing the proposed offer to a personalized fairness threshold proportional to self-earned entitlement, controlled by a fairness cutoff parameter. Offers below threshold produce a sharply increasing logarithmic penalty with learnable aversion strength. Contextual sensitivity is captured via a distinct bonus/penalty for high-stake trials. The model achieves nonoverlapping parameter influence, explicit loss/gain asymmetry, and nonlinear transitions, providing recoverable, interpretable mechanisms for fairness and context effects in choice.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['tau_fairness', 'alpha_unfair', 'gamma_context']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                offer = split_self  \nentitlement = (token_self / (token_self + token_opp)) * combined_earning  \nfairness_threshold = tau_fairness * entitlement  \npenalty = alpha_unfair * log(1 + max(0, fairness_threshold - offer))  \ncontext_bonus = gamma_context * indicator(combined_earning > median_combined_earning)  \nU = -penalty + context_bonus\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"offer\": {\n      \"description\": \"Amount offered to the participant in pounds (\\u00a3), same as split_self.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 500,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant in pre-task.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by opponent in pre-task.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money available for split (in pounds), 3 times sum of tokens.\",\n      \"range\": {\n        \"min\": 6,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed amount for participant (in pounds).\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"entitlement\": {\n      \"description\": \"Participant's data-derived entitlement, calculated as (token_self / (token_self + token_opp)) \\u00d7 combined_earning.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"fairness_threshold\": {\n      \"description\": \"Fairness threshold, proportional to entitlement: tau_fairness \\u00d7 entitlement.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 180,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"tau_fairness\": {\n      \"description\": \"Personal proportional fairness threshold multiplier. Shifts the cutoff for what offer is considered 'fair' for acceptance.\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha_unfair\": {\n      \"description\": \"Penalty strength for unfair (sub-threshold) offers. Governs how sharply utility decreases as offer drops below threshold.\",\n      \"range\": {\n        \"min\": 0.05,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"gamma_context\": {\n      \"description\": \"Additive utility bonus or penalty for trials with combined_earning greater than the dataset median.\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"median_combined_earning\": {\n      \"description\": \"Median combined_earning across all dataset trials, used for context split.\",\n      \"range\": {\n        \"min\": 6,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"penalty\": {\n      \"description\": \"Nonlinear penalty applied to offers below fairness_threshold; zero if offer equals or exceeds threshold.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"+inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"context_bonus\": {\n      \"description\": \"Additive utility shift if combined_earning exceeds median, zero otherwise.\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U\": {\n      \"description\": \"Latent utility of accepting the offer, prior to conversion to choice probability via logistic function.\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, tau_fairness=1.0, alpha_unfair=1.0, gamma_context=0.0):\n    \"\"\"\n    Simulates the fairness and context model.\n\n    Args:\n        trial_data (list of dict): A list of dictionaries, where each dictionary\n            represents a trial and contains the following keys:\n            - token_self (int): Number of tokens found by the participant.\n            - token_opp (int): Number of tokens found by the opponent.\n            - combined_earning (int): Total money available for split.\n            - split_self (float): Amount offered to the participant.\n            - median_combined_earning (float): Median combined earning across all trials.\n        tau_fairness (float): Personal proportional fairness threshold multiplier.\n        alpha_unfair (float): Penalty strength for unfair offers.\n        gamma_context (float): Additive utility bonus or penalty for trials with combined_earning greater than the dataset median.\n\n    Returns:\n        list of float: A list of model predictions (utilities) for each trial.\n    \"\"\"\n\n    model_predictions = []\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n        split_self = trial.get(\"split_self\", 0)\n        median_combined_earning = trial.get(\"median_combined_earning\", 0)\n\n\n        # Calculate entitlement\n        entitlement = (token_self / (token_self + token_opp)) * combined_earning if (token_self + token_opp) > 0 else 0\n\n        # Calculate fairness threshold\n        fairness_threshold = tau_fairness * entitlement\n\n        # Calculate penalty\n        penalty = 0.0\n        if split_self < fairness_threshold:\n            penalty = alpha_unfair * math.log(1 + (fairness_threshold - split_self))\n\n        # Calculate context bonus\n        context_bonus = 0.0\n        if combined_earning > median_combined_earning:\n            context_bonus = gamma_context\n\n        # Calculate utility\n        U = -penalty + context_bonus\n\n        model_predictions.append(U)\n\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"treatment\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 0, \"token_opp\": 2, \"token_self\": 1, \"combined_earning\": 9, \"split_opp\": 6.0, \"split_self\": 3.0, \"splitperc_opp\": 67, \"splitperc_self\": 33, \"accept\": 1.0, \"accepted_amount\": 3.0, \"accepted_perc\": 33.0, \"proposed_perc\": 33.0, \"proposed_amount\": 3.0, \"median_combined_earning\": 54.0}, {\"ID\": 1, \"group\": \"treatment\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 0, \"token_opp\": 2, \"token_self\": 1, \"combined_earning\": 9, \"split_opp\": 6.0, \"split_self\": 3.0, \"splitperc_opp\": 67, \"splitperc_self\": 33, \"accept\": 1.0, \"accepted_amount\": 3.0, \"accepted_perc\": 33.0, \"proposed_perc\": 33.0, \"proposed_amount\": 3.0, \"median_combined_earning\": 54.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1744726129.493726
    }
  ],
  "learnable_parameters": {
    "tau_fairness": {
      "description": "Personal proportional fairness threshold multiplier. Shifts the cutoff for what offer is considered 'fair' for acceptance.",
      "range": {
        "min": 0.5,
        "max": 1.5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha_unfair": {
      "description": "Penalty strength for unfair (sub-threshold) offers. Governs how sharply utility decreases as offer drops below threshold.",
      "range": {
        "min": 0.05,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "gamma_context": {
      "description": "Additive utility bonus or penalty for trials with combined_earning greater than the dataset median.",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "offer = split_self  \nentitlement = (token_self / (token_self + token_opp)) * combined_earning  \nfairness_threshold = tau_fairness * entitlement  \npenalty = alpha_unfair * log(1 + max(0, fairness_threshold - offer))  \ncontext_bonus = gamma_context * indicator(combined_earning > median_combined_earning)  \nU = -penalty + context_bonus",
  "variable_descriptions": {
    "offer": {
      "description": "Amount offered to the participant in pounds (\u00a3), same as split_self.",
      "range": {
        "min": 0,
        "max": 500,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by participant in pre-task.",
      "range": {
        "min": 0,
        "max": 20,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by opponent in pre-task.",
      "range": {
        "min": 0,
        "max": 20,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "combined_earning": {
      "description": "Total money available for split (in pounds), 3 times sum of tokens.",
      "range": {
        "min": 6,
        "max": 120,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_self": {
      "description": "Proposed amount for participant (in pounds).",
      "range": {
        "min": 0,
        "max": 120,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "entitlement": {
      "description": "Participant's data-derived entitlement, calculated as (token_self / (token_self + token_opp)) \u00d7 combined_earning.",
      "range": {
        "min": 0,
        "max": 120,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "fairness_threshold": {
      "description": "Fairness threshold, proportional to entitlement: tau_fairness \u00d7 entitlement.",
      "range": {
        "min": 0,
        "max": 180,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "tau_fairness": {
      "description": "Personal proportional fairness threshold multiplier. Shifts the cutoff for what offer is considered 'fair' for acceptance.",
      "range": {
        "min": 0.5,
        "max": 1.5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha_unfair": {
      "description": "Penalty strength for unfair (sub-threshold) offers. Governs how sharply utility decreases as offer drops below threshold.",
      "range": {
        "min": 0.05,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "gamma_context": {
      "description": "Additive utility bonus or penalty for trials with combined_earning greater than the dataset median.",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "median_combined_earning": {
      "description": "Median combined_earning across all dataset trials, used for context split.",
      "range": {
        "min": 6,
        "max": 120,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "penalty": {
      "description": "Nonlinear penalty applied to offers below fairness_threshold; zero if offer equals or exceeds threshold.",
      "range": {
        "min": 0,
        "max": "+inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "context_bonus": {
      "description": "Additive utility shift if combined_earning exceeds median, zero otherwise.",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "U": {
      "description": "Latent utility of accepting the offer, prior to conversion to choice probability via logistic function.",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    }
  },
  "model_summary": "This model predicts responder acceptance by comparing the proposed offer to a personalized fairness threshold proportional to self-earned entitlement, controlled by a fairness cutoff parameter. Offers below threshold produce a sharply increasing logarithmic penalty with learnable aversion strength. Contextual sensitivity is captured via a distinct bonus/penalty for high-stake trials. The model achieves nonoverlapping parameter influence, explicit loss/gain asymmetry, and nonlinear transitions, providing recoverable, interpretable mechanisms for fairness and context effects in choice.",
  "target_variable": "accept",
  "full_reasoning": "Let me proceed step by step as instructed, following the constraints.\n\n---\n\n**Step 1: Identify critical psychological processes and dataset variables**\n\n- Participants as responders judge proposals using fairness and context (stake size, proportional and absolute offer).\n- They compare the offer to a 'fairness threshold' based on their entitlement, i.e., relative contribution: **entitlement = (token_self / (token_self + token_opp)) \u00d7 combined_earning**.\n- Acceptance is asymmetric: accepting below-threshold offers is costly (penalty/aversion), whereas generosity above threshold may have a different effect (reward or steepness, possibly less than penalty).\n- Stakes matter: people may be more/less likely to accept unfair offers if the stakes are low/high (context bonus), worth potentially encoding as a gain context effect, but (per challenge C), this must NOT be implemented by scaling penalty/bonus, but as a separate additive effect.\n\n**Step 2: Mechanistic mapping onto parameters**\n\n**Strict nonoverlap:**  \n- \u03c4_fairness: Proportional fairness threshold, controls left-right placement of the utility curve wrt entitlement (NOT scaling penalty or bonus).\n- alpha_unfair: Nonlinearly penalizes offers below threshold (only for unfair/offer < threshold; e.g., logarithmic penalty)\u2014does NOT scale threshold or generosity.\n- gamma_context: Additive context bonus, ONLY applicable to trials with large combined_earning (not scaling or modulating penalty/bonus), e.g., a step or indicator adding gamma_context if combined_earning exceeds a datum-specific limit. This would be a strictly separable effect.\n\n**Nonlinear transition (Challenge B):**  \n- Below threshold: Penalty is log(threshold - offer + 1), so as offer sinks below threshold, penalty sharply increases.\n- Above threshold: Accept baseline utility (no proportional bonus), but could add a mild boost but that's not required here.\n\n**Explicit asymmetry (Challenge A):**  \n- Loss/gain sides have structurally different forms: log penalty for unfair, but no bonus for generous offers (or could add a symmetric bonus with a different parameter, but capping at 3 parameters per instruction).\n\n**Contextual bonus/penalty (Challenge C):**  \n- Add a fixed bonus if the combined earnings are high (e.g., above median combined_earning), representing increased willingness to accept large-stake offers.\n\n---\n\n**Step 3: Constructing the mathematical model**\n\nLet:\n- offer = split_self (the amount offered to participant)\n- entitlement = (token_self / (token_self + token_opp)) * combined_earning\n- fairness_threshold = tau_fairness \u00d7 entitlement\n\nDefine:\n- penalty = alpha_unfair \u00d7 log(1 + max(0, fairness_threshold - offer))\n- context_bonus = gamma_context \u00d7 indicator[combined_earning > median_combined_earning]\n\nSo:\n\nU = - penalty + context_bonus\n\nWhere\n- If offer >= fairness_threshold: penalty = 0\n- If offer < fairness_threshold: penalty = alpha_unfair \u00d7 log(1 + [fairness_threshold - offer])\n\nAll parameters appear in distinct, nonoverlapping regions by construction.\n\n---\n\n**Step 4: Parameter recoverability / interpretability**\n\n- **tau_fairness**: shifts the acceptance curve left (more generous threshold) or right (stricter threshold). A fairness fundamentalist will fit tau_fairness ~=1; a greedy egalitarian may have tau_fairness <1.\n- **alpha_unfair**: determines how steeply utility drops for unfair offers\u2014higher for \"angry responders\", low for tolerant or self-interested ones.\n- **gamma_context**: induces an upward shift in utility *only* for large-stake trials, without affecting shape or threshold.\n\nProfiles:\n\n- **Fairness Fundamentalist**: tau_fairness\u22481, alpha_unfair high, gamma_context zero or low (rejects all unfair deals, even at high stakes).\n- **Greedy Egalitarian**: tau_fairness low (accepting little), alpha_unfair low (not much penalty for unfairness), gamma_context high (accepts more when pot is big).\n\n---\n\n**Step 5: Choosing context threshold**\n\n- Let\u2019s set **median_combined_earning** as the data-driven threshold (computed over all observations). This ensures implementation flexibility and interpretability.\n\n- All variables in formula are either from data, computed per-trial, or learnable parameters.\n\n---\n\n**Step 6: Variable Descriptions & Ranges**\n\n- tau_fairness: \u03c4_fairness in [0.5, 1.5] (since fairness personal thresholds may be lower or higher than entitlement, but must avoid degenerate cases).\n- alpha_unfair: \u03b1_unfair in [0.05, 10] (prevents zero penalty and wild sensitivity).\n- gamma_context: \u03b3_context in [-10, 10] (allows positive/negative context bonus: could be more or less accepting for big pots).\n\n---\n\n**Step 7: Model output & tags**\n\n- Model outputs per-trial utility, targeting 'accept', where P(accept) = sigmoid(U).\n\n---\n\n### Final Specification\n\n<MODEL>\noffer = split_self  \nentitlement = (token_self / (token_self + token_opp)) * combined_earning  \nfairness_threshold = tau_fairness * entitlement  \npenalty = alpha_unfair * log(1 + max(0, fairness_threshold - offer))  \ncontext_bonus = gamma_context * indicator(combined_earning > median_combined_earning)  \nU = -penalty + context_bonus\n</MODEL>\n\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"offer\": {\n      \"description\": \"Amount offered to the participant in pounds (\u00a3), same as split_self.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 500,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant in pre-task.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by opponent in pre-task.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money available for split (in pounds), 3 times sum of tokens.\",\n      \"range\": {\n        \"min\": 6,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed amount for participant (in pounds).\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"entitlement\": {\n      \"description\": \"Participant's data-derived entitlement, calculated as (token_self / (token_self + token_opp)) \u00d7 combined_earning.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"fairness_threshold\": {\n      \"description\": \"Fairness threshold, proportional to entitlement: tau_fairness \u00d7 entitlement.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 180,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"tau_fairness\": {\n      \"description\": \"Personal proportional fairness threshold multiplier. Shifts the cutoff for what offer is considered 'fair' for acceptance.\",\n      \"range\": {\n        \"min\": 0.5,\n        \"max\": 1.5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha_unfair\": {\n      \"description\": \"Penalty strength for unfair (sub-threshold) offers. Governs how sharply utility decreases as offer drops below threshold.\",\n      \"range\": {\n        \"min\": 0.05,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"gamma_context\": {\n      \"description\": \"Additive utility bonus or penalty for trials with combined_earning greater than the dataset median.\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"median_combined_earning\": {\n      \"description\": \"Median combined_earning across all dataset trials, used for context split.\",\n      \"range\": {\n        \"min\": 6,\n        \"max\": 120,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"penalty\": {\n      \"description\": \"Nonlinear penalty applied to offers below fairness_threshold; zero if offer equals or exceeds threshold.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"+inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"context_bonus\": {\n      \"description\": \"Additive utility shift if combined_earning exceeds median, zero otherwise.\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"U\": {\n      \"description\": \"Latent utility of accepting the offer, prior to conversion to choice probability via logistic function.\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nThis model predicts responder acceptance by comparing the proposed offer to a personalized fairness threshold proportional to self-earned entitlement, controlled by a fairness cutoff parameter. Offers below threshold produce a sharply increasing logarithmic penalty with learnable aversion strength. Contextual sensitivity is captured via a distinct bonus/penalty for high-stake trials. The model achieves nonoverlapping parameter influence, explicit loss/gain asymmetry, and nonlinear transitions, providing recoverable, interpretable mechanisms for fairness and context effects in choice.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: offer = split_self  \nentitlement = (token_self / (token_self + token_opp)) * combined_earning  \nfairness_threshold = tau_fairness * entitlement  \npenalty = alpha_unfair * log(1 + max(0, fairness_threshold - offer))  \ncontext_bonus = gamma_context * indicator(combined_earning > median_combined_earning)  \nU = -penalty + context_bonus\nSummary: This model predicts responder acceptance by comparing the proposed offer to a personalized fairness threshold proportional to self-earned entitlement, controlled by a fairness cutoff parameter. Offers below threshold produce a sharply increasing logarithmic penalty with learnable aversion strength. Contextual sensitivity is captured via a distinct bonus/penalty for high-stake trials. The model achieves nonoverlapping parameter influence, explicit loss/gain asymmetry, and nonlinear transitions, providing recoverable, interpretable mechanisms for fairness and context effects in choice.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, tau_fairness=1.0, alpha_unfair=1.0, gamma_context=0.0):\n    \"\"\"\n    Simulates the fairness and context model.\n\n    Args:\n        trial_data (list of dict): A list of dictionaries, where each dictionary\n            represents a trial and contains the following keys:\n            - token_self (int): Number of tokens found by the participant.\n            - token_opp (int): Number of tokens found by the opponent.\n            - combined_earning (int): Total money available for split.\n            - split_self (float): Amount offered to the participant.\n            - median_combined_earning (float): Median combined earning across all trials.\n        tau_fairness (float): Personal proportional fairness threshold multiplier.\n        alpha_unfair (float): Penalty strength for unfair offers.\n        gamma_context (float): Additive utility bonus or penalty for trials with combined_earning greater than the dataset median.\n\n    Returns:\n        list of float: A list of model predictions (utilities) for each trial.\n    \"\"\"\n\n    model_predictions = []\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n        split_self = trial.get(\"split_self\", 0)\n        median_combined_earning = trial.get(\"median_combined_earning\", 0)\n\n\n        # Calculate entitlement\n        entitlement = (token_self / (token_self + token_opp)) * combined_earning if (token_self + token_opp) > 0 else 0\n\n        # Calculate fairness threshold\n        fairness_threshold = tau_fairness * entitlement\n\n        # Calculate penalty\n        penalty = 0.0\n        if split_self < fairness_threshold:\n            penalty = alpha_unfair * math.log(1 + (fairness_threshold - split_self))\n\n        # Calculate context bonus\n        context_bonus = 0.0\n        if combined_earning > median_combined_earning:\n            context_bonus = gamma_context\n\n        # Calculate utility\n        U = -penalty + context_bonus\n\n        model_predictions.append(U)\n\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.7957875457875458,
  "group_accuracies": {
    "Control": 0.8488372093023255,
    "Cocaine": 0.7482638888888888
  },
  "group_parameter_averages": {
    "Control": {
      "tau_fairness": 0.6875515668188369,
      "alpha_unfair": 4.180860902041257,
      "gamma_context": 4.463134219031109
    },
    "Cocaine": {
      "tau_fairness": 0.7725364996028827,
      "alpha_unfair": 3.8659318144777526,
      "gamma_context": 1.75498458174541
    }
  },
  "fitting_results": [
    {
      "tau_fairness": 0.7499999969261908,
      "alpha_unfair": 10.0,
      "gamma_context": -1.1381798422254852,
      "success": true,
      "log_likelihood": -6.814631593029514,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8685888355746025,
      "alpha_unfair": 3.041971592359175,
      "gamma_context": 2.646333239466717,
      "success": true,
      "log_likelihood": -16.07049697902499,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7386923103911103,
      "alpha_unfair": 5.207478281577263,
      "gamma_context": 3.308209851892703,
      "success": true,
      "log_likelihood": -10.774471042460899,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6666666466108823,
      "alpha_unfair": 4.408240886464008,
      "gamma_context": 1.1548797065734118,
      "success": true,
      "log_likelihood": -15.870243873448,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8315806317338735,
      "alpha_unfair": 6.001433866863018,
      "gamma_context": 4.145100083091609,
      "success": true,
      "log_likelihood": -8.480069174246841,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6870801046040925,
      "alpha_unfair": 4.573533428603491,
      "gamma_context": 3.337285894908393,
      "success": true,
      "log_likelihood": -12.040030678248934,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 10.0,
      "success": true,
      "log_likelihood": -0.0016515497260602495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7000004365139375,
      "alpha_unfair": 5.173115263585417,
      "gamma_context": 4.238839702781407,
      "success": true,
      "log_likelihood": -10.546166269769179,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 10.0,
      "success": true,
      "log_likelihood": -0.0016515497260602495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8930224143221672,
      "alpha_unfair": 4.256108900735854,
      "gamma_context": 2.904360108155173,
      "success": true,
      "log_likelihood": -11.495131759943986,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6867254169058998,
      "alpha_unfair": 10.0,
      "gamma_context": 2.8218253108704423,
      "success": true,
      "log_likelihood": -4.295084160625191,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499999960530167,
      "alpha_unfair": 10.0,
      "gamma_context": -0.7378395171342841,
      "success": true,
      "log_likelihood": -7.7371855526285165,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7798742466049298,
      "alpha_unfair": 6.34222895329786,
      "gamma_context": 3.1292033874506546,
      "success": true,
      "log_likelihood": -8.270161803548541,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 10.0,
      "success": true,
      "log_likelihood": -0.0016515497260602495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 10.0,
      "success": true,
      "log_likelihood": -0.0016515497260602492,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 10.0,
      "success": true,
      "log_likelihood": -0.0016515497260602495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 10.0,
      "success": true,
      "log_likelihood": -0.0016515497260602492,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 5.730473876049458,
      "gamma_context": 4.288919782243992,
      "success": true,
      "log_likelihood": -6.4758431781843875,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6336264704904913,
      "alpha_unfair": 7.05465281728166,
      "gamma_context": 8.343925835632275,
      "success": true,
      "log_likelihood": -5.022474485322238,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 10.0,
      "success": true,
      "log_likelihood": -0.0016515497260602492,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8243178150708746,
      "alpha_unfair": 4.433399184013178,
      "gamma_context": 4.027711199741915,
      "success": true,
      "log_likelihood": -12.38193954332005,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6250009155359686,
      "alpha_unfair": 5.271470816791152,
      "gamma_context": 5.808864739600888,
      "success": true,
      "log_likelihood": -6.920965563164579,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.583333326796316,
      "alpha_unfair": 10.0,
      "gamma_context": 0.31894388340716107,
      "success": true,
      "log_likelihood": -14.96838680928763,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7328246397186313,
      "alpha_unfair": 2.7017385533950007,
      "gamma_context": 1.7308302722060107,
      "success": true,
      "log_likelihood": -18.089077317635105,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.9723971277008031,
      "alpha_unfair": 5.842531859063426,
      "gamma_context": 6.516796532135216,
      "success": true,
      "log_likelihood": -9.977625923062627,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499999759502053,
      "alpha_unfair": 3.4030367681085787,
      "gamma_context": 2.693065849390013,
      "success": true,
      "log_likelihood": -15.253873704143414,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7000000640887203,
      "alpha_unfair": 4.043292186259148,
      "gamma_context": 2.4174609340123223,
      "success": true,
      "log_likelihood": -14.014867498730068,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.874999997119581,
      "alpha_unfair": 9.591604914233686,
      "gamma_context": 3.430716735142012,
      "success": true,
      "log_likelihood": -4.321232004148899,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6957575294097823,
      "alpha_unfair": 4.128830539902781,
      "gamma_context": 5.831337897935159,
      "success": true,
      "log_likelihood": -7.684894886338392,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8342067764319369,
      "alpha_unfair": 3.231757031795039,
      "gamma_context": 4.641358274055923,
      "success": true,
      "log_likelihood": -13.278347348229222,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 10.0,
      "success": true,
      "log_likelihood": -0.0016515497260602495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499999899176545,
      "alpha_unfair": 10.0,
      "gamma_context": -1.126525541761645,
      "success": true,
      "log_likelihood": -6.814507800160345,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5833333322402238,
      "alpha_unfair": 2.569705302495819,
      "gamma_context": 0.2945381758809849,
      "success": true,
      "log_likelihood": -20.88679215589972,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499906624994993,
      "alpha_unfair": 3.036590584276064,
      "gamma_context": 1.1188922967915829,
      "success": true,
      "log_likelihood": -16.65552457270519,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 10.0,
      "success": true,
      "log_likelihood": -0.0016515497260602492,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 4.6633602690289475,
      "gamma_context": 1.455718479206188,
      "success": true,
      "log_likelihood": -16.857036191574124,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5392664304550863,
      "alpha_unfair": 8.106003916836885,
      "gamma_context": 3.1053935857386183,
      "success": true,
      "log_likelihood": -8.624848526613823,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8634157593791103,
      "alpha_unfair": 3.920461200988126,
      "gamma_context": 3.413714133457908,
      "success": true,
      "log_likelihood": -13.368543337156344,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8750004465861978,
      "alpha_unfair": 3.047030604768682,
      "gamma_context": 3.8376257885525833,
      "success": true,
      "log_likelihood": -15.718836684867151,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7311310835282014,
      "alpha_unfair": 2.846005841623181,
      "gamma_context": 5.032704571488676,
      "success": true,
      "log_likelihood": -7.880400564721753,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8938822426606486,
      "alpha_unfair": 3.3389544190431466,
      "gamma_context": 3.9052191745015494,
      "success": true,
      "log_likelihood": -15.395916682307245,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.0000017513893509,
      "alpha_unfair": 3.3120069283340126,
      "gamma_context": 1.451738235435749,
      "success": true,
      "log_likelihood": -11.531505407320198,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 3.565802657711856,
      "success": true,
      "log_likelihood": -4.579959824860516,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5213993237116918,
      "alpha_unfair": 5.253613788861276,
      "gamma_context": 1.1534406428351571,
      "success": true,
      "log_likelihood": -17.5258690025736,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5833305076358675,
      "alpha_unfair": 4.4193587779293715,
      "gamma_context": 1.816127072440394,
      "success": true,
      "log_likelihood": -14.858937150836605,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5272480348099623,
      "alpha_unfair": 2.1452762471659743,
      "gamma_context": 1.5885193234846755,
      "success": true,
      "log_likelihood": -18.634888764424918,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7999999005610257,
      "alpha_unfair": 3.0516389324915503,
      "gamma_context": 2.3976959119602377,
      "success": true,
      "log_likelihood": -16.26946472344624,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8750000904768059,
      "alpha_unfair": 1.8382244652742166,
      "gamma_context": 1.0133500161087636,
      "success": true,
      "log_likelihood": -19.824713239760072,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.0503026192277123,
      "alpha_unfair": 6.646341876478519,
      "gamma_context": 7.218412463307342,
      "success": true,
      "log_likelihood": -8.418936963673314,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7000000199268683,
      "alpha_unfair": 0.7610158492802787,
      "gamma_context": 0.4888861058237325,
      "success": true,
      "log_likelihood": -24.183637649400033,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5291988978421275,
      "alpha_unfair": 2.5933479115577573,
      "gamma_context": 1.5671087992275794,
      "success": true,
      "log_likelihood": -18.473680423881174,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499999838009948,
      "alpha_unfair": 2.35327097921536,
      "gamma_context": -0.887750707452557,
      "success": true,
      "log_likelihood": -12.288605931990002,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6733422615713156,
      "alpha_unfair": 2.2413351943977355,
      "gamma_context": 1.1744311077427394,
      "success": true,
      "log_likelihood": -20.321744349764124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 1.670240996728727,
      "gamma_context": 0.6813271653434493,
      "success": true,
      "log_likelihood": -22.963815414403282,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.0000021694826964,
      "alpha_unfair": 2.3084648272628483,
      "gamma_context": 1.9165094555779056,
      "success": true,
      "log_likelihood": -18.033453196237335,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499999907014915,
      "alpha_unfair": 10.0,
      "gamma_context": 2.1843213211007826,
      "success": true,
      "log_likelihood": -5.3823069588155406,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7000000005237429,
      "alpha_unfair": 10.0,
      "gamma_context": 0.7514511174669986,
      "success": true,
      "log_likelihood": -8.926431000888067,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5,
      "alpha_unfair": 0.05,
      "gamma_context": 2.089868233022036,
      "success": true,
      "log_likelihood": -12.599738970164038,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499999914923534,
      "alpha_unfair": 9.11346637395379,
      "gamma_context": -0.7886916272234651,
      "success": true,
      "log_likelihood": -7.770470584228163,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7027663094814546,
      "alpha_unfair": 1.6189918197867181,
      "gamma_context": 2.5179215302561553,
      "success": true,
      "log_likelihood": -15.702005117849186,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7500153818953104,
      "alpha_unfair": 2.0030364440160375,
      "gamma_context": 1.8309970389406016,
      "success": true,
      "log_likelihood": -19.69816190868598,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.828542372685322,
      "alpha_unfair": 4.148490955999237,
      "gamma_context": 3.3021420125143286,
      "success": true,
      "log_likelihood": -12.783654580521759,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.9999999286551862,
      "alpha_unfair": 2.834240211286677,
      "gamma_context": -0.45701529802986374,
      "success": true,
      "log_likelihood": -7.486696672789649,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5833346222021392,
      "alpha_unfair": 3.491360537072934,
      "gamma_context": 0.5804883296203334,
      "success": true,
      "log_likelihood": -19.23524946643165,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7262657751226215,
      "alpha_unfair": 5.46267582650612,
      "gamma_context": 0.19559402134534284,
      "success": true,
      "log_likelihood": -11.955736512050343,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7166410595769936,
      "alpha_unfair": 3.3238829850562515,
      "gamma_context": 2.1664632826359327,
      "success": true,
      "log_likelihood": -16.001468678603516,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6262529132255067,
      "alpha_unfair": 3.300876842072022,
      "gamma_context": 4.796575871521019,
      "success": true,
      "log_likelihood": -7.113938679892459,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8079208085629084,
      "alpha_unfair": 3.4813239370750844,
      "gamma_context": 5.580196575676239,
      "success": true,
      "log_likelihood": -10.111393084021337,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.542996964476089,
      "alpha_unfair": 3.739589972095941,
      "gamma_context": 1.0854938883866818,
      "success": true,
      "log_likelihood": -18.903518187784243,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.0524384102465851,
      "alpha_unfair": 1.842631940527872,
      "gamma_context": 4.9742213938785005,
      "success": true,
      "log_likelihood": -9.31245826038466,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.9999999975579661,
      "alpha_unfair": 10.0,
      "gamma_context": 1.6373910974734627,
      "success": true,
      "log_likelihood": -1.4541971693060227,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499999308337443,
      "alpha_unfair": 3.01163028620697,
      "gamma_context": -0.7681669706344936,
      "success": true,
      "log_likelihood": -11.561602459422131,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7000008731019403,
      "alpha_unfair": 3.2827167896695446,
      "gamma_context": 1.816530161640669,
      "success": true,
      "log_likelihood": -16.54000764754887,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5070764437840927,
      "alpha_unfair": 2.582602642399768,
      "gamma_context": 2.0025398805005303,
      "success": true,
      "log_likelihood": -15.869861394797024,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.799999909137101,
      "alpha_unfair": 1.4080641970490673,
      "gamma_context": 1.0121946130343829,
      "success": true,
      "log_likelihood": -22.29066331520525,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6666116911849149,
      "alpha_unfair": 4.050145471236917,
      "gamma_context": 3.262152421516627,
      "success": true,
      "log_likelihood": -12.623777741144577,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7498726415554382,
      "alpha_unfair": 3.12340990127111,
      "gamma_context": 6.006858055045908,
      "success": true,
      "log_likelihood": -5.842009095846857,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.9999999719440349,
      "alpha_unfair": 1.6826617166620779,
      "gamma_context": -1.4428518202430847,
      "success": true,
      "log_likelihood": -6.806553970071586,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8750007827293225,
      "alpha_unfair": 0.6712940478210028,
      "gamma_context": 1.0439795275295465,
      "success": true,
      "log_likelihood": -23.378610505892603,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499999850834955,
      "alpha_unfair": 9.263730255023345,
      "gamma_context": -1.1603098803357792,
      "success": true,
      "log_likelihood": -6.831799636798416,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.0000000863990886,
      "alpha_unfair": 1.0789840830035857,
      "gamma_context": 1.2285034170456022,
      "success": true,
      "log_likelihood": -23.391042591142828,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.5,
      "alpha_unfair": 0.05,
      "gamma_context": 2.485349435345876,
      "success": true,
      "log_likelihood": -10.32499615150891,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.7499999408879532,
      "alpha_unfair": 10.0,
      "gamma_context": -0.7367400821313588,
      "success": true,
      "log_likelihood": -7.737161897419833,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.583333327140503,
      "alpha_unfair": 10.0,
      "gamma_context": 1.4491418475929139,
      "success": true,
      "log_likelihood": -10.674461312601794,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.1188467826138366,
      "alpha_unfair": 1.1311617636651208,
      "gamma_context": 1.9590485396147619,
      "success": true,
      "log_likelihood": -22.566832799236238,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 1.0290360493610768,
      "alpha_unfair": 3.9045650526165567,
      "gamma_context": 0.6601994658782059,
      "success": true,
      "log_likelihood": -7.508391417794395,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8749998952613407,
      "alpha_unfair": 2.4138975539532908,
      "gamma_context": 3.286417349213205,
      "success": true,
      "log_likelihood": -16.997557858960004,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8750000638332612,
      "alpha_unfair": 3.3550663412933486,
      "gamma_context": 0.6447689588087141,
      "success": true,
      "log_likelihood": -12.0068562241039,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.6835701259480588,
      "alpha_unfair": 3.726673321534768,
      "gamma_context": 6.292830791962604,
      "success": true,
      "log_likelihood": -5.232844324156693,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.5214050656174405,
      "alpha_unfair": 10.0,
      "gamma_context": 0.8939779773184738,
      "success": true,
      "log_likelihood": -15.47888504739036,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau_fairness": 0.8000000790689914,
      "alpha_unfair": 1.135425979433353,
      "gamma_context": 1.7273600900918697,
      "success": true,
      "log_likelihood": -20.624711700653663,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "tau_fairness": {
        "r": 0.5116879056652995,
        "p": 5.303209863285436e-08
      },
      "alpha_unfair": {
        "r": 0.6540473420644679,
        "p": 1.6019354805516643e-13
      },
      "gamma_context": {
        "r": 0.7689102800557998,
        "p": 9.53289924358189e-21
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 33.81947410281372,
  "bic_Control": 28.38367015696192,
  "bic_Cocaine": 38.68904847097263,
  "bic_results": {
    "average_bic": 33.81947410281372,
    "individual_bics": [
      24.37982000142736,
      42.89155077341832,
      32.29949890029013,
      42.491044562264335,
      27.710695163862013,
      34.8306181718662,
      10.75385991482045,
      31.84288935490669,
      10.75385991482045,
      33.7408203352563,
      19.340725136618712,
      26.224927920625362,
      27.290880422465413,
      10.75385991482045,
      10.75385991482045,
      10.75385991482045,
      10.75385991482045,
      23.702243171737106,
      20.795505786012807,
      10.75385991482045,
      35.51443590200843,
      24.592487941697488,
      40.687330433943586,
      46.928711450638545,
      30.705808661493585,
      41.25830422365516,
      38.780291812828466,
      19.393020823666127,
      26.120346588045116,
      37.30725151182678,
      10.75385991482045,
      24.37957241568902,
      52.524141127167766,
      44.06160596077871,
      10.75385991482045,
      44.464629198516576,
      28.000253868595976,
      37.48764348968102,
      42.18823018510263,
      26.511357944811834,
      41.54239017998282,
      33.81356763000873,
      19.91047646508936,
      45.80229482051553,
      40.46843111704154,
      48.02033434421817,
      43.28948626226081,
      50.39998329488847,
      27.58843074271496,
      59.117832114168394,
      47.69791766313068,
      35.32776867934834,
      51.394045514896575,
      56.6781876441749,
      46.817463207843005,
      21.51517073299941,
      28.603418817144465,
      35.95003475569641,
      26.291497983824655,
      42.15456705106671,
      50.14688063274029,
      36.31786597641185,
      25.72395016094763,
      49.221055748231635,
      34.662029839469014,
      42.75349417257536,
      24.978434175153247,
      30.973342983411005,
      48.557593190936814,
      29.375473336137652,
      13.658951153980375,
      33.87376173421259,
      43.830572110466065,
      42.49027960496238,
      55.33188344577883,
      35.998112297657485,
      22.434575007062044,
      24.3636647555115,
      57.50777782715353,
      24.414156088965164,
      57.53264199765398,
      31.400549118386152,
      26.224880610207997,
      32.09947944057192,
      55.8842224138408,
      25.76733965095712,
      44.74567253328834,
      34.76426926357613,
      21.216245463681716,
      41.708326910149054,
      51.99998021667565
    ],
    "group_enabled": true,
    "group_bics": {
      "Control": [
        24.37982000142736,
        42.89155077341832,
        32.29949890029013,
        42.491044562264335,
        27.710695163862013,
        34.8306181718662,
        10.75385991482045,
        31.84288935490669,
        10.75385991482045,
        33.7408203352563,
        19.340725136618712,
        26.224927920625362,
        27.290880422465413,
        10.75385991482045,
        10.75385991482045,
        10.75385991482045,
        10.75385991482045,
        23.702243171737106,
        20.795505786012807,
        10.75385991482045,
        35.51443590200843,
        24.592487941697488,
        40.687330433943586,
        46.928711450638545,
        30.705808661493585,
        41.25830422365516,
        38.780291812828466,
        19.393020823666127,
        26.120346588045116,
        37.30725151182678,
        10.75385991482045,
        24.37957241568902,
        52.524141127167766,
        44.06160596077871,
        10.75385991482045,
        44.464629198516576,
        28.000253868595976,
        37.48764348968102,
        42.18823018510263,
        26.511357944811834,
        41.54239017998282,
        33.81356763000873,
        19.91047646508936
      ],
      "Cocaine": [
        45.80229482051553,
        40.46843111704154,
        48.02033434421817,
        43.28948626226081,
        50.39998329488847,
        27.58843074271496,
        59.117832114168394,
        47.69791766313068,
        35.32776867934834,
        51.394045514896575,
        56.6781876441749,
        46.817463207843005,
        21.51517073299941,
        28.603418817144465,
        35.95003475569641,
        26.291497983824655,
        42.15456705106671,
        50.14688063274029,
        36.31786597641185,
        25.72395016094763,
        49.221055748231635,
        34.662029839469014,
        42.75349417257536,
        24.978434175153247,
        30.973342983411005,
        48.557593190936814,
        29.375473336137652,
        13.658951153980375,
        33.87376173421259,
        43.830572110466065,
        42.49027960496238,
        55.33188344577883,
        35.998112297657485,
        22.434575007062044,
        24.3636647555115,
        57.50777782715353,
        24.414156088965164,
        57.53264199765398,
        31.400549118386152,
        26.224880610207997,
        32.09947944057192,
        55.8842224138408,
        25.76733965095712,
        44.74567253328834,
        34.76426926357613,
        21.216245463681716,
        41.708326910149054,
        51.99998021667565
      ]
    },
    "num_parameters": 3,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- tau_fairness: r = 0.512\n- alpha_unfair: r = 0.654\n- gamma_context: r = 0.769",
  "previous_models": [
    "Specification: offer = split_self  \nentitlement = (token_self / (token_self + token_opp)) * combined_earning  \nfairness_threshold = tau_fairness * entitlement  \npenalty = alpha_unfair * log(1 + max(0, fairness_threshold - offer))  \ncontext_bonus = gamma_context * indicator(combined_earning > median_combined_earning)  \nU = -penalty + context_bonus\nSummary: This model predicts responder acceptance by comparing the proposed offer to a personalized fairness threshold proportional to self-earned entitlement, controlled by a fairness cutoff parameter. Offers below threshold produce a sharply increasing logarithmic penalty with learnable aversion strength. Contextual sensitivity is captured via a distinct bonus/penalty for high-stake trials. The model achieves nonoverlapping parameter influence, explicit loss/gain asymmetry, and nonlinear transitions, providing recoverable, interpretable mechanisms for fairness and context effects in choice.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 33.81947410281372\nOverall Accuracy: 0.7958\nGroup Accuracies:\n- Control: 0.8488\n- Cocaine: 0.7483\n\nParameter Recovery:\n- tau_fairness: r = 0.512\n- alpha_unfair: r = 0.654\n- gamma_context: r = 0.769"
  ]
}