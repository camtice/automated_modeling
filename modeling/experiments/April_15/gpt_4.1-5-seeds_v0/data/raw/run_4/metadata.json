{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\nThe previous model, although hybrid and flexible, produced extremely poor parameter recovery (correlations near zero or negative), low accuracy (53%), and moderate BIC, signaling severe identifiability and specification issues. This suggests the following problems:\n\n1. **Parameter Redundancy and Entanglement:** The hybrid model sought to capture both threshold and fairness aversion via a soft sigmoid threshold and a fairness penalty. However, both control the slope and midpoint, leading to collinearity or mutual compensation, which destroys interpretability and recovery.\n2. **Over-parameterization:** Four learnable parameters (threshold location, steepness, fairness weight, lapse rate) is aggressive, especially if their influence is not orthogonalized.\n3. **Form of Nonlinearity:** The model\u2019s use of multiplicative elements (sigmoid \u00d7 fairness penalty) is interpretable, but may be too smooth or too subtle to separate out real behavioral discontinuities (e.g., hard rejections below threshold).\n4. **Lapse Parameter:** Lapse rates typically recover well only if there is clear evidence of pure randomness. If confounded with poor model fit on systematic behavior, this parameter can become noise-catching and confound others.\n5. **Lack of Strong \u201cDiscontinuity\u201d/Categoriality:** Human data often shows abrupt drop-offs below minimum acceptable shares; modeling this as a very soft sigmoid can mask sharp realities.\n\nGiven these, new instructions should:\n- Enforce a much more *parsimonious* parameterization per model (e.g., at least one proposal with only one genuinely learnable parameter, such as an exact threshold).\n- Explicitly *separate* functional forms: try models with only a step/indicator function (hard or very steep), and others with only fairness penalty, not both at once unless identifiability is proven (e.g., by simulated recovery checks).\n- Make the modeler formally check identifiability logic: for each parameter, \u201cWhat distinct behavioral curve, pattern, or feature uniquely identifies this parameter, even if all others are known or set to default? Would I recover it if fitting to realistic (non-ideal) data?\u201d\n- Encourage the modeler to:\n    - Propose at least one \"hard\" (piecewise, discontinuous) function and justify when such abrupt changes are appropriate.\n    - Try at least one \"winner-take-all\" or \"max/min\"-style model; e.g., utility = min(self-interest, fairness discrepancy effect), or utility switches as soon as a threshold or criterion is met.\n    - Consider drastically reducing the number of learnable parameters to maximize recovery and avoid co-linearity.\n- Emphasize that *linear additive* models with clear separation of terms and bounds often outperform complex hybrids on recovery, despite being \"simpler.\"\n- Suggest modeling the *step location* (minimum acceptable percent) as the *sole* parameter in a minimalist model\u2014see if a step function alone can outperform cumbersome hybrids.\n- Insist on thorough, argument-based *parameter separability*: for any multi-parameter proposal, include a sketch of how each parameter would be estimated from data features, or what would force model degeneracy.\n- Make the modeler explicitly model and discuss \u201ccatastrophic collinearity\u201d and set minimal model as baseline, justifying any additional parameter as showing incremental, not overlapping, value.\n- Encourage at least one \u201creference-dependent\u201d or \u201crelative loss aversion\u201d model that penalizes either direction of deviation from a fairness standard\u2014consider non-monotonic or symmetric penalty (e.g., quadratic or absolute deviation), and avoid making both threshold and penalty strengths \u201cfree\u201d unless data can distinguish.\n- Suggest testing their models on *simulated data* for extreme test cases: strong threshold-only responder, fairness-only responder, noiseless/random responder; ensure the model recovers parameters when data is drawn from such known patterns.\n\nEncourage truly creative, not just \u201cincrementally different\u201d models.\n</think>\n\n**Updated Instructions for Run 4:**\n\n**Goal:** Produce computational models that maximize prediction accuracy, minimize BIC, and\u2014critically\u2014ensure robust experimental recovery of all parameter values. Interpretability and mathematical identifiability must both be maximized.\n\n**Stepwise Modeling Process (complete rigorously before formalization):**\n\n1. **Map psychological mechanisms** underlying responder choice (self-interest, fairness, thresholding, satisficing, random lapses, etc). Explicitly consider abrupt (discontinuous) as well as graded (continuous) behavioral responses.\n\n2. **Construct at least three distinctly structured candidate models:**\n    - One using a *hard step* or *indicator* function (abrupt cutoff at a learned threshold).\n    - One using a *single* fairness penalty or deviation (absolute difference or quadratic) from a reference point (e.g., equality or merit), without threshold.\n    - One \u201cnon-standard\u201d form\u2014e.g., \"max/min\", switch/floor, or piecewise, or even mixed discrete-continuous (e.g., acceptance only if both threshold and fairness proximity are met).\n    - At least one model should use only a *single* learnable parameter (e.g., the location of the threshold), to test for best recovery; multi-parameter models should only admit extra parameters with clear justification.\n\n3. **For multi-parameter models,** for *each parameter*, explicitly specify:\n    - The exact data feature or behavioral difference it uniquely governs, and how you would distinguish its effect from other parameters.\n    - How parameter values will be recoverable in the presence of random lapses or non-standard responding.\n\n4. **For every model:**\n    - Argue why this functional form is appropriate for the observed (or simulated) behavioral data (e.g., abrupt transitions, smooth gradients, symmetric vs. asymmetric tendencies).\n    - Test for catastrophic collinearity; do any pairs of parameters try to explain the same variance or are otherwise substitutable?\n    - Limit the number of learnable parameters\u2014prefer parsimonious, minimal models and justify any increase in complexity with simulation-based evidence.\n\n5. **Recommend at least one \u201creference-dependent\u201d or \u201crelative\u201d model:** Utility based solely on deviations (absolute or signed) from a social or merit-based standard (e.g., U = -alpha \u00d7 |splitperc_self \u2013 fairness_reference|, where only alpha is learned).\n\n6. **For all parameter bounds:** Base on theoretical or empirical limits. E.g., minimum acceptable percent in [0,100], penalty strengths \u2265 0. Avoid bounds that risk extremal-fitting or unbounded drift.\n\n7. **Modeling Lapses and Noise:** Consider omitting lapse parameters if not necessary, or use fixed plausible values\u2014do not attach lapse/noise parameters only to absorb unexplained variance.\n\n8. **Argue for parameter recoverability, not just fit.** For every candidate, demonstrate via simulated logic which data patterns would yield high vs. low parameter estimates, and what tradeoffs exist.\n\n**When writing your mathematical model and variable specifications:**\n\n- Present only one final candidate (the most promising for recovery and accuracy), but include a short justification why this choice was made from your earlier theoretical exercise.\n- Use clean, unambiguous mathematical notation and variable names.\n- Ensure all variable and parameter descriptions are precise, bounded, and justified.\n\n**Encouragement:**\n- Challenge yourself to propose at least one model form that is entirely \u201coutside the box\u201d\u2014e.g., using max/min, hard indicators, or a mixture of discontinuous and continuous functions, and justify its relevance to behavioral data.\n- Consider models that are deliberately minimalist\u2014sometimes a 1-parameter cutoff matches the data as well as complex forms but is vastly more recoverable.\n- Model \"winning\" patterns\u2014thresholds (\u201cI accept if \u2265 x%\u201d), fairness only (\u201caccept if close to fair\u201d), or interaction (\u201cit must be both fair and enough for me\u201d). Demonstrate via logic or toy simulation which best suits data like this.\n\nAfter these rigorous steps and justifications, then present your one formal model specification, variable descriptions, and accompanying short summary as previously instructed.",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: <think>\nThe previous model, although hybrid and flexible, produced extremely poor parameter recovery (correlations near zero or negative), low accuracy (53%), and moderate BIC, signaling severe identifiability and specification issues. This suggests the following problems:\n\n1. **Parameter Redundancy and Entanglement:** The hybrid model sought to capture both threshold and fairness aversion via a soft sigmoid threshold and a fairness penalty. However, both control the slope and midpoint, leading to collinearity or mutual compensation, which destroys interpretability and recovery.\n2. **Over-parameterization:** Four learnable parameters (threshold location, steepness, fairness weight, lapse rate) is aggressive, especially if their influence is not orthogonalized.\n3. **Form of Nonlinearity:** The model\u2019s use of multiplicative elements (sigmoid \u00d7 fairness penalty) is interpretable, but may be too smooth or too subtle to separate out real behavioral discontinuities (e.g., hard rejections below threshold).\n4. **Lapse Parameter:** Lapse rates typically recover well only if there is clear evidence of pure randomness. If confounded with poor model fit on systematic behavior, this parameter can become noise-catching and confound others.\n5. **Lack of Strong \u201cDiscontinuity\u201d/Categoriality:** Human data often shows abrupt drop-offs below minimum acceptable shares; modeling this as a very soft sigmoid can mask sharp realities.\n\nGiven these, new instructions should:\n- Enforce a much more *parsimonious* parameterization per model (e.g., at least one proposal with only one genuinely learnable parameter, such as an exact threshold).\n- Explicitly *separate* functional forms: try models with only a step/indicator function (hard or very steep), and others with only fairness penalty, not both at once unless identifiability is proven (e.g., by simulated recovery checks).\n- Make the modeler formally check identifiability logic: for each parameter, \u201cWhat distinct behavioral curve, pattern, or feature uniquely identifies this parameter, even if all others are known or set to default? Would I recover it if fitting to realistic (non-ideal) data?\u201d\n- Encourage the modeler to:\n    - Propose at least one \"hard\" (piecewise, discontinuous) function and justify when such abrupt changes are appropriate.\n    - Try at least one \"winner-take-all\" or \"max/min\"-style model; e.g., utility = min(self-interest, fairness discrepancy effect), or utility switches as soon as a threshold or criterion is met.\n    - Consider drastically reducing the number of learnable parameters to maximize recovery and avoid co-linearity.\n- Emphasize that *linear additive* models with clear separation of terms and bounds often outperform complex hybrids on recovery, despite being \"simpler.\"\n- Suggest modeling the *step location* (minimum acceptable percent) as the *sole* parameter in a minimalist model\u2014see if a step function alone can outperform cumbersome hybrids.\n- Insist on thorough, argument-based *parameter separability*: for any multi-parameter proposal, include a sketch of how each parameter would be estimated from data features, or what would force model degeneracy.\n- Make the modeler explicitly model and discuss \u201ccatastrophic collinearity\u201d and set minimal model as baseline, justifying any additional parameter as showing incremental, not overlapping, value.\n- Encourage at least one \u201creference-dependent\u201d or \u201crelative loss aversion\u201d model that penalizes either direction of deviation from a fairness standard\u2014consider non-monotonic or symmetric penalty (e.g., quadratic or absolute deviation), and avoid making both threshold and penalty strengths \u201cfree\u201d unless data can distinguish.\n- Suggest testing their models on *simulated data* for extreme test cases: strong threshold-only responder, fairness-only responder, noiseless/random responder; ensure the model recovers parameters when data is drawn from such known patterns.\n\nEncourage truly creative, not just \u201cincrementally different\u201d models.\n</think>\n\n**Updated Instructions for Run 4:**\n\n**Goal:** Produce computational models that maximize prediction accuracy, minimize BIC, and\u2014critically\u2014ensure robust experimental recovery of all parameter values. Interpretability and mathematical identifiability must both be maximized.\n\n**Stepwise Modeling Process (complete rigorously before formalization):**\n\n1. **Map psychological mechanisms** underlying responder choice (self-interest, fairness, thresholding, satisficing, random lapses, etc). Explicitly consider abrupt (discontinuous) as well as graded (continuous) behavioral responses.\n\n2. **Construct at least three distinctly structured candidate models:**\n    - One using a *hard step* or *indicator* function (abrupt cutoff at a learned threshold).\n    - One using a *single* fairness penalty or deviation (absolute difference or quadratic) from a reference point (e.g., equality or merit), without threshold.\n    - One \u201cnon-standard\u201d form\u2014e.g., \"max/min\", switch/floor, or piecewise, or even mixed discrete-continuous (e.g., acceptance only if both threshold and fairness proximity are met).\n    - At least one model should use only a *single* learnable parameter (e.g., the location of the threshold), to test for best recovery; multi-parameter models should only admit extra parameters with clear justification.\n\n3. **For multi-parameter models,** for *each parameter*, explicitly specify:\n    - The exact data feature or behavioral difference it uniquely governs, and how you would distinguish its effect from other parameters.\n    - How parameter values will be recoverable in the presence of random lapses or non-standard responding.\n\n4. **For every model:**\n    - Argue why this functional form is appropriate for the observed (or simulated) behavioral data (e.g., abrupt transitions, smooth gradients, symmetric vs. asymmetric tendencies).\n    - Test for catastrophic collinearity; do any pairs of parameters try to explain the same variance or are otherwise substitutable?\n    - Limit the number of learnable parameters\u2014prefer parsimonious, minimal models and justify any increase in complexity with simulation-based evidence.\n\n5. **Recommend at least one \u201creference-dependent\u201d or \u201crelative\u201d model:** Utility based solely on deviations (absolute or signed) from a social or merit-based standard (e.g., U = -alpha \u00d7 |splitperc_self \u2013 fairness_reference|, where only alpha is learned).\n\n6. **For all parameter bounds:** Base on theoretical or empirical limits. E.g., minimum acceptable percent in [0,100], penalty strengths \u2265 0. Avoid bounds that risk extremal-fitting or unbounded drift.\n\n7. **Modeling Lapses and Noise:** Consider omitting lapse parameters if not necessary, or use fixed plausible values\u2014do not attach lapse/noise parameters only to absorb unexplained variance.\n\n8. **Argue for parameter recoverability, not just fit.** For every candidate, demonstrate via simulated logic which data patterns would yield high vs. low parameter estimates, and what tradeoffs exist.\n\n**When writing your mathematical model and variable specifications:**\n\n- Present only one final candidate (the most promising for recovery and accuracy), but include a short justification why this choice was made from your earlier theoretical exercise.\n- Use clean, unambiguous mathematical notation and variable names.\n- Ensure all variable and parameter descriptions are precise, bounded, and justified.\n\n**Encouragement:**\n- Challenge yourself to propose at least one model form that is entirely \u201coutside the box\u201d\u2014e.g., using max/min, hard indicators, or a mixture of discontinuous and continuous functions, and justify its relevance to behavioral data.\n- Consider models that are deliberately minimalist\u2014sometimes a 1-parameter cutoff matches the data as well as complex forms but is vastly more recoverable.\n- Model \"winning\" patterns\u2014thresholds (\u201cI accept if \u2265 x%\u201d), fairness only (\u201caccept if close to fair\u201d), or interaction (\u201cit must be both fair and enough for me\u201d). Demonstrate via logic or toy simulation which best suits data like this.\n\nAfter these rigorous steps and justifications, then present your one formal model specification, variable descriptions, and accompanying short summary as previously instructed.\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "Absolutely, let's approach this with maximum rigor and explicit reasoning around identifiability, interpretability, and recovery. I\u2019ll proceed through each requested modeling stage and then give one formal model as specified.\n\n---\n\n### 1. Mapping Psychological Mechanisms\n\nFor responder behavior in ultimatum-like tasks, observed patterns generally include:\n- **Abrupt cutoff/thresholding behaviors:** \u201cI accept if and only if my share is at least X%.\u201d\n- **Deviation from fairness/merit:** \u201cI dislike deviation from the fair or deserved share, the further away, the worse.\u201d\n- **Satisficing/uncompensated threshold:** \u201cI need at least Y (regardless of fairness) or else reject.\u201d\n- **Random lapses:** Unexplained acceptance/rejection.\n\n### 2. Candidate Model Structures (Summary)\n\n#### (A) Hard Threshold Model (1 parameter)\n- Accept if and only if: `splitperc_self >= tau`\n- Utility: Very high above threshold, very low below.\n- **Strengths:** Parameter (tau) is directly recoverable from a step in subject acceptance curves.\n- **Form:**  \n  - U = High if splitperc_self \u2265 tau  \n  - U = Low if splitperc_self < tau\n\n#### (B) Fairness Penalty Model (1 parameter)\n- Utility is a penalty of the distance from an ideal (e.g., equality or merit-based share).\n- U = -alpha * |splitperc_self - fairness_reference|\n- **Strengths:** Parameter (alpha) is slope/sensitivity: steeper curves for higher penalty. Reference can be equality, or `token_self/(token_self + token_opp) * 100`.\n- **Form:**  \n  - U = -alpha \u00d7 |splitperc_self \u2212 reference|  \n\n#### (C) Min-Rule Model (\u201cboth conditions must be met\u201d)\n- Accept if (splitperc_self >= tau) AND (fairness deviation \u2264 delta): multiple satisficing constraints.\n- Utility: Max(low threshold, fairness deviation).\n- **Strengths:** Captures participants who demand both \u201cenough\u201d and \u201cfairness,\u201d leads to non-orthogonal, but interpretable, parameter boundary curves.\n\n#### (D) Absolute Share Model\n- Accept if and only if: split_self >= tau_abs (\u00a3, not percent)\n- Less plausible for this task: prior evidence shows relative, not absolute, thresholds.\n\n---\n\nNow, to *concretely meet the challenge requirements* and the goal of maximizing recovery and parsimony, I\u2019ll select one model to formalize.\n\n### 3. Parameter Identifiability/Recovery\n\n#### (A) Hard Threshold Model (Percent Split)\n- Only one parameter (tau): the minimum acceptable percent (in their share).\n- *How is it identified?* Directly visible as the step/cutoff in subject acceptance rates. No other parameter can explain a systematic drop precisely at this value.\n- *Catastrophic collinearity?* Impossible, as only one parameter.\n- *Simulation test:* If data is generated with a hard threshold at 35%, tau will be clearly recovered.\n- *Downside:* Cannot explain gradation or tolerance for small deviations from threshold\u2014may underfit when participants are probabilistic or respond to fairness, not just self-share.\n\n#### (B) Fairness Penalty Model (Deviation from Reference)\n- Only one parameter (alpha): aversion to deviation from the reference fairness point.\n- *How is it identified?* The steeper the alpha, the steeper logistic acceptance decrease/increase as offers deviate from fairness.\n- *Simulation test:* If alpha is high, nearly full rejection for deviation; if alpha is near zero, responses near random.\n- *Catastrophic collinearity?* Only if reference is mis-specified.\n\n#### (C) \u201cBoth Conditions Must Be Met\u201d (More complex, 2+ parameters)\n- At least two parameters: threshold tau and tolerance delta.\n- *Recovery logic:* Each is recoverable only if some trials violate one but not the other. If data only ever shows both, then collinearity is possible.\n- *Not chosen for formalization due to the above, and to comply with minimal parameterization preference.* Would consider if justified by improved fit and simulation-based gain.\n\n---\n\n### 4. Rationale for Final Model Selection\n\n**I choose Model (A): Hard Threshold on splitperc_self (participant\u2019s share percent).**\n\n- *Justification:*\n    - Maximally parsimonious (one learnable parameter).\n    - Recovery is unambiguous: the threshold is exactly where behavior changes.\n    - Matches established findings in ultimatum literature, where responders often have discernible minimum acceptable shares.\n    - If participants are more graded in reality, this model is an underfit baseline\u2014the direction and location of fit error itself will be diagnostically valuable.\n\n- *Functional form:*  \n    - U = 0 if splitperc_self < tau;  \n    - U = 1 if splitperc_self \u2265 tau;\n    - Logistic noise will still allow for smooth probability estimation.\n\n---\n\n## **Formal Model Specification, as requested**\n\n<MODEL>\nU = splitperc_self - tau\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposed offer in this trial\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage share of the proposed reward allocated to the participant (range 0 to 100, computed as split_self / combined_earning \u00d7 100)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"tau\": {\n      \"description\": \"Minimum acceptable percentage share for the participant (threshold); participant accepts if their share is at least tau percent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": 0,\n          \"high\": 100\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA strictly minimal, interpretable model where the responder\u2019s choice is governed by a single parameter: the minimum acceptable percentage share (tau). The utility of accepting is simply the difference between the offered percentage share (splitperc_self) and tau. Acceptance probability rises sharply from 0 to 1 at this threshold, ensuring robust parameter recovery and interpretability by capturing abrupt behavioral cutoffs observed in responder data.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['tau']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = splitperc_self - tau\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposed offer in this trial\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage share of the proposed reward allocated to the participant (range 0 to 100, computed as split_self / combined_earning \\u00d7 100)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"tau\": {\n      \"description\": \"Minimum acceptable percentage share for the participant (threshold); participant accepts if their share is at least tau percent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": 0,\n          \"high\": 100\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, tau=50):\n    \"\"\"\n    Simulates a simple threshold model for ultimatum game decisions.\n\n    Args:\n        trial_data (list of dict): A list of dictionaries, where each dictionary represents a trial\n                                  and contains the 'splitperc_self' key.\n        tau (float): The minimum acceptable percentage share for the participant (threshold).\n\n    Returns:\n        list of float: A list of model predictions (utilities) for each trial.\n    \"\"\"\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)  # Default to 0 if missing\n        utility = splitperc_self - tau\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n",
      "timestamp": 1744723781.166829
    }
  ],
  "learnable_parameters": {
    "tau": {
      "description": "Minimum acceptable percentage share for the participant (threshold); participant accepts if their share is at least tau percent",
      "range": {
        "min": 0,
        "max": 100,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {
          "low": 0,
          "high": 100
        }
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = splitperc_self - tau",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposed offer in this trial",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "splitperc_self": {
      "description": "Percentage share of the proposed reward allocated to the participant (range 0 to 100, computed as split_self / combined_earning \u00d7 100)",
      "range": {
        "min": 0,
        "max": 100,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "tau": {
      "description": "Minimum acceptable percentage share for the participant (threshold); participant accepts if their share is at least tau percent",
      "range": {
        "min": 0,
        "max": 100,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {
          "low": 0,
          "high": 100
        }
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_summary": "A strictly minimal, interpretable model where the responder\u2019s choice is governed by a single parameter: the minimum acceptable percentage share (tau). The utility of accepting is simply the difference between the offered percentage share (splitperc_self) and tau. Acceptance probability rises sharply from 0 to 1 at this threshold, ensuring robust parameter recovery and interpretability by capturing abrupt behavioral cutoffs observed in responder data.",
  "target_variable": "accept",
  "full_reasoning": "Absolutely, let's approach this with maximum rigor and explicit reasoning around identifiability, interpretability, and recovery. I\u2019ll proceed through each requested modeling stage and then give one formal model as specified.\n\n---\n\n### 1. Mapping Psychological Mechanisms\n\nFor responder behavior in ultimatum-like tasks, observed patterns generally include:\n- **Abrupt cutoff/thresholding behaviors:** \u201cI accept if and only if my share is at least X%.\u201d\n- **Deviation from fairness/merit:** \u201cI dislike deviation from the fair or deserved share, the further away, the worse.\u201d\n- **Satisficing/uncompensated threshold:** \u201cI need at least Y (regardless of fairness) or else reject.\u201d\n- **Random lapses:** Unexplained acceptance/rejection.\n\n### 2. Candidate Model Structures (Summary)\n\n#### (A) Hard Threshold Model (1 parameter)\n- Accept if and only if: `splitperc_self >= tau`\n- Utility: Very high above threshold, very low below.\n- **Strengths:** Parameter (tau) is directly recoverable from a step in subject acceptance curves.\n- **Form:**  \n  - U = High if splitperc_self \u2265 tau  \n  - U = Low if splitperc_self < tau\n\n#### (B) Fairness Penalty Model (1 parameter)\n- Utility is a penalty of the distance from an ideal (e.g., equality or merit-based share).\n- U = -alpha * |splitperc_self - fairness_reference|\n- **Strengths:** Parameter (alpha) is slope/sensitivity: steeper curves for higher penalty. Reference can be equality, or `token_self/(token_self + token_opp) * 100`.\n- **Form:**  \n  - U = -alpha \u00d7 |splitperc_self \u2212 reference|  \n\n#### (C) Min-Rule Model (\u201cboth conditions must be met\u201d)\n- Accept if (splitperc_self >= tau) AND (fairness deviation \u2264 delta): multiple satisficing constraints.\n- Utility: Max(low threshold, fairness deviation).\n- **Strengths:** Captures participants who demand both \u201cenough\u201d and \u201cfairness,\u201d leads to non-orthogonal, but interpretable, parameter boundary curves.\n\n#### (D) Absolute Share Model\n- Accept if and only if: split_self >= tau_abs (\u00a3, not percent)\n- Less plausible for this task: prior evidence shows relative, not absolute, thresholds.\n\n---\n\nNow, to *concretely meet the challenge requirements* and the goal of maximizing recovery and parsimony, I\u2019ll select one model to formalize.\n\n### 3. Parameter Identifiability/Recovery\n\n#### (A) Hard Threshold Model (Percent Split)\n- Only one parameter (tau): the minimum acceptable percent (in their share).\n- *How is it identified?* Directly visible as the step/cutoff in subject acceptance rates. No other parameter can explain a systematic drop precisely at this value.\n- *Catastrophic collinearity?* Impossible, as only one parameter.\n- *Simulation test:* If data is generated with a hard threshold at 35%, tau will be clearly recovered.\n- *Downside:* Cannot explain gradation or tolerance for small deviations from threshold\u2014may underfit when participants are probabilistic or respond to fairness, not just self-share.\n\n#### (B) Fairness Penalty Model (Deviation from Reference)\n- Only one parameter (alpha): aversion to deviation from the reference fairness point.\n- *How is it identified?* The steeper the alpha, the steeper logistic acceptance decrease/increase as offers deviate from fairness.\n- *Simulation test:* If alpha is high, nearly full rejection for deviation; if alpha is near zero, responses near random.\n- *Catastrophic collinearity?* Only if reference is mis-specified.\n\n#### (C) \u201cBoth Conditions Must Be Met\u201d (More complex, 2+ parameters)\n- At least two parameters: threshold tau and tolerance delta.\n- *Recovery logic:* Each is recoverable only if some trials violate one but not the other. If data only ever shows both, then collinearity is possible.\n- *Not chosen for formalization due to the above, and to comply with minimal parameterization preference.* Would consider if justified by improved fit and simulation-based gain.\n\n---\n\n### 4. Rationale for Final Model Selection\n\n**I choose Model (A): Hard Threshold on splitperc_self (participant\u2019s share percent).**\n\n- *Justification:*\n    - Maximally parsimonious (one learnable parameter).\n    - Recovery is unambiguous: the threshold is exactly where behavior changes.\n    - Matches established findings in ultimatum literature, where responders often have discernible minimum acceptable shares.\n    - If participants are more graded in reality, this model is an underfit baseline\u2014the direction and location of fit error itself will be diagnostically valuable.\n\n- *Functional form:*  \n    - U = 0 if splitperc_self < tau;  \n    - U = 1 if splitperc_self \u2265 tau;\n    - Logistic noise will still allow for smooth probability estimation.\n\n---\n\n## **Formal Model Specification, as requested**\n\n<MODEL>\nU = splitperc_self - tau\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposed offer in this trial\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage share of the proposed reward allocated to the participant (range 0 to 100, computed as split_self / combined_earning \u00d7 100)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"tau\": {\n      \"description\": \"Minimum acceptable percentage share for the participant (threshold); participant accepts if their share is at least tau percent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {\n          \"low\": 0,\n          \"high\": 100\n        }\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA strictly minimal, interpretable model where the responder\u2019s choice is governed by a single parameter: the minimum acceptable percentage share (tau). The utility of accepting is simply the difference between the offered percentage share (splitperc_self) and tau. Acceptance probability rises sharply from 0 to 1 at this threshold, ensuring robust parameter recovery and interpretability by capturing abrupt behavioral cutoffs observed in responder data.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = splitperc_self - tau\nSummary: A strictly minimal, interpretable model where the responder\u2019s choice is governed by a single parameter: the minimum acceptable percentage share (tau). The utility of accepting is simply the difference between the offered percentage share (splitperc_self) and tau. Acceptance probability rises sharply from 0 to 1 at this threshold, ensuring robust parameter recovery and interpretability by capturing abrupt behavioral cutoffs observed in responder data.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, tau=50):\n    \"\"\"\n    Simulates a simple threshold model for ultimatum game decisions.\n\n    Args:\n        trial_data (list of dict): A list of dictionaries, where each dictionary represents a trial\n                                  and contains the 'splitperc_self' key.\n        tau (float): The minimum acceptable percentage share for the participant (threshold).\n\n    Returns:\n        list of float: A list of model predictions (utilities) for each trial.\n    \"\"\"\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)  # Default to 0 if missing\n        utility = splitperc_self - tau\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8186813186813187,
  "group_accuracies": {
    "Control": 0.8481912144702842,
    "Cocaine": 0.7922453703703703
  },
  "group_parameter_averages": {
    "Control": {
      "tau": 22.64759511316758
    },
    "Cocaine": {
      "tau": 32.03462262382879
    }
  },
  "fitting_results": [
    {
      "tau": 45.352379014135195,
      "success": true,
      "log_likelihood": -0.05716311444679255,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 29.999999740949065,
      "success": true,
      "log_likelihood": -49.240009753650455,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 29.348621092191486,
      "success": true,
      "log_likelihood": -53.91899822776523,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 32.50005571909061,
      "success": true,
      "log_likelihood": -35.953307221202735,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 31.43262650432652,
      "success": true,
      "log_likelihood": -67.89477092948094,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 25.651378060212593,
      "success": true,
      "log_likelihood": -23.918869782273454,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 0.0,
      "success": true,
      "log_likelihood": -0.00013620554845160133,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 24.999826329423463,
      "success": true,
      "log_likelihood": -29.239742309741025,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 0.0,
      "success": true,
      "log_likelihood": -0.00013620554845160125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 33.567521310985605,
      "success": true,
      "log_likelihood": -47.89465474308196,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 31.432559341661147,
      "success": true,
      "log_likelihood": -32.89478991574033,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 41.61830947265,
      "success": true,
      "log_likelihood": -7.71215003193686,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 32.50003763217863,
      "success": true,
      "log_likelihood": -35.953333872536476,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 0.0,
      "success": true,
      "log_likelihood": -0.00013620554845160133,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 0.0,
      "success": true,
      "log_likelihood": -0.0001362055484516013,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 0.0,
      "success": true,
      "log_likelihood": -0.0001362055484516013,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 0.0,
      "success": true,
      "log_likelihood": -0.00013620554845160133,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 19.292033236715,
      "success": true,
      "log_likelihood": -3.8395262422282896,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 18.38166840657361,
      "success": true,
      "log_likelihood": -17.712149595031896,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 1.4975304414908331e-16,
      "success": true,
      "log_likelihood": -0.00013620554845160136,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 28.56773859895511,
      "success": true,
      "log_likelihood": -52.894780226082204,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 19.29210119637781,
      "success": true,
      "log_likelihood": -18.839521744695055,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 34.348624819430704,
      "success": true,
      "log_likelihood": -13.918869818463115,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 29.34851277383463,
      "success": true,
      "log_likelihood": -33.919002482254754,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 29.99994005769702,
      "success": true,
      "log_likelihood": -74.24000316339435,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 27.500514136029086,
      "success": true,
      "log_likelihood": -45.95330735489823,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 28.567390053187665,
      "success": true,
      "log_likelihood": -7.894790568713572,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 36.43364409079187,
      "success": true,
      "log_likelihood": -27.893652978901347,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 18.381795590502783,
      "success": true,
      "log_likelihood": -32.71213812691518,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 21.423814526716882,
      "success": true,
      "log_likelihood": -52.88507047145408,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 0.0,
      "success": true,
      "log_likelihood": -0.00013620554845160127,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 45.3523763840346,
      "success": true,
      "log_likelihood": -0.05716311444658901,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 32.50002358084151,
      "success": true,
      "log_likelihood": -25.953334054492206,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 33.5674707980275,
      "success": true,
      "log_likelihood": -27.894732837541117,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 0.0,
      "success": true,
      "log_likelihood": -0.00013620554845160127,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 25.65137364929722,
      "success": true,
      "log_likelihood": -13.918869818460397,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 24.3481651923815,
      "success": true,
      "log_likelihood": -18.91848789660424,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 30.000008616725452,
      "success": true,
      "log_likelihood": -59.24000755102652,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 24.999647187746046,
      "success": true,
      "log_likelihood": -59.23941315365125,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 14.6416234955511,
      "success": true,
      "log_likelihood": -25.05716093853908,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 26.43325167440832,
      "success": true,
      "log_likelihood": -52.89465456691039,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 38.576378725523895,
      "success": true,
      "log_likelihood": -42.88507101590656,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 7.835178857052334,
      "success": true,
      "log_likelihood": -22.138222711760243,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Control",
      "n_trials": 36
    },
    {
      "tau": 28.567816405283388,
      "success": true,
      "log_likelihood": -17.894781439069078,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 26.432494308506566,
      "success": true,
      "log_likelihood": -27.894732780434506,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 80.57563076756627,
      "success": true,
      "log_likelihood": -598.6705457943452,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.2777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 29.348542081070303,
      "success": true,
      "log_likelihood": -18.919003638596937,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 33.56761208774054,
      "success": true,
      "log_likelihood": -52.89457723076649,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 32.500090451292046,
      "success": true,
      "log_likelihood": -75.95327984103675,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 29.383586188144395,
      "success": true,
      "log_likelihood": -83.86818389823334,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 24.348192108642056,
      "success": true,
      "log_likelihood": -53.91768623338188,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 40.70796613971565,
      "success": true,
      "log_likelihood": -8.839526212256118,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 29.34858767700507,
      "success": true,
      "log_likelihood": -28.91900362556605,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 27.500154111721095,
      "success": true,
      "log_likelihood": -70.95328004575624,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 34.18267353461003,
      "success": true,
      "log_likelihood": -121.6898981125274,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 35.00017159215833,
      "success": true,
      "log_likelihood": -39.239740106719985,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 36.43363924914359,
      "success": true,
      "log_likelihood": -22.893653040678018,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 9.487175408068342,
      "success": true,
      "log_likelihood": -80.83797221647215,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 41.6183010442755,
      "success": true,
      "log_likelihood": -2.7121501063476967,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 20.636476784738072,
      "success": true,
      "log_likelihood": -28.897570852877507,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 26.43378633862485,
      "success": true,
      "log_likelihood": -57.89336337165138,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 29.9999939604401,
      "success": true,
      "log_likelihood": -44.2400075955533,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 45.352375026781736,
      "success": true,
      "log_likelihood": -0.05716311444663601,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 31.616815914082753,
      "success": true,
      "log_likelihood": -42.69445549165085,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 38.576224161918525,
      "success": true,
      "log_likelihood": -7.885081213886749,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 28.567391746088596,
      "success": true,
      "log_likelihood": -17.894790042555872,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 14.647697902434448,
      "success": true,
      "log_likelihood": -20.057163051517595,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 19.292028812388196,
      "success": true,
      "log_likelihood": -33.83951621513734,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 28.56739671877732,
      "success": true,
      "log_likelihood": -22.894790498988762,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 10.69272057536458,
      "success": true,
      "log_likelihood": -44.93406943772357,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 41.61832651937705,
      "success": true,
      "log_likelihood": -12.712149669330092,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 40.70796709396274,
      "success": true,
      "log_likelihood": -3.839526242228742,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 29.36580542851899,
      "success": true,
      "log_likelihood": -38.89381740021348,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 21.423797020721025,
      "success": true,
      "log_likelihood": -37.8850715285982,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 29.406294495602673,
      "success": true,
      "log_likelihood": -89.40448952981708,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 24.348168425208478,
      "success": true,
      "log_likelihood": -18.918487896606138,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 10.688240675833155,
      "success": true,
      "log_likelihood": -26.909930967299665,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 50.69266112808635,
      "success": true,
      "log_likelihood": -24.935062699528054,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 26.432418125622373,
      "success": true,
      "log_likelihood": -102.89047164200188,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 45.352377048589986,
      "success": true,
      "log_likelihood": -0.05716311444660334,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 29.34796942581669,
      "success": true,
      "log_likelihood": -78.91899498579193,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 14.654282933767004,
      "success": true,
      "log_likelihood": -50.05624070229381,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 41.61833387868527,
      "success": true,
      "log_likelihood": -17.712149595077925,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 29.99999987140292,
      "success": true,
      "log_likelihood": -9.240012045337844,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 25.81840451217819,
      "success": true,
      "log_likelihood": -116.68929576214927,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 41.61830846016165,
      "success": true,
      "log_likelihood": -2.7121501063201214,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 24.34826226885906,
      "success": true,
      "log_likelihood": -58.91831326525711,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 38.576525909006214,
      "success": true,
      "log_likelihood": -72.88498316599167,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 83.25821050045808,
      "success": true,
      "log_likelihood": -782.8788236495805,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.05555555555555555,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 30.65147337721999,
      "success": true,
      "log_likelihood": -8.919007843237717,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    },
    {
      "tau": 24.34851774812146,
      "success": true,
      "log_likelihood": -68.91797678608239,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "group": "Cocaine",
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "tau": {
        "r": 0.915993565511346,
        "p": 1.1779335717474668e-40
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 99.5672343228414,
  "bic_Control": 55.470911784231134,
  "bic_Cocaine": 139.07018993034643,
  "bic_results": {
    "average_bic": 99.5672343228414,
    "individual_bics": [
      3.697845167349695,
      102.06353844575702,
      111.42151539398657,
      75.49013338086158,
      139.373060797418,
      51.42125850300302,
      3.583791349553013,
      62.06300355793816,
      3.583791349553013,
      99.37282842462002,
      69.37309876993677,
      19.00781900232983,
      75.49018668352906,
      3.583791349553013,
      3.583791349553013,
      3.583791349553013,
      3.583791349553013,
      11.26257142291269,
      39.0078181285199,
      3.583791349553013,
      109.37307939062052,
      41.26256242784622,
      31.42125857538234,
      71.42152390296562,
      152.0635252652448,
      95.49013364825257,
      19.373100075883254,
      59.3708248962588,
      69.00779519228647,
      109.35365988136427,
      3.583791349553013,
      3.697845167349288,
      55.49018704744052,
      59.37298461353834,
      3.583791349553013,
      31.421258575376903,
      41.42049473166459,
      122.06353404050915,
      122.0623452457586,
      53.69784081553427,
      109.37282807227689,
      89.35366097026923,
      47.859964361976594,
      39.373081816594265,
      59.37298449932512,
      1200.9246105271466,
      41.42152621564998,
      109.37267339998908,
      155.4900786205296,
      171.3198867349228,
      111.41889140521987,
      21.262571362968345,
      61.42152618958821,
      145.4900790299686,
      246.9633151635109,
      82.06299915189608,
      49.370825019812145,
      165.25946337140041,
      9.007819151151503,
      61.378660644211124,
      119.37024568175887,
      92.06353412956271,
      3.697845167349382,
      88.97242992175781,
      19.35368136622961,
      39.37309902356785,
      43.6978450414913,
      71.26255136873078,
      49.37309993643363,
      93.45165781390325,
      29.007818277116293,
      11.262571422913595,
      81.37115373888307,
      79.35366199565252,
      182.39249799809028,
      41.420494731668384,
      57.40338087305544,
      53.453644337512216,
      209.36446222245988,
      3.6978451673493167,
      161.42150891003996,
      103.69600034304374,
      39.00781812861196,
      22.063543029131797,
      236.96211046275465,
      9.007819151096353,
      121.42014546897033,
      149.35348527043945,
      1569.341166237617,
      21.421534624931542,
      141.4194725106209
    ],
    "group_enabled": true,
    "group_bics": {
      "Control": [
        3.697845167349695,
        102.06353844575702,
        111.42151539398657,
        75.49013338086158,
        139.373060797418,
        51.42125850300302,
        3.583791349553013,
        62.06300355793816,
        3.583791349553013,
        99.37282842462002,
        69.37309876993677,
        19.00781900232983,
        75.49018668352906,
        3.583791349553013,
        3.583791349553013,
        3.583791349553013,
        3.583791349553013,
        11.26257142291269,
        39.0078181285199,
        3.583791349553013,
        109.37307939062052,
        41.26256242784622,
        31.42125857538234,
        71.42152390296562,
        152.0635252652448,
        95.49013364825257,
        19.373100075883254,
        59.3708248962588,
        69.00779519228647,
        109.35365988136427,
        3.583791349553013,
        3.697845167349288,
        55.49018704744052,
        59.37298461353834,
        3.583791349553013,
        31.421258575376903,
        41.42049473166459,
        122.06353404050915,
        122.0623452457586,
        53.69784081553427,
        109.37282807227689,
        89.35366097026923,
        47.859964361976594
      ],
      "Cocaine": [
        39.373081816594265,
        59.37298449932512,
        1200.9246105271466,
        41.42152621564998,
        109.37267339998908,
        155.4900786205296,
        171.3198867349228,
        111.41889140521987,
        21.262571362968345,
        61.42152618958821,
        145.4900790299686,
        246.9633151635109,
        82.06299915189608,
        49.370825019812145,
        165.25946337140041,
        9.007819151151503,
        61.378660644211124,
        119.37024568175887,
        92.06353412956271,
        3.697845167349382,
        88.97242992175781,
        19.35368136622961,
        39.37309902356785,
        43.6978450414913,
        71.26255136873078,
        49.37309993643363,
        93.45165781390325,
        29.007818277116293,
        11.262571422913595,
        81.37115373888307,
        79.35366199565252,
        182.39249799809028,
        41.420494731668384,
        57.40338087305544,
        53.453644337512216,
        209.36446222245988,
        3.6978451673493167,
        161.42150891003996,
        103.69600034304374,
        39.00781812861196,
        22.063543029131797,
        236.96211046275465,
        9.007819151096353,
        121.42014546897033,
        149.35348527043945,
        1569.341166237617,
        21.421534624931542,
        141.4194725106209
      ]
    },
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- tau: r = 0.916",
  "previous_models": [
    "Specification: U = splitperc_self - tau\nSummary: A strictly minimal, interpretable model where the responder\u2019s choice is governed by a single parameter: the minimum acceptable percentage share (tau). The utility of accepting is simply the difference between the offered percentage share (splitperc_self) and tau. Acceptance probability rises sharply from 0 to 1 at this threshold, ensuring robust parameter recovery and interpretability by capturing abrupt behavioral cutoffs observed in responder data.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 99.5672343228414\nOverall Accuracy: 0.8187\nGroup Accuracies:\n- Control: 0.8482\n- Cocaine: 0.7922\n\nParameter Recovery:\n- tau: r = 0.916"
  ]
}