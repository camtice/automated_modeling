run_number,average_bic,bic_control,bic_cocaine,overall_accuracy,model_specification,model_summary,version,instructions_used,alpha_disadv_recovery,alpha_adv_recovery,intercept_bias_recovery,alpha_recovery
2,30.417160847596072,24.759394368365047,35.485576651907195,0.7863247863247863,U = intercept_bias + alpha × log(split_self / (combined_earning × (token_self / (token_self + token_opp)))),"A two-parameter model in which responders evaluate offers using the log-ratio of proposed share to their deserved share based on contribution, plus a baseline acceptance bias. This structure ensures high parameter identifiability, as the intercept and fairness sensitivity have distinct, non-collinear influences. It accommodates both proportional and absolute fairness differences, scales across different pot sizes, and is optimized for both accuracy and parameter recovery.",v2,"<think>
Reviewing the previous run results, we observe that parameter recovery is inadequate (r≈0.5 for both parameters). BIC is rather high, and accuracy only moderate. This suggests a model with learnable parameters that are either collinear (covarying), or that the mathematical structure makes it difficult to tease apart their individual contributions. The model penalized both disadvantageous and advantageous inequity separately, but evidence often suggests participants are more sensitive to disadvantageous inequity, and the separation may make parameter estimation noisy.

To improve, we want:
- Lower parameter redundancy/covariance (for recovery).
- Potentially a model structure where effects of fairness and self-interest interact, not just add.
- A baseline bias/threshold parameter (people may have default tendencies to accept/reject, regardless of fairness/value).
- Careful reasoning about nonlinearity (e.g., using proportional utility, or ratios, or thresholding) can help identifiability.
- Encourage considering non-additive or alternative norms (e.g. absolute vs proportional, reference-dependent, etc).
- Encourage modeling with no more than 2-3 parameters, but with at least one controlling overall bias/sensitivity.
- Push for use of the existing data (especially mapping token contributions and offers to perceived fairness), but try psychological mechanisms beyond ""vanilla"" inequity aversion.

Specific instructions should encourage: (1) reasoning about bias/intercept or alternatives to double-parameter inequity aversion, (2) alternatives in functional form (e.g., nonlinear, ratio, sigmoidal), (3) justification for parameter choice based on improving both fit and recoverability.

</think>

For this run, do the following:

1. Explicitly consider the potential for high covariance between parameters; if introducing more than one learnable parameter, justify how your model structure helps distinguish their influence.
2. Carefully consider whether both advantageous and disadvantageous inequity aversion should be separate parameters, or whether another structure (e.g., a single inequity aversion parameter with an adjustable bias/intercept term) would be more identifiable.
3. Strongly consider including a bias/intercept parameter in your model to capture baseline acceptance/rejection tendencies, which may be more recoverable and improve prediction accuracy.
4. Encourage trying at least one non-additive or nonlinear approach to mapping contributions and/or splits to utility (e.g., ratio of offer to proportional share, or using a threshold, or non-linear penalty).
5. Before producing the final model, explicitly discuss (step by step) how each proposed component or parameter will help with parameter recovery, accuracy, and model identifiability.
6. Keep the total number of learnable parameters to no more than 3, and justify each.

First, step through:
- Identifying key psychological/decision processes (including acceptance bias, fairness evaluation, and value sensitivity).
- Exploring whether nonlinear, ratio, or threshold forms could model behavior better than simple additivity.
- Determining, for each proposed parameter, how its effect on the model can be clearly distinguished from the others.

Then, propose your model in the required format. Use clear notation. For each learnable parameter, provide a thoughtful explanation of why it is needed and how it supports both model fit and identifiability. Prioritize designs likely to yield high parameter recovery, low BIC, and higher predictive accuracy over simple psychological fidelity.

Think creatively—consider, for example, models that compare the offered share to a fairness norm by ratio rather than difference, or that include decision thresholds, or non-linear transforms. If you use more than one fairness-related parameter, explicitly describe why and how their effects can be empirically separated using the available data.

Remember: parameter recovery (identifiability) is as important as accuracy.",,,0.896437569252736,0.8493782482144077
1,104.72196558814753,89.32337808405481,118.51653356056393,0.5802808302808303,"tags.

---

# Final Model Specification

<MODEL>
U = split_self - alpha_disadv * max(proportional_share - split_self, 0) - alpha_adv * max(split_self - proportional_share, 0)","A utility model of responder behavior with two learnable parameters quantifying aversion to disadvantageous and advantageous inequity. The model computes the participant's utility for accepting an offer as the sum of the offered amount, minus a penalty for receiving less than their proportional share (weighted by alpha_disadv), and a penalty for receiving more than their proportional share (weighted by alpha_adv). Utility is transformed via a logistic function for binary prediction of accept/reject decisions. This model balances psychological fidelity with parameter identifiability.",v2,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",0.5581926071808277,0.5073155411372332,,
