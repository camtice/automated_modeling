run_number,average_bic,bic_control,bic_cocaine,overall_accuracy,model_specification,model_summary,version,instructions_used,w_gain_recovery,w_penalty_token_recovery,beta_0_recovery,beta_deviation_sensitivity_recovery,beta_intercept_recovery,beta_amount_recovery,beta_fairness_penalty_recovery,gamma_contribution_weight_recovery,beta_self_gain_recovery,beta_inequity_aversion_recovery,sensitivity_own_payoff_recovery,aversion_disadvantageous_inequality_recovery,aversion_advantageous_inequality_recovery,beta_percentage_recovery,intercept_recovery,alpha_param_recovery,beta_param_recovery,omega_0_recovery,omega_1_recovery,omega_2_recovery,beta_prop_recovery
5,34.62718157445364,27.89853510305991,40.654927371743845,0.8134920634920634,U_accept = beta_0 + beta_self_gain * split_self - beta_inequity_aversion * disadvantageous_inequity_amount,"A utility model predicting the decision to accept or reject an ultimatum game offer. The utility of accepting is calculated as a linear combination of a base utility, the participant's proposed monetary gain, and a penalty for disadvantageous inequity. Disadvantageous inequity is quantified as the amount by which the proposed share for the participant falls below an equitable share determined by their token contributions. The model incorporates three learnable parameters: a base tendency (beta_0), sensitivity to monetary gain (beta_self_gain), and sensitivity to disadvantageous inequity (beta_inequity_aversion).",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",,,0.7881054238808572,,,,,,0.7509425830879802,0.7347410136156473,,,,,,,,,,,
2,63.28304006828402,50.00300207538768,75.17974077025367,0.8244810744810744,"U_accept = w_gain * split_perc_self - w_penalty_token * max(0, perc_fair_tokens - split_perc_self)","A utility model predicting responder acceptance based on a combination of self-interest and fairness concerns. Utility increases linearly with the percentage offered to the participant (weighted by `w_gain`). A penalty is applied if the offered percentage falls below the percentage deemed fair based on token contributions, with the magnitude of the penalty weighted by `w_penalty_token`. The model uses two learnable, non-negative parameters constrained within [0, 10]. The utility maps to acceptance probability via a logistic function with temperature 1.",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",0.6005983787225211,0.6291338819721518,,,,,,,,,,,,,,,,,,,
3,66.06436979723472,62.09735892162273,69.61815037330379,0.7783882783882783,utility_accept = beta_0 + beta_deviation_sensitivity * (split_percentage_self - 100 * token_self_proportion),"A utility model predicting responder acceptance based on self-interest and contribution-based fairness. Utility to accept is a linear function of a baseline parameter (beta_0) and a term capturing the deviation of the proposed percentage from the participant's proportional contribution percentage, scaled by a sensitivity parameter (beta_deviation_sensitivity). Utility is converted to acceptance probability via a logistic function (temperature 1). The model uses two learnable parameters with generous finite bounds [-10, 10].",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",,,0.5894506245819425,0.9001833210560056,,,,,,,,,,,,,,,,,
10,68.56777725657766,60.941609279713916,75.39955273585142,0.768009768009768,Utility = split_self - beta_prop * proportional_fair_amount,"A single-parameter computational model predicting responder acceptance utility. The utility of accepting an offer is calculated as the proposed monetary amount for the participant minus a weighted reference point. The reference point is the calculated amount corresponding to a proportional split of the total earnings based on initial token contributions. The learnable parameter, beta_prop, scales this proportional fair amount reference point, capturing individual differences in the demand for contribution-based equity when evaluating offers.",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",,,,,,,,,,,,,,,,,,,,,0.5958280385451813
9,104.07394883177831,84.52197091487177,121.58926238234041,0.8495115995115995,utility_accept = omega_0 + omega_1 * split_perc_self + omega_2 * (split_perc_self - token_ratio_self_perc_ref),"A linear utility model predicting the decision to accept or reject an ultimatum game offer. Utility is based on a baseline, the percentage of the total earning proposed to the participant, and the difference between the proposed percentage and a fairness reference percentage derived from relative token contributions. The model uses three learnable parameters: an intercept (omega_0), sensitivity to the proposed percentage (omega_1), and sensitivity to the deviation from the contribution-based fairness reference (omega_2). The probability of acceptance is determined by a logistic function applied to the utility.",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",,,,,,,,,,,,,,,,,,0.6017578693662837,0.6385410227465695,0.6786733969925752,
4,127.65733465858514,93.02927395879746,158.67830570214494,0.7753357753357754,"contribution_percentage = token_self / (token_self + token_opp)
reference_percentage = gamma_contribution_weight * contribution_percentage + (1 - gamma_contribution_weight) * 0.5
underpayment_penalty_driver = max(0, reference_percentage - split_perc_self)
utility = beta_intercept + beta_amount * split_self - beta_fairness_penalty * underpayment_penalty_driver","A utility model predicting the responder's acceptance decision based on the proposed monetary amount and a fairness calculation influenced by individual contributions. The model incorporates a baseline utility (beta_intercept), sensitivity to the proposed amount (beta_amount), and a penalty for offers perceived as unfair relative to a reference percentage. The fairness reference point is a blend of an equal split (50%) and the participant's contribution percentage, weighted by a learnable parameter (gamma_contribution_weight). The sensitivity to this fairness penalty is controlled by beta_fairness_penalty. Utility is converted to probability of acceptance using a logistic function.",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",,,,,0.6792677134010678,0.3135240933397631,0.06635089331200406,0.008234442342922508,,,,,,,,,,,,,
8,163.08694859611347,162.4627632735461,163.64611461424676,0.7503052503052502,"U_accept = split_self - alpha_param * max(0, fair_amount_equal - split_self) - beta_param * max(0, split_self - fair_amount_equal)
fair_amount_equal = combined_earning / 2","A utility-based model predicting acceptance in the responder role of an ultimatum game. The utility of accepting a proposal is calculated as the proposed amount for the self, penalized by deviations from an equal split of the total earnings. The model includes two learnable parameters: 'alpha_param' for sensitivity to disadvantageous inequality and 'beta_param' for sensitivity to advantageous inequality, both constrained to [0, 100]. The utility is converted to a probability of acceptance using a logistic function, assuming the utility of rejecting is zero.",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",,,,,,,,,,,,,,,,0.10196164679502294,0.31804198601131894,,,,
6,250.1418092424669,231.8325572789276,266.5438474598042,0.7985347985347986,"U = sensitivity_own_payoff * split_self - aversion_disadvantageous_inequality * disadvantageous_inequality - aversion_advantageous_inequality * advantageous_inequality
disadvantageous_inequality = max(0, split_opp - split_self)
advantageous_inequality = max(0, split_self - split_opp)","A utility model for responder decisions based on the proposed monetary split. The model calculates utility as a weighted sum of the participant's own proposed share and terms representing aversion to disadvantageous and advantageous inequality. It includes three learnable parameters: sensitivity to own payoff (sensitivity_own_payoff), aversion to disadvantageous inequality (aversion_disadvantageous_inequality), and aversion to advantageous inequality (aversion_advantageous_inequality). Utility is converted to probability of acceptance using a logistic function.",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",,,,,,,,,,,0.37892217548879537,0.43711909428511064,0.07882082808007551,,,,,,,,
7,397.2324916015356,420.28163853856375,376.58429747044795,0.7191697191697192,U = beta_amount * split_self + beta_percentage * split_perc_self + intercept,"A utility model for the responder's acceptance decision in an ultimatum game. The utility of accepting a proposal is a linear combination of the absolute amount offered to the participant (split_self) and the percentage of the total pot offered to the participant (split_perc_self), plus a baseline intercept. The model includes three learnable parameters: sensitivity to absolute amount (beta_amount), sensitivity to percentage (beta_percentage), and a baseline (intercept), all bounded within [-10, 10]. Utility is mapped to the probability of acceptance via a logistic function.",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",,,,,,0.0034835346361041866,,,,,,,,0.756473369410132,-0.04260471503798537,,,,,,
1,,,,,"` section and vice-versa, and variable names are Python-friendly. Define intermediate calculated variables used in the model formula within the `<VARIABLES>` block as well. Adjust data variable ranges to be generously wide based on the description (e.g., total tokens could be up to 100+100=200, total earning £600, split £0-600). Using [0, 400] for split_self and [0, 100] for tokens seems a safe, generous range assuming tokens are per participant and total is not excessively large, aligning with 'a number of tokens'. Let's stick with the slightly larger range considered in thinking: [0, 400] for `split_self` and [0, 100] for tokens, which supports `combined_earning` up to £300.

This thought process leads directly to the provided model structure, variable definitions, and parameter bounds.

```xml
<MODEL>
fair_share_self_percentage = (token_self / (token_self + token_opp)) * 100
fairness_deviation = split_perc_self - fair_share_self_percentage
utility_accept = beta_intercept + beta_split_self * split_self + beta_fairness_positive_deviation * max(0, fairness_deviation) + beta_fairness_negative_deviation * max(0, -fairness_deviation)","A utility-based model predicting responder acceptance decisions based on the monetary amount offered and the fairness of the offer relative to contribution. The utility of accepting is a function of a baseline bias, a parameter controlling sensitivity to the absolute amount received, and parameters controlling sensitivity to receiving more or less than the contribution-proportional fair percentage share. Utility is converted to acceptance probability using a logistic function. The model has four learnable parameters: beta_intercept, beta_split_self, beta_fairness_positive_deviation, and beta_fairness_negative_deviation.",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",,,,,,,,,,,,,,,,,,,,,
