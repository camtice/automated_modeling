{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility should be modeled such that a utility less than 0 corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The responder's decision (accept or reject) is the target variable. The main factors influencing this decision likely involve fairness considerations, the actual split proposed, and the contributions of both the participant and the opponent. \n\nThe dataset includes variables like token_self and token_opp, which represent the contributions each made. The combined_earning is the total money, which is 3*(token_self + token_opp). The split_self and split_opp are the proposed splits in pounds. The trial_type indicates whether contributions were equal, opponent more, or participant more. \n\nSo, the responder's utility probably depends on how fair the proposed split is relative to their own contribution. For example, if the participant contributed more tokens, they might expect a larger share. The model should capture this expectation and compare it to the actual proposal.\n\nA common approach in such scenarios is to model the utility as a function of the difference between the proposed split and some reference point, which could be based on equity (contribution-based) or equality (equal split). \n\nLet me think about possible parameters. One parameter could be the weight given to equity considerations versus equality. Another could be the sensitivity to unfairness, either when the participant is getting less than their fair share or when they're getting more. Also, there might be a baseline acceptance tendency.\n\nBut since the model needs to have a manageable number of parameters to avoid high covariance, maybe limit to 2-3 learnable parameters. \n\nHere's an outline of the model components:\n\n1. Equity Fairness: The fair share based on contributions. The participant's fair share would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair share could be a factor.\n\n2. Equality Fairness: The equal split (50% of combined_earning). The difference between split_self and 50% could be another factor.\n\n3. The utility could be a weighted combination of these two fairness components. For example, a parameter alpha determines the weight given to equity vs equality. \n\n4. Additionally, participants might have different sensitivities to being under-benefited vs over-benefited. For example, they might dislike being under-benefited more than they care about being over-benefited. So, maybe a parameter beta that scales the negative difference (when split_self is less than fair share) more than the positive.\n\nBut to keep it simple, maybe model the utility as the difference between the proposed split and the fair share (equity), and also include a term for the absolute difference from equality. Alternatively, use a nonlinear function like Fehr-Schmidt inequality aversion, but that might require more parameters.\n\nAlternatively, use a linear combination of the proposed split's deviation from equity and equality. Let's formalize this.\n\nLet\u2019s define:\n\n- fair_equity = (token_self / (token_self + token_opp)) * combined_earning\n- fair_equality = 0.5 * combined_earning\n\nThen, the utility could be a function of how much the proposed split_self deviates from these two references. For example:\n\nU = \u03b8 * (split_self - fair_equity) + (1 - \u03b8) * (split_self - fair_equality) \n\nHere, theta is a learnable parameter between 0 and 1 that determines the weight given to equity versus equality. If theta is 1, the responder only cares about equity; if 0, only equality.\n\nBut maybe participants have different sensitivities. Also, the model needs to account for the fact that negative deviations (split_self < fair) might have a stronger effect. So perhaps a parameter that scales the negative part.\n\nAlternatively, model the utility as:\n\nU = alpha * (split_self - beta * fair_equity) + gamma * (split_self - 0.5 * combined_earning)\n\nBut this introduces multiple parameters (alpha, beta, gamma), which could complicate parameter recovery.\n\nAlternatively, use a single parameter that captures the trade-off between equity and equality. Let's say:\n\nU = (split_self - ( (1 - alpha) * 0.5 + alpha * (token_self / total_tokens) ) * combined_earning )\n\nHere, alpha is between 0 and 1. When alpha=0, the reference is 50% (equality); when alpha=1, it's the equity share. The utility is the difference between the proposed split and a weighted average of equity and equality.\n\nThis way, alpha determines how much the responder weighs their own contribution versus an equal split. The utility would be positive if the split is above the weighted reference, leading to higher acceptance probability.\n\nBut maybe participants also have a non-linear sensitivity. For example, they might be more sensitive to receiving less than their reference, compared to receiving more. So, perhaps include a parameter that differentiates between under- and over-compensation.\n\nThis could be modeled using an asymmetry parameter. For example:\n\ndifference = split_self - reference\nU = difference * (1 + delta * (difference < 0))\n\nHere, delta is a parameter that amplifies the negative difference. If delta is positive, negative differences (split_self < reference) reduce utility more than positive differences increase it. But this adds another parameter.\n\nAlternatively, use a piecewise linear function. For example:\n\nIf split_self >= reference:\n    U = split_self - reference\nElse:\n    U = lambda * (split_self - reference)\n\nWhere lambda > 1 would mean that negative differences are penalized more. But this introduces lambda as a learnable parameter.\n\nBut considering the need to limit parameters, perhaps start with a simpler model. Let's consider the first approach with alpha as the weight between equity and equality, and a single parameter for the reference point. Then, the utility is:\n\nreference = alpha * (token_self / total_tokens) * combined_earning + (1 - alpha) * 0.5 * combined_earning\nU = split_self - reference\n\nBut this is linear. Then, the probability of accepting is logistic(U). If U > 0, probability > 0.5; if U < 0, probability < 0.5. This meets the requirement that utility < 0 corresponds to <50% chance.\n\nBut maybe participants also have a baseline tendency to accept or reject, independent of the split. So adding an intercept term, say beta, which captures this bias. For example:\n\nU = beta + (split_self - reference)\n\nBeta could be a learnable parameter. If beta is positive, the participant is more likely to accept even if the split is slightly below the reference. If beta is negative, they are more likely to reject even if the split meets the reference.\n\nSo now, two parameters: alpha (weight between equity and equality) and beta (bias).\n\nBut wait, the combined_earning is 3*(token_self + token_opp), so token_self + token_opp = combined_earning / 3. Therefore, token_self / (token_self + token_opp) = token_self * 3 / combined_earning. So the equity reference can be rewritten as (token_self * 3 / combined_earning) * combined_earning = 3 * token_self. Wait, no. Wait, combined_earning is 3*(token_self + token_opp). So token_self / (token_self + token_opp) is token_self / (combined_earning / 3). So the equity share is (token_self / (combined_earning / 3)) * combined_earning = 3 * token_self. Wait, that can't be right. Let me recalculate:\n\nWait, combined_earning = 3*(token_self + token_opp). So the equity share would be (token_self / (token_self + token_opp)) * combined_earning = (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3 * token_self. So the equity share is exactly 3 * token_self. Similarly, the equality share is 0.5 * combined_earning = 1.5*(token_self + token_opp).\n\nWait, that's interesting. So the equity-based fair share is 3 * token_self, and the equality-based is 1.5*(token_self + token_opp). Then, the reference point is a weighted average of these two. So:\n\nreference = alpha * (3 * token_self) + (1 - alpha) * (1.5 * (token_self + token_opp))\n\nThen, the utility is split_self - reference + beta.\n\nBut split_self is the proposed amount for the participant. So the utility is how much they get compared to their reference. If split_self is higher than the reference, utility is positive. If lower, negative.\n\nBut this seems a bit counterintuitive. Let's see. For example, if alpha is 1 (full equity), then reference is 3* token_self. So if the participant contributed 10 tokens, their fair share is \u00a330. If the proposed split_self is \u00a325, then utility is 25 -30 + beta = -5 + beta. If beta is 0, this is negative, leading to rejection. If beta is 5, then utility is 0, 50% chance.\n\nBut maybe the model should also consider the opponent's split. Wait, split_self + split_opp = combined_earning. So split_opp is combined_earning - split_self. But the responder's decision is based on their own split. However, the trial_type (whether the split is equal, opponent more, or participant more) might influence the responder's perception. But trial_type is derived from token contributions, so maybe it's redundant if we already use token_self and token_opp.\n\nAlternatively, the model could include a term that compares the split percentage (splitperc_self) to the equity percentage (token_self / (token_self + token_opp) * 100). But splitperc_self is already a variable, but perhaps using the actual token counts is better.\n\nWait, the equity percentage would be (token_self / (token_self + token_opp)) * 100. The split percentage is splitperc_self. The difference between these two could be a factor. For example, if the split percentage is less than the equity percentage, the responder feels treated unfairly.\n\nSo, another approach: the utility is based on the difference between the proposed percentage and the equity percentage, scaled by the combined earning. But perhaps that's similar to the previous approach.\n\nAlternatively, model the utility as:\n\nU = beta + theta * (split_self - equity_reference) + gamma * (split_self - equality_reference)\n\nBut theta and gamma are parameters. However, this introduces two parameters, which might be redundant if they are correlated. Maybe combine them into a single parameter as a weight.\n\nAlternatively, use a single parameter alpha that determines the weight between equity and equality, as before.\n\nSo, the reference point is a weighted combination of equity and equality:\n\nreference = alpha * equity_reference + (1 - alpha) * equality_reference\n\nThen, utility is split_self - reference + beta.\n\nBut equity_reference is 3 * token_self, and equality_reference is 1.5*(token_self + token_opp). So:\n\nreference = alpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)\n\nThen, utility U = split_self - reference + beta\n\nThis way, alpha (between 0 and 1) determines the weight between equity and equality. Beta is a bias term, which could be positive or negative, capturing the general tendency to accept or reject.\n\nBut wait, the split_self is in pounds, and the reference is also in pounds. So the difference is in pounds. Then, beta is in pounds as well. For example, if beta is 2, that means the participant has a baseline utility equivalent to getting \u00a32 more than their reference. So even if the split is \u00a32 below the reference, the utility would be zero (50% chance).\n\nBut perhaps beta should be scaled differently. Alternatively, the model could have beta as a scaling factor for the difference. But maybe it's okay as is.\n\nNow, the parameters to learn are alpha (weight between equity and equality) and beta (bias). These parameters need to have bounds. For alpha, between 0 and 1. For beta, perhaps between -K and K, where K is a reasonable value. Since combined_earning can vary, but split_self is up to combined_earning. For example, if combined_earning is up to, say, 3*(36 + 36) = 216 (assuming each trial has up to 36 tokens each?), but the actual maximum would depend on the data. But beta's range should be generous. Maybe set beta between -10 and 10, which allows for a bias of up to \u00a310 in either direction.\n\nBut the problem states that the utility variable may be unbounded, but the parameters must have finite bounds. So alpha is between 0 and 1 (inclusive), and beta between, say, -20 and 20 (inclusive). These are generous bounds.\n\nAnother consideration: the model should use variables available in the dataset. The variables used here are token_self, token_opp, combined_earning, split_self. All of these are present in the dataset.\n\nNow, the utility function would be:\n\nU = split_self - [alpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)] + beta\n\nSimplify the terms inside the brackets:\n\nalpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)\n\nLet me expand this:\n\n= 3 alpha token_self + 1.5 (1 - alpha) (token_self + token_opp)\n\n= 3 alpha token_self + 1.5 (1 - alpha) token_self + 1.5 (1 - alpha) token_opp\n\nCombine like terms:\n\ntoken_self [3 alpha + 1.5 (1 - alpha)] + 1.5 (1 - alpha) token_opp\n\n= token_self [3 alpha + 1.5 - 1.5 alpha] + 1.5 (1 - alpha) token_opp\n\n= token_self [1.5 alpha + 1.5] + 1.5 (1 - alpha) token_opp\n\nFactor out 1.5:\n\n= 1.5 [ token_self (alpha + 1) + (1 - alpha) token_opp ]\n\nHmm, interesting. So the reference can be written as 1.5 times [ token_self (alpha +1) + token_opp (1 - alpha) ]\n\nBut maybe this isn't necessary. The key point is that the reference is a combination of equity and equality.\n\nSo the utility U is:\n\nU = split_self - 1.5 [ alpha * 2 * token_self + (1 - alpha) (token_self + token_opp) ] + beta\n\nWait, perhaps not. Let me re-express the original reference:\n\nreference = alpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)\n\nBut 3 * token_self is the equity share (since each token is \u00a33, so token_self *3 is the participant's contribution in \u00a3, and combined_earning is 3*(token_self + token_opp). Wait, no. Wait, the equity share would be (participant's contribution / total contribution) * combined_earning. The participant's contribution is token_self *3, and the total contribution is (token_self + token_opp)*3. So the equity share is (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so this simplifies to 3*token_self. So yes, the equity share is 3*token_self. The equality share is 0.5*combined_earning = 1.5*(token_self + token_opp).\n\nSo the reference is a weighted average of equity and equality. Then, the utility is split_self minus this reference plus a bias term beta.\n\nThis model has two learnable parameters: alpha (weight between equity and equality) and beta (bias). The parameters alpha is bounded between 0 and 1, beta between -20 and 20. These ranges are generous enough to capture various behaviors.\n\nThe probability of acceptance is then calculated using the logistic function: P(accept) = 1 / (1 + exp(-U)). So when U is positive, the probability is greater than 0.5; when negative, less than 0.5.\n\nThis model should be able to capture responders who base their decisions on a combination of equity and equality, with a personal bias. The parameters alpha and beta can be estimated from the data, and their recovery should be feasible as they are not too correlated if the experimental design includes variations in token contributions and splits.\n\nNow, checking if all variables used are present in the dataset:\n\n- split_self: yes\n- token_self: yes\n- token_opp: yes\n- combined_earning: yes (though it can be derived from token_self and token_opp, but it's provided)\n\nSo the model uses variables available in the dataset.\n\nAnother consideration: the trial_role is 1 for responder, so the model should only consider trials where trial_role is 1. But the model itself is only for responder behavior, so the data used for fitting will filter to those trials.\n\nNow, the variables in the model:\n\n- split_self: from data\n- token_self: from data\n- token_opp: from data\n- alpha: learnable parameter (0 to 1)\n- beta: learnable parameter (-20 to 20)\n- U: calculated utility\n\nThe mathematical formula would be:\n\nU = split_self - [alpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)] + beta\n\nAlternatively, simplifying:\n\nU = split_self - 1.5 * [2 * alpha * token_self + (1 - alpha) * (token_self + token_opp)] + beta\n\nBut perhaps keeping it as the original expression is clearer.\n\nNow, writing this in mathematical notation within the <MODEL> tags:\n\n<MODEL>\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\n</MODEL>\n\nWait, because 0.5 * combined_earning is the equality reference (since combined_earning is 3*(token_self + token_opp), so 0.5 * combined_earning = 1.5*(token_self + token_opp)). So this is equivalent to the previous expression. So this might be a simpler way to write it, using combined_earning directly.\n\nYes, because 0.5 * combined_earning is the equality reference. So the model can be written as:\n\nU = split_self - [alpha * (3 * token_self) + (1 - alpha) * (0.5 * combined_earning)] + beta\n\nWhich uses combined_earning from the data. This is better because combined_earning is a variable in the dataset, so we don't need to compute it from token_self and token_opp. This makes the model directly use the provided variables.\n\nSo the formula becomes:\n\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\n\nThis is cleaner and uses the available variables. So the model uses split_self, token_self, combined_earning, and the parameters alpha and beta.\n\nNow, the variables section:\n\nVariables:\n\n- split_self: from data (float64)\n- token_self: from data (int64)\n- combined_earning: from data (int64)\n- alpha: learnable parameter, range [0, 1]\n- beta: learnable parameter, range [-20, 20]\n- U: calculated utility\n\nSo in the variables JSON:\n\nFor alpha:\n- description: \"Weighting factor between equity-based (token_self) and equality-based (50% split) reference points\"\n- range: min 0, max 1, inclusive both\n- learnable: true\n- source: learnable\n\nFor beta:\n- description: \"Baseline bias term influencing acceptance tendency\"\n- range: min -20, max 20, inclusive both\n- learnable: true\n- source: learnable\n\nsplit_self, token_self, combined_earning are all from data, learnable: false.\n\nU is calculated, with range -inf to inf.\n\nThe target variable is 'accept', which is binary (0 or 1). The model's utility U is passed through a logistic function to get the probability of acceptance.\n\nThis model has two learnable parameters, which is manageable for parameter recovery. The parameters alpha and beta are unlikely to have high covariance if the experimental design varies token contributions and splits appropriately.\n\nAnother check: does the model account for the fact that two equal split_self values might not be equally fair if combined_earning differs? Yes, because the reference point depends on combined_earning (for the equality part) and token_self (for the equity part). So even if split_self is the same, a different combined_earning or token_self would change the reference, hence the utility.\n\nFor example, if split_self is \u00a310 in two trials:\n\nTrial 1: combined_earning = \u00a320, token_self = 5 (so equity reference is 3*5=15). If alpha=1, reference is 15. U = 10 -15 + beta = -5 + beta.\n\nTrial 2: combined_earning = \u00a330, token_self=5 (equity reference 15). If alpha=0.5, reference is 0.5*15 + 0.5*15 (since combined_earning is 30, equality is 15). Wait, no. Wait, combined_earning is 30, so equality reference is 15. So if alpha=0.5, reference is 0.5*15 (equity) + 0.5*15 (equality) = 15. So U = 10 -15 + beta = same as before. But if combined_earning is different, but token_self is same, then the equity reference is same, but equality reference changes. Wait, no. Wait, combined_earning is 3*(token_self + token_opp). So if token_self is 5 and combined_earning is 30, then token_opp is 5 (since 3*(5+5)=30). So equity reference is 15, equality reference is 15. So split_self=10 would have U=10-15+beta.\n\nBut if combined_earning is 30, token_self=5, token_opp=5, and split_self=10, then equity and equality references are both 15. So U=10-15+beta.\n\nBut if combined_earning is 30, token_self=10, token_opp=0 (so combined_earning=3*10=30), then equity reference is 3*10=30, equality reference is 15. If alpha=0.5, reference is 0.5*30 + 0.5*15 = 22.5. So split_self=20 would give U=20-22.5 + beta = -2.5 + beta.\n\nSo the model correctly accounts for different scenarios where the same split_self can have different utilities based on contributions and total earnings.\n\nThus, this model should meet the requirements. It uses the necessary variables, has two learnable parameters with reasonable bounds, and the utility directly influences the probability of acceptance via a logistic function.\n</think>\n\n<MODEL>\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Weighting factor between equity-based (3*token_self) and equality-based (50% of combined_earning) reference points\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline bias term influencing general acceptance tendency\",\n      \"range\": {\n        \"min\": -20,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money earned in the trial (\u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the deviation of the proposed split from a weighted combination of equity (proportional to tokens found) and equality (50% split). The model includes two learnable parameters: alpha (trade-off between fairness norms) and beta (baseline bias). Utility is converted to acceptance probability via logistic function.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                tags:\n\n<MODEL>\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Weighting factor between equity-based (3*token_self) and equality-based (50% of combined_earning) reference points\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline bias term influencing general acceptance tendency\",\n      \"range\": {\n        \"min\": -20,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money earned in the trial (\\u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=0.5, beta=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n\n        utility = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\n\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"A\", \"trial_number\": 1, \"trial_type\": 2, \"trial_role\": 1, \"token_opp\": 16, \"token_self\": 32, \"combined_earning\": 144, \"split_opp\": 72.0, \"split_self\": 72.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 72.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 72.0}, {\"ID\": 1, \"group\": \"A\", \"trial_number\": 2, \"trial_type\": 2, \"trial_role\": 1, \"token_opp\": 16, \"token_self\": 32, \"combined_earning\": 144, \"split_opp\": 96.0, \"split_self\": 48.0, \"splitperc_opp\": 67, \"splitperc_self\": 33, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 33.0, \"proposed_amount\": 48.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1744017600.3205252
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Weighting factor between equity-based (3*token_self) and equality-based (50% of combined_earning) reference points",
      "range": {
        "min": 0,
        "max": 1,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Baseline bias term influencing general acceptance tendency",
      "range": {
        "min": -20,
        "max": 20,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "tags:\n\n<MODEL>\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta",
  "variable_descriptions": {
    "alpha": {
      "description": "Weighting factor between equity-based (3*token_self) and equality-based (50% of combined_earning) reference points",
      "range": {
        "min": 0,
        "max": 1,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Baseline bias term influencing general acceptance tendency",
      "range": {
        "min": -20,
        "max": 20,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "combined_earning": {
      "description": "Total money earned in the trial (\u00a3)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    }
  },
  "model_summary": "A utility model where acceptance probability depends on the deviation of the proposed split from a weighted combination of equity (proportional to tokens found) and equality (50% split). The model includes two learnable parameters: alpha (trade-off between fairness norms) and beta (baseline bias). Utility is converted to acceptance probability via logistic function.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility should be modeled such that a utility less than 0 corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The responder's decision (accept or reject) is the target variable. The main factors influencing this decision likely involve fairness considerations, the actual split proposed, and the contributions of both the participant and the opponent. \n\nThe dataset includes variables like token_self and token_opp, which represent the contributions each made. The combined_earning is the total money, which is 3*(token_self + token_opp). The split_self and split_opp are the proposed splits in pounds. The trial_type indicates whether contributions were equal, opponent more, or participant more. \n\nSo, the responder's utility probably depends on how fair the proposed split is relative to their own contribution. For example, if the participant contributed more tokens, they might expect a larger share. The model should capture this expectation and compare it to the actual proposal.\n\nA common approach in such scenarios is to model the utility as a function of the difference between the proposed split and some reference point, which could be based on equity (contribution-based) or equality (equal split). \n\nLet me think about possible parameters. One parameter could be the weight given to equity considerations versus equality. Another could be the sensitivity to unfairness, either when the participant is getting less than their fair share or when they're getting more. Also, there might be a baseline acceptance tendency.\n\nBut since the model needs to have a manageable number of parameters to avoid high covariance, maybe limit to 2-3 learnable parameters. \n\nHere's an outline of the model components:\n\n1. Equity Fairness: The fair share based on contributions. The participant's fair share would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair share could be a factor.\n\n2. Equality Fairness: The equal split (50% of combined_earning). The difference between split_self and 50% could be another factor.\n\n3. The utility could be a weighted combination of these two fairness components. For example, a parameter alpha determines the weight given to equity vs equality. \n\n4. Additionally, participants might have different sensitivities to being under-benefited vs over-benefited. For example, they might dislike being under-benefited more than they care about being over-benefited. So, maybe a parameter beta that scales the negative difference (when split_self is less than fair share) more than the positive.\n\nBut to keep it simple, maybe model the utility as the difference between the proposed split and the fair share (equity), and also include a term for the absolute difference from equality. Alternatively, use a nonlinear function like Fehr-Schmidt inequality aversion, but that might require more parameters.\n\nAlternatively, use a linear combination of the proposed split's deviation from equity and equality. Let's formalize this.\n\nLet\u2019s define:\n\n- fair_equity = (token_self / (token_self + token_opp)) * combined_earning\n- fair_equality = 0.5 * combined_earning\n\nThen, the utility could be a function of how much the proposed split_self deviates from these two references. For example:\n\nU = \u03b8 * (split_self - fair_equity) + (1 - \u03b8) * (split_self - fair_equality) \n\nHere, theta is a learnable parameter between 0 and 1 that determines the weight given to equity versus equality. If theta is 1, the responder only cares about equity; if 0, only equality.\n\nBut maybe participants have different sensitivities. Also, the model needs to account for the fact that negative deviations (split_self < fair) might have a stronger effect. So perhaps a parameter that scales the negative part.\n\nAlternatively, model the utility as:\n\nU = alpha * (split_self - beta * fair_equity) + gamma * (split_self - 0.5 * combined_earning)\n\nBut this introduces multiple parameters (alpha, beta, gamma), which could complicate parameter recovery.\n\nAlternatively, use a single parameter that captures the trade-off between equity and equality. Let's say:\n\nU = (split_self - ( (1 - alpha) * 0.5 + alpha * (token_self / total_tokens) ) * combined_earning )\n\nHere, alpha is between 0 and 1. When alpha=0, the reference is 50% (equality); when alpha=1, it's the equity share. The utility is the difference between the proposed split and a weighted average of equity and equality.\n\nThis way, alpha determines how much the responder weighs their own contribution versus an equal split. The utility would be positive if the split is above the weighted reference, leading to higher acceptance probability.\n\nBut maybe participants also have a non-linear sensitivity. For example, they might be more sensitive to receiving less than their reference, compared to receiving more. So, perhaps include a parameter that differentiates between under- and over-compensation.\n\nThis could be modeled using an asymmetry parameter. For example:\n\ndifference = split_self - reference\nU = difference * (1 + delta * (difference < 0))\n\nHere, delta is a parameter that amplifies the negative difference. If delta is positive, negative differences (split_self < reference) reduce utility more than positive differences increase it. But this adds another parameter.\n\nAlternatively, use a piecewise linear function. For example:\n\nIf split_self >= reference:\n    U = split_self - reference\nElse:\n    U = lambda * (split_self - reference)\n\nWhere lambda > 1 would mean that negative differences are penalized more. But this introduces lambda as a learnable parameter.\n\nBut considering the need to limit parameters, perhaps start with a simpler model. Let's consider the first approach with alpha as the weight between equity and equality, and a single parameter for the reference point. Then, the utility is:\n\nreference = alpha * (token_self / total_tokens) * combined_earning + (1 - alpha) * 0.5 * combined_earning\nU = split_self - reference\n\nBut this is linear. Then, the probability of accepting is logistic(U). If U > 0, probability > 0.5; if U < 0, probability < 0.5. This meets the requirement that utility < 0 corresponds to <50% chance.\n\nBut maybe participants also have a baseline tendency to accept or reject, independent of the split. So adding an intercept term, say beta, which captures this bias. For example:\n\nU = beta + (split_self - reference)\n\nBeta could be a learnable parameter. If beta is positive, the participant is more likely to accept even if the split is slightly below the reference. If beta is negative, they are more likely to reject even if the split meets the reference.\n\nSo now, two parameters: alpha (weight between equity and equality) and beta (bias).\n\nBut wait, the combined_earning is 3*(token_self + token_opp), so token_self + token_opp = combined_earning / 3. Therefore, token_self / (token_self + token_opp) = token_self * 3 / combined_earning. So the equity reference can be rewritten as (token_self * 3 / combined_earning) * combined_earning = 3 * token_self. Wait, no. Wait, combined_earning is 3*(token_self + token_opp). So token_self / (token_self + token_opp) is token_self / (combined_earning / 3). So the equity share is (token_self / (combined_earning / 3)) * combined_earning = 3 * token_self. Wait, that can't be right. Let me recalculate:\n\nWait, combined_earning = 3*(token_self + token_opp). So the equity share would be (token_self / (token_self + token_opp)) * combined_earning = (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3 * token_self. So the equity share is exactly 3 * token_self. Similarly, the equality share is 0.5 * combined_earning = 1.5*(token_self + token_opp).\n\nWait, that's interesting. So the equity-based fair share is 3 * token_self, and the equality-based is 1.5*(token_self + token_opp). Then, the reference point is a weighted average of these two. So:\n\nreference = alpha * (3 * token_self) + (1 - alpha) * (1.5 * (token_self + token_opp))\n\nThen, the utility is split_self - reference + beta.\n\nBut split_self is the proposed amount for the participant. So the utility is how much they get compared to their reference. If split_self is higher than the reference, utility is positive. If lower, negative.\n\nBut this seems a bit counterintuitive. Let's see. For example, if alpha is 1 (full equity), then reference is 3* token_self. So if the participant contributed 10 tokens, their fair share is \u00a330. If the proposed split_self is \u00a325, then utility is 25 -30 + beta = -5 + beta. If beta is 0, this is negative, leading to rejection. If beta is 5, then utility is 0, 50% chance.\n\nBut maybe the model should also consider the opponent's split. Wait, split_self + split_opp = combined_earning. So split_opp is combined_earning - split_self. But the responder's decision is based on their own split. However, the trial_type (whether the split is equal, opponent more, or participant more) might influence the responder's perception. But trial_type is derived from token contributions, so maybe it's redundant if we already use token_self and token_opp.\n\nAlternatively, the model could include a term that compares the split percentage (splitperc_self) to the equity percentage (token_self / (token_self + token_opp) * 100). But splitperc_self is already a variable, but perhaps using the actual token counts is better.\n\nWait, the equity percentage would be (token_self / (token_self + token_opp)) * 100. The split percentage is splitperc_self. The difference between these two could be a factor. For example, if the split percentage is less than the equity percentage, the responder feels treated unfairly.\n\nSo, another approach: the utility is based on the difference between the proposed percentage and the equity percentage, scaled by the combined earning. But perhaps that's similar to the previous approach.\n\nAlternatively, model the utility as:\n\nU = beta + theta * (split_self - equity_reference) + gamma * (split_self - equality_reference)\n\nBut theta and gamma are parameters. However, this introduces two parameters, which might be redundant if they are correlated. Maybe combine them into a single parameter as a weight.\n\nAlternatively, use a single parameter alpha that determines the weight between equity and equality, as before.\n\nSo, the reference point is a weighted combination of equity and equality:\n\nreference = alpha * equity_reference + (1 - alpha) * equality_reference\n\nThen, utility is split_self - reference + beta.\n\nBut equity_reference is 3 * token_self, and equality_reference is 1.5*(token_self + token_opp). So:\n\nreference = alpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)\n\nThen, utility U = split_self - reference + beta\n\nThis way, alpha (between 0 and 1) determines the weight between equity and equality. Beta is a bias term, which could be positive or negative, capturing the general tendency to accept or reject.\n\nBut wait, the split_self is in pounds, and the reference is also in pounds. So the difference is in pounds. Then, beta is in pounds as well. For example, if beta is 2, that means the participant has a baseline utility equivalent to getting \u00a32 more than their reference. So even if the split is \u00a32 below the reference, the utility would be zero (50% chance).\n\nBut perhaps beta should be scaled differently. Alternatively, the model could have beta as a scaling factor for the difference. But maybe it's okay as is.\n\nNow, the parameters to learn are alpha (weight between equity and equality) and beta (bias). These parameters need to have bounds. For alpha, between 0 and 1. For beta, perhaps between -K and K, where K is a reasonable value. Since combined_earning can vary, but split_self is up to combined_earning. For example, if combined_earning is up to, say, 3*(36 + 36) = 216 (assuming each trial has up to 36 tokens each?), but the actual maximum would depend on the data. But beta's range should be generous. Maybe set beta between -10 and 10, which allows for a bias of up to \u00a310 in either direction.\n\nBut the problem states that the utility variable may be unbounded, but the parameters must have finite bounds. So alpha is between 0 and 1 (inclusive), and beta between, say, -20 and 20 (inclusive). These are generous bounds.\n\nAnother consideration: the model should use variables available in the dataset. The variables used here are token_self, token_opp, combined_earning, split_self. All of these are present in the dataset.\n\nNow, the utility function would be:\n\nU = split_self - [alpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)] + beta\n\nSimplify the terms inside the brackets:\n\nalpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)\n\nLet me expand this:\n\n= 3 alpha token_self + 1.5 (1 - alpha) (token_self + token_opp)\n\n= 3 alpha token_self + 1.5 (1 - alpha) token_self + 1.5 (1 - alpha) token_opp\n\nCombine like terms:\n\ntoken_self [3 alpha + 1.5 (1 - alpha)] + 1.5 (1 - alpha) token_opp\n\n= token_self [3 alpha + 1.5 - 1.5 alpha] + 1.5 (1 - alpha) token_opp\n\n= token_self [1.5 alpha + 1.5] + 1.5 (1 - alpha) token_opp\n\nFactor out 1.5:\n\n= 1.5 [ token_self (alpha + 1) + (1 - alpha) token_opp ]\n\nHmm, interesting. So the reference can be written as 1.5 times [ token_self (alpha +1) + token_opp (1 - alpha) ]\n\nBut maybe this isn't necessary. The key point is that the reference is a combination of equity and equality.\n\nSo the utility U is:\n\nU = split_self - 1.5 [ alpha * 2 * token_self + (1 - alpha) (token_self + token_opp) ] + beta\n\nWait, perhaps not. Let me re-express the original reference:\n\nreference = alpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)\n\nBut 3 * token_self is the equity share (since each token is \u00a33, so token_self *3 is the participant's contribution in \u00a3, and combined_earning is 3*(token_self + token_opp). Wait, no. Wait, the equity share would be (participant's contribution / total contribution) * combined_earning. The participant's contribution is token_self *3, and the total contribution is (token_self + token_opp)*3. So the equity share is (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so this simplifies to 3*token_self. So yes, the equity share is 3*token_self. The equality share is 0.5*combined_earning = 1.5*(token_self + token_opp).\n\nSo the reference is a weighted average of equity and equality. Then, the utility is split_self minus this reference plus a bias term beta.\n\nThis model has two learnable parameters: alpha (weight between equity and equality) and beta (bias). The parameters alpha is bounded between 0 and 1, beta between -20 and 20. These ranges are generous enough to capture various behaviors.\n\nThe probability of acceptance is then calculated using the logistic function: P(accept) = 1 / (1 + exp(-U)). So when U is positive, the probability is greater than 0.5; when negative, less than 0.5.\n\nThis model should be able to capture responders who base their decisions on a combination of equity and equality, with a personal bias. The parameters alpha and beta can be estimated from the data, and their recovery should be feasible as they are not too correlated if the experimental design includes variations in token contributions and splits.\n\nNow, checking if all variables used are present in the dataset:\n\n- split_self: yes\n- token_self: yes\n- token_opp: yes\n- combined_earning: yes (though it can be derived from token_self and token_opp, but it's provided)\n\nSo the model uses variables available in the dataset.\n\nAnother consideration: the trial_role is 1 for responder, so the model should only consider trials where trial_role is 1. But the model itself is only for responder behavior, so the data used for fitting will filter to those trials.\n\nNow, the variables in the model:\n\n- split_self: from data\n- token_self: from data\n- token_opp: from data\n- alpha: learnable parameter (0 to 1)\n- beta: learnable parameter (-20 to 20)\n- U: calculated utility\n\nThe mathematical formula would be:\n\nU = split_self - [alpha * 3 * token_self + (1 - alpha) * 1.5 * (token_self + token_opp)] + beta\n\nAlternatively, simplifying:\n\nU = split_self - 1.5 * [2 * alpha * token_self + (1 - alpha) * (token_self + token_opp)] + beta\n\nBut perhaps keeping it as the original expression is clearer.\n\nNow, writing this in mathematical notation within the <MODEL> tags:\n\n<MODEL>\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\n</MODEL>\n\nWait, because 0.5 * combined_earning is the equality reference (since combined_earning is 3*(token_self + token_opp), so 0.5 * combined_earning = 1.5*(token_self + token_opp)). So this is equivalent to the previous expression. So this might be a simpler way to write it, using combined_earning directly.\n\nYes, because 0.5 * combined_earning is the equality reference. So the model can be written as:\n\nU = split_self - [alpha * (3 * token_self) + (1 - alpha) * (0.5 * combined_earning)] + beta\n\nWhich uses combined_earning from the data. This is better because combined_earning is a variable in the dataset, so we don't need to compute it from token_self and token_opp. This makes the model directly use the provided variables.\n\nSo the formula becomes:\n\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\n\nThis is cleaner and uses the available variables. So the model uses split_self, token_self, combined_earning, and the parameters alpha and beta.\n\nNow, the variables section:\n\nVariables:\n\n- split_self: from data (float64)\n- token_self: from data (int64)\n- combined_earning: from data (int64)\n- alpha: learnable parameter, range [0, 1]\n- beta: learnable parameter, range [-20, 20]\n- U: calculated utility\n\nSo in the variables JSON:\n\nFor alpha:\n- description: \"Weighting factor between equity-based (token_self) and equality-based (50% split) reference points\"\n- range: min 0, max 1, inclusive both\n- learnable: true\n- source: learnable\n\nFor beta:\n- description: \"Baseline bias term influencing acceptance tendency\"\n- range: min -20, max 20, inclusive both\n- learnable: true\n- source: learnable\n\nsplit_self, token_self, combined_earning are all from data, learnable: false.\n\nU is calculated, with range -inf to inf.\n\nThe target variable is 'accept', which is binary (0 or 1). The model's utility U is passed through a logistic function to get the probability of acceptance.\n\nThis model has two learnable parameters, which is manageable for parameter recovery. The parameters alpha and beta are unlikely to have high covariance if the experimental design varies token contributions and splits appropriately.\n\nAnother check: does the model account for the fact that two equal split_self values might not be equally fair if combined_earning differs? Yes, because the reference point depends on combined_earning (for the equality part) and token_self (for the equity part). So even if split_self is the same, a different combined_earning or token_self would change the reference, hence the utility.\n\nFor example, if split_self is \u00a310 in two trials:\n\nTrial 1: combined_earning = \u00a320, token_self = 5 (so equity reference is 3*5=15). If alpha=1, reference is 15. U = 10 -15 + beta = -5 + beta.\n\nTrial 2: combined_earning = \u00a330, token_self=5 (equity reference 15). If alpha=0.5, reference is 0.5*15 + 0.5*15 (since combined_earning is 30, equality is 15). Wait, no. Wait, combined_earning is 30, so equality reference is 15. So if alpha=0.5, reference is 0.5*15 (equity) + 0.5*15 (equality) = 15. So U = 10 -15 + beta = same as before. But if combined_earning is different, but token_self is same, then the equity reference is same, but equality reference changes. Wait, no. Wait, combined_earning is 3*(token_self + token_opp). So if token_self is 5 and combined_earning is 30, then token_opp is 5 (since 3*(5+5)=30). So equity reference is 15, equality reference is 15. So split_self=10 would have U=10-15+beta.\n\nBut if combined_earning is 30, token_self=5, token_opp=5, and split_self=10, then equity and equality references are both 15. So U=10-15+beta.\n\nBut if combined_earning is 30, token_self=10, token_opp=0 (so combined_earning=3*10=30), then equity reference is 3*10=30, equality reference is 15. If alpha=0.5, reference is 0.5*30 + 0.5*15 = 22.5. So split_self=20 would give U=20-22.5 + beta = -2.5 + beta.\n\nSo the model correctly accounts for different scenarios where the same split_self can have different utilities based on contributions and total earnings.\n\nThus, this model should meet the requirements. It uses the necessary variables, has two learnable parameters with reasonable bounds, and the utility directly influences the probability of acceptance via a logistic function.\n</think>\n\n<MODEL>\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Weighting factor between equity-based (3*token_self) and equality-based (50% of combined_earning) reference points\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline bias term influencing general acceptance tendency\",\n      \"range\": {\n        \"min\": -20,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money earned in the trial (\u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the deviation of the proposed split from a weighted combination of equity (proportional to tokens found) and equality (50% split). The model includes two learnable parameters: alpha (trade-off between fairness norms) and beta (baseline bias). Utility is converted to acceptance probability via logistic function.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: tags:\n\n<MODEL>\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\nSummary: A utility model where acceptance probability depends on the deviation of the proposed split from a weighted combination of equity (proportional to tokens found) and equality (50% split). The model includes two learnable parameters: alpha (trade-off between fairness norms) and beta (baseline bias). Utility is converted to acceptance probability via logistic function.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=0.5, beta=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n\n        utility = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\n\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.7854090354090355,
  "fitting_results": [
    {
      "alpha": 0.0,
      "beta": -0.6984817589888852,
      "success": true,
      "log_likelihood": -5.4289286165243915,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6379903083212725,
      "beta": 2.343508366895632,
      "success": true,
      "log_likelihood": -17.291375669082512,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 2.558040739007391,
      "success": true,
      "log_likelihood": -13.568034934090766,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6392812712127447,
      "beta": 1.8332496790073924,
      "success": true,
      "log_likelihood": -17.406721062111174,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 1.9899964550357614,
      "success": true,
      "log_likelihood": -11.239955527404575,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5438605368694468,
      "beta": 3.2067932362999443,
      "success": true,
      "log_likelihood": -13.469775814401542,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.007643793661976979,
      "beta": 18.99728891926121,
      "success": true,
      "log_likelihood": -8.598689926491293e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7800233190321147,
      "beta": 3.4579814976913643,
      "success": true,
      "log_likelihood": -12.153723964833315,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7902745629712435,
      "beta": 20.0,
      "success": true,
      "log_likelihood": -5.101234370833954e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 1.6083271361202587,
      "success": true,
      "log_likelihood": -13.339212569561132,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 1.9899964382260733,
      "success": true,
      "log_likelihood": -11.83995552726785,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.34497187926189066,
      "beta": -0.388742024230536,
      "success": true,
      "log_likelihood": -7.933973044966631,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 1.7998608100604774,
      "success": true,
      "log_likelihood": -11.784972398746653,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.15283128105646418,
      "beta": 19.251689597137098,
      "success": true,
      "log_likelihood": -6.803921980621605e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 20.0,
      "success": true,
      "log_likelihood": -3.1522766924639433e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.31656683574031136,
      "beta": 20.0,
      "success": true,
      "log_likelihood": -3.4361007945902474e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3520518183822077,
      "beta": 20.0,
      "success": true,
      "log_likelihood": -3.504854971381435e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.34143549205066925,
      "beta": 5.030677537968131,
      "success": true,
      "log_likelihood": -10.175076696576737,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 5.667698800088972,
      "success": true,
      "log_likelihood": -6.638510325107157,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6112685343879672,
      "beta": 20.0,
      "success": true,
      "log_likelihood": -4.265681763768071e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 2.748412116490699,
      "success": true,
      "log_likelihood": -14.514906197265052,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7030870328398185,
      "beta": 5.141567434538778,
      "success": true,
      "log_likelihood": -8.345863737457742,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 1.5384311520557312,
      "success": true,
      "log_likelihood": -16.376012970022494,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.31366564905020144,
      "beta": 2.489343293624599,
      "success": true,
      "log_likelihood": -18.845792351378517,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 2.368544626880079,
      "success": true,
      "log_likelihood": -11.831278405416354,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8742122870107341,
      "beta": 2.910912669332923,
      "success": true,
      "log_likelihood": -18.062389955814744,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.15908695872883183,
      "beta": 2.6444154379245757,
      "success": true,
      "log_likelihood": -13.48387153192899,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 0.8094024331315873,
      "success": true,
      "log_likelihood": -9.498576532337276,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 5.667698853913797,
      "success": true,
      "log_likelihood": -7.9885103240319495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7665795315866625,
      "beta": 4.31982105819932,
      "success": true,
      "log_likelihood": -14.318532979004216,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5960090224910523,
      "beta": 20.0,
      "success": true,
      "log_likelihood": -4.207185096887191e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -0.698481706526622,
      "success": true,
      "log_likelihood": -5.428928616524396,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 1.854855798556573,
      "success": true,
      "log_likelihood": -17.181189356153496,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.31348652273857497,
      "beta": 1.6891709078801753,
      "success": true,
      "log_likelihood": -15.490129898667668,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11809304932438229,
      "beta": 19.079527276942315,
      "success": true,
      "log_likelihood": -8.016721625936531e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.725740043710011e-08,
      "beta": 3.149806919557566,
      "success": true,
      "log_likelihood": -19.027943913683124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8289576880224337,
      "beta": 3.6757662903582364,
      "success": true,
      "log_likelihood": -13.388059490633022,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 2.368544569423558,
      "success": true,
      "log_likelihood": -14.231278404864632,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.78002135640791,
      "beta": 3.457979536262003,
      "success": true,
      "log_likelihood": -16.20372396351105,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.776880254057677,
      "success": true,
      "log_likelihood": -7.804862137704213,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5204604030929951,
      "beta": 3.021032097197898,
      "success": true,
      "log_likelihood": -14.783034476403643,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.912716597768048,
      "beta": 0.41525571857263943,
      "success": true,
      "log_likelihood": -12.614045481612333,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 7.175574986574611,
      "success": true,
      "log_likelihood": -4.654468959004844,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.641490461421515,
      "success": true,
      "log_likelihood": -15.85301975719348,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5204651204869595,
      "beta": 3.021030265792444,
      "success": true,
      "log_likelihood": -16.133034475934913,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 3.7236852178179163,
      "success": true,
      "log_likelihood": -19.239181835068166,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1556388462357089,
      "beta": 2.4828857867550935,
      "success": true,
      "log_likelihood": -13.947312708047164,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3134863271349655,
      "beta": 1.6891710003616702,
      "success": true,
      "log_likelihood": -20.440129895434417,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 1.7998607134199582,
      "success": true,
      "log_likelihood": -10.734972399079467,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.4807873285857944,
      "success": true,
      "log_likelihood": -28.01389046398854,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1974850605231712,
      "beta": 3.5316536603669144,
      "success": true,
      "log_likelihood": -18.363461886404792,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -0.039086478291532514,
      "success": true,
      "log_likelihood": -8.973279121886037,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.480773323209012,
      "success": true,
      "log_likelihood": -19.31389047055976,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.8058477857055717,
      "success": true,
      "log_likelihood": -25.42969678519719,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 1.6083272305051246,
      "success": true,
      "log_likelihood": -19.039212567135568,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 1.217781062508747,
      "success": true,
      "log_likelihood": -10.617415393082117,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7686953299324726,
      "beta": 0.9001694648663509,
      "success": true,
      "log_likelihood": -11.877736891027958,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.35074656010315,
      "success": true,
      "log_likelihood": -19.21113842695831,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.8470248972631597e-08,
      "beta": -0.33657777214985113,
      "success": true,
      "log_likelihood": -7.139578237892255,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 4.415545535620098,
      "success": true,
      "log_likelihood": -16.369728346427642,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16973924512867805,
      "beta": 2.9799301066042223,
      "success": true,
      "log_likelihood": -19.11262796906905,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8176089328587761,
      "beta": 2.355297257795497,
      "success": true,
      "log_likelihood": -14.401339079539698,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -0.6984810621762539,
      "success": true,
      "log_likelihood": -5.42892861652499,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.0106982299532423,
      "success": true,
      "log_likelihood": -21.548353607304037,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.41756058964821097,
      "beta": 0.6049917355250135,
      "success": true,
      "log_likelihood": -11.275391019225038,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.15906847297461785,
      "beta": 2.644510075521349,
      "success": true,
      "log_likelihood": -16.183871501578988,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.9266125935518114e-08,
      "beta": 5.776876091425842,
      "success": true,
      "log_likelihood": -6.904862138989561,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7030991730392374,
      "beta": 5.14157124562062,
      "success": true,
      "log_likelihood": -10.595863736193442,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.963605488853615e-08,
      "beta": 2.641490533340211,
      "success": true,
      "log_likelihood": -20.05301975621477,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 6.132049385386995,
      "success": true,
      "log_likelihood": -11.697920696217992,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": -0.7560041145502902,
      "success": true,
      "log_likelihood": -5.870137449588674,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -0.03908649535391524,
      "success": true,
      "log_likelihood": -8.223279122466247,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4766453142358544,
      "beta": 2.500210758793821,
      "success": true,
      "log_likelihood": -17.50385322640654,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1152297369698533e-07,
      "beta": 4.1667887223666105,
      "success": true,
      "log_likelihood": -18.559075645048175,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.322704453171499,
      "success": true,
      "log_likelihood": -24.715448922247433,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1974848240757033,
      "beta": 3.531652637811243,
      "success": true,
      "log_likelihood": -10.56346188947586,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 6.751957923154655,
      "success": true,
      "log_likelihood": -6.623573324884793,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -1.1781015316392618,
      "success": true,
      "log_likelihood": -6.304294816530568,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16973761190336023,
      "beta": 2.979928161088813,
      "success": true,
      "log_likelihood": -30.062627963028092,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -0.6984818056251437,
      "success": true,
      "log_likelihood": -5.428928616524394,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.480773670837129,
      "success": true,
      "log_likelihood": -23.063890469643543,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.776877284477511,
      "success": true,
      "log_likelihood": -12.004862133070278,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -0.3365777965315902,
      "success": true,
      "log_likelihood": -8.789578236867369,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.30906931815305166,
      "beta": 2.3278678201397707,
      "success": true,
      "log_likelihood": -17.80420478403993,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3422475234090421,
      "beta": 2.995154628623393,
      "success": true,
      "log_likelihood": -27.729162929088687,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.34494631214024923,
      "beta": -0.38874986789773575,
      "success": true,
      "log_likelihood": -6.583973043815518,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.607592320794652,
      "beta": 3.6066219402247377,
      "success": true,
      "log_likelihood": -17.313582803499195,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 0.3681298406215432,
      "success": true,
      "log_likelihood": -14.583350772330812,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0,
      "beta": 6.75195923691125,
      "success": true,
      "log_likelihood": -5.273573323597564,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.166327688404727,
      "success": true,
      "log_likelihood": -19.459863315917257,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.026213116511767e-08,
      "beta": 3.522555016375746,
      "success": true,
      "log_likelihood": -21.661347330441465,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.53316756898427,
        "p": 0.1125177063588042
      },
      "beta": {
        "r": 0.9404587716568399,
        "p": 5.11535743805723e-05
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 32.49373218068033,
  "bic_results": {
    "average_bic": 32.49373218068033,
    "individual_bics": [
      18.024895109961,
      41.74978921507724,
      34.303107745093754,
      41.980480001134566,
      29.64694893172137,
      34.106589505715306,
      7.1670550742920724,
      31.474485806578848,
      7.167048079380962,
      33.845463016034486,
      30.84694893144792,
      23.03498396684548,
      30.736982674405525,
      7.167051484756181,
      7.167044181465605,
      7.167044749113809,
      7.167044886622163,
      27.517191270065695,
      20.444058527126536,
      7.1670464082757475,
      36.19685027144232,
      23.858765351827707,
      39.919063816957205,
      44.85862257966925,
      30.82959468774493,
      43.29181778854171,
      34.1347809407702,
      26.164190941586774,
      23.144058524976117,
      35.80410383492065,
      7.167046291282413,
      18.02489510996101,
      41.52941658921921,
      38.147297674247554,
      7.167053910355472,
      45.22292570427847,
      33.94315685817826,
      35.62959468664148,
      39.574485803934316,
      22.776762152320646,
      36.7331068297195,
      32.395128840136884,
      16.475975794921908,
      38.87307739129918,
      39.433106828782044,
      45.64540154704855,
      35.061663293006546,
      48.04729766778105,
      28.636982675071152,
      63.1948188048893,
      43.8939616497218,
      25.113596120684292,
      45.794818818031736,
      58.0264314473066,
      45.245463011183354,
      28.401868663076456,
      30.922511658968133,
      45.58931473082884,
      21.44619435269673,
      39.9064945697675,
      45.392293815050316,
      35.96971603599162,
      18.024895109962202,
      50.26374509152029,
      29.7178199153623,
      39.534780880070194,
      20.97676215489134,
      28.358765349299105,
      47.27307738934176,
      30.5628792693482,
      18.90731277608957,
      23.613596121844715,
      42.1747443297253,
      44.28518916700857,
      56.59793572140708,
      28.29396165586394,
      20.414184526681808,
      19.775627509973354,
      67.2922938029684,
      18.024895109961008,
      53.2948188161993,
      31.176762143052777,
      24.74619435064696,
      42.775447444992075,
      62.62536373508959,
      20.334983964543255,
      41.79420348391061,
      36.333739421573846,
      17.714184524107345,
      46.08676450874673,
      50.48973253779515
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.533\n- beta: r = 0.940",
  "previous_models": [
    "Specification: tags:\n\n<MODEL>\nU = split_self - (alpha * 3 * token_self + (1 - alpha) * 0.5 * combined_earning) + beta\nSummary: A utility model where acceptance probability depends on the deviation of the proposed split from a weighted combination of equity (proportional to tokens found) and equality (50% split). The model includes two learnable parameters: alpha (trade-off between fairness norms) and beta (baseline bias). Utility is converted to acceptance probability via logistic function.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 32.49373218068033\nOverall Accuracy: 0.7854\n\nParameter Recovery:\n- alpha: r = 0.533\n- beta: r = 0.940"
  ]
}