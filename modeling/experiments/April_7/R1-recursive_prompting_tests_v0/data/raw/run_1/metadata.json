{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: THIS IS A TEST TO SEE IF THESE INSTRUCTIONS ARE ACTUALLY BEING DISPLAYED WITHIN THE PIPELINE\n\nBe VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where the trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). \n\nFirst, I need to understand the variables provided and how they relate to the responder's decision. The key variables here are the tokens found by the participant (token_self) and the opponent (token_opp), the combined earnings, and the proposed splits (split_self and split_opp). The trial_type indicates whether contributions are equal, opponent more, or participant more. \n\nThe responder's decision likely depends on the fairness of the proposed split relative to their own contribution. Since each token is worth \u00a33, the combined_earning is 3*(token_self + token_opp). The split_self is the amount offered to the responder, and split_opp is what the proposer keeps. \n\nA common approach in such scenarios is to model the utility of accepting the offer. The responder might compare the proposed split to what they consider a fair share. The fair share could be based on their contribution (token_self) relative to the total (token_self + token_opp). For example, if the responder contributed 40% of the tokens, they might expect 40% of the combined earnings. \n\nSo, the model should calculate the utility of accepting the offer. If the utility is positive, the responder is more likely to accept. The utility could be the difference between the proposed split and the fair share, adjusted by some parameters. \n\nBut fairness isn't the only factor. People might have different sensitivities to inequity. For instance, some might reject offers that are slightly below their fair share, while others might accept even if it's unfair. Also, the absolute amount might matter\u2014people might be more sensitive to differences when the stakes are higher.\n\nAnother consideration is the participant's own contribution (token_self). If they contributed more, they might expect a higher share. The model should account for the proportion of tokens they contributed. \n\nLet me outline possible components for the utility function:\n\n1. **Fairness Component**: The difference between the proposed split (split_self) and the fair share based on their contribution. The fair share would be (token_self / (token_self + token_opp)) * combined_earning. Let's call this expected_fair. The difference would be split_self - expected_fair. \n\n2. **Inequity Aversion**: The responder might dislike inequity, so even if the split is fair, they might be sensitive to the absolute difference between split_self and split_opp. But since split_opp is the proposer's share, maybe they compare their split to the proposer's. However, in the responder role, the proposer's split is determined by the proposer, so the responder might focus on their own share relative to their contribution.\n\n3. **Non-linear Sensitivity**: The utility might not be linear. For example, the responder could have diminishing sensitivity to larger deviations from fairness. But to keep the model simple, maybe a linear term with a parameter to scale it would suffice.\n\n4. **Loss Aversion**: The responder might be more sensitive to offers below their fair share than above. This could be modeled with separate parameters for positive and negative deviations. However, adding more parameters could complicate the model and affect parameter recovery. Since the task mentions avoiding high covariance between parameters, maybe a single parameter for the sensitivity to unfairness would be better.\n\n5. **Alternative Formulation**: Another approach is to model the utility as the proposed split minus some reference point (like the fair share), scaled by a parameter. The probability of accepting is then a logistic function of this utility.\n\nPutting this together, the utility (U) could be:\n\nU = \u03b1 * (split_self - expected_fair) + \u03b2 * (split_self / combined_earning - split_opp / combined_earning)\n\nBut this introduces two parameters (\u03b1 and \u03b2). However, the problem states that parameter recovery is important, so we need to balance model complexity. Maybe combining these into a single term.\n\nAlternatively, the utility could be based on the difference between the proposed percentage (splitperc_self) and the fair percentage (token_self / (token_self + token_opp) * 100). Let's call this fair_perc. Then, the utility is the difference between splitperc_self and fair_perc, scaled by a parameter. \n\nU = \u03b1 * (splitperc_self - fair_perc)\n\nIf U is positive, the responder is more likely to accept. The probability of acceptance would be 1 / (1 + exp(-U)). \n\nBut maybe the responder's sensitivity depends on the magnitude of the difference. For example, a larger negative difference (unfair offer) might have a stronger effect. Alternatively, the model could include a non-linear term, but again, simplicity is key.\n\nAnother angle: the responder's decision could be influenced by both the absolute amount offered (split_self) and the relative fairness. For example, even if the offer is slightly unfair, if the absolute amount is high, they might accept. But this complicates the model with more parameters.\n\nWait, the problem mentions that two equal split_self variables don't represent the same fairness because combined_earning varies. So the model must account for the proportion of the split relative to the total. Using percentages (splitperc_self) might already handle this, as it's a percentage of the combined earnings. \n\nSo, the fair percentage is (token_self / (token_self + token_opp)) * 100. The difference between the proposed percentage (splitperc_self) and the fair percentage is the key variable. Let's call this difference \u0394 = splitperc_self - fair_perc. \n\nThe utility could be U = \u03b1 * \u0394. The probability of accepting is logistic(U). If \u0394 is positive (offer is more than fair), U is positive, leading to higher acceptance probability. If \u0394 is negative, U is negative, lower probability. \n\nBut maybe the responder has a non-linear sensitivity. For example, they might be more sensitive to unfairness when the offer is below the fair percentage. This could be modeled with a parameter that scales negative differences more. However, adding a parameter for this would increase model complexity. \n\nAlternatively, the model could include a parameter that represents the responder's threshold for acceptance. For example, they might accept if the offer is above (fair_perc - \u03b8), where \u03b8 is a learnable parameter representing their tolerance. But this would make the model deterministic, whereas we need a probabilistic model. \n\nWait, the problem states that the model should predict utility, where less than 0 is less than 50% likely. So the utility is passed through a logistic function to get the probability. So the utility function must be designed such that when U < 0, the probability is below 0.5, and vice versa. \n\nSo, the core idea is to model the utility as a function of how much the proposed split deviates from the fair split (based on contribution), scaled by a parameter. \n\nLet me formalize this:\n\nfair_perc = (token_self / (token_self + token_opp)) * 100\n\u0394 = splitperc_self - fair_perc\nU = \u03b1 * \u0394\n\nThen, the probability of accepting is \u03c3(U), where \u03c3 is the logistic function. \n\nThis model has one learnable parameter, \u03b1, which represents the sensitivity to the deviation from the fair percentage. A positive \u03b1 means that as the offer becomes more favorable (higher than fair), the utility increases, leading to higher acceptance probability. \n\nBut maybe the responder also cares about the absolute amount, not just the percentage. For example, getting 50% of \u00a310 is different from 50% of \u00a3100. However, since combined_earning varies, and the split_self is in \u00a3, perhaps the absolute amount is already considered. But in the model above, using percentages might normalize this. \n\nAlternatively, the utility could be based on the difference in monetary terms:\n\nfair_self = (token_self / (token_self + token_opp)) * combined_earning\n\u0394 = split_self - fair_self\nU = \u03b1 * \u0394\n\nBut this is similar to the percentage approach, just scaled by combined_earning. Since combined_earning is 3*(token_self + token_opp), the two approaches are related. \n\nHowever, using the monetary difference might make the parameter \u03b1 have units of utility per \u00a3, whereas using percentage difference would have units of utility per percentage point. Depending on the data, one might be more appropriate. \n\nBut given that the problem states that two equal split_self values don't represent the same fairness because combined_earning varies, using the percentage approach (splitperc_self) would naturally account for this, as the same split_self in \u00a3 would correspond to different percentages depending on combined_earning. \n\nTherefore, using the percentage difference seems appropriate. \n\nAnother consideration is the role of trial_type. For example, if the trial_type indicates whether the participant contributed more, the responder's expectations might vary. However, including trial_type would require additional parameters. Since the model should be kept simple, perhaps trial_type is captured indirectly through the token_self and token_opp variables. \n\nSo, the model would have one learnable parameter \u03b1, which scales the difference between the proposed percentage and the fair percentage. \n\nBut wait, maybe there's a baseline tendency to accept or reject regardless of fairness. For example, some participants might have a general tendency to accept offers (e.g., due to risk aversion or other factors). To account for this, we could add an intercept term \u03b2. \n\nSo, the utility would be:\n\nU = \u03b1 * \u0394 + \u03b2\n\nHere, \u03b2 represents a bias term. If \u03b2 is positive, the participant is more likely to accept even if the offer is slightly unfair. If negative, they are more likely to reject even fair offers. \n\nThis introduces a second parameter. However, the problem mentions that parameter recovery is a priority, so having two parameters might be manageable. But we need to check for identifiability. If \u03b1 and \u03b2 are both learnable, there could be covariance. For example, a higher \u03b2 could compensate for a lower \u03b1. But with enough data (36 trials per participant), it might still be feasible. \n\nAlternatively, maybe the intercept is not necessary. If the model is purely based on the fairness deviation, then when \u0394=0 (offer is exactly fair), the utility is zero, leading to a 50% chance of accepting. But in reality, participants might have a default tendency to accept fair offers. However, the problem states that the utility should predict such that U < 0 corresponds to less than 50% probability. So if the model is U = \u03b1 * \u0394, then when \u0394=0, U=0, probability is 0.5, which is correct. If the offer is unfair (\u0394 < 0), U < 0, probability < 0.5. If fair or generous, U >=0, probability >=0.5. \n\nBut perhaps participants have different baseline acceptance rates. For example, some might generally accept unless the offer is very unfair, while others might reject more often. To capture this, the intercept \u03b2 could be useful. However, adding \u03b2 increases the number of parameters. Since the task advises a slight bias towards fewer parameters, maybe starting with just \u03b1 is better. \n\nAlternatively, perhaps the intercept is necessary. Let's think: if the utility is U = \u03b1 * \u0394 + \u03b2, then even when \u0394=0, the utility is \u03b2. If \u03b2 is positive, the baseline probability is higher than 50%, and vice versa. This could capture a general acceptance bias. However, without \u03b2, the model assumes that exactly fair offers (\u0394=0) have a 50% chance of being accepted. If the data shows that participants accept fair offers more than 50% of the time, then \u03b2 would be needed. \n\nGiven that the problem states the model should predict utility where less than 0 is less than 50% likely, but doesn't specify the baseline, including \u03b2 might be necessary to capture individual differences in baseline acceptance rates. \n\nSo, the model would have two parameters: \u03b1 (sensitivity to fairness deviation) and \u03b2 (baseline utility). \n\nAnother consideration is the scaling of the parameters. For example, \u03b1 should be positive, as a higher \u0394 (more favorable offer) should increase utility. So we can set a bound for \u03b1, say [0, 10], to ensure it's positive. \u03b2 could be in a range like [-5, 5], allowing for baseline tendencies. \n\nBut how to determine the bounds? The problem says to specify finite numerical bounds that are generous. For \u03b1, since it's a scaling factor on the percentage difference (which can be up to \u00b1100), a range of [0, 10] would mean that each percentage point difference contributes up to 10 utility units. For \u03b2, a range of [-5, 5] allows the baseline utility to shift the acceptance probability between roughly 0.007 (for \u03b2=-5) to 0.993 (for \u03b2=5) when \u0394=0. \n\nAlternatively, using a logistic function with temperature 1, the utility directly maps to probability via sigmoid. So, the parameters' scales should be such that typical \u0394 values (say, \u00b120%) multiplied by \u03b1 plus \u03b2 would cover a reasonable range of utilities. \n\nBut perhaps the bounds can be set as \u03b1 in [0, 10] and \u03b2 in [-5, 5], which are generous enough. \n\nAnother possible factor is the participant's own contribution (token_self). Maybe the utility also depends on the absolute number of tokens they contributed. For example, if they contributed more tokens, they might feel more entitled, affecting their acceptance decision. However, this is already captured in the fair_perc calculation, as token_self directly influences fair_perc. \n\nAlternatively, the model could include a term for the participant's own tokens, but that might introduce redundancy. \n\nSo, the core model remains U = \u03b1 * (splitperc_self - fair_perc) + \u03b2. \n\nNow, let's check the variables in the dataset. The variables available include splitperc_self, token_self, token_opp, etc. The fair_perc can be calculated as (token_self / (token_self + token_opp)) * 100. \n\nBut wait, token_self and token_opp are the number of tokens found by the participant and opponent, respectively. The combined_earning is 3*(token_self + token_opp). So, the fair share in percentage is indeed (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the model's variables are:\n\n- splitperc_self: from the data.\n- fair_perc: calculated as (token_self / (token_self + token_opp)) * 100.\n- \u0394 = splitperc_self - fair_perc.\n- U = \u03b1 * \u0394 + \u03b2.\n\nThe parameters \u03b1 and \u03b2 are learnable. \n\nNow, the variables section needs to describe each variable in the model. The variables in the model are U, \u03b1, \u03b2, splitperc_self, token_self, token_opp, fair_perc (calculated), and \u0394 (calculated). However, in the variables JSON, we need to list all variables used in the model formula. \n\nWait, the model formula as written would be:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nSo, the variables directly used are splitperc_self, token_self, token_opp, alpha, beta. The fair_perc is a calculated intermediate variable. \n\nTherefore, in the variables section, we need to include:\n\n- splitperc_self: from data, int64.\n- token_self: from data, int64.\n- token_opp: from data, int64.\n- alpha: learnable parameter, range [0, 10].\n- beta: learnable parameter, range [-5, 5].\n\nThe utility U is calculated. \n\nSo, the model formula in <MODEL> tags would be:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nBut to make it more concise, perhaps we can define fair_perc as (token_self / (token_self + token_opp)) * 100, and then U = alpha * (splitperc_self - fair_perc) + beta. However, in the model formula, we need to use the variables as per the dataset. \n\nAlternatively, since the dataset includes splitperc_self and splitperc_opp, but fair_perc isn't a variable, we need to compute it from token_self and token_opp. \n\nSo, the model formula would directly compute fair_perc as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the mathematical model is:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nThis way, all variables used are from the dataset (splitperc_self, token_self, token_opp) and the parameters alpha and beta. \n\nNow, the variables section in JSON should describe each variable in the model. \n\nsplitperc_self is a data variable (int64), token_self and token_opp are data variables (int64). alpha and beta are learnable parameters with specified ranges. \n\nFor alpha, since it's a sensitivity parameter, it should be positive. So the range is min 0, max perhaps 10, inclusive. Beta can be between -5 and 5. \n\nThe variables JSON would look like:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from fair percentage\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe target variable is 'accept', so enclosed in <target_variable> tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept</target_variable>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair percentage\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline utility bias (general acceptance tendency)\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA linear utility model where acceptance probability depends on the deviation between the proposed percentage and the participant's contribution-based fair share. The model uses two parameters: a sensitivity to fairness deviations (alpha) and a baseline utility bias (beta). The utility is converted to acceptance probability via a logistic function, with negative utilities corresponding to <50% acceptance likelihood.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                tags would be:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nBut to make it more concise, perhaps we can define fair_perc as (token_self / (token_self + token_opp)) * 100, and then U = alpha * (splitperc_self - fair_perc) + beta. However, in the model formula, we need to use the variables as per the dataset. \n\nAlternatively, since the dataset includes splitperc_self and splitperc_opp, but fair_perc isn't a variable, we need to compute it from token_self and token_opp. \n\nSo, the model formula would directly compute fair_perc as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the mathematical model is:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nThis way, all variables used are from the dataset (splitperc_self, token_self, token_opp) and the parameters alpha and beta. \n\nNow, the variables section in JSON should describe each variable in the model. \n\nsplitperc_self is a data variable (int64), token_self and token_opp are data variables (int64). alpha and beta are learnable parameters with specified ranges. \n\nFor alpha, since it's a sensitivity parameter, it should be positive. So the range is min 0, max perhaps 10, inclusive. Beta can be between -5 and 5. \n\nThe variables JSON would look like:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from fair percentage\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe target variable is 'accept', so enclosed in <target_variable> tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair percentage\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline utility bias (general acceptance tendency)\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=0):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        # Calculate fair percentage\n        if (token_self + token_opp) == 0:\n            fair_perc = 50  # Avoid division by zero, assume 50/50 if both have 0 tokens\n        else:\n            fair_perc = (token_self / (token_self + token_opp)) * 100\n\n        # Calculate utility\n        utility = alpha * (splitperc_self - fair_perc) + beta\n\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '''\n[\n    {\n        \"ID\": 1,\n        \"group\": \"A\",\n        \"trial_number\": 1,\n        \"trial_type\": 1,\n        \"trial_role\": 1,\n        \"token_opp\": 5,\n        \"token_self\": 5,\n        \"combined_earning\": 30,\n        \"split_opp\": 15.0,\n        \"split_self\": 15.0,\n        \"splitperc_opp\": 50,\n        \"splitperc_self\": 50,\n        \"accept\": 1.0,\n        \"accepted_amount\": 15.0,\n        \"accepted_perc\": 50.0,\n        \"proposed_perc\": 50.0,\n        \"proposed_amount\": 15.0\n    },\n    {\n        \"ID\": 2,\n        \"group\": \"A\",\n        \"trial_number\": 2,\n        \"trial_type\": 1,\n        \"trial_role\": 1,\n        \"token_opp\": 5,\n        \"token_self\": 5,\n        \"combined_earning\": 30,\n        \"split_opp\": 21.0,\n        \"split_self\": 9.0,\n        \"splitperc_opp\": 70,\n        \"splitperc_self\": 30,\n        \"accept\": 0.0,\n        \"accepted_amount\": 0.0,\n        \"accepted_perc\": 0.0,\n        \"proposed_perc\": 30.0,\n        \"proposed_amount\": 9.0\n    }\n]\n'''\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1744025476.372939
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair percentage",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Baseline utility bias (general acceptance tendency)",
      "range": {
        "min": -5,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "tags would be:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nBut to make it more concise, perhaps we can define fair_perc as (token_self / (token_self + token_opp)) * 100, and then U = alpha * (splitperc_self - fair_perc) + beta. However, in the model formula, we need to use the variables as per the dataset. \n\nAlternatively, since the dataset includes splitperc_self and splitperc_opp, but fair_perc isn't a variable, we need to compute it from token_self and token_opp. \n\nSo, the model formula would directly compute fair_perc as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the mathematical model is:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nThis way, all variables used are from the dataset (splitperc_self, token_self, token_opp) and the parameters alpha and beta. \n\nNow, the variables section in JSON should describe each variable in the model. \n\nsplitperc_self is a data variable (int64), token_self and token_opp are data variables (int64). alpha and beta are learnable parameters with specified ranges. \n\nFor alpha, since it's a sensitivity parameter, it should be positive. So the range is min 0, max perhaps 10, inclusive. Beta can be between -5 and 5. \n\nThe variables JSON would look like:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from fair percentage\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe target variable is 'accept', so enclosed in <target_variable> tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta",
  "variable_descriptions": {
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair percentage",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Baseline utility bias (general acceptance tendency)",
      "range": {
        "min": -5,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "splitperc_self": {
      "description": "Proposed percentage share for the participant",
      "range": {
        "min": 0,
        "max": 100,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A linear utility model where acceptance probability depends on the deviation between the proposed percentage and the participant's contribution-based fair share. The model uses two parameters: a sensitivity to fairness deviations (alpha) and a baseline utility bias (beta). The utility is converted to acceptance probability via a logistic function, with negative utilities corresponding to <50% acceptance likelihood.",
  "target_variable": "tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where the trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). \n\nFirst, I need to understand the variables provided and how they relate to the responder's decision. The key variables here are the tokens found by the participant (token_self) and the opponent (token_opp), the combined earnings, and the proposed splits (split_self and split_opp). The trial_type indicates whether contributions are equal, opponent more, or participant more. \n\nThe responder's decision likely depends on the fairness of the proposed split relative to their own contribution. Since each token is worth \u00a33, the combined_earning is 3*(token_self + token_opp). The split_self is the amount offered to the responder, and split_opp is what the proposer keeps. \n\nA common approach in such scenarios is to model the utility of accepting the offer. The responder might compare the proposed split to what they consider a fair share. The fair share could be based on their contribution (token_self) relative to the total (token_self + token_opp). For example, if the responder contributed 40% of the tokens, they might expect 40% of the combined earnings. \n\nSo, the model should calculate the utility of accepting the offer. If the utility is positive, the responder is more likely to accept. The utility could be the difference between the proposed split and the fair share, adjusted by some parameters. \n\nBut fairness isn't the only factor. People might have different sensitivities to inequity. For instance, some might reject offers that are slightly below their fair share, while others might accept even if it's unfair. Also, the absolute amount might matter\u2014people might be more sensitive to differences when the stakes are higher.\n\nAnother consideration is the participant's own contribution (token_self). If they contributed more, they might expect a higher share. The model should account for the proportion of tokens they contributed. \n\nLet me outline possible components for the utility function:\n\n1. **Fairness Component**: The difference between the proposed split (split_self) and the fair share based on their contribution. The fair share would be (token_self / (token_self + token_opp)) * combined_earning. Let's call this expected_fair. The difference would be split_self - expected_fair. \n\n2. **Inequity Aversion**: The responder might dislike inequity, so even if the split is fair, they might be sensitive to the absolute difference between split_self and split_opp. But since split_opp is the proposer's share, maybe they compare their split to the proposer's. However, in the responder role, the proposer's split is determined by the proposer, so the responder might focus on their own share relative to their contribution.\n\n3. **Non-linear Sensitivity**: The utility might not be linear. For example, the responder could have diminishing sensitivity to larger deviations from fairness. But to keep the model simple, maybe a linear term with a parameter to scale it would suffice.\n\n4. **Loss Aversion**: The responder might be more sensitive to offers below their fair share than above. This could be modeled with separate parameters for positive and negative deviations. However, adding more parameters could complicate the model and affect parameter recovery. Since the task mentions avoiding high covariance between parameters, maybe a single parameter for the sensitivity to unfairness would be better.\n\n5. **Alternative Formulation**: Another approach is to model the utility as the proposed split minus some reference point (like the fair share), scaled by a parameter. The probability of accepting is then a logistic function of this utility.\n\nPutting this together, the utility (U) could be:\n\nU = \u03b1 * (split_self - expected_fair) + \u03b2 * (split_self / combined_earning - split_opp / combined_earning)\n\nBut this introduces two parameters (\u03b1 and \u03b2). However, the problem states that parameter recovery is important, so we need to balance model complexity. Maybe combining these into a single term.\n\nAlternatively, the utility could be based on the difference between the proposed percentage (splitperc_self) and the fair percentage (token_self / (token_self + token_opp) * 100). Let's call this fair_perc. Then, the utility is the difference between splitperc_self and fair_perc, scaled by a parameter. \n\nU = \u03b1 * (splitperc_self - fair_perc)\n\nIf U is positive, the responder is more likely to accept. The probability of acceptance would be 1 / (1 + exp(-U)). \n\nBut maybe the responder's sensitivity depends on the magnitude of the difference. For example, a larger negative difference (unfair offer) might have a stronger effect. Alternatively, the model could include a non-linear term, but again, simplicity is key.\n\nAnother angle: the responder's decision could be influenced by both the absolute amount offered (split_self) and the relative fairness. For example, even if the offer is slightly unfair, if the absolute amount is high, they might accept. But this complicates the model with more parameters.\n\nWait, the problem mentions that two equal split_self variables don't represent the same fairness because combined_earning varies. So the model must account for the proportion of the split relative to the total. Using percentages (splitperc_self) might already handle this, as it's a percentage of the combined earnings. \n\nSo, the fair percentage is (token_self / (token_self + token_opp)) * 100. The difference between the proposed percentage (splitperc_self) and the fair percentage is the key variable. Let's call this difference \u0394 = splitperc_self - fair_perc. \n\nThe utility could be U = \u03b1 * \u0394. The probability of accepting is logistic(U). If \u0394 is positive (offer is more than fair), U is positive, leading to higher acceptance probability. If \u0394 is negative, U is negative, lower probability. \n\nBut maybe the responder has a non-linear sensitivity. For example, they might be more sensitive to unfairness when the offer is below the fair percentage. This could be modeled with a parameter that scales negative differences more. However, adding a parameter for this would increase model complexity. \n\nAlternatively, the model could include a parameter that represents the responder's threshold for acceptance. For example, they might accept if the offer is above (fair_perc - \u03b8), where \u03b8 is a learnable parameter representing their tolerance. But this would make the model deterministic, whereas we need a probabilistic model. \n\nWait, the problem states that the model should predict utility, where less than 0 is less than 50% likely. So the utility is passed through a logistic function to get the probability. So the utility function must be designed such that when U < 0, the probability is below 0.5, and vice versa. \n\nSo, the core idea is to model the utility as a function of how much the proposed split deviates from the fair split (based on contribution), scaled by a parameter. \n\nLet me formalize this:\n\nfair_perc = (token_self / (token_self + token_opp)) * 100\n\u0394 = splitperc_self - fair_perc\nU = \u03b1 * \u0394\n\nThen, the probability of accepting is \u03c3(U), where \u03c3 is the logistic function. \n\nThis model has one learnable parameter, \u03b1, which represents the sensitivity to the deviation from the fair percentage. A positive \u03b1 means that as the offer becomes more favorable (higher than fair), the utility increases, leading to higher acceptance probability. \n\nBut maybe the responder also cares about the absolute amount, not just the percentage. For example, getting 50% of \u00a310 is different from 50% of \u00a3100. However, since combined_earning varies, and the split_self is in \u00a3, perhaps the absolute amount is already considered. But in the model above, using percentages might normalize this. \n\nAlternatively, the utility could be based on the difference in monetary terms:\n\nfair_self = (token_self / (token_self + token_opp)) * combined_earning\n\u0394 = split_self - fair_self\nU = \u03b1 * \u0394\n\nBut this is similar to the percentage approach, just scaled by combined_earning. Since combined_earning is 3*(token_self + token_opp), the two approaches are related. \n\nHowever, using the monetary difference might make the parameter \u03b1 have units of utility per \u00a3, whereas using percentage difference would have units of utility per percentage point. Depending on the data, one might be more appropriate. \n\nBut given that the problem states that two equal split_self values don't represent the same fairness because combined_earning varies, using the percentage approach (splitperc_self) would naturally account for this, as the same split_self in \u00a3 would correspond to different percentages depending on combined_earning. \n\nTherefore, using the percentage difference seems appropriate. \n\nAnother consideration is the role of trial_type. For example, if the trial_type indicates whether the participant contributed more, the responder's expectations might vary. However, including trial_type would require additional parameters. Since the model should be kept simple, perhaps trial_type is captured indirectly through the token_self and token_opp variables. \n\nSo, the model would have one learnable parameter \u03b1, which scales the difference between the proposed percentage and the fair percentage. \n\nBut wait, maybe there's a baseline tendency to accept or reject regardless of fairness. For example, some participants might have a general tendency to accept offers (e.g., due to risk aversion or other factors). To account for this, we could add an intercept term \u03b2. \n\nSo, the utility would be:\n\nU = \u03b1 * \u0394 + \u03b2\n\nHere, \u03b2 represents a bias term. If \u03b2 is positive, the participant is more likely to accept even if the offer is slightly unfair. If negative, they are more likely to reject even fair offers. \n\nThis introduces a second parameter. However, the problem mentions that parameter recovery is a priority, so having two parameters might be manageable. But we need to check for identifiability. If \u03b1 and \u03b2 are both learnable, there could be covariance. For example, a higher \u03b2 could compensate for a lower \u03b1. But with enough data (36 trials per participant), it might still be feasible. \n\nAlternatively, maybe the intercept is not necessary. If the model is purely based on the fairness deviation, then when \u0394=0 (offer is exactly fair), the utility is zero, leading to a 50% chance of accepting. But in reality, participants might have a default tendency to accept fair offers. However, the problem states that the utility should predict such that U < 0 corresponds to less than 50% probability. So if the model is U = \u03b1 * \u0394, then when \u0394=0, U=0, probability is 0.5, which is correct. If the offer is unfair (\u0394 < 0), U < 0, probability < 0.5. If fair or generous, U >=0, probability >=0.5. \n\nBut perhaps participants have different baseline acceptance rates. For example, some might generally accept unless the offer is very unfair, while others might reject more often. To capture this, the intercept \u03b2 could be useful. However, adding \u03b2 increases the number of parameters. Since the task advises a slight bias towards fewer parameters, maybe starting with just \u03b1 is better. \n\nAlternatively, perhaps the intercept is necessary. Let's think: if the utility is U = \u03b1 * \u0394 + \u03b2, then even when \u0394=0, the utility is \u03b2. If \u03b2 is positive, the baseline probability is higher than 50%, and vice versa. This could capture a general acceptance bias. However, without \u03b2, the model assumes that exactly fair offers (\u0394=0) have a 50% chance of being accepted. If the data shows that participants accept fair offers more than 50% of the time, then \u03b2 would be needed. \n\nGiven that the problem states the model should predict utility where less than 0 is less than 50% likely, but doesn't specify the baseline, including \u03b2 might be necessary to capture individual differences in baseline acceptance rates. \n\nSo, the model would have two parameters: \u03b1 (sensitivity to fairness deviation) and \u03b2 (baseline utility). \n\nAnother consideration is the scaling of the parameters. For example, \u03b1 should be positive, as a higher \u0394 (more favorable offer) should increase utility. So we can set a bound for \u03b1, say [0, 10], to ensure it's positive. \u03b2 could be in a range like [-5, 5], allowing for baseline tendencies. \n\nBut how to determine the bounds? The problem says to specify finite numerical bounds that are generous. For \u03b1, since it's a scaling factor on the percentage difference (which can be up to \u00b1100), a range of [0, 10] would mean that each percentage point difference contributes up to 10 utility units. For \u03b2, a range of [-5, 5] allows the baseline utility to shift the acceptance probability between roughly 0.007 (for \u03b2=-5) to 0.993 (for \u03b2=5) when \u0394=0. \n\nAlternatively, using a logistic function with temperature 1, the utility directly maps to probability via sigmoid. So, the parameters' scales should be such that typical \u0394 values (say, \u00b120%) multiplied by \u03b1 plus \u03b2 would cover a reasonable range of utilities. \n\nBut perhaps the bounds can be set as \u03b1 in [0, 10] and \u03b2 in [-5, 5], which are generous enough. \n\nAnother possible factor is the participant's own contribution (token_self). Maybe the utility also depends on the absolute number of tokens they contributed. For example, if they contributed more tokens, they might feel more entitled, affecting their acceptance decision. However, this is already captured in the fair_perc calculation, as token_self directly influences fair_perc. \n\nAlternatively, the model could include a term for the participant's own tokens, but that might introduce redundancy. \n\nSo, the core model remains U = \u03b1 * (splitperc_self - fair_perc) + \u03b2. \n\nNow, let's check the variables in the dataset. The variables available include splitperc_self, token_self, token_opp, etc. The fair_perc can be calculated as (token_self / (token_self + token_opp)) * 100. \n\nBut wait, token_self and token_opp are the number of tokens found by the participant and opponent, respectively. The combined_earning is 3*(token_self + token_opp). So, the fair share in percentage is indeed (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the model's variables are:\n\n- splitperc_self: from the data.\n- fair_perc: calculated as (token_self / (token_self + token_opp)) * 100.\n- \u0394 = splitperc_self - fair_perc.\n- U = \u03b1 * \u0394 + \u03b2.\n\nThe parameters \u03b1 and \u03b2 are learnable. \n\nNow, the variables section needs to describe each variable in the model. The variables in the model are U, \u03b1, \u03b2, splitperc_self, token_self, token_opp, fair_perc (calculated), and \u0394 (calculated). However, in the variables JSON, we need to list all variables used in the model formula. \n\nWait, the model formula as written would be:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nSo, the variables directly used are splitperc_self, token_self, token_opp, alpha, beta. The fair_perc is a calculated intermediate variable. \n\nTherefore, in the variables section, we need to include:\n\n- splitperc_self: from data, int64.\n- token_self: from data, int64.\n- token_opp: from data, int64.\n- alpha: learnable parameter, range [0, 10].\n- beta: learnable parameter, range [-5, 5].\n\nThe utility U is calculated. \n\nSo, the model formula in <MODEL> tags would be:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nBut to make it more concise, perhaps we can define fair_perc as (token_self / (token_self + token_opp)) * 100, and then U = alpha * (splitperc_self - fair_perc) + beta. However, in the model formula, we need to use the variables as per the dataset. \n\nAlternatively, since the dataset includes splitperc_self and splitperc_opp, but fair_perc isn't a variable, we need to compute it from token_self and token_opp. \n\nSo, the model formula would directly compute fair_perc as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the mathematical model is:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nThis way, all variables used are from the dataset (splitperc_self, token_self, token_opp) and the parameters alpha and beta. \n\nNow, the variables section in JSON should describe each variable in the model. \n\nsplitperc_self is a data variable (int64), token_self and token_opp are data variables (int64). alpha and beta are learnable parameters with specified ranges. \n\nFor alpha, since it's a sensitivity parameter, it should be positive. So the range is min 0, max perhaps 10, inclusive. Beta can be between -5 and 5. \n\nThe variables JSON would look like:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from fair percentage\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe target variable is 'accept', so enclosed in <target_variable> tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept</target_variable>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair percentage\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline utility bias (general acceptance tendency)\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA linear utility model where acceptance probability depends on the deviation between the proposed percentage and the participant's contribution-based fair share. The model uses two parameters: a sensitivity to fairness deviations (alpha) and a baseline utility bias (beta). The utility is converted to acceptance probability via a logistic function, with negative utilities corresponding to <50% acceptance likelihood.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: tags would be:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nBut to make it more concise, perhaps we can define fair_perc as (token_self / (token_self + token_opp)) * 100, and then U = alpha * (splitperc_self - fair_perc) + beta. However, in the model formula, we need to use the variables as per the dataset. \n\nAlternatively, since the dataset includes splitperc_self and splitperc_opp, but fair_perc isn't a variable, we need to compute it from token_self and token_opp. \n\nSo, the model formula would directly compute fair_perc as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the mathematical model is:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nThis way, all variables used are from the dataset (splitperc_self, token_self, token_opp) and the parameters alpha and beta. \n\nNow, the variables section in JSON should describe each variable in the model. \n\nsplitperc_self is a data variable (int64), token_self and token_opp are data variables (int64). alpha and beta are learnable parameters with specified ranges. \n\nFor alpha, since it's a sensitivity parameter, it should be positive. So the range is min 0, max perhaps 10, inclusive. Beta can be between -5 and 5. \n\nThe variables JSON would look like:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from fair percentage\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe target variable is 'accept', so enclosed in <target_variable> tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\nSummary: A linear utility model where acceptance probability depends on the deviation between the proposed percentage and the participant's contribution-based fair share. The model uses two parameters: a sensitivity to fairness deviations (alpha) and a baseline utility bias (beta). The utility is converted to acceptance probability via a logistic function, with negative utilities corresponding to <50% acceptance likelihood.\nTarget Variable: tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=0):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        # Calculate fair percentage\n        if (token_self + token_opp) == 0:\n            fair_perc = 50  # Avoid division by zero, assume 50/50 if both have 0 tokens\n        else:\n            fair_perc = (token_self / (token_self + token_opp)) * 100\n\n        # Calculate utility\n        utility = alpha * (splitperc_self - fair_perc) + beta\n\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "fitting_warnings": [
    "Error fitting participant 1001: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1002: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1003: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1004: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1005: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1006: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1007: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1008: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1009: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1010: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1011: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1012: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1013: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1015: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1016: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1017: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1018: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1019: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1020: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1021: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1022: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1023: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1024: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1025: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1026: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1027: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1028: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1029: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1030: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1031: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1032: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1033: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1034: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1035: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1036: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1037: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1038: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1039: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1040: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1041: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1042: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1043: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1044: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2001: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2002: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2003: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2004: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2005: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2006: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2007: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2008: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2009: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2010: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2011: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2012: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2013: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2014: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2015: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2016: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2017: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2018: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2019: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2020: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2021: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2022: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2023: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2024: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2025: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2026: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2027: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2028: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2029: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2030: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2031: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2032: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2033: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2034: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2035: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2036: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2037: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2038: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2039: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2040: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2041: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2042: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2043: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2044: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2045: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2046: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2047: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2048: Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
  ],
  "skipped_participants": [
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept' column in data"
    }
  ],
  "num_skipped_participants": 91,
  "fitting_error": "No successful parameter fits obtained",
  "recovery_error": "No fitting results found",
  "previous_models": [
    "Specification: tags would be:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nBut to make it more concise, perhaps we can define fair_perc as (token_self / (token_self + token_opp)) * 100, and then U = alpha * (splitperc_self - fair_perc) + beta. However, in the model formula, we need to use the variables as per the dataset. \n\nAlternatively, since the dataset includes splitperc_self and splitperc_opp, but fair_perc isn't a variable, we need to compute it from token_self and token_opp. \n\nSo, the model formula would directly compute fair_perc as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the mathematical model is:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n\nThis way, all variables used are from the dataset (splitperc_self, token_self, token_opp) and the parameters alpha and beta. \n\nNow, the variables section in JSON should describe each variable in the model. \n\nsplitperc_self is a data variable (int64), token_self and token_opp are data variables (int64). alpha and beta are learnable parameters with specified ranges. \n\nFor alpha, since it's a sensitivity parameter, it should be positive. So the range is min 0, max perhaps 10, inclusive. Beta can be between -5 and 5. \n\nThe variables JSON would look like:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from fair percentage\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe target variable is 'accept', so enclosed in <target_variable> tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\nSummary: A linear utility model where acceptance probability depends on the deviation between the proposed percentage and the participant's contribution-based fair share. The model uses two parameters: a sensitivity to fairness deviations (alpha) and a baseline utility bias (beta). The utility is converted to acceptance probability via a logistic function, with negative utilities corresponding to <50% acceptance likelihood.\nTarget Variable: tags.\n\nThe summary should describe the model concisely. It's a utility model combining fairness deviation and baseline bias. The fairness deviation is the difference between the proposed percentage and the participant's contribution-based fair percentage. The utility is a linear function of this deviation with sensitivity and bias parameters, predicting acceptance probability via logistic function.\n\nSo, the summary would be:\n\nA linear utility model combining the deviation from the contribution-based fair percentage and a baseline bias. The model uses two learnable parameters: a sensitivity parameter (alpha) constrained to [0,10] and a baseline utility parameter (beta) in [-5,5]. The utility is calculated as the sensitivity-weighted fairness deviation plus the baseline, determining the probability of accepting the proposal via logistic transformation.\n\nThis model has two learnable parameters, which is manageable for parameter recovery, and the variables are all present in the dataset. The model directly addresses the fairness consideration based on the participant's contribution, which is crucial for predicting responder behavior.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta\n</MODEL>\n\n<target_variable>accept\nPrediction Type: utility"
  ]
}