{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: THIS IS A TEST TO SEE IF THESE INSTRUCTIONS ARE ACTUALLY BEING DISPLAYED WITHIN THE PIPELINE\n\nBe VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0), so a logistic model makes sense here.\n\nFirst, I need to think about the factors that influence a responder's decision. The key variables provided are token_self, token_opp, split_self, split_opp, combined_earning, and trial_type. The main idea is that the responder's utility depends on their own share compared to some notion of fairness, possibly based on contributions (tokens) and the total combined earnings.\n\nIn many economic games, fairness is a crucial factor. The responder might compare the proposed split to what they consider a fair split based on contributions. For example, if the participant contributed more tokens (token_self), they might expect a larger share. The trial_type variable indicates whether the contributions are equal, opponent more, or participant more, which could influence fairness perceptions.\n\nSo, the utility could be a function of the difference between the proposed split (split_self) and the fair split. The fair split might be calculated as the proportion of tokens contributed by the participant. For instance, fair_share = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and fair_share would be a key term. If the proposed split is higher than the fair share, the responder is more likely to accept, and vice versa.\n\nBut maybe participants have different sensitivities to unfairness. Some might be more tolerant of being undercompensated, while others might strongly reject it. So, parameters could capture the sensitivity to advantageous and disadvantageous inequality. This is similar to the Fehr-Schmidt model of inequity aversion.\n\nAlternatively, the model could use a logistic function where the utility is the difference between the proposed split and the fair split, scaled by a parameter. The probability of accepting would then be a logistic function of this utility.\n\nAnother consideration is the absolute amount of the split. Even if the split is fair, if the absolute amount is too low, the responder might reject. But the problem states that two equal split_self variables don't represent the same fairness because combined_earning varies. So, the model should account for the proportion relative to contributions, not just absolute amounts.\n\nLet's outline possible variables:\n\n1. Fair share based on tokens: fair_share = (token_self / (token_self + token_opp)) * combined_earning\n2. Difference between proposed split and fair share: delta = split_self - fair_share\n3. Utility could be a function of delta. For example, utility = alpha * delta, where alpha is a sensitivity parameter.\n4. The probability of accepting is logistic(utility). If utility > 0, higher chance to accept.\n\nBut maybe participants also care about the opponent's share. For example, if the opponent is getting more than their fair share, the responder might feel envious. Alternatively, if the responder is getting more than their fair share, they might feel guilty. So, parameters could capture aversion to advantageous and disadvantageous inequality.\n\nWait, the Fehr-Schmidt model has two parameters: alpha (disutility from disadvantageous inequality) and beta (disutility from advantageous inequality). But in this case, since the responder is deciding to accept or reject, their utility might be based on their own share compared to the fair share, not directly the opponent's share. However, the opponent's split (split_opp) is part of the data. But perhaps the key comparison is between split_self and the fair share based on tokens.\n\nAlternatively, the responder's utility could be split_self minus their fair share, scaled by a parameter. If this is positive, they accept; if negative, they might reject. But the model needs to translate this into a probability.\n\nAnother angle: the utility could be the proposed split_self minus the minimum acceptable amount. The minimum acceptable amount could be a function of their contribution. For example, min_accept = gamma * fair_share. Then, utility = split_self - gamma * fair_share. If this is positive, accept; else, reject. Gamma here is a parameter indicating the threshold (e.g., gamma=0.5 means they accept if they get at least half of their fair share).\n\nBut how to model this? The utility would be split_self - gamma * fair_share. Then, the probability of accepting is 1 / (1 + exp(-utility)). Wait, but the utility here is linear in parameters. However, gamma would be a learnable parameter. But maybe we need to include a temperature parameter (beta) to scale the utility. So, the probability would be logistic(beta * (split_self - gamma * fair_share)).\n\nAlternatively, the model could have parameters that weight the difference between the proposed split and the fair share. For example, utility = theta * (split_self - fair_share). Theta is a learnable parameter. The probability of accepting is 1 / (1 + exp(-utility)).\n\nBut perhaps participants have different fairness thresholds. For example, they might accept if the proposed split is at least a certain percentage of their fair share. So, maybe split_self >= lambda * fair_share. Lambda is a parameter between 0 and 1. But how to model this in a utility function that can be used in a logistic regression. Maybe the utility is split_self - lambda * fair_share. Then, the probability of accepting is sigmoid(utility * beta), where beta is a scaling parameter.\n\nAlternatively, the model could use the ratio of split_self to fair_share. For example, utility = beta * (split_self / fair_share - 1). If this is positive, higher chance to accept. But if fair_share is zero, this could be problematic, but in the dataset, token_self and token_opp are non-negative integers, and combined_earning is their sum times \u00a33. So, fair_share would be zero only if token_self is zero, which might be possible. But maybe in such cases, the model can handle it by adding a small epsilon, but perhaps the data doesn't have such cases. Alternatively, the model could use the difference instead of the ratio.\n\nAnother consideration is the trial_type variable. It indicates whether contributions are equal, opponent more, or participant more. This might affect the responder's perception. For example, if the participant contributed more (trial_type=3), they might have a higher expectation, so the parameter lambda could vary based on trial_type. But adding interactions with trial_type would increase the number of parameters, which might complicate the model. Since the model needs to pass parameter recovery tests and avoid high covariance, it's better to keep the number of parameters low.\n\nAlternatively, trial_type could be incorporated into the fair_share calculation. Wait, trial_type is derived from token_self and token_opp. For example, trial_type=1 when token_self = token_opp, 2 when token_opp > token_self, and 3 when token_self > token_opp. So, trial_type is redundant with token_self and token_opp. Therefore, including token_self and token_opp in the model would already capture the information in trial_type. So, perhaps trial_type isn't necessary in the model.\n\nSo, the key variables are token_self, token_opp, split_self, combined_earning. The fair_share is (token_self / (token_self + token_opp)) * combined_earning. The difference between split_self and fair_share is delta. The utility is a function of delta, possibly scaled by a parameter.\n\nBut maybe participants also consider the absolute amount they receive. For example, even if the split is fair, if the absolute amount is too low, they might reject. But combined_earning varies, so the split_self is in absolute terms. However, the problem states that two equal split_self values don't represent the same fairness because combined_earning varies. So, the model should account for the proportion relative to contributions, not just the absolute split_self.\n\nTherefore, the core idea is to model the utility as the difference between the proposed split_self and the fair_share, scaled by a parameter. Additionally, there might be a parameter that captures the participant's sensitivity to this difference. The probability of accepting is then a logistic function of this utility.\n\nSo, the utility U could be:\n\nU = \u03b1 * (split_self - fair_share) + \u03b2 * split_self\n\nWait, but split_self is part of the fair_share calculation. Alternatively, maybe the model should have two components: one for the fairness (split_self vs fair_share) and another for the absolute amount. But the problem states that split_self's fairness depends on combined_earning, so the absolute amount is already considered in the fair_share.\n\nAlternatively, the utility could be:\n\nU = \u03b8 * (split_self - fair_share) + \u03b3 * split_self\n\nBut this might lead to multicollinearity if split_self and fair_share are correlated. Alternatively, since fair_share is (token_self / total_tokens) * combined_earning, and split_self is part of the split, perhaps the key term is the difference between split_self and fair_share.\n\nAnother approach is to model the utility as the ratio of split_self to fair_share. For example, U = \u03b8 * (split_self / fair_share - 1). If split_self equals fair_share, U is zero. If it's higher, U is positive; lower, negative. Then, the probability of accepting is logistic(U). But this ratio could be problematic if fair_share is zero, but in the dataset, token_self and token_opp are non-negative, and combined_earning is their sum times 3. So, fair_share would be zero only if token_self is zero, which might be a rare case. But perhaps in such cases, the model can handle it by setting a default behavior.\n\nAlternatively, use the difference instead of the ratio to avoid division by zero. So, U = \u03b8 * (split_self - fair_share). Theta is a learnable parameter. The probability of accepting is 1 / (1 + exp(-U)).\n\nBut maybe participants have a non-linear sensitivity to the difference. For example, they might be more sensitive to under-offers than over-offers. So, the model could have separate parameters for positive and negative differences. For example:\n\nU = \u03b8_plus * max(split_self - fair_share, 0) + \u03b8_minus * min(split_self - fair_share, 0)\n\nThis way, \u03b8_plus captures the sensitivity to advantageous offers (split_self > fair_share), and \u03b8_minus captures sensitivity to disadvantageous offers. However, this introduces two parameters, which might complicate the model but could capture different effects for over and under offers.\n\nAlternatively, use a single parameter but apply different signs. For example, U = \u03b8 * (split_self - fair_share). If \u03b8 is positive, then higher split_self relative to fair_share increases utility. But if participants are more sensitive to negative differences, \u03b8 could be a parameter that is multiplied by the difference. However, this might not capture asymmetric sensitivity.\n\nBut given the need to keep the model simple and avoid high covariance between parameters, perhaps a single parameter is better. Let's start with a simple model and see if it can be expanded.\n\nSo, the basic model would be:\n\nUtility U = \u03b8 * (split_self - fair_share)\n\nProbability of accept = 1 / (1 + exp(-U))\n\nBut \u03b8 is a learnable parameter. However, this model assumes that the participant's decision is based solely on the difference between the proposed split and their fair share. If \u03b8 is positive, a higher split_self than fair_share increases the probability of accepting.\n\nBut maybe participants also consider the absolute amount they receive. For example, even if the split is fair, if the absolute amount is too low, they might reject. However, since combined_earning varies, the split_self's absolute value is tied to the total. But perhaps participants have a minimum acceptable amount that's a proportion of the combined_earning. For example, they might accept if split_self >= \u03bb * combined_earning, where \u03bb is a parameter. But this could be part of the utility function.\n\nAlternatively, the utility could be a combination of the difference from fair share and the absolute split. For example:\n\nU = \u03b8 * (split_self - fair_share) + \u03b3 * split_self\n\nBut this introduces another parameter (\u03b3), which might be necessary if participants care about both fairness and the absolute amount. However, this could lead to multicollinearity if split_self and fair_share are correlated. Let's check: fair_share is (token_self / (token_self + token_opp)) * combined_earning. split_self is part of the split, which is a portion of combined_earning. So, split_self = s * combined_earning, where s is the percentage (splitperc_self / 100). Similarly, fair_share is f * combined_earning, where f = token_self / (token_self + token_opp). So, split_self - fair_share = (s - f) * combined_earning. Therefore, the difference is scaled by combined_earning. So, the utility U = \u03b8 * (s - f) * combined_earning. If \u03b8 is positive, higher (s - f) leads to higher utility. But split_self itself is s * combined_earning. So, including split_self in the model would be s * combined_earning. But if we already have (s - f) * combined_earning, then adding split_self would be redundant unless there's an additional effect.\n\nAlternatively, perhaps the utility should be based on the proportional difference rather than the absolute difference. For example, U = \u03b8 * (s / f - 1), which is the percentage difference from the fair share. This way, the utility is unitless and might be more comparable across trials with different combined_earning. However, this again introduces the issue of division by zero if f is zero.\n\nBut given that token_self and token_opp are non-negative integers, f can only be zero if token_self is zero, which might be a rare case. In such cases, perhaps the fair_share is zero, and the participant's decision is based solely on the absolute split_self. But the model needs to handle that. However, for simplicity, maybe we can proceed under the assumption that f is positive, or handle those cases separately. But given the data, perhaps it's safe to proceed.\n\nSo, the utility could be:\n\nU = \u03b8 * (split_self / fair_share - 1)\n\nIf split_self equals fair_share, U is zero. If it's higher, U is positive. The probability of accepting is logistic(U). But \u03b8 here scales the effect of the proportional difference. However, this approach might not account for the absolute amount, but since combined_earning varies, the proportional approach might be better.\n\nAlternatively, combining both proportional and absolute terms. But again, this complicates the model.\n\nAnother consideration is the role of the opponent's contribution. The variable token_opp represents the opponent's contribution. The responder might compare their own contribution to the opponent's. For example, if the responder contributed more, they might expect a higher share. This is already captured in the fair_share calculation, which is based on their own and the opponent's tokens.\n\nSo, the fair_share is already a function of both token_self and token_opp. Therefore, including the fair_share in the model accounts for both contributions.\n\nPutting this together, the model's utility function should compare the proposed split_self to the fair_share. The parameters would determine how sensitive the participant is to this difference.\n\nTo account for possible non-linearities or different sensitivities to over vs. under offers, we could introduce two parameters: one for when the offer is above the fair share (advantageous inequality) and one for when it's below (disadvantageous inequality). This is similar to the Fehr-Schmidt model.\n\nSo, the utility could be:\n\nU = \u03b8_adv * max(split_self - fair_share, 0) + \u03b8_dis * min(split_self - fair_share, 0)\n\nHere, \u03b8_adv is the sensitivity to advantageous offers (split_self > fair_share), and \u03b8_dis is the sensitivity to disadvantageous offers (split_self < fair_share). If \u03b8_adv is positive, it increases utility when the offer is above fair share, encouraging acceptance. If \u03b8_dis is negative, it decreases utility when the offer is below fair share, encouraging rejection.\n\nBut in the Fehr-Schmidt model, the utility function includes terms for both advantageous and disadvantageous inequality. However, in this case, since the responder is deciding to accept or reject, their utility is based on their own share relative to the fair share. The Fehr-Schmidt model's utility function for player i is:\n\nU_i = x_i - \u03b1 * max(x_j - x_i, 0) - \u03b2 * max(x_i - x_j, 0)\n\nWhere x_i is player i's payoff, and x_j is the other player's. \u03b1 and \u03b2 are parameters for disadvantageous and advantageous inequality aversion, respectively.\n\nBut in our case, the responder's payoff is split_self if accepted, and possibly zero if rejected (but the dataset's accepted_amount is the amount gained, which is split_self if accept=1, else zero). However, the model needs to predict accept/reject based on the proposed split, not the outcome. So, the utility of accepting is based on split_self and split_opp, while rejecting gives zero. But the problem states that the model should predict accept based on the proposed split.\n\nWait, in the ultimatum game, the responder's decision depends on their share compared to the proposer's. If the proposer keeps too much, the responder might reject to punish, even if their share is positive. But in this task, the description mentions that the combined_earning is split between the participant and opponent. So, split_self + split_opp = combined_earning. Therefore, the responder might compare their split to the opponent's split. But the fair_share based on contributions is another reference point.\n\nSo, perhaps the utility should include both the comparison to the fair share and the comparison to the opponent's split. For example:\n\nU = \u03b8_fair * (split_self - fair_share) + \u03b8_inequity * (split_self - split_opp)\n\nBut this introduces two parameters. However, split_opp is equal to combined_earning - split_self, so split_self - split_opp = 2*split_self - combined_earning. This might be redundant with split_self and combined_earning. Alternatively, the inequity term could be split_self - (split_opp), which is 2*split_self - combined_earning. But this might complicate the model.\n\nAlternatively, the inequity term could be split_self / split_opp, but that could be problematic if split_opp is zero.\n\nGiven the complexity, perhaps the model should focus on the difference between split_self and fair_share, as that directly relates to the participant's contribution. Including both terms might lead to overparameterization and high covariance between \u03b8_fair and \u03b8_inequity.\n\nTherefore, to keep the model simple and ensure parameter recovery, let's start with a single parameter model based on the difference from the fair share.\n\nSo, the utility function is:\n\nU = \u03b8 * (split_self - fair_share)\n\nThe probability of accepting is logistic(U). If U > 0, the probability is greater than 0.5.\n\nBut wait, the problem states that the utility should be such that less than 0 corresponds to less than 50% likelihood. That's exactly what the logistic function does: when U < 0, the probability is < 0.5.\n\nHowever, this model assumes that the participant's decision is solely based on how much they receive compared to their fair share. If they get more than fair, they're more likely to accept; less, more likely to reject. The parameter \u03b8 scales the sensitivity to this difference.\n\nBut perhaps participants have a different baseline. For example, they might require a certain minimum above the fair share. So, the utility could be:\n\nU = \u03b8 * (split_self - (fair_share + delta))\n\nWhere delta is a parameter representing a required surplus. But this adds another parameter. However, the problem states that the model should have a slight bias towards fewer parameters. So, maybe stick with one parameter.\n\nAlternatively, the model could include a parameter for the slope (\u03b8) and a parameter for the threshold (delta), leading to two parameters. But this increases the complexity.\n\nAnother consideration is the role of the trial_role. Since the model is only for responders (trial_role=1), we don't need to account for proposer behavior. So, all data used in the model is when trial_role=1.\n\nNow, considering the variables in the dataset, the fair_share can be calculated as (token_self / (token_self + token_opp)) * combined_earning. However, combined_earning is token_self * 3 + token_opp * 3 = 3*(token_self + token_opp). Wait, no: each token is \u00a33, so combined_earning is 3*(token_self + token_opp). Therefore, fair_share = (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3 * token_self. Wait, that's interesting. Because combined_earning is 3*(token_self + token_opp), and the fair_share based on tokens would be (token_self / (token_self + token_opp)) * combined_earning = 3 * token_self. So, the fair_share is simply 3 * token_self. That's a key insight!\n\nWait, let's verify:\n\ntoken_self is the number of tokens found by the participant. Each token is \u00a33. So, the participant's contribution is 3 * token_self. Similarly, the opponent's contribution is 3 * token_opp. The combined_earning is 3*(token_self + token_opp). The fair_share based on contributions would be (3*token_self) / (3*(token_self + token_opp)) * combined_earning. Wait, no: the fair_share is the proportion of the combined_earning that matches their contribution. Since the combined_earning is the sum of both contributions, the fair_share would be equal to their own contribution. Because:\n\nIf each contributed token is \u00a33, then the participant's contribution is 3*token_self, opponent's is 3*token_opp. The total is 3*(token_self + token_opp) = combined_earning. So, the fair_share based on contribution would be exactly the participant's contribution: 3*token_self. Because that's their contribution to the pot. Therefore, the fair_share is 3*token_self.\n\nWait, that's a crucial point. So, the fair_share is simply 3 * token_self, which is the participant's own contribution. The split_self is the amount offered to the participant from the combined_earning. So, the difference between split_self and fair_share is split_self - 3*token_self. If split_self is greater than 3*token_self, the participant is getting more than their contribution; if less, they're getting less.\n\nTherefore, the utility could be based on this difference. So, U = \u03b8 * (split_self - 3*token_self). The parameter \u03b8 represents the sensitivity to this difference. If \u03b8 is positive, then when split_self > 3*token_self, utility is positive, increasing the probability of acceptance. If \u03b8 is negative, the opposite.\n\nBut wait, in the task description, it says that \"two equal split_self variables do not represent the same 'fairness'\". For example, if combined_earning is higher, the same split_self might be a smaller percentage. But according to this calculation, the fair_share is 3*token_self, which is fixed based on the participant's tokens. So, split_self is compared to this fixed value. However, combined_earning is 3*(token_self + token_opp), so split_self can vary from 0 to combined_earning.\n\nBut this approach simplifies the model because fair_share is directly 3*token_self. Therefore, the difference split_self - fair_share is split_self - 3*token_self. This difference can be positive or negative. The utility is then \u03b8 multiplied by this difference.\n\nSo, the model becomes:\n\nU = \u03b8 * (split_self - 3 * token_self)\n\nProbability of accept = 1 / (1 + exp(-U))\n\nThis model has one learnable parameter, \u03b8, which can be positive or negative. If \u03b8 is positive, participants are more likely to accept when split_self exceeds their contribution. If \u03b8 is negative, the opposite. But typically, we'd expect \u03b8 to be positive, as getting more than your contribution is better.\n\nHowever, this model assumes that participants base their decision solely on whether they're getting at least their contribution back. But in reality, people might have different reference points. For example, some might accept if they get any positive amount, while others might want at least half of their contribution. But this model captures the basic idea of comparing the offer to their contribution.\n\nAlternatively, the model could use a parameter that represents the proportion of their contribution they require. For example, U = \u03b8 * (split_self - \u03bb * 3 * token_self). Here, \u03bb is a parameter between 0 and 1, representing the minimum proportion of their contribution they require. If \u03bb=1, they require the full contribution; if \u03bb=0.5, they accept if they get at least half. But this introduces two parameters (\u03b8 and \u03bb), which might complicate the model but allows for more flexibility.\n\nHowever, the problem states that the model should prioritize parameter recovery, which is easier with fewer parameters. So, starting with one parameter is better.\n\nAnother consideration is the intercept. Maybe participants have a baseline tendency to accept or reject regardless of the offer. So, the utility could include an intercept term:\n\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\n\nHere, \u03b2_0 is a bias term. If \u03b2_0 is positive, there's a higher baseline acceptance rate. This adds another parameter, but it might be necessary if there's a general tendency to accept or reject.\n\nBut again, to keep the model simple, perhaps omit the intercept. However, without an intercept, the model assumes that when split_self equals 3*token_self, the utility is zero, leading to a 50% chance of accepting. But maybe participants have a different baseline. Including an intercept allows for that.\n\nSo, the model with an intercept would be:\n\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\n\nThis has two learnable parameters: \u03b2_0 and \u03b8. The intercept \u03b2_0 could capture the general willingness to accept offers even when split_self is below the fair share. For example, if \u03b2_0 is positive, even when split_self is slightly below fair_share, the participant might still accept.\n\nBut the problem states that the model should have a slight bias towards fewer parameters. So, perhaps start with one parameter. However, including the intercept might be necessary for better fit. Let's think: if we omit the intercept, when split_self equals fair_share, the probability is 50%. But maybe participants have a default acceptance rate higher or lower than 50% when the offer is exactly fair. Including the intercept allows for that.\n\nBut given the need to balance simplicity and accuracy, perhaps include the intercept. So, two parameters: \u03b2_0 and \u03b8.\n\nAnother angle: the utility could be based on the ratio of split_self to fair_share. For example:\n\nU = \u03b8 * (split_self / (3 * token_self) - 1)\n\nThis way, the utility is zero when split_self equals fair_share. If split_self is double the fair_share, U is \u03b8. But this introduces division by zero if token_self is zero. However, in such cases, the fair_share is zero, and split_self would be part of the combined_earning. But if token_self is zero, the participant contributed nothing, so their fair_share is zero. In that case, any positive split_self is more than fair. But the model would have U = \u03b8 * (split_self / 0 - 1), which is undefined. Therefore, this approach is problematic.\n\nThus, the difference approach is better to avoid division by zero.\n\nBack to the difference model with intercept:\n\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\n\nThis model has two parameters. The intercept \u03b2_0 could be bounded between, say, -5 and 5, and \u03b8 between -5 and 5. These are generous bounds. The probability of acceptance is logistic(U).\n\nBut wait, the problem states that the utility variable may be unbounded, but learnable parameters must have finite bounds. So, \u03b2_0 and \u03b8 would have finite bounds, but the utility U itself is unbounded as it's a linear combination.\n\nThis model uses variables split_self and token_self (from the dataset), calculates 3*token_self, and uses the difference. The parameters \u03b2_0 and \u03b8 are learnable.\n\nAnother consideration: maybe the participant's decision is influenced by how much the opponent is getting. For example, if the opponent is getting a lot more, even if the participant's share is fair, they might reject out of envy. But this would require including split_opp in the model. However, split_opp is equal to combined_earning - split_self. Since combined_earning is 3*(token_self + token_opp), split_opp = 3*(token_self + token_opp) - split_self. Including split_opp might add redundancy, but perhaps the participant cares about the opponent's share relative to their contribution.\n\nAlternatively, the participant might compare the opponent's share to their contribution. The opponent's fair share would be 3*token_opp. So, the opponent's split_opp minus their fair share is split_opp - 3*token_opp. If the opponent is getting more than their fair share, the participant might be more likely to reject. So, the utility could include both terms:\n\nU = \u03b2_0 + \u03b8_self * (split_self - 3*token_self) + \u03b8_opp * (split_opp - 3*token_opp)\n\nBut split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self. So, split_opp - 3*token_opp = 3*token_self + 3*token_opp - split_self - 3*token_opp = 3*token_self - split_self. Therefore, this term is equal to -(split_self - 3*token_self). So, the two terms are negatives of each other. Therefore, including both would lead to perfect multicollinearity. Hence, this approach isn't feasible.\n\nTherefore, including both terms isn't useful. The model should focus on the participant's own share relative to their contribution.\n\nThus, the model with intercept and one parameter seems reasonable:\n\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\n\nBut perhaps the intercept is redundant. For example, if when split_self equals 3*token_self, the utility is \u03b2_0. If \u03b2_0 is positive, the baseline acceptance is higher than 50% even when the offer is fair. If it's zero, then at fair offers, the probability is 50%. Including \u03b2_0 allows for this flexibility.\n\nHowever, adding \u03b2_0 introduces another parameter. Given the need to balance simplicity and accuracy, perhaps include it. But the problem states that parameter recovery is roughly equal in priority to accuracy. So, a model with two parameters (\u03b2_0 and \u03b8) is manageable.\n\nAlternatively, if we omit \u03b2_0, the model has one parameter. Let's compare:\n\nModel 1 (1 parameter):\nU = \u03b8 * (split_self - 3 * token_self)\nProbability = 1 / (1 + exp(-U))\n\nModel 2 (2 parameters):\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\nProbability = 1 / (1 + exp(-U))\n\nModel 2 allows for a baseline tendency to accept or reject when the offer is exactly fair. For example, if \u03b8 is positive and \u03b2_0 is positive, even offers slightly below fair might still be accepted. If \u03b2_0 is negative, even fair offers might be rejected.\n\nGiven that participants might have different baseline tendencies, including \u03b2_0 could improve the model's accuracy. However, it adds another parameter, which might affect parameter recovery. But with 36 trials per participant, two parameters should be recoverable.\n\nTherefore, proceeding with Model 2.\n\nNow, the variables in the model:\n\n- split_self: from the dataset (float64)\n- token_self: from the dataset (int64)\n- 3 * token_self: calculated as fair_share\n- split_self - 3 * token_self: calculated difference\n- \u03b2_0: learnable parameter (intercept)\n- \u03b8: learnable parameter (sensitivity to difference)\n\nThe utility U is \u03b2_0 + \u03b8*(split_self - 3*token_self). The probability of accept is logistic(U).\n\nThis model uses variables split_self and token_self from the dataset. The parameters \u03b2_0 and \u03b8 are learnable, with bounds. For example, \u03b2_0 could be bounded between -10 and 10, \u03b8 between -10 and 10. These are generous bounds to allow for different sensitivities.\n\nAnother consideration: the difference (split_self - 3*token_self) can be positive or negative. If \u03b8 is positive, positive differences increase utility, leading to higher acceptance. If \u03b8 is negative, the opposite. However, typically, we'd expect \u03b8 to be positive, as higher offers should increase acceptance. But the model allows for both possibilities.\n\nNow, checking the dataset variables:\n\nThe dataset has split_self (float64) and token_self (int64). So, the model uses these variables directly.\n\nThe target variable is 'accept' (float64, 0 or 1). The model predicts the probability of accept=1.\n\nThis model meets the requirements: it uses only the provided variables, predicts responder behavior (trial_role=1), and the utility is unbounded, with learnable parameters having finite bounds.\n\nNow, specifying the mathematical model:\n\n<MODEL>\nU = beta_0 + theta * (split_self - 3 * token_self)\n</MODEL>\n\nVariables:\n\n- beta_0: learnable intercept, bounded between -10 and 10.\n- theta: learnable sensitivity parameter, bounded between -10 and 10.\n- split_self: from data.\n- token_self: from data.\n\nThe utility U is passed through a logistic function to get the probability of acceptance.\n\nIn the variables section:\n\n- beta_0: learnable, range [-10, 10]\n- theta: learnable, range [-10, 10]\n- split_self: data variable\n- token_self: data variable\n\nThe calculated terms (3*token_self and the difference) are part of the model formula but not variables in the JSON, as they are derived from data variables.\n\nNow, the variable descriptions in JSON:\n\nFor each variable in the model:\n\n- beta_0: learnable, intercept term.\n- theta: learnable, sensitivity to the difference between split_self and fair share.\n- split_self: from data, the proposed share for the participant.\n- token_self: from data, the number of tokens found by the participant.\n\nThe calculated terms (like 3*token_self) are not variables in the JSON since they are derived from existing data variables.\n\nThus, the JSON variables section includes beta_0 and theta as learnable parameters, and split_self and token_self as data variables.\n\nThe summary should explain that the model uses an intercept and a sensitivity parameter to the difference between the proposed share and the participant's contribution-based fair share. The probability of acceptance is determined by a logistic function of this utility.\n\nThis model is parsimonious with two parameters, which should aid in parameter recovery while capturing the key psychological mechanism of comparing the offer to one's contribution.\n</think>\n\n<MODEL>\nU = beta_0 + theta * (split_self - 3 * token_self)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"beta_0\": {\n      \"description\": \"Intercept term capturing baseline acceptance tendency\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA logistic choice model where acceptance probability depends on a linear combination of baseline tendency and sensitivity to deviations from contribution-based fairness. The utility compares the proposed share (split_self) to the participant's contribution (3 * token_self), with learnable parameters for intercept (beta_0) and fairness sensitivity (theta). The model captures both general acceptance bias and context-sensitive responses to equitable offers.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['beta_0', 'theta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = beta_0 + theta * (split_self - 3 * token_self)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"beta_0\": {\n      \"description\": \"Intercept term capturing baseline acceptance tendency\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, beta_0=0, theta=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        utility = beta_0 + theta * (split_self - 3 * token_self)\n        \n        # Logistic function to convert utility to probability\n        probability = 1 / (1 + math.exp(-utility))\n        \n        model_predictions.append(probability)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 8, \"token_self\": 8, \"combined_earning\": 48, \"split_opp\": 24.0, \"split_self\": 24.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 24.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 24.0}, {\"ID\": 1, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 8, \"token_self\": 8, \"combined_earning\": 48, \"split_opp\": 36.0, \"split_self\": 12.0, \"splitperc_opp\": 75, \"splitperc_self\": 25, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 25.0, \"proposed_amount\": 12.0}, {\"ID\": 2, \"group\": \"control\", \"trial_number\": 3, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 8, \"token_self\": 8, \"combined_earning\": 48, \"split_opp\": 28.8, \"split_self\": 19.2, \"splitperc_opp\": 60, \"splitperc_self\": 40, \"accept\": 1.0, \"accepted_amount\": 19.2, \"accepted_perc\": 40.0, \"proposed_perc\": 40.0, \"proposed_amount\": 19.2}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1744025609.08209
    }
  ],
  "learnable_parameters": {
    "beta_0": {
      "description": "Intercept term capturing baseline acceptance tendency",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "theta": {
      "description": "Sensitivity to deviation from contribution-based fair share (3 * token_self)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = beta_0 + theta * (split_self - 3 * token_self)",
  "variable_descriptions": {
    "beta_0": {
      "description": "Intercept term capturing baseline acceptance tendency",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "theta": {
      "description": "Sensitivity to deviation from contribution-based fair share (3 * token_self)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A logistic choice model where acceptance probability depends on a linear combination of baseline tendency and sensitivity to deviations from contribution-based fairness. The utility compares the proposed share (split_self) to the participant's contribution (3 * token_self), with learnable parameters for intercept (beta_0) and fairness sensitivity (theta). The model captures both general acceptance bias and context-sensitive responses to equitable offers.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0), so a logistic model makes sense here.\n\nFirst, I need to think about the factors that influence a responder's decision. The key variables provided are token_self, token_opp, split_self, split_opp, combined_earning, and trial_type. The main idea is that the responder's utility depends on their own share compared to some notion of fairness, possibly based on contributions (tokens) and the total combined earnings.\n\nIn many economic games, fairness is a crucial factor. The responder might compare the proposed split to what they consider a fair split based on contributions. For example, if the participant contributed more tokens (token_self), they might expect a larger share. The trial_type variable indicates whether the contributions are equal, opponent more, or participant more, which could influence fairness perceptions.\n\nSo, the utility could be a function of the difference between the proposed split (split_self) and the fair split. The fair split might be calculated as the proportion of tokens contributed by the participant. For instance, fair_share = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and fair_share would be a key term. If the proposed split is higher than the fair share, the responder is more likely to accept, and vice versa.\n\nBut maybe participants have different sensitivities to unfairness. Some might be more tolerant of being undercompensated, while others might strongly reject it. So, parameters could capture the sensitivity to advantageous and disadvantageous inequality. This is similar to the Fehr-Schmidt model of inequity aversion.\n\nAlternatively, the model could use a logistic function where the utility is the difference between the proposed split and the fair split, scaled by a parameter. The probability of accepting would then be a logistic function of this utility.\n\nAnother consideration is the absolute amount of the split. Even if the split is fair, if the absolute amount is too low, the responder might reject. But the problem states that two equal split_self variables don't represent the same fairness because combined_earning varies. So, the model should account for the proportion relative to contributions, not just absolute amounts.\n\nLet's outline possible variables:\n\n1. Fair share based on tokens: fair_share = (token_self / (token_self + token_opp)) * combined_earning\n2. Difference between proposed split and fair share: delta = split_self - fair_share\n3. Utility could be a function of delta. For example, utility = alpha * delta, where alpha is a sensitivity parameter.\n4. The probability of accepting is logistic(utility). If utility > 0, higher chance to accept.\n\nBut maybe participants also care about the opponent's share. For example, if the opponent is getting more than their fair share, the responder might feel envious. Alternatively, if the responder is getting more than their fair share, they might feel guilty. So, parameters could capture aversion to advantageous and disadvantageous inequality.\n\nWait, the Fehr-Schmidt model has two parameters: alpha (disutility from disadvantageous inequality) and beta (disutility from advantageous inequality). But in this case, since the responder is deciding to accept or reject, their utility might be based on their own share compared to the fair share, not directly the opponent's share. However, the opponent's split (split_opp) is part of the data. But perhaps the key comparison is between split_self and the fair share based on tokens.\n\nAlternatively, the responder's utility could be split_self minus their fair share, scaled by a parameter. If this is positive, they accept; if negative, they might reject. But the model needs to translate this into a probability.\n\nAnother angle: the utility could be the proposed split_self minus the minimum acceptable amount. The minimum acceptable amount could be a function of their contribution. For example, min_accept = gamma * fair_share. Then, utility = split_self - gamma * fair_share. If this is positive, accept; else, reject. Gamma here is a parameter indicating the threshold (e.g., gamma=0.5 means they accept if they get at least half of their fair share).\n\nBut how to model this? The utility would be split_self - gamma * fair_share. Then, the probability of accepting is 1 / (1 + exp(-utility)). Wait, but the utility here is linear in parameters. However, gamma would be a learnable parameter. But maybe we need to include a temperature parameter (beta) to scale the utility. So, the probability would be logistic(beta * (split_self - gamma * fair_share)).\n\nAlternatively, the model could have parameters that weight the difference between the proposed split and the fair share. For example, utility = theta * (split_self - fair_share). Theta is a learnable parameter. The probability of accepting is 1 / (1 + exp(-utility)).\n\nBut perhaps participants have different fairness thresholds. For example, they might accept if the proposed split is at least a certain percentage of their fair share. So, maybe split_self >= lambda * fair_share. Lambda is a parameter between 0 and 1. But how to model this in a utility function that can be used in a logistic regression. Maybe the utility is split_self - lambda * fair_share. Then, the probability of accepting is sigmoid(utility * beta), where beta is a scaling parameter.\n\nAlternatively, the model could use the ratio of split_self to fair_share. For example, utility = beta * (split_self / fair_share - 1). If this is positive, higher chance to accept. But if fair_share is zero, this could be problematic, but in the dataset, token_self and token_opp are non-negative integers, and combined_earning is their sum times \u00a33. So, fair_share would be zero only if token_self is zero, which might be possible. But maybe in such cases, the model can handle it by adding a small epsilon, but perhaps the data doesn't have such cases. Alternatively, the model could use the difference instead of the ratio.\n\nAnother consideration is the trial_type variable. It indicates whether contributions are equal, opponent more, or participant more. This might affect the responder's perception. For example, if the participant contributed more (trial_type=3), they might have a higher expectation, so the parameter lambda could vary based on trial_type. But adding interactions with trial_type would increase the number of parameters, which might complicate the model. Since the model needs to pass parameter recovery tests and avoid high covariance, it's better to keep the number of parameters low.\n\nAlternatively, trial_type could be incorporated into the fair_share calculation. Wait, trial_type is derived from token_self and token_opp. For example, trial_type=1 when token_self = token_opp, 2 when token_opp > token_self, and 3 when token_self > token_opp. So, trial_type is redundant with token_self and token_opp. Therefore, including token_self and token_opp in the model would already capture the information in trial_type. So, perhaps trial_type isn't necessary in the model.\n\nSo, the key variables are token_self, token_opp, split_self, combined_earning. The fair_share is (token_self / (token_self + token_opp)) * combined_earning. The difference between split_self and fair_share is delta. The utility is a function of delta, possibly scaled by a parameter.\n\nBut maybe participants also consider the absolute amount they receive. For example, even if the split is fair, if the absolute amount is too low, they might reject. But combined_earning varies, so the split_self is in absolute terms. However, the problem states that two equal split_self values don't represent the same fairness because combined_earning varies. So, the model should account for the proportion relative to contributions, not just the absolute split_self.\n\nTherefore, the core idea is to model the utility as the difference between the proposed split_self and the fair_share, scaled by a parameter. Additionally, there might be a parameter that captures the participant's sensitivity to this difference. The probability of accepting is then a logistic function of this utility.\n\nSo, the utility U could be:\n\nU = \u03b1 * (split_self - fair_share) + \u03b2 * split_self\n\nWait, but split_self is part of the fair_share calculation. Alternatively, maybe the model should have two components: one for the fairness (split_self vs fair_share) and another for the absolute amount. But the problem states that split_self's fairness depends on combined_earning, so the absolute amount is already considered in the fair_share.\n\nAlternatively, the utility could be:\n\nU = \u03b8 * (split_self - fair_share) + \u03b3 * split_self\n\nBut this might lead to multicollinearity if split_self and fair_share are correlated. Alternatively, since fair_share is (token_self / total_tokens) * combined_earning, and split_self is part of the split, perhaps the key term is the difference between split_self and fair_share.\n\nAnother approach is to model the utility as the ratio of split_self to fair_share. For example, U = \u03b8 * (split_self / fair_share - 1). If split_self equals fair_share, U is zero. If it's higher, U is positive; lower, negative. Then, the probability of accepting is logistic(U). But this ratio could be problematic if fair_share is zero, but in the dataset, token_self and token_opp are non-negative, and combined_earning is their sum times 3. So, fair_share would be zero only if token_self is zero, which might be a rare case. But perhaps in such cases, the model can handle it by setting a default behavior.\n\nAlternatively, use the difference instead of the ratio to avoid division by zero. So, U = \u03b8 * (split_self - fair_share). Theta is a learnable parameter. The probability of accepting is 1 / (1 + exp(-U)).\n\nBut maybe participants have a non-linear sensitivity to the difference. For example, they might be more sensitive to under-offers than over-offers. So, the model could have separate parameters for positive and negative differences. For example:\n\nU = \u03b8_plus * max(split_self - fair_share, 0) + \u03b8_minus * min(split_self - fair_share, 0)\n\nThis way, \u03b8_plus captures the sensitivity to advantageous offers (split_self > fair_share), and \u03b8_minus captures sensitivity to disadvantageous offers. However, this introduces two parameters, which might complicate the model but could capture different effects for over and under offers.\n\nAlternatively, use a single parameter but apply different signs. For example, U = \u03b8 * (split_self - fair_share). If \u03b8 is positive, then higher split_self relative to fair_share increases utility. But if participants are more sensitive to negative differences, \u03b8 could be a parameter that is multiplied by the difference. However, this might not capture asymmetric sensitivity.\n\nBut given the need to keep the model simple and avoid high covariance between parameters, perhaps a single parameter is better. Let's start with a simple model and see if it can be expanded.\n\nSo, the basic model would be:\n\nUtility U = \u03b8 * (split_self - fair_share)\n\nProbability of accept = 1 / (1 + exp(-U))\n\nBut \u03b8 is a learnable parameter. However, this model assumes that the participant's decision is based solely on the difference between the proposed split and their fair share. If \u03b8 is positive, a higher split_self than fair_share increases the probability of accepting.\n\nBut maybe participants also consider the absolute amount they receive. For example, even if the split is fair, if the absolute amount is too low, they might reject. However, since combined_earning varies, the split_self's absolute value is tied to the total. But perhaps participants have a minimum acceptable amount that's a proportion of the combined_earning. For example, they might accept if split_self >= \u03bb * combined_earning, where \u03bb is a parameter. But this could be part of the utility function.\n\nAlternatively, the utility could be a combination of the difference from fair share and the absolute split. For example:\n\nU = \u03b8 * (split_self - fair_share) + \u03b3 * split_self\n\nBut this introduces another parameter (\u03b3), which might be necessary if participants care about both fairness and the absolute amount. However, this could lead to multicollinearity if split_self and fair_share are correlated. Let's check: fair_share is (token_self / (token_self + token_opp)) * combined_earning. split_self is part of the split, which is a portion of combined_earning. So, split_self = s * combined_earning, where s is the percentage (splitperc_self / 100). Similarly, fair_share is f * combined_earning, where f = token_self / (token_self + token_opp). So, split_self - fair_share = (s - f) * combined_earning. Therefore, the difference is scaled by combined_earning. So, the utility U = \u03b8 * (s - f) * combined_earning. If \u03b8 is positive, higher (s - f) leads to higher utility. But split_self itself is s * combined_earning. So, including split_self in the model would be s * combined_earning. But if we already have (s - f) * combined_earning, then adding split_self would be redundant unless there's an additional effect.\n\nAlternatively, perhaps the utility should be based on the proportional difference rather than the absolute difference. For example, U = \u03b8 * (s / f - 1), which is the percentage difference from the fair share. This way, the utility is unitless and might be more comparable across trials with different combined_earning. However, this again introduces the issue of division by zero if f is zero.\n\nBut given that token_self and token_opp are non-negative integers, f can only be zero if token_self is zero, which might be a rare case. In such cases, perhaps the fair_share is zero, and the participant's decision is based solely on the absolute split_self. But the model needs to handle that. However, for simplicity, maybe we can proceed under the assumption that f is positive, or handle those cases separately. But given the data, perhaps it's safe to proceed.\n\nSo, the utility could be:\n\nU = \u03b8 * (split_self / fair_share - 1)\n\nIf split_self equals fair_share, U is zero. If it's higher, U is positive. The probability of accepting is logistic(U). But \u03b8 here scales the effect of the proportional difference. However, this approach might not account for the absolute amount, but since combined_earning varies, the proportional approach might be better.\n\nAlternatively, combining both proportional and absolute terms. But again, this complicates the model.\n\nAnother consideration is the role of the opponent's contribution. The variable token_opp represents the opponent's contribution. The responder might compare their own contribution to the opponent's. For example, if the responder contributed more, they might expect a higher share. This is already captured in the fair_share calculation, which is based on their own and the opponent's tokens.\n\nSo, the fair_share is already a function of both token_self and token_opp. Therefore, including the fair_share in the model accounts for both contributions.\n\nPutting this together, the model's utility function should compare the proposed split_self to the fair_share. The parameters would determine how sensitive the participant is to this difference.\n\nTo account for possible non-linearities or different sensitivities to over vs. under offers, we could introduce two parameters: one for when the offer is above the fair share (advantageous inequality) and one for when it's below (disadvantageous inequality). This is similar to the Fehr-Schmidt model.\n\nSo, the utility could be:\n\nU = \u03b8_adv * max(split_self - fair_share, 0) + \u03b8_dis * min(split_self - fair_share, 0)\n\nHere, \u03b8_adv is the sensitivity to advantageous offers (split_self > fair_share), and \u03b8_dis is the sensitivity to disadvantageous offers (split_self < fair_share). If \u03b8_adv is positive, it increases utility when the offer is above fair share, encouraging acceptance. If \u03b8_dis is negative, it decreases utility when the offer is below fair share, encouraging rejection.\n\nBut in the Fehr-Schmidt model, the utility function includes terms for both advantageous and disadvantageous inequality. However, in this case, since the responder is deciding to accept or reject, their utility is based on their own share relative to the fair share. The Fehr-Schmidt model's utility function for player i is:\n\nU_i = x_i - \u03b1 * max(x_j - x_i, 0) - \u03b2 * max(x_i - x_j, 0)\n\nWhere x_i is player i's payoff, and x_j is the other player's. \u03b1 and \u03b2 are parameters for disadvantageous and advantageous inequality aversion, respectively.\n\nBut in our case, the responder's payoff is split_self if accepted, and possibly zero if rejected (but the dataset's accepted_amount is the amount gained, which is split_self if accept=1, else zero). However, the model needs to predict accept/reject based on the proposed split, not the outcome. So, the utility of accepting is based on split_self and split_opp, while rejecting gives zero. But the problem states that the model should predict accept based on the proposed split.\n\nWait, in the ultimatum game, the responder's decision depends on their share compared to the proposer's. If the proposer keeps too much, the responder might reject to punish, even if their share is positive. But in this task, the description mentions that the combined_earning is split between the participant and opponent. So, split_self + split_opp = combined_earning. Therefore, the responder might compare their split to the opponent's split. But the fair_share based on contributions is another reference point.\n\nSo, perhaps the utility should include both the comparison to the fair share and the comparison to the opponent's split. For example:\n\nU = \u03b8_fair * (split_self - fair_share) + \u03b8_inequity * (split_self - split_opp)\n\nBut this introduces two parameters. However, split_opp is equal to combined_earning - split_self, so split_self - split_opp = 2*split_self - combined_earning. This might be redundant with split_self and combined_earning. Alternatively, the inequity term could be split_self - (split_opp), which is 2*split_self - combined_earning. But this might complicate the model.\n\nAlternatively, the inequity term could be split_self / split_opp, but that could be problematic if split_opp is zero.\n\nGiven the complexity, perhaps the model should focus on the difference between split_self and fair_share, as that directly relates to the participant's contribution. Including both terms might lead to overparameterization and high covariance between \u03b8_fair and \u03b8_inequity.\n\nTherefore, to keep the model simple and ensure parameter recovery, let's start with a single parameter model based on the difference from the fair share.\n\nSo, the utility function is:\n\nU = \u03b8 * (split_self - fair_share)\n\nThe probability of accepting is logistic(U). If U > 0, the probability is greater than 0.5.\n\nBut wait, the problem states that the utility should be such that less than 0 corresponds to less than 50% likelihood. That's exactly what the logistic function does: when U < 0, the probability is < 0.5.\n\nHowever, this model assumes that the participant's decision is solely based on how much they receive compared to their fair share. If they get more than fair, they're more likely to accept; less, more likely to reject. The parameter \u03b8 scales the sensitivity to this difference.\n\nBut perhaps participants have a different baseline. For example, they might require a certain minimum above the fair share. So, the utility could be:\n\nU = \u03b8 * (split_self - (fair_share + delta))\n\nWhere delta is a parameter representing a required surplus. But this adds another parameter. However, the problem states that the model should have a slight bias towards fewer parameters. So, maybe stick with one parameter.\n\nAlternatively, the model could include a parameter for the slope (\u03b8) and a parameter for the threshold (delta), leading to two parameters. But this increases the complexity.\n\nAnother consideration is the role of the trial_role. Since the model is only for responders (trial_role=1), we don't need to account for proposer behavior. So, all data used in the model is when trial_role=1.\n\nNow, considering the variables in the dataset, the fair_share can be calculated as (token_self / (token_self + token_opp)) * combined_earning. However, combined_earning is token_self * 3 + token_opp * 3 = 3*(token_self + token_opp). Wait, no: each token is \u00a33, so combined_earning is 3*(token_self + token_opp). Therefore, fair_share = (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3 * token_self. Wait, that's interesting. Because combined_earning is 3*(token_self + token_opp), and the fair_share based on tokens would be (token_self / (token_self + token_opp)) * combined_earning = 3 * token_self. So, the fair_share is simply 3 * token_self. That's a key insight!\n\nWait, let's verify:\n\ntoken_self is the number of tokens found by the participant. Each token is \u00a33. So, the participant's contribution is 3 * token_self. Similarly, the opponent's contribution is 3 * token_opp. The combined_earning is 3*(token_self + token_opp). The fair_share based on contributions would be (3*token_self) / (3*(token_self + token_opp)) * combined_earning. Wait, no: the fair_share is the proportion of the combined_earning that matches their contribution. Since the combined_earning is the sum of both contributions, the fair_share would be equal to their own contribution. Because:\n\nIf each contributed token is \u00a33, then the participant's contribution is 3*token_self, opponent's is 3*token_opp. The total is 3*(token_self + token_opp) = combined_earning. So, the fair_share based on contribution would be exactly the participant's contribution: 3*token_self. Because that's their contribution to the pot. Therefore, the fair_share is 3*token_self.\n\nWait, that's a crucial point. So, the fair_share is simply 3 * token_self, which is the participant's own contribution. The split_self is the amount offered to the participant from the combined_earning. So, the difference between split_self and fair_share is split_self - 3*token_self. If split_self is greater than 3*token_self, the participant is getting more than their contribution; if less, they're getting less.\n\nTherefore, the utility could be based on this difference. So, U = \u03b8 * (split_self - 3*token_self). The parameter \u03b8 represents the sensitivity to this difference. If \u03b8 is positive, then when split_self > 3*token_self, utility is positive, increasing the probability of acceptance. If \u03b8 is negative, the opposite.\n\nBut wait, in the task description, it says that \"two equal split_self variables do not represent the same 'fairness'\". For example, if combined_earning is higher, the same split_self might be a smaller percentage. But according to this calculation, the fair_share is 3*token_self, which is fixed based on the participant's tokens. So, split_self is compared to this fixed value. However, combined_earning is 3*(token_self + token_opp), so split_self can vary from 0 to combined_earning.\n\nBut this approach simplifies the model because fair_share is directly 3*token_self. Therefore, the difference split_self - fair_share is split_self - 3*token_self. This difference can be positive or negative. The utility is then \u03b8 multiplied by this difference.\n\nSo, the model becomes:\n\nU = \u03b8 * (split_self - 3 * token_self)\n\nProbability of accept = 1 / (1 + exp(-U))\n\nThis model has one learnable parameter, \u03b8, which can be positive or negative. If \u03b8 is positive, participants are more likely to accept when split_self exceeds their contribution. If \u03b8 is negative, the opposite. But typically, we'd expect \u03b8 to be positive, as getting more than your contribution is better.\n\nHowever, this model assumes that participants base their decision solely on whether they're getting at least their contribution back. But in reality, people might have different reference points. For example, some might accept if they get any positive amount, while others might want at least half of their contribution. But this model captures the basic idea of comparing the offer to their contribution.\n\nAlternatively, the model could use a parameter that represents the proportion of their contribution they require. For example, U = \u03b8 * (split_self - \u03bb * 3 * token_self). Here, \u03bb is a parameter between 0 and 1, representing the minimum proportion of their contribution they require. If \u03bb=1, they require the full contribution; if \u03bb=0.5, they accept if they get at least half. But this introduces two parameters (\u03b8 and \u03bb), which might complicate the model but allows for more flexibility.\n\nHowever, the problem states that the model should prioritize parameter recovery, which is easier with fewer parameters. So, starting with one parameter is better.\n\nAnother consideration is the intercept. Maybe participants have a baseline tendency to accept or reject regardless of the offer. So, the utility could include an intercept term:\n\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\n\nHere, \u03b2_0 is a bias term. If \u03b2_0 is positive, there's a higher baseline acceptance rate. This adds another parameter, but it might be necessary if there's a general tendency to accept or reject.\n\nBut again, to keep the model simple, perhaps omit the intercept. However, without an intercept, the model assumes that when split_self equals 3*token_self, the utility is zero, leading to a 50% chance of accepting. But maybe participants have a different baseline. Including an intercept allows for that.\n\nSo, the model with an intercept would be:\n\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\n\nThis has two learnable parameters: \u03b2_0 and \u03b8. The intercept \u03b2_0 could capture the general willingness to accept offers even when split_self is below the fair share. For example, if \u03b2_0 is positive, even when split_self is slightly below fair_share, the participant might still accept.\n\nBut the problem states that the model should have a slight bias towards fewer parameters. So, perhaps start with one parameter. However, including the intercept might be necessary for better fit. Let's think: if we omit the intercept, when split_self equals fair_share, the probability is 50%. But maybe participants have a default acceptance rate higher or lower than 50% when the offer is exactly fair. Including the intercept allows for that.\n\nBut given the need to balance simplicity and accuracy, perhaps include the intercept. So, two parameters: \u03b2_0 and \u03b8.\n\nAnother angle: the utility could be based on the ratio of split_self to fair_share. For example:\n\nU = \u03b8 * (split_self / (3 * token_self) - 1)\n\nThis way, the utility is zero when split_self equals fair_share. If split_self is double the fair_share, U is \u03b8. But this introduces division by zero if token_self is zero. However, in such cases, the fair_share is zero, and split_self would be part of the combined_earning. But if token_self is zero, the participant contributed nothing, so their fair_share is zero. In that case, any positive split_self is more than fair. But the model would have U = \u03b8 * (split_self / 0 - 1), which is undefined. Therefore, this approach is problematic.\n\nThus, the difference approach is better to avoid division by zero.\n\nBack to the difference model with intercept:\n\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\n\nThis model has two parameters. The intercept \u03b2_0 could be bounded between, say, -5 and 5, and \u03b8 between -5 and 5. These are generous bounds. The probability of acceptance is logistic(U).\n\nBut wait, the problem states that the utility variable may be unbounded, but learnable parameters must have finite bounds. So, \u03b2_0 and \u03b8 would have finite bounds, but the utility U itself is unbounded as it's a linear combination.\n\nThis model uses variables split_self and token_self (from the dataset), calculates 3*token_self, and uses the difference. The parameters \u03b2_0 and \u03b8 are learnable.\n\nAnother consideration: maybe the participant's decision is influenced by how much the opponent is getting. For example, if the opponent is getting a lot more, even if the participant's share is fair, they might reject out of envy. But this would require including split_opp in the model. However, split_opp is equal to combined_earning - split_self. Since combined_earning is 3*(token_self + token_opp), split_opp = 3*(token_self + token_opp) - split_self. Including split_opp might add redundancy, but perhaps the participant cares about the opponent's share relative to their contribution.\n\nAlternatively, the participant might compare the opponent's share to their contribution. The opponent's fair share would be 3*token_opp. So, the opponent's split_opp minus their fair share is split_opp - 3*token_opp. If the opponent is getting more than their fair share, the participant might be more likely to reject. So, the utility could include both terms:\n\nU = \u03b2_0 + \u03b8_self * (split_self - 3*token_self) + \u03b8_opp * (split_opp - 3*token_opp)\n\nBut split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self. So, split_opp - 3*token_opp = 3*token_self + 3*token_opp - split_self - 3*token_opp = 3*token_self - split_self. Therefore, this term is equal to -(split_self - 3*token_self). So, the two terms are negatives of each other. Therefore, including both would lead to perfect multicollinearity. Hence, this approach isn't feasible.\n\nTherefore, including both terms isn't useful. The model should focus on the participant's own share relative to their contribution.\n\nThus, the model with intercept and one parameter seems reasonable:\n\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\n\nBut perhaps the intercept is redundant. For example, if when split_self equals 3*token_self, the utility is \u03b2_0. If \u03b2_0 is positive, the baseline acceptance is higher than 50% even when the offer is fair. If it's zero, then at fair offers, the probability is 50%. Including \u03b2_0 allows for this flexibility.\n\nHowever, adding \u03b2_0 introduces another parameter. Given the need to balance simplicity and accuracy, perhaps include it. But the problem states that parameter recovery is roughly equal in priority to accuracy. So, a model with two parameters (\u03b2_0 and \u03b8) is manageable.\n\nAlternatively, if we omit \u03b2_0, the model has one parameter. Let's compare:\n\nModel 1 (1 parameter):\nU = \u03b8 * (split_self - 3 * token_self)\nProbability = 1 / (1 + exp(-U))\n\nModel 2 (2 parameters):\nU = \u03b2_0 + \u03b8 * (split_self - 3 * token_self)\nProbability = 1 / (1 + exp(-U))\n\nModel 2 allows for a baseline tendency to accept or reject when the offer is exactly fair. For example, if \u03b8 is positive and \u03b2_0 is positive, even offers slightly below fair might still be accepted. If \u03b2_0 is negative, even fair offers might be rejected.\n\nGiven that participants might have different baseline tendencies, including \u03b2_0 could improve the model's accuracy. However, it adds another parameter, which might affect parameter recovery. But with 36 trials per participant, two parameters should be recoverable.\n\nTherefore, proceeding with Model 2.\n\nNow, the variables in the model:\n\n- split_self: from the dataset (float64)\n- token_self: from the dataset (int64)\n- 3 * token_self: calculated as fair_share\n- split_self - 3 * token_self: calculated difference\n- \u03b2_0: learnable parameter (intercept)\n- \u03b8: learnable parameter (sensitivity to difference)\n\nThe utility U is \u03b2_0 + \u03b8*(split_self - 3*token_self). The probability of accept is logistic(U).\n\nThis model uses variables split_self and token_self from the dataset. The parameters \u03b2_0 and \u03b8 are learnable, with bounds. For example, \u03b2_0 could be bounded between -10 and 10, \u03b8 between -10 and 10. These are generous bounds to allow for different sensitivities.\n\nAnother consideration: the difference (split_self - 3*token_self) can be positive or negative. If \u03b8 is positive, positive differences increase utility, leading to higher acceptance. If \u03b8 is negative, the opposite. However, typically, we'd expect \u03b8 to be positive, as higher offers should increase acceptance. But the model allows for both possibilities.\n\nNow, checking the dataset variables:\n\nThe dataset has split_self (float64) and token_self (int64). So, the model uses these variables directly.\n\nThe target variable is 'accept' (float64, 0 or 1). The model predicts the probability of accept=1.\n\nThis model meets the requirements: it uses only the provided variables, predicts responder behavior (trial_role=1), and the utility is unbounded, with learnable parameters having finite bounds.\n\nNow, specifying the mathematical model:\n\n<MODEL>\nU = beta_0 + theta * (split_self - 3 * token_self)\n</MODEL>\n\nVariables:\n\n- beta_0: learnable intercept, bounded between -10 and 10.\n- theta: learnable sensitivity parameter, bounded between -10 and 10.\n- split_self: from data.\n- token_self: from data.\n\nThe utility U is passed through a logistic function to get the probability of acceptance.\n\nIn the variables section:\n\n- beta_0: learnable, range [-10, 10]\n- theta: learnable, range [-10, 10]\n- split_self: data variable\n- token_self: data variable\n\nThe calculated terms (3*token_self and the difference) are part of the model formula but not variables in the JSON, as they are derived from data variables.\n\nNow, the variable descriptions in JSON:\n\nFor each variable in the model:\n\n- beta_0: learnable, intercept term.\n- theta: learnable, sensitivity to the difference between split_self and fair share.\n- split_self: from data, the proposed share for the participant.\n- token_self: from data, the number of tokens found by the participant.\n\nThe calculated terms (like 3*token_self) are not variables in the JSON since they are derived from existing data variables.\n\nThus, the JSON variables section includes beta_0 and theta as learnable parameters, and split_self and token_self as data variables.\n\nThe summary should explain that the model uses an intercept and a sensitivity parameter to the difference between the proposed share and the participant's contribution-based fair share. The probability of acceptance is determined by a logistic function of this utility.\n\nThis model is parsimonious with two parameters, which should aid in parameter recovery while capturing the key psychological mechanism of comparing the offer to one's contribution.\n</think>\n\n<MODEL>\nU = beta_0 + theta * (split_self - 3 * token_self)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"beta_0\": {\n      \"description\": \"Intercept term capturing baseline acceptance tendency\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA logistic choice model where acceptance probability depends on a linear combination of baseline tendency and sensitivity to deviations from contribution-based fairness. The utility compares the proposed share (split_self) to the participant's contribution (3 * token_self), with learnable parameters for intercept (beta_0) and fairness sensitivity (theta). The model captures both general acceptance bias and context-sensitive responses to equitable offers.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = beta_0 + theta * (split_self - 3 * token_self)\nSummary: A logistic choice model where acceptance probability depends on a linear combination of baseline tendency and sensitivity to deviations from contribution-based fairness. The utility compares the proposed share (split_self) to the participant's contribution (3 * token_self), with learnable parameters for intercept (beta_0) and fairness sensitivity (theta). The model captures both general acceptance bias and context-sensitive responses to equitable offers.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, beta_0=0, theta=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        utility = beta_0 + theta * (split_self - 3 * token_self)\n        \n        # Logistic function to convert utility to probability\n        probability = 1 / (1 + math.exp(-utility))\n        \n        model_predictions.append(probability)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.5876068376068376,
  "fitting_results": [
    {
      "beta_0": -10.0,
      "theta": 9.66952697189948,
      "success": true,
      "log_likelihood": -24.58379662469009,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 7.209564410598085,
      "success": true,
      "log_likelihood": -20.92965327083198,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 4.847602399196804,
      "success": true,
      "log_likelihood": -20.442756255033323,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 9.868264637763867,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -23.26480244892157,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 6.5590011100737815,
      "success": true,
      "log_likelihood": -20.844772790221327,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 3.83194304985285,
      "success": true,
      "log_likelihood": -19.69628867238976,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.7751598204068871,
      "success": true,
      "log_likelihood": -11.277593380904126,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 3.1002668266255613,
      "success": true,
      "log_likelihood": -18.443290505536797,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.9173362403518146,
      "success": true,
      "log_likelihood": -11.277590414900114,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -21.284839664909903,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -21.28416523801097,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -10.0,
      "theta": 9.66872221349195,
      "success": true,
      "log_likelihood": -24.583796591153106,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -21.28416896660878,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.9150175366760707,
      "success": true,
      "log_likelihood": -11.277590415794034,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.923141026259959,
      "success": true,
      "log_likelihood": -11.277590419187211,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.9382828722025417,
      "success": true,
      "log_likelihood": -11.277590473947756,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.9184738025187308,
      "success": true,
      "log_likelihood": -11.27759041500639,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 2.2987717328934645,
      "success": true,
      "log_likelihood": -15.73599368285292,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 2.1792343323151773,
      "success": true,
      "log_likelihood": -14.4024878294688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.9136290599596534,
      "success": true,
      "log_likelihood": -11.277590417043724,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 5.2352384532166445,
      "success": true,
      "log_likelihood": -19.55890936390969,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 2.2550955437588898,
      "success": true,
      "log_likelihood": -15.177972408703404,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 1.106181456752745,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -23.71417342752116,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 4.1376351782281295,
      "success": true,
      "log_likelihood": -22.109798370166864,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 6.263558230849395,
      "success": true,
      "log_likelihood": -19.93372917742293,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 4.187981889472235,
      "success": true,
      "log_likelihood": -20.139682760553114,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 9.999999999999998,
      "success": true,
      "log_likelihood": -21.284168132914704,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 8.20846245674274,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -22.187859738440785,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 1.7554111845227869,
      "success": true,
      "log_likelihood": -14.500789563152976,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 3.479137993876411,
      "success": true,
      "log_likelihood": -17.830984367429963,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.9173708630595112,
      "success": true,
      "log_likelihood": -11.277590414898063,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -10.0,
      "theta": 9.66963125678826,
      "success": true,
      "log_likelihood": -24.583796625295452,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -2.091702553832946,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -24.201768190963726,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 8.208473991196085,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -22.187859599467426,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.9154231991772511,
      "success": true,
      "log_likelihood": -11.277590415529998,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 2.7784863173894077,
      "success": true,
      "log_likelihood": -22.004493087654083,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 3.1558809347711008,
      "success": true,
      "log_likelihood": -18.62635220605781,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 7.184426211783966,
      "success": true,
      "log_likelihood": -20.9276054357437,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 3.3091453526669663,
      "success": true,
      "log_likelihood": -19.57619748972836,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.8884634609788156,
      "success": true,
      "log_likelihood": -14.277587778794436,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 4.982800630931615,
      "success": true,
      "log_likelihood": -19.93466803742411,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 4.213484509244716,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -22.94527060342179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.9039733346801916,
      "success": true,
      "log_likelihood": -12.277588687580991,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 8.557613006247845,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -22.162053279535083,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 6.602587327794334,
      "success": true,
      "log_likelihood": -20.849770776587544,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 2.539523225327808,
      "success": true,
      "log_likelihood": -19.886559935211928,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 5.349620269540457,
      "success": true,
      "log_likelihood": -21.53335856329014,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 8.145989335742605,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -23.18219587438375,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -21.28416524007212,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 2.6052882193802485,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -24.49949295634535,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 2.410694380073787,
      "success": true,
      "log_likelihood": -20.386343395440555,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -10.0,
      "theta": 9.669951177532832,
      "success": true,
      "log_likelihood": -24.583796490589318,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 8.554043455702214,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -23.161860850122693,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 0.5422657536442842,
      "theta": 0.5125509409429891,
      "success": true,
      "log_likelihood": -24.20741666689014,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 4.8390750494744665,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -22.85147050674975,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 9.879356904365112,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -22.265836620301336,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 2.606905132609621,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -23.499759133907784,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.6057448278207697,
      "success": true,
      "log_likelihood": -15.277508426416196,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -10.0,
      "theta": 9.670009632547762,
      "success": true,
      "log_likelihood": -24.583796627907542,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.5515357231130157,
      "success": true,
      "log_likelihood": -18.277546577827145,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 5.319625068705843,
      "success": true,
      "log_likelihood": -21.533092427371717,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 5.35006841832532,
      "success": true,
      "log_likelihood": -20.536459008938333,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -10.0,
      "theta": 9.66412809385171,
      "success": true,
      "log_likelihood": -24.583796728640618,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -2.425167333268878,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -24.206839051076667,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 1.1159278874095526,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -23.712514987716894,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -21.28416813301732,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.8687137468191495,
      "success": true,
      "log_likelihood": -14.277587602271765,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 1.5462809378665483,
      "success": true,
      "log_likelihood": -15.867852365974356,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -22.28359243171345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.9477293863386883,
      "success": true,
      "log_likelihood": -14.277585549704385,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 1.0642998478128467,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -23.721475284408374,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -10.0,
      "theta": 9.669961410877892,
      "success": true,
      "log_likelihood": -24.58379662744944,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 4.542451760161586,
      "success": true,
      "log_likelihood": -21.747309986019424,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 9.991171501156616,
      "theta": 0.4824105291084862,
      "success": true,
      "log_likelihood": -19.27736151797948,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -9.666317564986356,
      "theta": 0.36575277394560624,
      "success": true,
      "log_likelihood": -24.95320113752544,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 2.8663497475860997,
      "success": true,
      "log_likelihood": -18.212283862269537,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.8922387729328534,
      "success": true,
      "log_likelihood": -13.277589014053948,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -10.0,
      "theta": 0.9544842299854362,
      "success": true,
      "log_likelihood": -24.95356073278919,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 4.839509682367612,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -22.8514318496202,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -10.0,
      "theta": 9.666899138521407,
      "success": true,
      "log_likelihood": -24.583796642101763,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 2.606330998708749,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -23.49975893766371,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": -0.8434371984092062,
      "success": true,
      "log_likelihood": -14.277551586819015,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -10.0,
      "theta": 0.8091658101522841,
      "success": true,
      "log_likelihood": -24.95350550990723,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 3.9777330790937966,
      "success": true,
      "log_likelihood": -23.029860901578235,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 8.570023682202013,
      "success": true,
      "log_likelihood": -22.03991014816798,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": -2.4437966070013446,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -24.207153688216152,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 3.941460406073,
      "success": true,
      "log_likelihood": -20.026229718183256,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 9.243594782016057,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -24.34765004399724,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 1.491144832957217,
      "success": true,
      "log_likelihood": -12.944487627245028,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 1.1260789531257709,
      "theta": 10.0,
      "success": true,
      "log_likelihood": -23.71080210634727,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta_0": 10.0,
      "theta": 2.7206628093786533,
      "success": true,
      "log_likelihood": -21.082207393470966,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "beta_0": {
        "r": 0.491592093760076,
        "p": 2.061376665610401e-07
      },
      "theta": {
        "r": 0.3571239525167385,
        "p": 0.00026492190423627775
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 47.026328621615626,
  "bic_results": {
    "average_bic": 47.026328621615626,
    "individual_bics": [
      56.3346311262924,
      49.02634441857618,
      48.052550386978865,
      53.69664277475536,
      48.85658345735487,
      46.559615221691736,
      29.72222463872047,
      44.05361888798581,
      29.722218706712447,
      49.736717206732024,
      49.73536835293416,
      56.33463105921843,
      49.73537581012978,
      29.722218708500286,
      29.722218715286644,
      29.722218824807733,
      29.722218706925,
      38.63902524261806,
      35.97201353584982,
      29.72221871099967,
      46.2848566047316,
      37.522982694319026,
      54.59538473195454,
      51.386634617245946,
      47.03449623175808,
      47.44640339801845,
      49.73537414274163,
      51.54275735379379,
      36.16861700321817,
      42.829006611772144,
      29.722218706708347,
      56.33463112750312,
      55.57057425883967,
      51.54275707584707,
      29.722218707972218,
      51.176024052220384,
      44.41974228902784,
      49.02224874839962,
      46.31943285636894,
      35.72221343450109,
      47.03637395176044,
      53.0575790837558,
      31.7222152520742,
      51.491144435982385,
      48.866579430087306,
      46.94015774733607,
      50.2337550034925,
      53.53142962567972,
      49.73536835705646,
      56.16602378960292,
      47.93972466779333,
      56.334630858090854,
      53.490759577157604,
      55.5818712106925,
      52.869978890411716,
      51.69871111751489,
      54.166556144727785,
      37.72205472974461,
      56.3346311327273,
      43.72213103256651,
      50.23322273165565,
      48.239955894788885,
      56.33463133419345,
      55.58071597906555,
      54.592067852346005,
      49.73537414294686,
      35.72221308145575,
      38.90274260886093,
      51.73422274033912,
      35.72220897632099,
      54.609988445728966,
      56.3346311318111,
      50.661657848951066,
      45.721760912871176,
      57.0734401519631,
      43.59160560145129,
      33.722215905020114,
      57.074159342490596,
      52.86990157615262,
      56.334631161115745,
      54.16655575223964,
      35.72214105055025,
      57.07404889672668,
      53.22675968006869,
      51.24685817324818,
      55.58134525334452,
      47.21949731327873,
      55.8623379649067,
      33.056013131402274,
      54.58864208960676,
      49.33145266385415
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- beta_0: r = 0.492\n- theta: r = 0.357",
  "previous_models": [
    "Specification: U = beta_0 + theta * (split_self - 3 * token_self)\nSummary: A logistic choice model where acceptance probability depends on a linear combination of baseline tendency and sensitivity to deviations from contribution-based fairness. The utility compares the proposed share (split_self) to the participant's contribution (3 * token_self), with learnable parameters for intercept (beta_0) and fairness sensitivity (theta). The model captures both general acceptance bias and context-sensitive responses to equitable offers.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 47.026328621615626\nOverall Accuracy: 0.5876\n\nParameter Recovery:\n- beta_0: r = 0.492\n- theta: r = 0.357"
  ]
}