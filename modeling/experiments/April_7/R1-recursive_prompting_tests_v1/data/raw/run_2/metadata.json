{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: THIS IS A TEST TO SEE IF THESE INSTRUCTIONS ARE ACTUALLY BEING DISPLAYED WITHIN THE PIPELINE AND GIVEN TO THE MODEL. IF THIS STRING OF TEXT IS WITHIN YOUR INSTRUCTIONS, PLESAE START YOUR RESPONSE WITH 'I have recieved the test instructions\n\nBe VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on utility, where utility less than 0 corresponds to less than 50% probability of acceptance. \n\nLooking at the dataset variables, important factors include the split percentages (splitperc_self and splitperc_opp), the combined earnings, and the tokens found by both the participant and the opponent. Since the trial_role is 1 for responder, we'll filter those trials. \n\nThe psychological mechanisms here likely involve fairness considerations and self-interest. Participants might compare their proposed share to what they perceive as fair. The fairness could be influenced by their own contribution (token_self) versus the opponent's (token_opp). For example, if the participant contributed more tokens, they might expect a larger share. \n\nA common approach in such models is to use a utility function that combines the monetary gain with a term representing inequity aversion. The Fehr-Schmidt model is a classic example, where utility decreases if there's inequality, whether advantageous or disadvantageous. However, since the combined earnings vary, the fairness might be based on the proportion contributed rather than absolute amounts.\n\nLet's consider the utility as the proposed share (split_self) minus the fair share based on contributions. The fair share could be calculated as (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split and this fair share would affect the utility. If the proposed split is less than the fair share, the participant might be more likely to reject, especially if they are sensitive to unfairness.\n\nAdditionally, there might be a parameter representing the participant's sensitivity to unfairness. For example, a parameter \u03b1 that scales the difference between the fair share and the proposed share. The utility could then be modeled as:\n\nUtility = split_self - (fair_share) - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b1 represents the aversion to being undercompensated relative to their contribution. If the proposed split is less than the fair share, the utility decreases by \u03b1 times the deficit. This would make rejection more likely as the deficit increases, depending on \u03b1's value.\n\nAnother consideration is the efficiency or the total combined earning. Participants might be more reluctant to reject higher total amounts even if the split is slightly unfair. However, the problem statement mentions that two equal split_self values don't represent the same fairness due to varying combined earnings, so the model should account for the proportion of contribution relative to the total.\n\nWait, but the split percentages (splitperc_self) are already provided. Maybe instead of calculating fair_share, we can directly compare the proposed percentage to the participant's contribution percentage. For example, if the participant contributed 60% of the tokens (token_self / (token_self + token_opp)*100), then a proposed split of 50% would be unfair. The difference between the proposed percentage and their contribution percentage could be a factor.\n\nSo, the utility might be based on the difference between splitperc_self and the participant's contribution percentage (let's call this contrib_perc). The utility could then be:\n\nUtility = \u03b2 * (split_self) + \u03b1 * (splitperc_self - contrib_perc)\n\nBut since the model needs to predict accept/reject, the utility would be passed through a logistic function to get the probability of acceptance. However, the problem states that the utility variable may be unbounded, but the probability is determined by a logistic function with temperature 1. So, the utility function should be designed such that when utility is positive, the probability is above 50%, and vice versa.\n\nAlternatively, using a fairness term that penalizes deviations from the contribution percentage. Let's formalize this:\n\ncontrib_perc = (token_self / (token_self + token_opp)) * 100\nfairness_diff = splitperc_self - contrib_perc\n\nUtility = split_self + \u03b8 * fairness_diff\n\nHere, \u03b8 is a learnable parameter that determines how sensitive the participant is to the fairness difference. If \u03b8 is positive, a higher fairness_diff (i.e., getting more than their contribution) increases utility, but if \u03b8 is negative, it might decrease utility. Wait, but if splitperc_self is higher than contrib_perc, that's advantageous, so participants might not mind, but if it's lower, they might be averse. So maybe the model should have a term that only penalizes when splitperc_self is less than contrib_perc. So:\n\nunfairness = max(contrib_perc - splitperc_self, 0)\nUtility = split_self - \u03b1 * unfairness\n\nHere, \u03b1 is the sensitivity to being underpaid relative to their contribution. If the proposed percentage is less than their contribution percentage, the utility decreases by \u03b1 times the difference. This makes sense because participants might be willing to accept lower amounts if they're still getting a fair share based on their contribution.\n\nAdditionally, there might be a baseline term representing the participant's general willingness to accept, independent of the split. For example, a parameter \u03b2 that captures this baseline. So:\n\nUtility = \u03b2 + split_self - \u03b1 * unfairness\n\nBut since split_self is in pounds, and unfairness is a percentage difference, the units might not align. To address this, perhaps normalize split_self by the combined earnings to make it a percentage, but splitperc_self is already a percentage. Wait, split_self is the proposed amount in \u00a3, and splitperc_self is the percentage. Maybe using splitperc_self directly is better for the fairness comparison.\n\nWait, contrib_perc is (token_self / total_tokens) * 100, and splitperc_self is the percentage offered. So the unfairness is contrib_perc - splitperc_self. If this is positive, the participant is getting less than their contribution share. So:\n\nunfairness = max(contrib_perc - splitperc_self, 0)\nUtility = \u03b2 * splitperc_self - \u03b1 * unfairness\n\nBut splitperc_self is the percentage offered to the participant. So higher splitperc_self increases utility, but if it's below contrib_perc, the unfairness term reduces utility. Alternatively, maybe the utility is a combination of the absolute amount (split_self) and the fairness term.\n\nAlternatively, since combined_earning is variable, the absolute split_self is important. For example, getting \u00a35 in a \u00a310 total is different from \u00a35 in a \u00a320 total. But the problem states that two equal split_self values don't represent the same fairness. So perhaps the model should consider both the absolute amount and the fairness relative to contribution.\n\nBut to simplify, maybe focus on the percentage terms. Let's define:\n\nfair_share_perc = (token_self / (token_self + token_opp)) * 100\nunfairness = max(fair_share_perc - splitperc_self, 0)\n\nUtility = \u03b2 * split_self - \u03b1 * unfairness\n\nHere, \u03b2 represents the sensitivity to the absolute proposed amount, and \u03b1 represents the sensitivity to unfairness (receiving less than their fair share percentage). If \u03b1 is positive, unfairness reduces utility. The higher \u03b1, the more the participant is willing to reject offers that are below their fair share.\n\nBut split_self is in pounds, and unfairness is a percentage. To make the units compatible, perhaps scale the unfairness term by the combined earnings. For example, the monetary value of the unfairness is (fair_share_perc - splitperc_self)/100 * combined_earning. So:\n\nunfairness_amount = (fair_share_perc - splitperc_self)/100 * combined_earning\nunfairness_amount = max(unfairness_amount, 0)\n\nUtility = split_self - \u03b1 * unfairness_amount\n\nThis way, both terms are in pounds, and \u03b1 is a unitless parameter indicating how much each pound of unfairness affects the utility. If \u03b1 is high, even a small unfairness in monetary terms would significantly reduce utility.\n\nAlternatively, maybe participants compare their split to the fair share in absolute terms. For example, fair_share = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and fair_share is:\n\ndifference = fair_share - split_self\nunfairness = max(difference, 0)\n\nUtility = split_self - \u03b1 * unfairness\n\nHere, \u03b1 represents the aversion to receiving less than the fair share. If the proposed split_self is less than fair_share, the utility decreases by \u03b1 times the difference. This seems plausible because it directly compares the actual offer to what's fair based on contribution.\n\nThis model would have one learnable parameter, \u03b1, which must be non-negative (since receiving less than fair share should decrease utility). The utility is then split_self minus \u03b1 times the unfairness. If the utility is positive, the probability of accepting is greater than 50%; if negative, less than 50%.\n\nBut perhaps there's also a term for the proposer's share. For example, if the opponent is taking more than their fair share, the participant might be more inclined to reject. However, in the responder role, the participant only cares about their own share versus their contribution. The opponent's share is determined by the proposer (the opponent in this case), so the key comparison is between the participant's split and their contribution-based fair share.\n\nAnother consideration is the efficiency of the proposal: the total combined earning. Participants might be more likely to accept unfair offers if the total is high because rejecting means both get nothing. But the problem statement says that the model should predict based on the variables provided. The combined_earning is available, so maybe including it as a factor. For example, the utility could be:\n\nUtility = split_self + \u03b3 * combined_earning - \u03b1 * unfairness\n\nBut this complicates the model with an additional parameter \u03b3. However, the priority is parameter recovery, so keeping the number of parameters low is better. Maybe \u03b3 is not necessary, or perhaps combined_earning is accounted for in the split_self and fair_share terms.\n\nWait, split_self is part of the combined_earning, so perhaps the model doesn't need to explicitly include combined_earning beyond what's already in split_self and fair_share. For example, if combined_earning is high, split_self could be high even if the percentage is low. But the participant's decision might depend on both the absolute amount and the fairness. However, the problem states that two equal split_self values don't represent the same fairness, implying that the model must account for the context of combined_earning. Therefore, the fairness comparison (fair_share) inherently considers combined_earning because fair_share is (token_self / total_tokens) * combined_earning.\n\nSo, the model using fair_share and split_self difference captures the effect of combined_earning. For example, if combined_earning is \u00a330, a split_self of \u00a310 might be unfair if the fair_share is \u00a315 (because the participant contributed 50% of tokens), but if combined_earning is \u00a360, a split_self of \u00a320 with fair_share \u00a330 is similarly unfair. The model would capture this through the unfairness term.\n\nThus, the utility function could be:\n\nUtility = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nWhere fair_share = (token_self / (token_self + token_opp)) * combined_earning\n\nThis way, \u03b1 represents the sensitivity to being underpaid relative to the fair share. If \u03b1 is 0, the participant only cares about the absolute split_self. Higher \u03b1 means they are more sensitive to unfairness.\n\nBut maybe participants also have a baseline acceptance tendency, independent of the offer. For example, a parameter \u03b2 that captures this. So:\n\nUtility = \u03b2 + split_self - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b2 could be a constant term. If \u03b2 is positive, the participant has a higher baseline utility, making acceptance more likely. However, introducing \u03b2 adds another parameter, which might complicate parameter recovery. Since the priority is to have a model that can recover parameters well, perhaps it's better to start with a single parameter \u03b1. But if the data has a general bias towards acceptance or rejection, \u03b2 could capture that.\n\nAlternatively, the logistic function includes a bias term inherently. For example, the probability of acceptance is 1 / (1 + exp(-Utility)). If Utility is split_self - \u03b1 * unfairness, then when split_self is exactly equal to the fair_share, Utility is split_self, which is fair_share. But maybe participants have a different baseline. However, without \u03b2, the model assumes that when the offer is exactly fair, the utility is split_self, which is fair_share. But perhaps participants require more than just the fair share to accept, or maybe they have a general tendency to accept or reject. Including \u03b2 could help capture that.\n\nBut given the instruction to have a slight bias towards fewer parameters, maybe start with \u03b1 and \u03b2. Wait, the example in the problem statement had two learnable parameters. Let's see: the example had beta and epsilon as learnable. So perhaps having two parameters is acceptable.\n\nAlternatively, maybe the model should have \u03b1 (unfairness aversion) and \u03bb (altruism or other-regarding preference). But given the task is responder behavior, perhaps \u03b1 is sufficient.\n\nWait, another angle: the participant might also consider the opponent's share. For example, if the opponent is keeping a lot more than their fair share, the participant might be more likely to reject. But in the responder role, the opponent's split is determined by the proposer (the opponent), so the participant's decision is based on their own share versus their contribution. However, the model could include a term comparing the opponent's split to their contribution. For example:\n\nopponent_fair_share = (token_opp / (token_self + token_opp)) * combined_earning\nopponent_unfairness = max(split_opp - opponent_fair_share, 0)\n\nUtility = split_self - \u03b1 * (fair_share - split_self) - \u03b3 * opponent_unfairness\n\nBut this adds another parameter \u03b3, making the model more complex. Given the need for parameter recovery, it's better to keep it simple. The problem statement also mentions that the model should utilize the given variables appropriately but not necessarily all. So focusing on the key variables: token_self, token_opp, combined_earning, split_self, splitperc_self.\n\nGiven that, the core idea is that participants compare their proposed share to their contribution-based fair share. The utility decreases if the proposed share is below the fair share, with sensitivity determined by \u03b1.\n\nSo, the model would be:\n\nfair_share = (token_self / (token_self + token_opp)) * combined_earning\nunfairness = max(fair_share - split_self, 0)\nUtility = split_self - \u03b1 * unfairness\n\nThis model has one learnable parameter, \u03b1, which must be non-negative (since unfairness reduces utility). The range for \u03b1 could be [0, 10], for example, allowing for varying degrees of sensitivity.\n\nBut perhaps participants also have a non-linear sensitivity to unfairness. For example, the disutility could be quadratic in unfairness. However, that would introduce another parameter, which complicates the model. Given the need for simplicity and parameter recovery, a linear term is better.\n\nAnother consideration: the participant's own contribution (token_self) might influence their fairness perception. For example, if they contributed more, they might be more sensitive to unfairness. But this is already captured in the fair_share calculation.\n\nAlternatively, the model could include a term that weights the unfairness by the participant's contribution. But again, this complicates the model.\n\nIn summary, the model should calculate the fair share based on the participant's contribution, determine the unfairness as the difference between fair share and proposed split (if positive), and subtract a scaled version of this unfairness from the proposed split to get the utility. The probability of acceptance is then a logistic function of this utility.\n\nNow, considering the parameters:\n\n- \u03b1 (alpha): Sensitivity to unfairness. Must be \u2265 0. Bounds could be [0, 10], which is generous but finite.\n\nThis gives a single learnable parameter, which is good for parameter recovery. The utility is:\n\nUtility = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nBut wait, split_self is the proposed amount to the participant. If the proposed amount is less than the fair share, the utility decreases. The probability of acceptance is 1 / (1 + exp(-Utility)). So when Utility is positive, the probability is >0.5; when negative, <0.5.\n\nThis model assumes that the utility is linear in both the proposed split and the unfairness. The parameter \u03b1 determines how much each unit of unfairness reduces the utility. For example, if \u03b1 is 2, then each \u00a31 of unfairness reduces the utility by \u00a32, making the participant more likely to reject.\n\nTesting this model: If the proposed split equals the fair share, Utility = split_self, which is fair_share. If fair_share is, say, \u00a310, then Utility is \u00a310. The probability of acceptance would be close to 1 (since exp(-10) is negligible). If the proposed split is \u00a35 and fair_share is \u00a310, then unfairness is \u00a35, and Utility = 5 - \u03b1*5. If \u03b1 is 1, Utility is 0, so probability is 0.5. If \u03b1 is 2, Utility is -5, probability is ~0.007, so likely rejection.\n\nThis seems reasonable. The model captures the trade-off between the absolute offer and the fairness. Participants might accept a lower offer if it's close to fair, but reject if it's too unfair, depending on their \u03b1.\n\nNow, mapping the variables:\n\n- split_self: from the data (float64)\n- token_self: from the data (int64)\n- token_opp: from the data (int64)\n- combined_earning: from the data (int64)\n- fair_share: calculated as (token_self / (token_self + token_opp)) * combined_earning\n- unfairness: max(fair_share - split_self, 0)\n- alpha: learnable parameter, range [0, 10]\n\nThe utility is then split_self - alpha * unfairness.\n\nThis model uses the variables token_self, token_opp, combined_earning, and split_self from the dataset. The target variable is 'accept'.\n\nNow, checking the dataset variables: token_self, token_opp, combined_earning, split_self are all available. So the model is feasible.\n\nAnother possible addition is a term for the participant's own token count influencing their sensitivity. For example, someone who contributed more tokens might be more sensitive to unfairness. But this would require an interaction term, adding more parameters. To keep it simple, we'll stick with the initial model.\n\nThus, the final model is:\n\n<MODEL>\nUtility = split_self - alpha * max( ( (token_self / (token_self + token_opp) ) * combined_earning ) - split_self, 0 )\n</MODEL>\n\nVariables:\n\n- alpha: learnable, range [0, 10]\n- split_self: from data\n- token_self: from data\n- token_opp: from data\n- combined_earning: from data\n\nThe variable descriptions in JSON would include alpha as a learnable parameter with min 0 and max 10, inclusive. The other variables are from the data or calculated.\n\nNow, the summary should concisely describe the model. It's a utility model where the utility of accepting depends on the proposed amount minus the product of unfairness (difference between fair share and proposed amount) and a sensitivity parameter. The fair share is based on the participant's contribution relative to the total.\n</think>\n\nI have recieved the test instructions\n\n<MODEL>\nUtility = split_self - alpha * max( ( (token_self / (token_self + token_opp) ) * combined_earning ) - split_self, 0 )\n</MODEL>\n\n<target_variable>accept</target_variable>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to unfairness when proposed amount is below contribution-based fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Opponent's token contribution\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money in shared pot\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA fairness-based utility model where acceptance probability depends on the proposed amount relative to a contribution-adjusted fair share. Utility decreases linearly with the deficit between proposed amount and the fair share (calculated from token contributions), scaled by a learnable unfairness sensitivity parameter (alpha). Maintains one learnable parameter for robust recovery.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                Utility = split_self - alpha * max( ( (token_self / (token_self + token_opp) ) * combined_earning ) - split_self, 0 )\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to unfairness when proposed amount is below contribution-based fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Opponent's token contribution\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money in shared pot\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n\n        # Calculate fair share\n        total_tokens = token_self + token_opp\n        if total_tokens == 0:\n            fair_share = 0\n        else:\n            fair_share = (token_self / total_tokens) * combined_earning\n\n        # Calculate utility\n        unfairness = max(fair_share - split_self, 0)\n        utility = split_self - alpha * unfairness\n\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1744026301.313816
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to unfairness when proposed amount is below contribution-based fair share",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "Utility = split_self - alpha * max( ( (token_self / (token_self + token_opp) ) * combined_earning ) - split_self, 0 )",
  "variable_descriptions": {
    "alpha": {
      "description": "Sensitivity to unfairness when proposed amount is below contribution-based fair share",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Participant's token contribution",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Opponent's token contribution",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "combined_earning": {
      "description": "Total money in shared pot",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A fairness-based utility model where acceptance probability depends on the proposed amount relative to a contribution-adjusted fair share. Utility decreases linearly with the deficit between proposed amount and the fair share (calculated from token contributions), scaled by a learnable unfairness sensitivity parameter (alpha). Maintains one learnable parameter for robust recovery.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on utility, where utility less than 0 corresponds to less than 50% probability of acceptance. \n\nLooking at the dataset variables, important factors include the split percentages (splitperc_self and splitperc_opp), the combined earnings, and the tokens found by both the participant and the opponent. Since the trial_role is 1 for responder, we'll filter those trials. \n\nThe psychological mechanisms here likely involve fairness considerations and self-interest. Participants might compare their proposed share to what they perceive as fair. The fairness could be influenced by their own contribution (token_self) versus the opponent's (token_opp). For example, if the participant contributed more tokens, they might expect a larger share. \n\nA common approach in such models is to use a utility function that combines the monetary gain with a term representing inequity aversion. The Fehr-Schmidt model is a classic example, where utility decreases if there's inequality, whether advantageous or disadvantageous. However, since the combined earnings vary, the fairness might be based on the proportion contributed rather than absolute amounts.\n\nLet's consider the utility as the proposed share (split_self) minus the fair share based on contributions. The fair share could be calculated as (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split and this fair share would affect the utility. If the proposed split is less than the fair share, the participant might be more likely to reject, especially if they are sensitive to unfairness.\n\nAdditionally, there might be a parameter representing the participant's sensitivity to unfairness. For example, a parameter \u03b1 that scales the difference between the fair share and the proposed share. The utility could then be modeled as:\n\nUtility = split_self - (fair_share) - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b1 represents the aversion to being undercompensated relative to their contribution. If the proposed split is less than the fair share, the utility decreases by \u03b1 times the deficit. This would make rejection more likely as the deficit increases, depending on \u03b1's value.\n\nAnother consideration is the efficiency or the total combined earning. Participants might be more reluctant to reject higher total amounts even if the split is slightly unfair. However, the problem statement mentions that two equal split_self values don't represent the same fairness due to varying combined earnings, so the model should account for the proportion of contribution relative to the total.\n\nWait, but the split percentages (splitperc_self) are already provided. Maybe instead of calculating fair_share, we can directly compare the proposed percentage to the participant's contribution percentage. For example, if the participant contributed 60% of the tokens (token_self / (token_self + token_opp)*100), then a proposed split of 50% would be unfair. The difference between the proposed percentage and their contribution percentage could be a factor.\n\nSo, the utility might be based on the difference between splitperc_self and the participant's contribution percentage (let's call this contrib_perc). The utility could then be:\n\nUtility = \u03b2 * (split_self) + \u03b1 * (splitperc_self - contrib_perc)\n\nBut since the model needs to predict accept/reject, the utility would be passed through a logistic function to get the probability of acceptance. However, the problem states that the utility variable may be unbounded, but the probability is determined by a logistic function with temperature 1. So, the utility function should be designed such that when utility is positive, the probability is above 50%, and vice versa.\n\nAlternatively, using a fairness term that penalizes deviations from the contribution percentage. Let's formalize this:\n\ncontrib_perc = (token_self / (token_self + token_opp)) * 100\nfairness_diff = splitperc_self - contrib_perc\n\nUtility = split_self + \u03b8 * fairness_diff\n\nHere, \u03b8 is a learnable parameter that determines how sensitive the participant is to the fairness difference. If \u03b8 is positive, a higher fairness_diff (i.e., getting more than their contribution) increases utility, but if \u03b8 is negative, it might decrease utility. Wait, but if splitperc_self is higher than contrib_perc, that's advantageous, so participants might not mind, but if it's lower, they might be averse. So maybe the model should have a term that only penalizes when splitperc_self is less than contrib_perc. So:\n\nunfairness = max(contrib_perc - splitperc_self, 0)\nUtility = split_self - \u03b1 * unfairness\n\nHere, \u03b1 is the sensitivity to being underpaid relative to their contribution. If the proposed percentage is less than their contribution percentage, the utility decreases by \u03b1 times the difference. This makes sense because participants might be willing to accept lower amounts if they're still getting a fair share based on their contribution.\n\nAdditionally, there might be a baseline term representing the participant's general willingness to accept, independent of the split. For example, a parameter \u03b2 that captures this baseline. So:\n\nUtility = \u03b2 + split_self - \u03b1 * unfairness\n\nBut since split_self is in pounds, and unfairness is a percentage difference, the units might not align. To address this, perhaps normalize split_self by the combined earnings to make it a percentage, but splitperc_self is already a percentage. Wait, split_self is the proposed amount in \u00a3, and splitperc_self is the percentage. Maybe using splitperc_self directly is better for the fairness comparison.\n\nWait, contrib_perc is (token_self / total_tokens) * 100, and splitperc_self is the percentage offered. So the unfairness is contrib_perc - splitperc_self. If this is positive, the participant is getting less than their contribution share. So:\n\nunfairness = max(contrib_perc - splitperc_self, 0)\nUtility = \u03b2 * splitperc_self - \u03b1 * unfairness\n\nBut splitperc_self is the percentage offered to the participant. So higher splitperc_self increases utility, but if it's below contrib_perc, the unfairness term reduces utility. Alternatively, maybe the utility is a combination of the absolute amount (split_self) and the fairness term.\n\nAlternatively, since combined_earning is variable, the absolute split_self is important. For example, getting \u00a35 in a \u00a310 total is different from \u00a35 in a \u00a320 total. But the problem states that two equal split_self values don't represent the same fairness. So perhaps the model should consider both the absolute amount and the fairness relative to contribution.\n\nBut to simplify, maybe focus on the percentage terms. Let's define:\n\nfair_share_perc = (token_self / (token_self + token_opp)) * 100\nunfairness = max(fair_share_perc - splitperc_self, 0)\n\nUtility = \u03b2 * split_self - \u03b1 * unfairness\n\nHere, \u03b2 represents the sensitivity to the absolute proposed amount, and \u03b1 represents the sensitivity to unfairness (receiving less than their fair share percentage). If \u03b1 is positive, unfairness reduces utility. The higher \u03b1, the more the participant is willing to reject offers that are below their fair share.\n\nBut split_self is in pounds, and unfairness is a percentage. To make the units compatible, perhaps scale the unfairness term by the combined earnings. For example, the monetary value of the unfairness is (fair_share_perc - splitperc_self)/100 * combined_earning. So:\n\nunfairness_amount = (fair_share_perc - splitperc_self)/100 * combined_earning\nunfairness_amount = max(unfairness_amount, 0)\n\nUtility = split_self - \u03b1 * unfairness_amount\n\nThis way, both terms are in pounds, and \u03b1 is a unitless parameter indicating how much each pound of unfairness affects the utility. If \u03b1 is high, even a small unfairness in monetary terms would significantly reduce utility.\n\nAlternatively, maybe participants compare their split to the fair share in absolute terms. For example, fair_share = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and fair_share is:\n\ndifference = fair_share - split_self\nunfairness = max(difference, 0)\n\nUtility = split_self - \u03b1 * unfairness\n\nHere, \u03b1 represents the aversion to receiving less than the fair share. If the proposed split_self is less than fair_share, the utility decreases by \u03b1 times the difference. This seems plausible because it directly compares the actual offer to what's fair based on contribution.\n\nThis model would have one learnable parameter, \u03b1, which must be non-negative (since receiving less than fair share should decrease utility). The utility is then split_self minus \u03b1 times the unfairness. If the utility is positive, the probability of accepting is greater than 50%; if negative, less than 50%.\n\nBut perhaps there's also a term for the proposer's share. For example, if the opponent is taking more than their fair share, the participant might be more inclined to reject. However, in the responder role, the participant only cares about their own share versus their contribution. The opponent's share is determined by the proposer (the opponent in this case), so the key comparison is between the participant's split and their contribution-based fair share.\n\nAnother consideration is the efficiency of the proposal: the total combined earning. Participants might be more likely to accept unfair offers if the total is high because rejecting means both get nothing. But the problem statement says that the model should predict based on the variables provided. The combined_earning is available, so maybe including it as a factor. For example, the utility could be:\n\nUtility = split_self + \u03b3 * combined_earning - \u03b1 * unfairness\n\nBut this complicates the model with an additional parameter \u03b3. However, the priority is parameter recovery, so keeping the number of parameters low is better. Maybe \u03b3 is not necessary, or perhaps combined_earning is accounted for in the split_self and fair_share terms.\n\nWait, split_self is part of the combined_earning, so perhaps the model doesn't need to explicitly include combined_earning beyond what's already in split_self and fair_share. For example, if combined_earning is high, split_self could be high even if the percentage is low. But the participant's decision might depend on both the absolute amount and the fairness. However, the problem states that two equal split_self values don't represent the same fairness, implying that the model must account for the context of combined_earning. Therefore, the fairness comparison (fair_share) inherently considers combined_earning because fair_share is (token_self / total_tokens) * combined_earning.\n\nSo, the model using fair_share and split_self difference captures the effect of combined_earning. For example, if combined_earning is \u00a330, a split_self of \u00a310 might be unfair if the fair_share is \u00a315 (because the participant contributed 50% of tokens), but if combined_earning is \u00a360, a split_self of \u00a320 with fair_share \u00a330 is similarly unfair. The model would capture this through the unfairness term.\n\nThus, the utility function could be:\n\nUtility = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nWhere fair_share = (token_self / (token_self + token_opp)) * combined_earning\n\nThis way, \u03b1 represents the sensitivity to being underpaid relative to the fair share. If \u03b1 is 0, the participant only cares about the absolute split_self. Higher \u03b1 means they are more sensitive to unfairness.\n\nBut maybe participants also have a baseline acceptance tendency, independent of the offer. For example, a parameter \u03b2 that captures this. So:\n\nUtility = \u03b2 + split_self - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b2 could be a constant term. If \u03b2 is positive, the participant has a higher baseline utility, making acceptance more likely. However, introducing \u03b2 adds another parameter, which might complicate parameter recovery. Since the priority is to have a model that can recover parameters well, perhaps it's better to start with a single parameter \u03b1. But if the data has a general bias towards acceptance or rejection, \u03b2 could capture that.\n\nAlternatively, the logistic function includes a bias term inherently. For example, the probability of acceptance is 1 / (1 + exp(-Utility)). If Utility is split_self - \u03b1 * unfairness, then when split_self is exactly equal to the fair_share, Utility is split_self, which is fair_share. But maybe participants have a different baseline. However, without \u03b2, the model assumes that when the offer is exactly fair, the utility is split_self, which is fair_share. But perhaps participants require more than just the fair share to accept, or maybe they have a general tendency to accept or reject. Including \u03b2 could help capture that.\n\nBut given the instruction to have a slight bias towards fewer parameters, maybe start with \u03b1 and \u03b2. Wait, the example in the problem statement had two learnable parameters. Let's see: the example had beta and epsilon as learnable. So perhaps having two parameters is acceptable.\n\nAlternatively, maybe the model should have \u03b1 (unfairness aversion) and \u03bb (altruism or other-regarding preference). But given the task is responder behavior, perhaps \u03b1 is sufficient.\n\nWait, another angle: the participant might also consider the opponent's share. For example, if the opponent is keeping a lot more than their fair share, the participant might be more likely to reject. But in the responder role, the opponent's split is determined by the proposer (the opponent), so the participant's decision is based on their own share versus their contribution. However, the model could include a term comparing the opponent's split to their contribution. For example:\n\nopponent_fair_share = (token_opp / (token_self + token_opp)) * combined_earning\nopponent_unfairness = max(split_opp - opponent_fair_share, 0)\n\nUtility = split_self - \u03b1 * (fair_share - split_self) - \u03b3 * opponent_unfairness\n\nBut this adds another parameter \u03b3, making the model more complex. Given the need for parameter recovery, it's better to keep it simple. The problem statement also mentions that the model should utilize the given variables appropriately but not necessarily all. So focusing on the key variables: token_self, token_opp, combined_earning, split_self, splitperc_self.\n\nGiven that, the core idea is that participants compare their proposed share to their contribution-based fair share. The utility decreases if the proposed share is below the fair share, with sensitivity determined by \u03b1.\n\nSo, the model would be:\n\nfair_share = (token_self / (token_self + token_opp)) * combined_earning\nunfairness = max(fair_share - split_self, 0)\nUtility = split_self - \u03b1 * unfairness\n\nThis model has one learnable parameter, \u03b1, which must be non-negative (since unfairness reduces utility). The range for \u03b1 could be [0, 10], for example, allowing for varying degrees of sensitivity.\n\nBut perhaps participants also have a non-linear sensitivity to unfairness. For example, the disutility could be quadratic in unfairness. However, that would introduce another parameter, which complicates the model. Given the need for simplicity and parameter recovery, a linear term is better.\n\nAnother consideration: the participant's own contribution (token_self) might influence their fairness perception. For example, if they contributed more, they might be more sensitive to unfairness. But this is already captured in the fair_share calculation.\n\nAlternatively, the model could include a term that weights the unfairness by the participant's contribution. But again, this complicates the model.\n\nIn summary, the model should calculate the fair share based on the participant's contribution, determine the unfairness as the difference between fair share and proposed split (if positive), and subtract a scaled version of this unfairness from the proposed split to get the utility. The probability of acceptance is then a logistic function of this utility.\n\nNow, considering the parameters:\n\n- \u03b1 (alpha): Sensitivity to unfairness. Must be \u2265 0. Bounds could be [0, 10], which is generous but finite.\n\nThis gives a single learnable parameter, which is good for parameter recovery. The utility is:\n\nUtility = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nBut wait, split_self is the proposed amount to the participant. If the proposed amount is less than the fair share, the utility decreases. The probability of acceptance is 1 / (1 + exp(-Utility)). So when Utility is positive, the probability is >0.5; when negative, <0.5.\n\nThis model assumes that the utility is linear in both the proposed split and the unfairness. The parameter \u03b1 determines how much each unit of unfairness reduces the utility. For example, if \u03b1 is 2, then each \u00a31 of unfairness reduces the utility by \u00a32, making the participant more likely to reject.\n\nTesting this model: If the proposed split equals the fair share, Utility = split_self, which is fair_share. If fair_share is, say, \u00a310, then Utility is \u00a310. The probability of acceptance would be close to 1 (since exp(-10) is negligible). If the proposed split is \u00a35 and fair_share is \u00a310, then unfairness is \u00a35, and Utility = 5 - \u03b1*5. If \u03b1 is 1, Utility is 0, so probability is 0.5. If \u03b1 is 2, Utility is -5, probability is ~0.007, so likely rejection.\n\nThis seems reasonable. The model captures the trade-off between the absolute offer and the fairness. Participants might accept a lower offer if it's close to fair, but reject if it's too unfair, depending on their \u03b1.\n\nNow, mapping the variables:\n\n- split_self: from the data (float64)\n- token_self: from the data (int64)\n- token_opp: from the data (int64)\n- combined_earning: from the data (int64)\n- fair_share: calculated as (token_self / (token_self + token_opp)) * combined_earning\n- unfairness: max(fair_share - split_self, 0)\n- alpha: learnable parameter, range [0, 10]\n\nThe utility is then split_self - alpha * unfairness.\n\nThis model uses the variables token_self, token_opp, combined_earning, and split_self from the dataset. The target variable is 'accept'.\n\nNow, checking the dataset variables: token_self, token_opp, combined_earning, split_self are all available. So the model is feasible.\n\nAnother possible addition is a term for the participant's own token count influencing their sensitivity. For example, someone who contributed more tokens might be more sensitive to unfairness. But this would require an interaction term, adding more parameters. To keep it simple, we'll stick with the initial model.\n\nThus, the final model is:\n\n<MODEL>\nUtility = split_self - alpha * max( ( (token_self / (token_self + token_opp) ) * combined_earning ) - split_self, 0 )\n</MODEL>\n\nVariables:\n\n- alpha: learnable, range [0, 10]\n- split_self: from data\n- token_self: from data\n- token_opp: from data\n- combined_earning: from data\n\nThe variable descriptions in JSON would include alpha as a learnable parameter with min 0 and max 10, inclusive. The other variables are from the data or calculated.\n\nNow, the summary should concisely describe the model. It's a utility model where the utility of accepting depends on the proposed amount minus the product of unfairness (difference between fair share and proposed amount) and a sensitivity parameter. The fair share is based on the participant's contribution relative to the total.\n</think>\n\nI have recieved the test instructions\n\n<MODEL>\nUtility = split_self - alpha * max( ( (token_self / (token_self + token_opp) ) * combined_earning ) - split_self, 0 )\n</MODEL>\n\n<target_variable>accept</target_variable>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to unfairness when proposed amount is below contribution-based fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Opponent's token contribution\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money in shared pot\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA fairness-based utility model where acceptance probability depends on the proposed amount relative to a contribution-adjusted fair share. Utility decreases linearly with the deficit between proposed amount and the fair share (calculated from token contributions), scaled by a learnable unfairness sensitivity parameter (alpha). Maintains one learnable parameter for robust recovery.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: Utility = split_self - alpha * max( ( (token_self / (token_self + token_opp) ) * combined_earning ) - split_self, 0 )\nSummary: A fairness-based utility model where acceptance probability depends on the proposed amount relative to a contribution-adjusted fair share. Utility decreases linearly with the deficit between proposed amount and the fair share (calculated from token contributions), scaled by a learnable unfairness sensitivity parameter (alpha). Maintains one learnable parameter for robust recovery.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n\n        # Calculate fair share\n        total_tokens = token_self + token_opp\n        if total_tokens == 0:\n            fair_share = 0\n        else:\n            fair_share = (token_self / total_tokens) * combined_earning\n\n        # Calculate utility\n        unfairness = max(fair_share - split_self, 0)\n        utility = split_self - alpha * unfairness\n\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.789072039072039,
  "fitting_results": [
    {
      "alpha": 6.560687780207715,
      "success": true,
      "log_likelihood": -19.290984238861903,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3918959748343034,
      "success": true,
      "log_likelihood": -17.59432659605719,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.459287107744272,
      "success": true,
      "log_likelihood": -10.228883640726517,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6438425767673766,
      "success": true,
      "log_likelihood": -14.142834706802747,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9111396886860592,
      "success": true,
      "log_likelihood": -9.848709686242168,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0558815205064627,
      "success": true,
      "log_likelihood": -11.198187153561147,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -2.3703753451934495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0150754586686237,
      "success": true,
      "log_likelihood": -11.251236946184802,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -2.370375345193449,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0196407162776153,
      "success": true,
      "log_likelihood": -11.402688783828665,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8719670315719095,
      "success": true,
      "log_likelihood": -5.5835349370939165,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.26347413884768,
      "success": true,
      "log_likelihood": -18.44201461022876,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0080157309696394,
      "success": true,
      "log_likelihood": -8.854760654857754,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -2.370375345193449,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3830444799212494,
      "success": true,
      "log_likelihood": -6.214918122041387,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.36213586304113726,
      "success": true,
      "log_likelihood": -7.656146828788513,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -2.3703753451934495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3282783930645718,
      "success": true,
      "log_likelihood": -15.841966351222434,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.433516202709028,
      "success": true,
      "log_likelihood": -8.623736865238,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5891046930068433,
      "success": true,
      "log_likelihood": -23.282250660963637,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2055570863123486,
      "success": true,
      "log_likelihood": -18.3391199562246,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6367998953619405,
      "success": true,
      "log_likelihood": -14.538881710394906,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1240948407462479,
      "success": true,
      "log_likelihood": -15.182962254640646,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2580692347159448,
      "success": true,
      "log_likelihood": -11.656930369516102,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.6452825826095867,
      "success": true,
      "log_likelihood": -4.210645140337255,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.33506939270513963,
      "success": true,
      "log_likelihood": -9.476787731626334,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6035295199372385,
      "success": true,
      "log_likelihood": -16.643513671693608,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.560689814722408,
      "success": true,
      "log_likelihood": -19.290984238863093,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.312891859694331,
      "success": true,
      "log_likelihood": -28.886222226352217,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7493937289432486,
      "success": true,
      "log_likelihood": -17.48240082067635,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -2.370375345193449,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8324038367436518,
      "success": true,
      "log_likelihood": -13.374533139629312,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8928407334005495,
      "success": true,
      "log_likelihood": -8.625441841616638,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5313101308433184,
      "success": true,
      "log_likelihood": -14.738122138836866,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9111701956833353,
      "success": true,
      "log_likelihood": -19.198954265692876,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1816558928081924,
      "success": true,
      "log_likelihood": -9.550269052105032,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1066885800360415,
      "success": true,
      "log_likelihood": -19.452185122058992,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.9461705161747016,
      "success": true,
      "log_likelihood": -18.513254161284497,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -7.770375323053267,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1417465214012246,
      "success": true,
      "log_likelihood": -16.903222301404448,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0683901398781945,
      "success": true,
      "log_likelihood": -13.27023072061411,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6065709798176718,
      "success": true,
      "log_likelihood": -16.40275617697166,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.328278415533654,
      "success": true,
      "log_likelihood": -15.84196636341582,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.518937377526626,
      "success": true,
      "log_likelihood": -24.495655232787517,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0936834998221614,
      "success": true,
      "log_likelihood": -11.652269950459573,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8640502034571571,
      "success": true,
      "log_likelihood": -35.47951200693466,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7286071914836694,
      "success": true,
      "log_likelihood": -18.651005410331138,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.0272785381069234,
      "success": true,
      "log_likelihood": -29.917412584456848,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0725878042819839,
      "success": true,
      "log_likelihood": -21.959657512410335,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7679168735061812,
      "success": true,
      "log_likelihood": -28.104284474886033,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6725991415646104,
      "success": true,
      "log_likelihood": -27.547954303275784,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.6801673146593425,
      "success": true,
      "log_likelihood": -5.080842705203493,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.802914262971797,
      "success": true,
      "log_likelihood": -11.836822093246926,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -16.020375317788545,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.723136398600398,
      "success": true,
      "log_likelihood": -21.133285694318282,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3919768866588256,
      "success": true,
      "log_likelihood": -14.590537752838229,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9372873905523128,
      "success": true,
      "log_likelihood": -23.178561596518865,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.518936683540209,
      "success": true,
      "log_likelihood": -15.49565527133606,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.560693181430048,
      "success": true,
      "log_likelihood": -19.290984238869452,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2532034645199333,
      "success": true,
      "log_likelihood": -26.99527552590273,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.02727830004019,
      "success": true,
      "log_likelihood": -14.317412672931935,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1732472236417744,
      "success": true,
      "log_likelihood": -14.63791967565883,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.20232629201368618,
      "success": true,
      "log_likelihood": -8.477425857323839,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.38900101786693614,
      "success": true,
      "log_likelihood": -11.799111117732593,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.02312984394613,
      "success": true,
      "log_likelihood": -16.645507463640755,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1222859916656626,
      "success": true,
      "log_likelihood": -12.316454827602925,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -1.3827686923606062,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.265576253072923,
      "success": true,
      "log_likelihood": -26.811112937630867,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.277724574779776,
      "success": true,
      "log_likelihood": -16.29620774806537,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4631182939312162,
      "success": true,
      "log_likelihood": -15.451259084719993,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.07258820931124,
      "success": true,
      "log_likelihood": -32.45965747051343,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9074831352803496,
      "success": true,
      "log_likelihood": -13.4853531263629,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08489620188132276,
      "success": true,
      "log_likelihood": -7.8067164935615345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.978360521461228,
      "success": true,
      "log_likelihood": -28.96978052709512,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6870686982138673,
      "success": true,
      "log_likelihood": -30.981126395024486,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.560687551414779,
      "success": true,
      "log_likelihood": -19.290984238861896,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0110679429356852,
      "success": true,
      "log_likelihood": -32.55319751249074,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.02250228291120677,
      "success": true,
      "log_likelihood": -12.863417496525258,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.573746287627901,
      "success": true,
      "log_likelihood": -23.480385031218308,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3179988361210437,
      "success": true,
      "log_likelihood": -10.53890576754117,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7612940804647986,
      "success": true,
      "log_likelihood": -37.033665284107784,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.560686915327263,
      "success": true,
      "log_likelihood": -15.69098424251909,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7913324406549961,
      "success": true,
      "log_likelihood": -19.63571203226095,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.836613283012166,
      "success": true,
      "log_likelihood": -21.763874598196292,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12228566312051825,
      "success": true,
      "log_likelihood": -6.316454855482137,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2532034430896337,
      "success": true,
      "log_likelihood": -14.995275579339578,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6310317122844451,
      "success": true,
      "log_likelihood": -26.460218373179185,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.9203377667790817,
        "p": 9.713279322540773e-42
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 34.221569452521194,
  "bic_results": {
    "average_bic": 34.221569452521194,
    "individual_bics": [
      42.165487416179914,
      38.77217213057049,
      24.041286219909143,
      31.869188352061602,
      23.280938310940446,
      25.979893245578403,
      8.324269628843009,
      26.085992830825713,
      8.324269628843009,
      26.38889650611344,
      14.750588812643944,
      40.46754815891363,
      21.293040248171618,
      8.324269628843009,
      8.324269628843009,
      8.324269628843009,
      8.324269628843009,
      16.013355182538884,
      18.895812596033135,
      8.324269628843009,
      35.26745164090098,
      20.830992668932108,
      50.14802026038338,
      40.26175885090531,
      32.661282359245924,
      33.9494434477374,
      26.897379677488313,
      12.004809219130621,
      22.537094401708778,
      36.870546281843325,
      8.324269628843009,
      42.165487416182295,
      61.35596339116054,
      38.54832057980881,
      8.324269628843009,
      30.332585217714733,
      20.834402621689385,
      33.059763216129845,
      41.98142746984186,
      22.684057042666172,
      42.487889182574094,
      40.6100272610251,
      19.124269584562644,
      37.389963541265004,
      30.12398037968433,
      36.38903129239943,
      35.26745166528775,
      52.57482940403114,
      26.888058839375255,
      74.54254295232543,
      40.885529759118384,
      63.418344107369805,
      47.50283396327678,
      59.792087888228174,
      58.67942754500768,
      13.745204348863094,
      27.25716312494996,
      35.6242695740332,
      45.85009032709267,
      32.76459444413257,
      49.94064213149384,
      34.57482948112823,
      42.16548741619501,
      57.57406999026157,
      32.21834428431998,
      32.85935828977377,
      20.538370653103787,
      27.181741173921296,
      36.87453386573762,
      28.21642859366196,
      6.349056323177322,
      57.20574481371784,
      36.17593443458685,
      34.486037107896095,
      68.50283387948296,
      30.55422519118191,
      19.196951925579178,
      61.52307999264635,
      65.54577172850509,
      42.1654874161799,
      68.68991396343759,
      29.310353931506626,
      50.544289000892725,
      24.66133047353845,
      77.65084950667168,
      34.96548742349429,
      42.85494300297801,
      47.11126813484869,
      16.216428649420383,
      33.57407009713527,
      56.50395568481448
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.920",
  "previous_models": [
    "Specification: Utility = split_self - alpha * max( ( (token_self / (token_self + token_opp) ) * combined_earning ) - split_self, 0 )\nSummary: A fairness-based utility model where acceptance probability depends on the proposed amount relative to a contribution-adjusted fair share. Utility decreases linearly with the deficit between proposed amount and the fair share (calculated from token contributions), scaled by a learnable unfairness sensitivity parameter (alpha). Maintains one learnable parameter for robust recovery.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 34.221569452521194\nOverall Accuracy: 0.7891\n\nParameter Recovery:\n- alpha: r = 0.920"
  ]
}