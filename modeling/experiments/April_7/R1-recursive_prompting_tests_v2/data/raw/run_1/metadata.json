{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: THIS IS A TEST TO SEE IF THESE INSTRUCTIONS ARE ACTUALLY BEING DISPLAYED WITHIN THE PIPELINE AND GIVEN TO THE MODEL. IF THIS STRING OF TEXT IS WITHIN YOUR INSTRUCTIONS, PLESAE START YOUR RESPONSE WITH 'I have recieved the test instructions\n\nBe VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility should be structured such that a utility less than 0 corresponds to less than 50% probability of accepting.\n\nFirst, I need to understand the key variables involved. The main target is 'accept', which is binary (1 or 0). The model should predict this based on the utility of the proposed split. The variables provided include token contributions (self and opponent), combined earnings, split amounts, percentages, etc. \n\nThe key psychological mechanisms here likely revolve around fairness and self-interest. Participants might compare their own share to what they perceive as fair, considering both their contribution and the opponent's. The utility function should capture the difference between the proposed split and a reference point (like what's considered fair), weighted by some parameters.\n\nLet me think about how to model fairness. The combined earnings are calculated as (token_self + token_opp) * \u00a33. The split_self is the amount offered to the participant. A fair split might be proportional to their contribution. For example, if the participant contributed more tokens, they might expect a higher share. So, the fair share could be (token_self / (token_self + token_opp)) * combined_earning. \n\nThe model could compare the proposed split_self to this fair share. The difference between the two (split_self - fair_share) would be a factor in utility. If the proposed amount is higher than the fair share, utility increases; if lower, it decreases. \n\nAnother factor could be the absolute amount offered. Even if the split is unfair, a higher absolute amount might still be accepted. So, the utility might include a term for the proposed split_self itself. \n\nAdditionally, participants might have different sensitivities to unfairness. For example, some might strongly reject unfair offers even if the absolute amount is decent, while others might accept as long as the amount is above a certain threshold. This suggests parameters that weight the importance of fairness versus the absolute amount.\n\nLet's outline possible parameters:\n\n1. \u03b1 (alpha): Sensitivity to the difference between the proposed split and the fair share. Positive \u03b1 means participants are more likely to accept when the offer is above the fair share. This parameter could be bounded between, say, -5 and 5 to allow for varying sensitivity.\n\n2. \u03b2 (beta): Sensitivity to the absolute proposed amount (split_self). Higher \u03b2 means participants care more about the absolute amount. Bounds could be 0 to 5, as negative values wouldn't make sense here (since higher amounts should increase utility).\n\n3. \u03b3 (gamma): A baseline bias term, representing a general tendency to accept or reject. This could be bounded between -5 and 5. A negative gamma would indicate a baseline tendency to reject, positive to accept.\n\nThe utility function could then be:\n\nUtility = \u03b3 + \u03b1*(split_self - fair_share) + \u03b2*split_self\n\nThe fair_share is calculated as (token_self / (token_self + token_opp)) * combined_earning. Since combined_earning is (token_self + token_opp)*3, this simplifies to token_self * 3. Wait, no. Wait, combined_earning is (token_self + token_opp) * 3. So the fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is already (token_self + token_opp)*3, so the fair_share would be token_self * 3. Because (token_self / (token_self + token_opp)) * (token_self + token_opp)*3 = token_self *3. So the fair share based on contribution is exactly the participant's own tokens multiplied by \u00a33. That's interesting. So split_self is the proposed amount, and fair_share is 3*token_self. So the difference (split_self - fair_share) is how much more or less the participant is getting compared to their contribution-based fair share.\n\nSo the utility becomes:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nBut wait, maybe participants also consider the opponent's contribution. For example, if the opponent contributed more, maybe the participant expects a different split. Alternatively, the trial_type variable indicates whether contributions are equal, opponent more, or participant more. But trial_type is a categorical variable. However, since the model should use the variables provided, perhaps using token_self and token_opp directly is better.\n\nAlternatively, the fair share could be calculated as (token_self / (token_self + token_opp)) * combined_earning. Let's compute that:\n\nfair_share = (token_self / (token_self + token_opp)) * (token_self + token_opp)*3 = token_self *3. So it's the same as before. So the fair share based on contribution is always 3*token_self. Therefore, the difference (split_self - 3*token_self) represents how much the offer deviates from the contribution-based fair share.\n\nBut maybe participants have a different notion of fairness. For example, they might expect an equal split (50%) regardless of contributions. In that case, the fair share would be combined_earning / 2. So the difference would be split_self - (combined_earning / 2). This could be another term. But how to model this?\n\nAlternatively, the model could include both terms: one for contribution-based fairness and one for equal split fairness. But that might complicate the model and introduce more parameters. However, given the need to keep the number of parameters low to aid parameter recovery, perhaps we should choose one.\n\nAlternatively, the model could let the data determine which fairness norm is used by including a parameter that weights between the two. But that would add another parameter. Let's see.\n\nSuppose we have:\n\nfairness_diff = split_self - w*(3*token_self) - (1-w)*(combined_earning / 2)\n\nBut this introduces a weight parameter w, which would be between 0 and 1. However, this increases the number of parameters. Maybe that's too much. Alternatively, perhaps the model should focus on the difference between the proposed split and the contribution-based fair share, as that's directly tied to the variables provided (token_self and token_opp).\n\nAlternatively, the participant's perception of fairness could be a combination of their own contribution and the opponent's. For example, if the opponent contributed more, the participant might be more willing to accept a lower split. But how to model that?\n\nAlternatively, the model could use the split percentage (splitperc_self) compared to the fair percentage (token_self / (token_self + token_opp) * 100). The difference in percentages could be a factor. But the split percentage is already provided as splitperc_self. However, using splitperc_self might not account for the actual contribution. For example, if the participant contributed 10 tokens and the opponent 20, the fair percentage would be 33.3%. If the split is 40%, that's more than fair. But if the combined earning is higher, the absolute amount might be more important.\n\nHmm. The problem states that two equal split_self variables do not represent the same fairness because the combined earning varies. So the model must account for both the split amount and the context (contributions). Therefore, the utility should consider both the absolute amount offered (split_self) and how it compares to a fairness reference (like contribution-based or equal split).\n\nSo perhaps the utility function should have two terms: one for the absolute offer (split_self) and one for the difference between the offer and the fair share (split_self - fair_share). The fair_share could be based on contribution (3*token_self) or equal split (combined_earning / 2). Let's test both possibilities.\n\nIf we use contribution-based fair share:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nIf we use equal split:\n\nUtility = gamma + alpha*(split_self - combined_earning/2) + beta*split_self\n\nWhich one is better? The contribution-based approach directly uses the token_self and token_opp variables, which are provided. The equal split approach uses combined_earning, which is also provided. However, the problem mentions that trial_type is 1=equal, 2=opponent more, 3=participant more. So perhaps the model should account for the trial_type in the fairness calculation. But trial_type is a categorical variable, which complicates the model. Alternatively, the model could use the token_self and token_opp to compute the relative contribution.\n\nAlternatively, the model could compute the fair share as a weighted average between contribution-based and equal split, with a parameter determining the weight. But that adds another parameter. Let's see.\n\nBut the user mentioned that parameter recovery is a priority, so we should keep the number of parameters low. Let's start with a simple model.\n\nLet's assume that participants evaluate the offer based on two factors: the absolute amount they receive (split_self) and how fair that offer is relative to their contribution (split_self - 3*token_self). The fairness term (split_self - 3*token_self) captures whether the offer is more or less than what they contributed. The absolute term (split_self) captures self-interest.\n\nSo the utility would be:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nThen, the probability of accepting is logistic(Utility). If Utility > 0, the probability is >0.5.\n\nThis model has three parameters: gamma (baseline), alpha (fairness sensitivity), and beta (self-interest sensitivity). \n\nBut wait, combining alpha*(split_self - 3*token_self) and beta*split_self can be rewritten as (alpha + beta)*split_self - alpha*3*token_self + gamma. This might lead to identifiability issues between alpha and beta. For example, if alpha and beta are both positive, their effects on split_self could be confounded. To avoid high covariance, perhaps one of the parameters can be fixed, or the model can be reparameterized.\n\nAlternatively, maybe the model should have one parameter that captures the trade-off between fairness and self-interest. For example:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + (1 - alpha)*split_self\n\nBut this constrains the coefficients of the two terms to sum to 1, which might not be appropriate. Alternatively, use separate parameters but ensure they are identifiable.\n\nAlternatively, perhaps the model should include only the fairness term and the absolute term, allowing both parameters to vary. However, this could still lead to some covariance. To mitigate this, perhaps the model should include only one of these terms. For example, if we assume that participants care only about the fairness difference, then:\n\nUtility = gamma + alpha*(split_self - 3*token_self)\n\nOr if they care only about the absolute amount:\n\nUtility = gamma + beta*split_self\n\nBut combining both might better capture the decision-making process. However, with three parameters (gamma, alpha, beta), there's a risk of overfitting, but given 36 trials per participant, it's manageable. \n\nAnother consideration is the opponent's contribution. Maybe participants also consider how much the opponent contributed. For example, if the opponent contributed more, the participant might feel that the opponent deserves a larger share. But how to model that? The split_opp is the opponent's proposed share, but in the responder role, the participant is responding to the opponent's proposal. So split_opp is the amount the opponent is taking, and split_self is what's offered to the participant. \n\nAlternatively, the participant might compare their own split to the opponent's split. For example, the ratio of split_self to split_opp. But that could be another term. However, this complicates the model further. Let's stick to the initial approach for simplicity.\n\nNow, considering the parameters:\n\n- gamma: baseline utility. If all terms are zero, gamma determines the baseline tendency to accept. Bounds could be [-5, 5], as it's a bias term.\n\n- alpha: sensitivity to the fairness term (split_self - 3*token_self). This could be positive or negative. If positive, participants are more likely to accept when the offer exceeds their contribution-based fair share. Bounds could be [-5, 5].\n\n- beta: sensitivity to the absolute split_self. Expected to be positive, as higher offers should increase acceptance. Bounds [0, 5].\n\nBut including both alpha and beta might lead to covariance. For example, if split_self is highly correlated with (split_self - 3*token_self), then alpha and beta could be hard to distinguish. To check, let's see:\n\nsplit_self is part of both terms. Suppose in some trials, split_self is high when 3*token_self is also high. Then, the two terms could be correlated. However, in other trials, split_self might be high even if 3*token_self is low (e.g., the opponent is generous). So there might be enough variation to estimate both parameters. \n\nAlternatively, perhaps the model should use the difference term and the fair share term separately. Wait, the difference term is (split_self - fair_share), and the fair_share is 3*token_self. So the difference term is split_self - 3*token_self. The absolute term is split_self. So including both would be:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n= gamma + (alpha + beta)*split_self - alpha*3*token_self\n\nThis can be rewritten as:\n\nUtility = gamma + (alpha + beta)*split_self - 3*alpha*token_self\n\nBut this might make it hard to distinguish alpha and beta unless there's sufficient variation in token_self and split_self independently. For example, if token_self varies while split_self is fixed, then the coefficient for token_self would be -3*alpha, allowing estimation of alpha. Then, the coefficient for split_self is (alpha + beta), allowing estimation of beta once alpha is known. However, in practice, if split_self and token_self are correlated, this could lead to multicollinearity issues. \n\nAlternatively, perhaps the model should use the ratio of split_self to fair_share. For example, (split_self / fair_share) - 1. But that introduces division, which could complicate things. Alternatively, use the percentage split (splitperc_self) compared to the fair percentage (token_self / (token_self + token_opp)*100). But splitperc_self is already a variable. However, using splitperc_self might not account for the actual contribution, as the problem states that two equal split_self values don't represent the same fairness if combined_earning differs. Wait, but splitperc_self is the percentage of the combined_earning. So if combined_earning is higher, the same percentage means a higher absolute amount. But the problem says that split_self's fairness depends on the combined_earning. So perhaps the absolute amount is important, but the percentage relative to contribution is also a factor.\n\nThis is getting a bit complicated. Let's return to the initial model with three parameters: gamma, alpha, beta.\n\nAnother approach is to model the utility as a function of the offer's deviation from equality. For example, the equitable split is combined_earning / 2. So the difference split_self - (combined_earning / 2) could be a term. This represents how much more or less the participant is getting compared to an equal split. \n\nSo the utility could be:\n\nUtility = gamma + alpha*(split_self - combined_earning/2) + beta*split_self\n\nHere, alpha captures sensitivity to equity, and beta captures sensitivity to the absolute offer. This model also has three parameters. \n\nComparing the two approaches:\n\n1. Contribution-based fairness: split_self vs 3*token_self\n2. Equity-based fairness: split_self vs combined_earning/2\n\nWhich is more psychologically plausible? In ultimatum game studies, people often reject offers that are perceived as unfair, even if it means getting nothing. The fairness norm in such games is often around 40-50%, suggesting that equity (equal split) is a common reference. However, in this task, participants have contributed tokens, which might make contribution-based fairness more salient. \n\nThe problem statement mentions that trial_type is based on contributions (1=equal, 2=opponent more, 3=participant more), which suggests that the participants' contributions vary, and the model should account for that. Therefore, using the contribution-based fair share (3*token_self) might be more appropriate.\n\nBut to confirm, let's compute both fair shares:\n\n- Contribution-based fair share: 3*token_self\n- Equity-based fair share: combined_earning / 2 = (token_self + token_opp)*3 / 2\n\nThe difference between the two is that contribution-based depends only on the participant's tokens, while equity-based depends on both. \n\nIncluding both might be better, but that would add more parameters. Let's see:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*(split_self - (token_self + token_opp)*3/2) + delta*split_self\n\nBut this introduces four parameters, which might be too many. Alternatively, choose one fairness norm. \n\nGiven the task description emphasizes that the combined earning varies and that two equal split_self values aren't equally fair, the model must account for the context of contributions. Therefore, the contribution-based fair share seems essential. \n\nThus, proceeding with the initial model:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nThis model uses the difference between the offer and the contribution-based fair share, plus the absolute offer. The parameters alpha and beta capture the sensitivity to fairness and self-interest, respectively. Gamma is the baseline bias.\n\nNow, the probability of accepting is logistic(Utility). The model needs to ensure that when Utility < 0, the probability is <50%, which aligns with the requirement.\n\nRegarding parameter bounds:\n\n- gamma: Bounds could be [-5, 5], as it's a bias term. A wide range allows for strong biases in either direction.\n- alpha: Sensitivity to fairness. Could be positive (prefer offers above fair share) or negative (accept even if below fair share). Bounds [-5, 5].\n- beta: Sensitivity to absolute offer. Should be positive, as higher offers are better. Bounds [0, 5].\n\nThese bounds are generous but finite, aiding parameter recovery.\n\nNow, checking for identifiability. If alpha and beta are both positive, their effects on split_self are additive. For example, if split_self increases, both terms contribute positively. However, the term (split_self - 3*token_self) also depends on token_self. If token_self varies across trials, this can help distinguish alpha and beta. For instance, in trials where token_self is low, a given split_self might be much higher than 3*token_self, contributing more to utility. In trials where token_self is high, the same split_self might be closer to or below the fair share. This variation can help estimate alpha and beta separately.\n\nAnother consideration: the term (split_self - 3*token_self) can be rewritten as split_self - fair_share, where fair_share is 3*token_self. This term represents the surplus (or deficit) relative to the contribution-based fair share. Participants might be more likely to accept if this surplus is positive. \n\nIncluding both the surplus and the absolute split_self allows the model to capture participants who care about both getting a fair deal and maximizing their own earnings. For example, a participant might accept an unfair offer if the absolute amount is high enough (high beta), or reject a fair offer if the absolute amount is too low (low beta but high alpha).\n\nThis model should have three learnable parameters (gamma, alpha, beta) with the specified bounds. The variables used are split_self and token_self from the dataset, which are available when the participant is the responder (trial_role=1). \n\nNow, mapping the variables:\n\n- split_self: proposed share for the participant (data variable)\n- token_self: participant's tokens (data variable)\n- combined_earning is (token_self + token_opp)*3, but in the model, we use 3*token_self, which is derived from token_self.\n\nThe model doesn't use combined_earning directly, but 3*token_self is part of the calculation. However, since token_self is provided, the model can compute it.\n\nNow, the formal mathematical model would be:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nThe probability of acceptance is 1 / (1 + exp(-Utility)).\n\nVariables:\n\n- gamma: learnable, baseline utility. Range [-5,5]\n- alpha: learnable, fairness sensitivity. Range [-5,5]\n- beta: learnable, absolute offer sensitivity. Range [0,5]\n- split_self: from data\n- token_self: from data\n\nAll variables except the learnable parameters are sourced from the data. The calculated term (split_self - 3*token_self) is part of the utility function.\n\nNow, writing the variable descriptions in JSON:\n\nFor gamma:\n- description: Baseline utility bias\n- range: min -5, max 5, inclusive both\n- learnable: true\n- source: learnable\n\nFor alpha:\n- description: Sensitivity to the difference between the proposed split and the contribution-based fair share (split_self - 3*token_self)\n- range: min -5, max 5, inclusive both\n- learnable: true\n- source: learnable\n\nFor beta:\n- description: Sensitivity to the absolute proposed split amount (split_self)\n- range: min 0, max 5, inclusive both\n- learnable: true\n- source: learnable\n\nsplit_self and token_self are data variables, so they are marked as source: data, learnable: false.\n\nThe utility variable (U) is calculated, so it's not learnable.\n\nPutting it all together:\n\n<MODEL>\nU = gamma + alpha * (split_self - 3 * token_self) + beta * split_self\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"gamma\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (split_self - 3*token_self)\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute proposed amount (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining baseline bias, sensitivity to deviation from contribution-based fairness, and sensitivity to absolute offer amount. The model predicts acceptance based on whether the utility exceeds zero, with parameters capturing individual differences in fairness considerations and self-interest.\n</SUMMARY>\n\n<target_variable>accept</target_variable>\n</think>\n\nI have recieved the test instructions\n\n<MODEL>\nU = gamma + alpha * (split_self - 3 * token_self) + beta * split_self\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"gamma\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (split_self - 3*token_self)\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute proposed amount (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining baseline bias, sensitivity to deviation from contribution-based fairness, and sensitivity to absolute offer amount. The model predicts acceptance based on whether the utility exceeds zero, with parameters capturing individual differences in fairness considerations and self-interest.\n</SUMMARY>\n\n<target_variable>accept</target_variable>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['gamma', 'alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = gamma + alpha * (split_self - 3 * token_self) + beta * split_self\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"gamma\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (split_self - 3*token_self)\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute proposed amount (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, gamma=0, alpha=0, beta=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        U = gamma + alpha * (split_self - 3 * token_self) + beta * split_self\n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1744026597.245325
    }
  ],
  "learnable_parameters": {
    "gamma": {
      "description": "Baseline utility bias",
      "range": {
        "min": -5,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair share (split_self - 3*token_self)",
      "range": {
        "min": -5,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to absolute proposed amount (split_self)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = gamma + alpha * (split_self - 3 * token_self) + beta * split_self",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "gamma": {
      "description": "Baseline utility bias",
      "range": {
        "min": -5,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair share (split_self - 3*token_self)",
      "range": {
        "min": -5,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to absolute proposed amount (split_self)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model combining baseline bias, sensitivity to deviation from contribution-based fairness, and sensitivity to absolute offer amount. The model predicts acceptance based on whether the utility exceeds zero, with parameters capturing individual differences in fairness considerations and self-interest.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility should be structured such that a utility less than 0 corresponds to less than 50% probability of accepting.\n\nFirst, I need to understand the key variables involved. The main target is 'accept', which is binary (1 or 0). The model should predict this based on the utility of the proposed split. The variables provided include token contributions (self and opponent), combined earnings, split amounts, percentages, etc. \n\nThe key psychological mechanisms here likely revolve around fairness and self-interest. Participants might compare their own share to what they perceive as fair, considering both their contribution and the opponent's. The utility function should capture the difference between the proposed split and a reference point (like what's considered fair), weighted by some parameters.\n\nLet me think about how to model fairness. The combined earnings are calculated as (token_self + token_opp) * \u00a33. The split_self is the amount offered to the participant. A fair split might be proportional to their contribution. For example, if the participant contributed more tokens, they might expect a higher share. So, the fair share could be (token_self / (token_self + token_opp)) * combined_earning. \n\nThe model could compare the proposed split_self to this fair share. The difference between the two (split_self - fair_share) would be a factor in utility. If the proposed amount is higher than the fair share, utility increases; if lower, it decreases. \n\nAnother factor could be the absolute amount offered. Even if the split is unfair, a higher absolute amount might still be accepted. So, the utility might include a term for the proposed split_self itself. \n\nAdditionally, participants might have different sensitivities to unfairness. For example, some might strongly reject unfair offers even if the absolute amount is decent, while others might accept as long as the amount is above a certain threshold. This suggests parameters that weight the importance of fairness versus the absolute amount.\n\nLet's outline possible parameters:\n\n1. \u03b1 (alpha): Sensitivity to the difference between the proposed split and the fair share. Positive \u03b1 means participants are more likely to accept when the offer is above the fair share. This parameter could be bounded between, say, -5 and 5 to allow for varying sensitivity.\n\n2. \u03b2 (beta): Sensitivity to the absolute proposed amount (split_self). Higher \u03b2 means participants care more about the absolute amount. Bounds could be 0 to 5, as negative values wouldn't make sense here (since higher amounts should increase utility).\n\n3. \u03b3 (gamma): A baseline bias term, representing a general tendency to accept or reject. This could be bounded between -5 and 5. A negative gamma would indicate a baseline tendency to reject, positive to accept.\n\nThe utility function could then be:\n\nUtility = \u03b3 + \u03b1*(split_self - fair_share) + \u03b2*split_self\n\nThe fair_share is calculated as (token_self / (token_self + token_opp)) * combined_earning. Since combined_earning is (token_self + token_opp)*3, this simplifies to token_self * 3. Wait, no. Wait, combined_earning is (token_self + token_opp) * 3. So the fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is already (token_self + token_opp)*3, so the fair_share would be token_self * 3. Because (token_self / (token_self + token_opp)) * (token_self + token_opp)*3 = token_self *3. So the fair share based on contribution is exactly the participant's own tokens multiplied by \u00a33. That's interesting. So split_self is the proposed amount, and fair_share is 3*token_self. So the difference (split_self - fair_share) is how much more or less the participant is getting compared to their contribution-based fair share.\n\nSo the utility becomes:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nBut wait, maybe participants also consider the opponent's contribution. For example, if the opponent contributed more, maybe the participant expects a different split. Alternatively, the trial_type variable indicates whether contributions are equal, opponent more, or participant more. But trial_type is a categorical variable. However, since the model should use the variables provided, perhaps using token_self and token_opp directly is better.\n\nAlternatively, the fair share could be calculated as (token_self / (token_self + token_opp)) * combined_earning. Let's compute that:\n\nfair_share = (token_self / (token_self + token_opp)) * (token_self + token_opp)*3 = token_self *3. So it's the same as before. So the fair share based on contribution is always 3*token_self. Therefore, the difference (split_self - 3*token_self) represents how much the offer deviates from the contribution-based fair share.\n\nBut maybe participants have a different notion of fairness. For example, they might expect an equal split (50%) regardless of contributions. In that case, the fair share would be combined_earning / 2. So the difference would be split_self - (combined_earning / 2). This could be another term. But how to model this?\n\nAlternatively, the model could include both terms: one for contribution-based fairness and one for equal split fairness. But that might complicate the model and introduce more parameters. However, given the need to keep the number of parameters low to aid parameter recovery, perhaps we should choose one.\n\nAlternatively, the model could let the data determine which fairness norm is used by including a parameter that weights between the two. But that would add another parameter. Let's see.\n\nSuppose we have:\n\nfairness_diff = split_self - w*(3*token_self) - (1-w)*(combined_earning / 2)\n\nBut this introduces a weight parameter w, which would be between 0 and 1. However, this increases the number of parameters. Maybe that's too much. Alternatively, perhaps the model should focus on the difference between the proposed split and the contribution-based fair share, as that's directly tied to the variables provided (token_self and token_opp).\n\nAlternatively, the participant's perception of fairness could be a combination of their own contribution and the opponent's. For example, if the opponent contributed more, the participant might be more willing to accept a lower split. But how to model that?\n\nAlternatively, the model could use the split percentage (splitperc_self) compared to the fair percentage (token_self / (token_self + token_opp) * 100). The difference in percentages could be a factor. But the split percentage is already provided as splitperc_self. However, using splitperc_self might not account for the actual contribution. For example, if the participant contributed 10 tokens and the opponent 20, the fair percentage would be 33.3%. If the split is 40%, that's more than fair. But if the combined earning is higher, the absolute amount might be more important.\n\nHmm. The problem states that two equal split_self variables do not represent the same fairness because the combined earning varies. So the model must account for both the split amount and the context (contributions). Therefore, the utility should consider both the absolute amount offered (split_self) and how it compares to a fairness reference (like contribution-based or equal split).\n\nSo perhaps the utility function should have two terms: one for the absolute offer (split_self) and one for the difference between the offer and the fair share (split_self - fair_share). The fair_share could be based on contribution (3*token_self) or equal split (combined_earning / 2). Let's test both possibilities.\n\nIf we use contribution-based fair share:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nIf we use equal split:\n\nUtility = gamma + alpha*(split_self - combined_earning/2) + beta*split_self\n\nWhich one is better? The contribution-based approach directly uses the token_self and token_opp variables, which are provided. The equal split approach uses combined_earning, which is also provided. However, the problem mentions that trial_type is 1=equal, 2=opponent more, 3=participant more. So perhaps the model should account for the trial_type in the fairness calculation. But trial_type is a categorical variable, which complicates the model. Alternatively, the model could use the token_self and token_opp to compute the relative contribution.\n\nAlternatively, the model could compute the fair share as a weighted average between contribution-based and equal split, with a parameter determining the weight. But that adds another parameter. Let's see.\n\nBut the user mentioned that parameter recovery is a priority, so we should keep the number of parameters low. Let's start with a simple model.\n\nLet's assume that participants evaluate the offer based on two factors: the absolute amount they receive (split_self) and how fair that offer is relative to their contribution (split_self - 3*token_self). The fairness term (split_self - 3*token_self) captures whether the offer is more or less than what they contributed. The absolute term (split_self) captures self-interest.\n\nSo the utility would be:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nThen, the probability of accepting is logistic(Utility). If Utility > 0, the probability is >0.5.\n\nThis model has three parameters: gamma (baseline), alpha (fairness sensitivity), and beta (self-interest sensitivity). \n\nBut wait, combining alpha*(split_self - 3*token_self) and beta*split_self can be rewritten as (alpha + beta)*split_self - alpha*3*token_self + gamma. This might lead to identifiability issues between alpha and beta. For example, if alpha and beta are both positive, their effects on split_self could be confounded. To avoid high covariance, perhaps one of the parameters can be fixed, or the model can be reparameterized.\n\nAlternatively, maybe the model should have one parameter that captures the trade-off between fairness and self-interest. For example:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + (1 - alpha)*split_self\n\nBut this constrains the coefficients of the two terms to sum to 1, which might not be appropriate. Alternatively, use separate parameters but ensure they are identifiable.\n\nAlternatively, perhaps the model should include only the fairness term and the absolute term, allowing both parameters to vary. However, this could still lead to some covariance. To mitigate this, perhaps the model should include only one of these terms. For example, if we assume that participants care only about the fairness difference, then:\n\nUtility = gamma + alpha*(split_self - 3*token_self)\n\nOr if they care only about the absolute amount:\n\nUtility = gamma + beta*split_self\n\nBut combining both might better capture the decision-making process. However, with three parameters (gamma, alpha, beta), there's a risk of overfitting, but given 36 trials per participant, it's manageable. \n\nAnother consideration is the opponent's contribution. Maybe participants also consider how much the opponent contributed. For example, if the opponent contributed more, the participant might feel that the opponent deserves a larger share. But how to model that? The split_opp is the opponent's proposed share, but in the responder role, the participant is responding to the opponent's proposal. So split_opp is the amount the opponent is taking, and split_self is what's offered to the participant. \n\nAlternatively, the participant might compare their own split to the opponent's split. For example, the ratio of split_self to split_opp. But that could be another term. However, this complicates the model further. Let's stick to the initial approach for simplicity.\n\nNow, considering the parameters:\n\n- gamma: baseline utility. If all terms are zero, gamma determines the baseline tendency to accept. Bounds could be [-5, 5], as it's a bias term.\n\n- alpha: sensitivity to the fairness term (split_self - 3*token_self). This could be positive or negative. If positive, participants are more likely to accept when the offer exceeds their contribution-based fair share. Bounds could be [-5, 5].\n\n- beta: sensitivity to the absolute split_self. Expected to be positive, as higher offers should increase acceptance. Bounds [0, 5].\n\nBut including both alpha and beta might lead to covariance. For example, if split_self is highly correlated with (split_self - 3*token_self), then alpha and beta could be hard to distinguish. To check, let's see:\n\nsplit_self is part of both terms. Suppose in some trials, split_self is high when 3*token_self is also high. Then, the two terms could be correlated. However, in other trials, split_self might be high even if 3*token_self is low (e.g., the opponent is generous). So there might be enough variation to estimate both parameters. \n\nAlternatively, perhaps the model should use the difference term and the fair share term separately. Wait, the difference term is (split_self - fair_share), and the fair_share is 3*token_self. So the difference term is split_self - 3*token_self. The absolute term is split_self. So including both would be:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n= gamma + (alpha + beta)*split_self - alpha*3*token_self\n\nThis can be rewritten as:\n\nUtility = gamma + (alpha + beta)*split_self - 3*alpha*token_self\n\nBut this might make it hard to distinguish alpha and beta unless there's sufficient variation in token_self and split_self independently. For example, if token_self varies while split_self is fixed, then the coefficient for token_self would be -3*alpha, allowing estimation of alpha. Then, the coefficient for split_self is (alpha + beta), allowing estimation of beta once alpha is known. However, in practice, if split_self and token_self are correlated, this could lead to multicollinearity issues. \n\nAlternatively, perhaps the model should use the ratio of split_self to fair_share. For example, (split_self / fair_share) - 1. But that introduces division, which could complicate things. Alternatively, use the percentage split (splitperc_self) compared to the fair percentage (token_self / (token_self + token_opp)*100). But splitperc_self is already a variable. However, using splitperc_self might not account for the actual contribution, as the problem states that two equal split_self values don't represent the same fairness if combined_earning differs. Wait, but splitperc_self is the percentage of the combined_earning. So if combined_earning is higher, the same percentage means a higher absolute amount. But the problem says that split_self's fairness depends on the combined_earning. So perhaps the absolute amount is important, but the percentage relative to contribution is also a factor.\n\nThis is getting a bit complicated. Let's return to the initial model with three parameters: gamma, alpha, beta.\n\nAnother approach is to model the utility as a function of the offer's deviation from equality. For example, the equitable split is combined_earning / 2. So the difference split_self - (combined_earning / 2) could be a term. This represents how much more or less the participant is getting compared to an equal split. \n\nSo the utility could be:\n\nUtility = gamma + alpha*(split_self - combined_earning/2) + beta*split_self\n\nHere, alpha captures sensitivity to equity, and beta captures sensitivity to the absolute offer. This model also has three parameters. \n\nComparing the two approaches:\n\n1. Contribution-based fairness: split_self vs 3*token_self\n2. Equity-based fairness: split_self vs combined_earning/2\n\nWhich is more psychologically plausible? In ultimatum game studies, people often reject offers that are perceived as unfair, even if it means getting nothing. The fairness norm in such games is often around 40-50%, suggesting that equity (equal split) is a common reference. However, in this task, participants have contributed tokens, which might make contribution-based fairness more salient. \n\nThe problem statement mentions that trial_type is based on contributions (1=equal, 2=opponent more, 3=participant more), which suggests that the participants' contributions vary, and the model should account for that. Therefore, using the contribution-based fair share (3*token_self) might be more appropriate.\n\nBut to confirm, let's compute both fair shares:\n\n- Contribution-based fair share: 3*token_self\n- Equity-based fair share: combined_earning / 2 = (token_self + token_opp)*3 / 2\n\nThe difference between the two is that contribution-based depends only on the participant's tokens, while equity-based depends on both. \n\nIncluding both might be better, but that would add more parameters. Let's see:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*(split_self - (token_self + token_opp)*3/2) + delta*split_self\n\nBut this introduces four parameters, which might be too many. Alternatively, choose one fairness norm. \n\nGiven the task description emphasizes that the combined earning varies and that two equal split_self values aren't equally fair, the model must account for the context of contributions. Therefore, the contribution-based fair share seems essential. \n\nThus, proceeding with the initial model:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nThis model uses the difference between the offer and the contribution-based fair share, plus the absolute offer. The parameters alpha and beta capture the sensitivity to fairness and self-interest, respectively. Gamma is the baseline bias.\n\nNow, the probability of accepting is logistic(Utility). The model needs to ensure that when Utility < 0, the probability is <50%, which aligns with the requirement.\n\nRegarding parameter bounds:\n\n- gamma: Bounds could be [-5, 5], as it's a bias term. A wide range allows for strong biases in either direction.\n- alpha: Sensitivity to fairness. Could be positive (prefer offers above fair share) or negative (accept even if below fair share). Bounds [-5, 5].\n- beta: Sensitivity to absolute offer. Should be positive, as higher offers are better. Bounds [0, 5].\n\nThese bounds are generous but finite, aiding parameter recovery.\n\nNow, checking for identifiability. If alpha and beta are both positive, their effects on split_self are additive. For example, if split_self increases, both terms contribute positively. However, the term (split_self - 3*token_self) also depends on token_self. If token_self varies across trials, this can help distinguish alpha and beta. For instance, in trials where token_self is low, a given split_self might be much higher than 3*token_self, contributing more to utility. In trials where token_self is high, the same split_self might be closer to or below the fair share. This variation can help estimate alpha and beta separately.\n\nAnother consideration: the term (split_self - 3*token_self) can be rewritten as split_self - fair_share, where fair_share is 3*token_self. This term represents the surplus (or deficit) relative to the contribution-based fair share. Participants might be more likely to accept if this surplus is positive. \n\nIncluding both the surplus and the absolute split_self allows the model to capture participants who care about both getting a fair deal and maximizing their own earnings. For example, a participant might accept an unfair offer if the absolute amount is high enough (high beta), or reject a fair offer if the absolute amount is too low (low beta but high alpha).\n\nThis model should have three learnable parameters (gamma, alpha, beta) with the specified bounds. The variables used are split_self and token_self from the dataset, which are available when the participant is the responder (trial_role=1). \n\nNow, mapping the variables:\n\n- split_self: proposed share for the participant (data variable)\n- token_self: participant's tokens (data variable)\n- combined_earning is (token_self + token_opp)*3, but in the model, we use 3*token_self, which is derived from token_self.\n\nThe model doesn't use combined_earning directly, but 3*token_self is part of the calculation. However, since token_self is provided, the model can compute it.\n\nNow, the formal mathematical model would be:\n\nUtility = gamma + alpha*(split_self - 3*token_self) + beta*split_self\n\nThe probability of acceptance is 1 / (1 + exp(-Utility)).\n\nVariables:\n\n- gamma: learnable, baseline utility. Range [-5,5]\n- alpha: learnable, fairness sensitivity. Range [-5,5]\n- beta: learnable, absolute offer sensitivity. Range [0,5]\n- split_self: from data\n- token_self: from data\n\nAll variables except the learnable parameters are sourced from the data. The calculated term (split_self - 3*token_self) is part of the utility function.\n\nNow, writing the variable descriptions in JSON:\n\nFor gamma:\n- description: Baseline utility bias\n- range: min -5, max 5, inclusive both\n- learnable: true\n- source: learnable\n\nFor alpha:\n- description: Sensitivity to the difference between the proposed split and the contribution-based fair share (split_self - 3*token_self)\n- range: min -5, max 5, inclusive both\n- learnable: true\n- source: learnable\n\nFor beta:\n- description: Sensitivity to the absolute proposed split amount (split_self)\n- range: min 0, max 5, inclusive both\n- learnable: true\n- source: learnable\n\nsplit_self and token_self are data variables, so they are marked as source: data, learnable: false.\n\nThe utility variable (U) is calculated, so it's not learnable.\n\nPutting it all together:\n\n<MODEL>\nU = gamma + alpha * (split_self - 3 * token_self) + beta * split_self\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"gamma\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (split_self - 3*token_self)\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute proposed amount (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining baseline bias, sensitivity to deviation from contribution-based fairness, and sensitivity to absolute offer amount. The model predicts acceptance based on whether the utility exceeds zero, with parameters capturing individual differences in fairness considerations and self-interest.\n</SUMMARY>\n\n<target_variable>accept</target_variable>\n</think>\n\nI have recieved the test instructions\n\n<MODEL>\nU = gamma + alpha * (split_self - 3 * token_self) + beta * split_self\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"gamma\": {\n      \"description\": \"Baseline utility bias\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (split_self - 3*token_self)\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute proposed amount (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining baseline bias, sensitivity to deviation from contribution-based fairness, and sensitivity to absolute offer amount. The model predicts acceptance based on whether the utility exceeds zero, with parameters capturing individual differences in fairness considerations and self-interest.\n</SUMMARY>\n\n<target_variable>accept</target_variable>",
  "prediction_type": "utility",
  "current_model": "Specification: U = gamma + alpha * (split_self - 3 * token_self) + beta * split_self\nSummary: A utility model combining baseline bias, sensitivity to deviation from contribution-based fairness, and sensitivity to absolute offer amount. The model predicts acceptance based on whether the utility exceeds zero, with parameters capturing individual differences in fairness considerations and self-interest.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, gamma=0, alpha=0, beta=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        U = gamma + alpha * (split_self - 3 * token_self) + beta * split_self\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8150183150183151,
  "fitting_results": [
    {
      "gamma": -4.702415270265191,
      "alpha": 1.0668061107124736,
      "beta": 0.7614142101272756,
      "success": true,
      "log_likelihood": -5.290766357292302,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.22670919832970401,
      "alpha": 0.889197259798967,
      "beta": 0.5332580948884759,
      "success": true,
      "log_likelihood": -16.16940615733775,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.8644543922894241,
      "alpha": 1.86788624942902,
      "beta": 1.0872076888508257,
      "success": true,
      "log_likelihood": -9.88497287567915,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -2.4673306445091763,
      "alpha": 1.1470187999898365,
      "beta": 1.2562003653199116,
      "success": true,
      "log_likelihood": -12.169594408982716,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 2.575328877014693,
      "alpha": 2.203495967265181,
      "beta": 0.524380446926464,
      "success": true,
      "log_likelihood": -8.635340396459462,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 1.045603277797247,
      "alpha": 1.396491431523033,
      "beta": 1.0263581218698397,
      "success": true,
      "log_likelihood": -10.823233645675787,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": -5.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.495039493949563e-08,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 2.8893675976920745,
      "alpha": 1.6679757104729986,
      "beta": 0.8047735643070778,
      "success": true,
      "log_likelihood": -9.820149026179731,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": -5.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.4950394939495628e-08,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.04978012556126341,
      "alpha": 1.6061310600646257,
      "beta": 0.7395134300362818,
      "success": true,
      "log_likelihood": -11.09842045217265,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -1.8209669219355662,
      "alpha": 5.0,
      "beta": 3.4064057997876303,
      "success": true,
      "log_likelihood": -1.9794170943653286,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -4.924338716853085,
      "alpha": 0.9710020248047116,
      "beta": 0.9235006032287348,
      "success": true,
      "log_likelihood": -6.434409629702156,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 1.0382709497631684,
      "alpha": 2.251638220590177,
      "beta": 0.8844346176801455,
      "success": true,
      "log_likelihood": -8.60180383764124,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": -5.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.4950394939495628e-08,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": -5.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.4950394939495628e-08,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": -5.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.495039493949563e-08,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": -5.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.495039493949563e-08,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -5.0,
      "alpha": 1.2408012721973347,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.0971588902906013,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": 1.444024580907899,
      "beta": 0.9684221444848732,
      "success": true,
      "log_likelihood": -5.2749609122508785,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": -5.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.4950394939495628e-08,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 2.247429014164391,
      "alpha": 1.2217786304572809,
      "beta": 0.3069909004813432,
      "success": true,
      "log_likelihood": -13.927513269064981,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 3.4134114880237223,
      "alpha": 1.1781459157951009,
      "beta": 0.9344479879385449,
      "success": true,
      "log_likelihood": -7.02517285110226,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -3.303124166458623,
      "alpha": 0.4847713736171078,
      "beta": 1.0024611291360717,
      "success": true,
      "log_likelihood": -15.222858031526824,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -1.0075579954869067,
      "alpha": 0.6572151535430122,
      "beta": 0.7857783009707268,
      "success": true,
      "log_likelihood": -16.59483804295325,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 4.194584130861746,
      "alpha": 1.7763590573489652,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -10.386554193415211,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.1600944986352828,
      "alpha": 0.8701887367410102,
      "beta": 0.8063544874656094,
      "success": true,
      "log_likelihood": -14.842333128339515,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.7653355626974392,
      "alpha": 1.2979300511382696,
      "beta": 1.2401725561696977,
      "success": true,
      "log_likelihood": -11.47327964745404,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.3877201317518604,
      "alpha": 5.0,
      "beta": 1.3429267832992926,
      "success": true,
      "log_likelihood": -3.9220389042691357,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 4.418773158537869,
      "alpha": 0.9721902030572763,
      "beta": 0.3880903064266372,
      "success": true,
      "log_likelihood": -7.698150438283485,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 2.954965169995678,
      "alpha": 0.7412265911479131,
      "beta": 0.16952275545988665,
      "success": true,
      "log_likelihood": -14.119150339713642,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": -5.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.4950394939495634e-08,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -4.702274010107315,
      "alpha": 1.0668151618070552,
      "beta": 0.761388769665045,
      "success": true,
      "log_likelihood": -5.290766358758685,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -2.172353471468696,
      "alpha": 0.27378889121670075,
      "beta": 0.6787155159913119,
      "success": true,
      "log_likelihood": -19.253357050578277,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -1.1437906650696743,
      "alpha": 0.9109101179416955,
      "beta": 0.7103565458338078,
      "success": true,
      "log_likelihood": -14.892929145655057,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 5.0,
      "alpha": -5.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.495039493949564e-08,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -5.0,
      "alpha": 0.5698770809943016,
      "beta": 2.3747565268831528,
      "success": true,
      "log_likelihood": -9.471666568345288,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.10317486694244796,
      "alpha": 1.8344429686142396,
      "beta": 2.1022977984014806,
      "success": true,
      "log_likelihood": -7.007710699723572,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 1.764223312648023,
      "alpha": 1.3100821759085082,
      "beta": 0.3757282868742778,
      "success": true,
      "log_likelihood": -13.369226053338236,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 2.4557796099820677,
      "alpha": 0.8388054828549306,
      "beta": 0.17006501219468856,
      "success": true,
      "log_likelihood": -16.119960101720395,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 3.150949987774127,
      "alpha": 0.4993133232887456,
      "beta": 0.2865306317630541,
      "success": true,
      "log_likelihood": -8.525358822953677,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 2.833502445204417,
      "alpha": 1.0003560502266309,
      "beta": 0.08636199534016627,
      "success": true,
      "log_likelihood": -15.642986316931566,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -1.3743570484026555,
      "alpha": 1.052371415367287,
      "beta": 0.4616640127271461,
      "success": true,
      "log_likelihood": -11.564951828908256,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 4.442855431385797,
      "alpha": 0.2974587232859518,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -4.392356974190233,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.9743257930203152,
      "alpha": 0.7001322232132993,
      "beta": 0.8632167877850944,
      "success": true,
      "log_likelihood": -15.798544497072266,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.12176747212157817,
      "alpha": 0.9962446706517014,
      "beta": 0.9648698599091455,
      "success": true,
      "log_likelihood": -13.22821645658754,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -1.518348545571967,
      "alpha": 0.252490953874918,
      "beta": 1.0394624766405047,
      "success": true,
      "log_likelihood": -15.152597481382523,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.28927106629450605,
      "alpha": 0.9790768777946712,
      "beta": 0.6326740651150783,
      "success": true,
      "log_likelihood": -15.149795817516285,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -2.026219512930597,
      "alpha": 0.47797248400157133,
      "beta": 0.7134691164498378,
      "success": true,
      "log_likelihood": -17.49470923346962,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 4.152834223305782,
      "alpha": 2.2928346144083576,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -8.219775069396015,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -1.511489296156047,
      "alpha": 0.011055592603088317,
      "beta": 0.4720228030208065,
      "success": true,
      "log_likelihood": -22.681090231293282,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.20776791727096833,
      "alpha": 0.44575584988992667,
      "beta": 0.5629120883120403,
      "success": true,
      "log_likelihood": -17.272976428006057,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -3.079956046107241,
      "alpha": 0.4910268804551475,
      "beta": 0.5094533646383156,
      "success": true,
      "log_likelihood": -11.05310477379529,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -2.110542830960411,
      "alpha": 0.35571825643275423,
      "beta": 0.8999696899303,
      "success": true,
      "log_likelihood": -17.58705597419026,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -2.41188326833102,
      "alpha": -0.01776194637274197,
      "beta": 0.8071309861583694,
      "success": true,
      "log_likelihood": -19.769087439064034,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.8938924887710001,
      "alpha": 0.7207229205374577,
      "beta": 0.05291102762004887,
      "success": true,
      "log_likelihood": -18.584529262545914,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -2.3270667454651814,
      "alpha": 5.0,
      "beta": 2.608001979822674,
      "success": true,
      "log_likelihood": -3.9774558464794274,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.9094058191318044,
      "alpha": 1.5699577502271373,
      "beta": 0.661346417008363,
      "success": true,
      "log_likelihood": -10.038291954129402,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.23985555560628816,
      "alpha": -0.5446633634116832,
      "beta": 0.24617763520219155,
      "success": true,
      "log_likelihood": -11.136945884213224,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -4.336473020705765,
      "alpha": 0.8414237191502945,
      "beta": 0.7781563064215041,
      "success": true,
      "log_likelihood": -7.2215871134727205,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.9852820168048193,
      "alpha": 0.13574230461057774,
      "beta": 0.9482059956704327,
      "success": true,
      "log_likelihood": -13.844750346903917,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.5586103865978963,
      "alpha": 0.49233793771164736,
      "beta": 0.3409579959204685,
      "success": true,
      "log_likelihood": -19.56428036941027,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 1.7848374637196853,
      "alpha": 1.2473586465132365,
      "beta": 0.3282610225770505,
      "success": true,
      "log_likelihood": -13.858097591028,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -4.7024916870401094,
      "alpha": 1.0667829158126632,
      "beta": 0.7614199780638752,
      "success": true,
      "log_likelihood": -5.2907663594949375,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -2.15304497256911,
      "alpha": 0.2961379547190356,
      "beta": 0.7301199088049505,
      "success": true,
      "log_likelihood": -18.971720636508014,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -3.7978919341736974,
      "alpha": 1.3028785321959193,
      "beta": 1.1393164420972013,
      "success": true,
      "log_likelihood": -8.752611416587492,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -1.2551373970902822,
      "alpha": 0.8575086694244909,
      "beta": 1.0651397088355998,
      "success": true,
      "log_likelihood": -13.98013693642715,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 2.849566955410484,
      "alpha": 0.5732560263626186,
      "beta": 0.5166299655806863,
      "success": true,
      "log_likelihood": -7.689167092507902,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 3.2809498701381945,
      "alpha": 0.7375847132293657,
      "beta": 0.3187954411470365,
      "success": true,
      "log_likelihood": -10.36306766473482,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -4.0019940632496445,
      "alpha": 0.47384985296114845,
      "beta": 1.6427868061864717,
      "success": true,
      "log_likelihood": -12.51902879834921,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 3.351929641452739,
      "alpha": 0.32388202071270683,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -9.754721097779372,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -5.0,
      "alpha": 5.0,
      "beta": 1.185524330876946,
      "success": true,
      "log_likelihood": -0.45711366525709707,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -3.3799676503250002,
      "alpha": 0.5796326245280389,
      "beta": 0.600765654439789,
      "success": true,
      "log_likelihood": -10.23795929807563,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.44767013824146645,
      "alpha": 0.8562519813568398,
      "beta": 0.7609327818920474,
      "success": true,
      "log_likelihood": -15.479816616714743,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -1.1391945349838537,
      "alpha": 0.17161358014444167,
      "beta": 0.9643063094644174,
      "success": true,
      "log_likelihood": -14.523693379836375,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.70129549074101,
      "alpha": 0.20172973215792875,
      "beta": 0.33579488258527335,
      "success": true,
      "log_likelihood": -22.748613182503515,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 3.055731197982974,
      "alpha": 1.2790419995835174,
      "beta": 0.47215081924625246,
      "success": true,
      "log_likelihood": -11.823598339221782,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 4.757119589607003,
      "alpha": 0.6172217927929271,
      "beta": 0.07078347568020991,
      "success": true,
      "log_likelihood": -6.297198988845662,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -3.596475402050839,
      "alpha": 0.4048444994397105,
      "beta": 0.3409162647138504,
      "success": true,
      "log_likelihood": -6.61507415885711,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -1.2926933766313602,
      "alpha": 0.00754181474260166,
      "beta": 0.5202117018423844,
      "success": true,
      "log_likelihood": -21.682231411021135,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -4.702476092448389,
      "alpha": 1.0667997984494104,
      "beta": 0.7614289472238903,
      "success": true,
      "log_likelihood": -5.290766357446488,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.07815206987986938,
      "alpha": 0.27963388081526835,
      "beta": 0.2472454027775766,
      "success": true,
      "log_likelihood": -22.663456154232406,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 2.347741555204008,
      "alpha": 0.014578716088763969,
      "beta": 0.024282803262021695,
      "success": true,
      "log_likelihood": -10.321738257836888,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -3.3302117029192098,
      "alpha": 0.5401572284195338,
      "beta": 0.5033840095745282,
      "success": true,
      "log_likelihood": -9.360812183210117,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -5.0,
      "alpha": 1.7592851464190262,
      "beta": 2.660693872352144,
      "success": true,
      "log_likelihood": -5.06630316474302,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 1.0395338699826935,
      "alpha": 0.2338288452933405,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -23.201218078130015,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -2.654596389965334,
      "alpha": 1.6021084291724088,
      "beta": 0.4915500046133761,
      "success": true,
      "log_likelihood": -5.910497467441491,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 1.8536484757551255,
      "alpha": 0.6554877433359854,
      "beta": 0.2390082343170382,
      "success": true,
      "log_likelihood": -16.970271550059092,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -0.9635024786657439,
      "alpha": 0.7662637675187913,
      "beta": 0.2690346097580742,
      "success": true,
      "log_likelihood": -14.001591264208834,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 4.2009018895750865,
      "alpha": 0.7775751908439527,
      "beta": 0.5726718115875676,
      "success": true,
      "log_likelihood": -4.97897635169054,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": -5.0,
      "alpha": 0.8229204588360187,
      "beta": 1.9874305955210325,
      "success": true,
      "log_likelihood": -6.869580878926851,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "gamma": 0.7840186209086588,
      "alpha": 0.2561478813892487,
      "beta": 0.20797570164650106,
      "success": true,
      "log_likelihood": -20.562156097555572,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "gamma": {
        "r": 0.5553002107402086,
        "p": 2.021206999759008e-09
      },
      "alpha": {
        "r": 0.8703829714726034,
        "p": 6.444171534271911e-32
      },
      "beta": {
        "r": 0.7119797920956099,
        "p": 1.0041095437410693e-16
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 31.894283655015983,
  "bic_results": {
    "average_bic": 31.894283655015983,
    "individual_bics": [
      21.332089529952935,
      43.089369130043835,
      30.52050256672663,
      35.08974563333376,
      28.021237608287255,
      32.397024106719904,
      10.75055684526912,
      30.390854867727793,
      10.75055684526912,
      32.947397719713635,
      14.709391004098988,
      23.619376074772642,
      27.95416449065081,
      10.75055684526912,
      10.75055684526912,
      10.75055684526912,
      10.75055684526912,
      12.944874595949534,
      21.300478639870086,
      10.75055684526912,
      38.60558335349829,
      24.80090251757285,
      41.19627287842198,
      43.940232901274825,
      31.523665202198753,
      40.435223072047364,
      33.69711611027641,
      18.5946346239066,
      26.146857691935303,
      38.98885749479561,
      10.75055684526912,
      21.3320895328857,
      49.25727091652489,
      40.53641510667845,
      10.75055684526912,
      29.693889952058907,
      24.765978214815476,
      37.4890089220448,
      42.990477018809116,
      27.801274461275685,
      42.036529449231466,
      33.88046047318484,
      19.535270763748798,
      42.34764580951286,
      37.20698972854341,
      41.05575177813338,
      41.0501484504009,
      45.73997528230757,
      27.19010695416036,
      56.11273727795489,
      45.29650967138045,
      32.85676636295891,
      45.924668763748855,
      50.2887316934964,
      47.919615340460155,
      18.705468508327186,
      30.827140723627135,
      33.02444858379478,
      25.19373104231377,
      38.440057509176164,
      49.879117554188866,
      38.46675199742433,
      21.332089534358204,
      48.69399808838436,
      28.255779648543314,
      38.71083068822263,
      26.128891000384137,
      31.47669214483797,
      35.78861441206675,
      30.259999010927075,
      11.664784145882525,
      31.22647541151959,
      41.71019004879781,
      39.79794357504108,
      56.247783180375365,
      34.39775349381189,
      23.344954793059657,
      23.98070513308255,
      54.1150196374106,
      21.332089530261307,
      56.07746912383314,
      31.394033331042106,
      29.472181181788564,
      20.88316314485437,
      57.152992971628365,
      22.571551750251313,
      44.69109991548652,
      38.753739343785995,
      20.70850951874941,
      24.489718573222035,
      51.87486901047947
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 3,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- gamma: r = 0.555\n- alpha: r = 0.870\n- beta: r = 0.712",
  "previous_models": [
    "Specification: U = gamma + alpha * (split_self - 3 * token_self) + beta * split_self\nSummary: A utility model combining baseline bias, sensitivity to deviation from contribution-based fairness, and sensitivity to absolute offer amount. The model predicts acceptance based on whether the utility exceeds zero, with parameters capturing individual differences in fairness considerations and self-interest.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 31.894283655015983\nOverall Accuracy: 0.8150\n\nParameter Recovery:\n- gamma: r = 0.555\n- alpha: r = 0.870\n- beta: r = 0.712"
  ]
}