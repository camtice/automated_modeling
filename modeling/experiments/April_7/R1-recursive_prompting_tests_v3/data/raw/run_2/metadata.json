{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "```text\n<think>\nThe previous model achieved a decent accuracy (0.729), but the BIC (35.73) is relatively high, and the parameter recovery for beta is quite poor (r = 0.167). This suggests the model might be oversimplified or that the parameterization is not ideal, leading to identifiability issues and poor parameter recovery. The correlation between parameters is also likely high given the low parameter recovery.\n\nHere's how to improve the instructions:\n\n1.  **Emphasize more complex, interaction-based models:** Encourage the model to explore non-linear relationships between variables and consider interaction effects. The previous model was a simple linear combination, which may not fully capture the underlying psychological processes. It may be useful to suggest the idea of diminishing returns to absolute reward and also diminishing marginal impact of fairness after some threshold.\n2.  **Encourage a wider search for relevant variables:** The current model only uses 'split_self', 'token_self', and 'token_opp'. It might be beneficial to encourage exploration of other variables like 'combined_earning' or 'trial_type' to see if they improve model fit and parameter recovery.\n3.  **Address the parameter recovery issue directly:** Explain what poor parameter recovery means (model isn't identifiable) and suggest strategies to improve it.\n4. **Provide a more concrete example of reasoning:** Showing a good and bad example would allow the model to generate better responses.\n5. **Suggest thinking of other utility terms**: Mentioning social comparison, loss aversion, or fairness models might kickstart creativity.\n</think>\nBe VERY considerate of high covariance between learnable parameters in the model. Aim for an appropriate number of learnable parameters, with a slight bias for fewer parameters to avoid overfitting. Poor parameter recovery indicates that the model is not identifiable. You should aim for parameter recovery r values above 0.7 if possible. If parameter recovery is not possible, consider removing the parameter from your model to improve BIC, and focus on other terms.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved, including potential non-linear relationships or interaction effects (e.g., diminishing returns).\n* How these mechanisms interact, especially regarding fairness, loss aversion, and social comparison.\n* What mathematical form could capture these interactions. Consider functions beyond simple linear combinations; explore multiplicative or exponential terms.\n* What parameters would be needed to calibrate the model, focusing on parameters that are identifiable and have a clear psychological interpretation.\n* How the model variables map to observable behavior. Consider how different variables might have interacting effects on the ultimate decision to accept or reject.\n\nConsider incorporating insights from established utility theories, such as prospect theory, inequity aversion, or other social preference models. Can other variables, such as 'combined_earning' or 'trial_type', explain significant variance in responder behavior? Consider models beyond simple linear combinations. Perhaps the utility from 'split_self' exhibits diminishing returns, or the impact of fairness considerations is non-linear. Explore interaction effects between variables; for example, does the influence of 'token_self' on utility depend on the 'combined_earning'? Consider including other utility terms such as social comparison, loss aversion, or fairness models. The most obvious models are not often the best.\n\nExample of a good vs bad reasoning process:\n\nBAD:\n\"The key psychological mechanism is fairness. People want an even split.\"\nThis is far too simplistic.\n\nGOOD:\n\"The key psychological mechanisms in the Ultimatum Game involve both self-interest and fairness considerations. Responders evaluate offers based on the absolute amount offered ('split_self') and relative to their contribution ('token_self' vs 'token_opp'). However, the impact of fairness might diminish as the offer increases; a small unfairness might be tolerated for a large reward. Also, responders may be sensitive to the overall pot size ('combined_earning'); an unfair split from a large pot might be more acceptable than the same unfair split from a small pot. Loss aversion may also play a role, where responders are more sensitive to the potential loss of rejecting an offer than the gain of accepting it. Social comparison might also play a role, where the participant compares their split to the opponents split.\"\nThis is a much more nuanced and complete perspective to build a model from.\n```",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: THIS IS A TEST TO SEE IF THESE INSTRUCTIONS ARE ACTUALLY BEING DISPLAYED WITHIN THE PIPELINE AND GIVEN TO THE MODEL. IF THIS STRING OF TEXT IS WITHIN YOUR INSTRUCTIONS, PLESAE START YOUR RESPONSE WITH 'I have recieved the test instructions\n\n```text\n<think>\nThe previous model achieved a decent accuracy (0.729), but the BIC (35.73) is relatively high, and the parameter recovery for beta is quite poor (r = 0.167). This suggests the model might be oversimplified or that the parameterization is not ideal, leading to identifiability issues and poor parameter recovery. The correlation between parameters is also likely high given the low parameter recovery.\n\nHere's how to improve the instructions:\n\n1.  **Emphasize more complex, interaction-based models:** Encourage the model to explore non-linear relationships between variables and consider interaction effects. The previous model was a simple linear combination, which may not fully capture the underlying psychological processes. It may be useful to suggest the idea of diminishing returns to absolute reward and also diminishing marginal impact of fairness after some threshold.\n2.  **Encourage a wider search for relevant variables:** The current model only uses 'split_self', 'token_self', and 'token_opp'. It might be beneficial to encourage exploration of other variables like 'combined_earning' or 'trial_type' to see if they improve model fit and parameter recovery.\n3.  **Address the parameter recovery issue directly:** Explain what poor parameter recovery means (model isn't identifiable) and suggest strategies to improve it.\n4. **Provide a more concrete example of reasoning:** Showing a good and bad example would allow the model to generate better responses.\n5. **Suggest thinking of other utility terms**: Mentioning social comparison, loss aversion, or fairness models might kickstart creativity.\n</think>\nBe VERY considerate of high covariance between learnable parameters in the model. Aim for an appropriate number of learnable parameters, with a slight bias for fewer parameters to avoid overfitting. Poor parameter recovery indicates that the model is not identifiable. You should aim for parameter recovery r values above 0.7 if possible. If parameter recovery is not possible, consider removing the parameter from your model to improve BIC, and focus on other terms.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved, including potential non-linear relationships or interaction effects (e.g., diminishing returns).\n* How these mechanisms interact, especially regarding fairness, loss aversion, and social comparison.\n* What mathematical form could capture these interactions. Consider functions beyond simple linear combinations; explore multiplicative or exponential terms.\n* What parameters would be needed to calibrate the model, focusing on parameters that are identifiable and have a clear psychological interpretation.\n* How the model variables map to observable behavior. Consider how different variables might have interacting effects on the ultimate decision to accept or reject.\n\nConsider incorporating insights from established utility theories, such as prospect theory, inequity aversion, or other social preference models. Can other variables, such as 'combined_earning' or 'trial_type', explain significant variance in responder behavior? Consider models beyond simple linear combinations. Perhaps the utility from 'split_self' exhibits diminishing returns, or the impact of fairness considerations is non-linear. Explore interaction effects between variables; for example, does the influence of 'token_self' on utility depend on the 'combined_earning'? Consider including other utility terms such as social comparison, loss aversion, or fairness models. The most obvious models are not often the best.\n\nExample of a good vs bad reasoning process:\n\nBAD:\n\"The key psychological mechanism is fairness. People want an even split.\"\nThis is far too simplistic.\n\nGOOD:\n\"The key psychological mechanisms in the Ultimatum Game involve both self-interest and fairness considerations. Responders evaluate offers based on the absolute amount offered ('split_self') and relative to their contribution ('token_self' vs 'token_opp'). However, the impact of fairness might diminish as the offer increases; a small unfairness might be tolerated for a large reward. Also, responders may be sensitive to the overall pot size ('combined_earning'); an unfair split from a large pot might be more acceptable than the same unfair split from a small pot. Loss aversion may also play a role, where responders are more sensitive to the potential loss of rejecting an offer than the gain of accepting it. Social comparison might also play a role, where the participant compares their split to the opponents split.\"\nThis is a much more nuanced and complete perspective to build a model from.\n```\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "I have recieved the test instructions\nOkay, let's think this through step by step to design a computational model for responder behavior in this modified Ultimatum Game.\n\n**1. Psychological Mechanisms:**\n\n*   **Self-Interest:** Responders want to maximize their monetary gain. The 'split_self' variable directly reflects this. However, the utility of money might exhibit diminishing returns, meaning the difference between \u00a31 and \u00a32 is greater than between \u00a3100 and \u00a3101.\n*   **Fairness:** Responders are sensitive to the fairness of the offer, considering their contribution ('token_self') relative to the opponent's ('token_opp') and the proposed split ('split_self' vs 'split_opp'). Perceived unfairness can lead to rejection, even if it means sacrificing potential gains. The perceived fairness might also depend on the overall earnings; a smaller split might be acceptable if the overall pie ('combined_earning') is large enough. People might also be more sensitive to being treated worse than their opponent.\n*   **Loss Aversion:** Responders might be more sensitive to the potential loss of rejecting a favorable offer than the equivalent gain of accepting it. This can lead to accepting offers that are slightly unfair, particularly when the stakes are high.\n*   **Social Comparison:** Responders may compare their share to the proposer's share. Strong inequity aversion may reduce the utility of unequal splits.\n*   **Trial Type Sensitivity:** The framing of the interaction changes with each 'trial_type'. When the trial type is 'opponent more' or 'participant more', there is an explicit imbalance in contribution. A responder might be more lenient to accepting a 'less fair' split when the 'trial_type' shows the opponent contributed more.\n\n**2. Interaction of Mechanisms:**\n\nThese mechanisms interact in complex ways. For example, the impact of fairness might be modulated by the overall size of the pie ('combined_earning'). A small unfairness might be tolerated when the overall reward is high. Loss aversion could amplify the negative impact of perceived unfairness, leading to more frequent rejections. The reference point from which loss is calculated depends on the contributions of the parties and is likely context dependent.\n\n**3. Mathematical Form:**\n\nTo capture these interactions, the model should incorporate:\n\n*   A term representing the utility of the monetary offer, potentially with diminishing returns (e.g., a logarithmic or power function).\n*   A fairness term that compares the responder's share to the opponent's share and their relative contributions. This term could be weighted by a parameter reflecting the responder's sensitivity to fairness. The difference in contribution can also be weighted relative to the total earning.\n*   Consider the inclusion of interaction terms to model how the effect of fairness changes with earnings.\n*   A consideration of the trial type as a modifier to the fairness penalty.\n*   A loss aversion component that penalizes outcomes where the responder feels they are receiving less than their \"fair share.\"\n\n**4. Parameters:**\n\nThe model will need the following parameters:\n\n*   `alpha`: Sensitivity to the absolute monetary amount (split_self). Bounded between 0 and 5.\n*   `fairness_sensitivity`: Sensitivity to fairness considerations. This parameter weighs the relative importance of fairness versus self-interest. Bounded between 0 and 5.\n*    `inequity_aversion`: Sensitivity to disadvantageous inequality. This parameter will penalize getting less than the opponent. Bounded between 0 and 5.\n*   `loss_aversion`: Weighting of the loss aversion component. Bounded between 0 and 5.\n*   `trial_type_effect`: This parameter will adjust for the trial type penalty or bonus on fairness. Bounded between -2 and 2.\n\n**5. Mapping to Observable Behavior:**\n\nThe model will predict the probability of accepting an offer based on the calculated utility. A logistic function can be used to map the utility to a probability, with a temperature parameter controlling the stochasticity of the choice.\n\nWith these things in mind, here is the model:\n\n```\n<MODEL>\nU = alpha * log(split_self + 1) - fairness_sensitivity * (split_opp - split_self) / (combined_earning + 1) * trial_type_effect - inequity_aversion * max(0, split_opp - split_self) / (combined_earning + 1) - loss_aversion * max(0, (token_self / (token_self + token_opp)) * combined_earning - split_self) / (combined_earning + 1)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to the absolute monetary amount (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"The proposed share for the participant (in \u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"fairness_sensitivity\": {\n      \"description\": \"Sensitivity to fairness considerations\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_opp\": {\n      \"description\": \"The proposed share of the opponent (in \u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Amount of money earned in total\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"trial_type_effect\": {\n      \"description\": \"Adjustment based on trial type (1 = equal, 2 = opponent more, 3 = participant more). Penalty for opponent more, bonus for participant more\",\n      \"range\": {\n        \"min\": -2,\n        \"max\": 2,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n        \"inequity_aversion\": {\n      \"description\": \"Sensitivity to disadvantageous inequality.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"loss_aversion\": {\n      \"description\": \"Weighting of the loss aversion component\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n<target_variable>accept</target_variable>\n<SUMMARY>\nThis model predicts responder behavior in a modified Ultimatum Game, considering self-interest (absolute monetary amount), fairness (relative split and contributions), inequity aversion, loss aversion and the effect of trial type. The model calculates a utility score based on these factors and predicts the probability of acceptance using a logistic function. The model incorporates diminishing returns for monetary gains using a logarithmic function. It also considers relative contributions and the impact of trial framing on fairness perceptions. Finally, loss aversion is incorporated by penalizing outcomes where the responder receives less than their proportional share of the total earnings. All fairness and loss aversion terms are scaled based on combined earnings, to reduce the impacts of outliers and large combined earnings.\n</SUMMARY>\n```"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'fairness_sensitivity', 'trial_type_effect', 'inequity_aversion', 'loss_aversion']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = alpha * log(split_self + 1) - fairness_sensitivity * (split_opp - split_self) / (combined_earning + 1) * trial_type_effect - inequity_aversion * max(0, split_opp - split_self) / (combined_earning + 1) - loss_aversion * max(0, (token_self / (token_self + token_opp)) * combined_earning - split_self) / (combined_earning + 1)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to the absolute monetary amount (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"The proposed share for the participant (in \\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"fairness_sensitivity\": {\n      \"description\": \"Sensitivity to fairness considerations\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_opp\": {\n      \"description\": \"The proposed share of the opponent (in \\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Amount of money earned in total\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"trial_type_effect\": {\n      \"description\": \"Adjustment based on trial type (1 = equal, 2 = opponent more, 3 = participant more). Penalty for opponent more, bonus for participant more\",\n      \"range\": {\n        \"min\": -2,\n        \"max\": 2,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"inequity_aversion\": {\n      \"description\": \"Sensitivity to disadvantageous inequality.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"loss_aversion\": {\n      \"description\": \"Weighting of the loss aversion component\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, fairness_sensitivity=1, trial_type_effect=1, inequity_aversion=1, loss_aversion=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 1)  # Add 1 to avoid division by zero\n        trial_type = trial.get(\"trial_type\", 1)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        U = alpha * math.log(split_self + 1) - fairness_sensitivity * (split_opp - split_self) / (combined_earning + 1) * trial_type_effect - inequity_aversion * max(0, split_opp - split_self) / (combined_earning + 1) - loss_aversion * max(0, (token_self / (token_self + token_opp + 1e-6)) * combined_earning - split_self) / (combined_earning + 1)\n\n        model_predictions.append(U)\n    return model_predictions\n```",
      "timestamp": 1744026824.878674
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to the absolute monetary amount (split_self)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "fairness_sensitivity": {
      "description": "Sensitivity to fairness considerations",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "trial_type_effect": {
      "description": "Adjustment based on trial type (1 = equal, 2 = opponent more, 3 = participant more). Penalty for opponent more, bonus for participant more",
      "range": {
        "min": -2,
        "max": 2,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "inequity_aversion": {
      "description": "Sensitivity to disadvantageous inequality.",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "loss_aversion": {
      "description": "Weighting of the loss aversion component",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = alpha * log(split_self + 1) - fairness_sensitivity * (split_opp - split_self) / (combined_earning + 1) * trial_type_effect - inequity_aversion * max(0, split_opp - split_self) / (combined_earning + 1) - loss_aversion * max(0, (token_self / (token_self + token_opp)) * combined_earning - split_self) / (combined_earning + 1)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the offer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha": {
      "description": "Sensitivity to the absolute monetary amount (split_self)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "The proposed share for the participant (in \u00a3)",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "fairness_sensitivity": {
      "description": "Sensitivity to fairness considerations",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_opp": {
      "description": "The proposed share of the opponent (in \u00a3)",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "combined_earning": {
      "description": "Amount of money earned in total",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "trial_type_effect": {
      "description": "Adjustment based on trial type (1 = equal, 2 = opponent more, 3 = participant more). Penalty for opponent more, bonus for participant more",
      "range": {
        "min": -2,
        "max": 2,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "inequity_aversion": {
      "description": "Sensitivity to disadvantageous inequality.",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "loss_aversion": {
      "description": "Weighting of the loss aversion component",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "This model predicts responder behavior in a modified Ultimatum Game, considering self-interest (absolute monetary amount), fairness (relative split and contributions), inequity aversion, loss aversion and the effect of trial type. The model calculates a utility score based on these factors and predicts the probability of acceptance using a logistic function. The model incorporates diminishing returns for monetary gains using a logarithmic function. It also considers relative contributions and the impact of trial framing on fairness perceptions. Finally, loss aversion is incorporated by penalizing outcomes where the responder receives less than their proportional share of the total earnings. All fairness and loss aversion terms are scaled based on combined earnings, to reduce the impacts of outliers and large combined earnings.",
  "target_variable": "accept",
  "full_reasoning": "I have recieved the test instructions\nOkay, let's think this through step by step to design a computational model for responder behavior in this modified Ultimatum Game.\n\n**1. Psychological Mechanisms:**\n\n*   **Self-Interest:** Responders want to maximize their monetary gain. The 'split_self' variable directly reflects this. However, the utility of money might exhibit diminishing returns, meaning the difference between \u00a31 and \u00a32 is greater than between \u00a3100 and \u00a3101.\n*   **Fairness:** Responders are sensitive to the fairness of the offer, considering their contribution ('token_self') relative to the opponent's ('token_opp') and the proposed split ('split_self' vs 'split_opp'). Perceived unfairness can lead to rejection, even if it means sacrificing potential gains. The perceived fairness might also depend on the overall earnings; a smaller split might be acceptable if the overall pie ('combined_earning') is large enough. People might also be more sensitive to being treated worse than their opponent.\n*   **Loss Aversion:** Responders might be more sensitive to the potential loss of rejecting a favorable offer than the equivalent gain of accepting it. This can lead to accepting offers that are slightly unfair, particularly when the stakes are high.\n*   **Social Comparison:** Responders may compare their share to the proposer's share. Strong inequity aversion may reduce the utility of unequal splits.\n*   **Trial Type Sensitivity:** The framing of the interaction changes with each 'trial_type'. When the trial type is 'opponent more' or 'participant more', there is an explicit imbalance in contribution. A responder might be more lenient to accepting a 'less fair' split when the 'trial_type' shows the opponent contributed more.\n\n**2. Interaction of Mechanisms:**\n\nThese mechanisms interact in complex ways. For example, the impact of fairness might be modulated by the overall size of the pie ('combined_earning'). A small unfairness might be tolerated when the overall reward is high. Loss aversion could amplify the negative impact of perceived unfairness, leading to more frequent rejections. The reference point from which loss is calculated depends on the contributions of the parties and is likely context dependent.\n\n**3. Mathematical Form:**\n\nTo capture these interactions, the model should incorporate:\n\n*   A term representing the utility of the monetary offer, potentially with diminishing returns (e.g., a logarithmic or power function).\n*   A fairness term that compares the responder's share to the opponent's share and their relative contributions. This term could be weighted by a parameter reflecting the responder's sensitivity to fairness. The difference in contribution can also be weighted relative to the total earning.\n*   Consider the inclusion of interaction terms to model how the effect of fairness changes with earnings.\n*   A consideration of the trial type as a modifier to the fairness penalty.\n*   A loss aversion component that penalizes outcomes where the responder feels they are receiving less than their \"fair share.\"\n\n**4. Parameters:**\n\nThe model will need the following parameters:\n\n*   `alpha`: Sensitivity to the absolute monetary amount (split_self). Bounded between 0 and 5.\n*   `fairness_sensitivity`: Sensitivity to fairness considerations. This parameter weighs the relative importance of fairness versus self-interest. Bounded between 0 and 5.\n*    `inequity_aversion`: Sensitivity to disadvantageous inequality. This parameter will penalize getting less than the opponent. Bounded between 0 and 5.\n*   `loss_aversion`: Weighting of the loss aversion component. Bounded between 0 and 5.\n*   `trial_type_effect`: This parameter will adjust for the trial type penalty or bonus on fairness. Bounded between -2 and 2.\n\n**5. Mapping to Observable Behavior:**\n\nThe model will predict the probability of accepting an offer based on the calculated utility. A logistic function can be used to map the utility to a probability, with a temperature parameter controlling the stochasticity of the choice.\n\nWith these things in mind, here is the model:\n\n```\n<MODEL>\nU = alpha * log(split_self + 1) - fairness_sensitivity * (split_opp - split_self) / (combined_earning + 1) * trial_type_effect - inequity_aversion * max(0, split_opp - split_self) / (combined_earning + 1) - loss_aversion * max(0, (token_self / (token_self + token_opp)) * combined_earning - split_self) / (combined_earning + 1)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to the absolute monetary amount (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"The proposed share for the participant (in \u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"fairness_sensitivity\": {\n      \"description\": \"Sensitivity to fairness considerations\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_opp\": {\n      \"description\": \"The proposed share of the opponent (in \u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Amount of money earned in total\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"trial_type_effect\": {\n      \"description\": \"Adjustment based on trial type (1 = equal, 2 = opponent more, 3 = participant more). Penalty for opponent more, bonus for participant more\",\n      \"range\": {\n        \"min\": -2,\n        \"max\": 2,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n        \"inequity_aversion\": {\n      \"description\": \"Sensitivity to disadvantageous inequality.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"loss_aversion\": {\n      \"description\": \"Weighting of the loss aversion component\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n<target_variable>accept</target_variable>\n<SUMMARY>\nThis model predicts responder behavior in a modified Ultimatum Game, considering self-interest (absolute monetary amount), fairness (relative split and contributions), inequity aversion, loss aversion and the effect of trial type. The model calculates a utility score based on these factors and predicts the probability of acceptance using a logistic function. The model incorporates diminishing returns for monetary gains using a logarithmic function. It also considers relative contributions and the impact of trial framing on fairness perceptions. Finally, loss aversion is incorporated by penalizing outcomes where the responder receives less than their proportional share of the total earnings. All fairness and loss aversion terms are scaled based on combined earnings, to reduce the impacts of outliers and large combined earnings.\n</SUMMARY>\n```",
  "prediction_type": "utility",
  "current_model": "Specification: U = alpha * log(split_self + 1) - fairness_sensitivity * (split_opp - split_self) / (combined_earning + 1) * trial_type_effect - inequity_aversion * max(0, split_opp - split_self) / (combined_earning + 1) - loss_aversion * max(0, (token_self / (token_self + token_opp)) * combined_earning - split_self) / (combined_earning + 1)\nSummary: This model predicts responder behavior in a modified Ultimatum Game, considering self-interest (absolute monetary amount), fairness (relative split and contributions), inequity aversion, loss aversion and the effect of trial type. The model calculates a utility score based on these factors and predicts the probability of acceptance using a logistic function. The model incorporates diminishing returns for monetary gains using a logarithmic function. It also considers relative contributions and the impact of trial framing on fairness perceptions. Finally, loss aversion is incorporated by penalizing outcomes where the responder receives less than their proportional share of the total earnings. All fairness and loss aversion terms are scaled based on combined earnings, to reduce the impacts of outliers and large combined earnings.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1, fairness_sensitivity=1, trial_type_effect=1, inequity_aversion=1, loss_aversion=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 1)  # Add 1 to avoid division by zero\n        trial_type = trial.get(\"trial_type\", 1)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        U = alpha * math.log(split_self + 1) - fairness_sensitivity * (split_opp - split_self) / (combined_earning + 1) * trial_type_effect - inequity_aversion * max(0, split_opp - split_self) / (combined_earning + 1) - loss_aversion * max(0, (token_self / (token_self + token_opp + 1e-6)) * combined_earning - split_self) / (combined_earning + 1)\n\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.7942612942612942,
  "fitting_results": [
    {
      "alpha": 0.6764061483600723,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -1.9180671676110554,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4756141827387255,
      "fairness_sensitivity": 0.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 3.563268153088757,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -17.05461900304407,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5568263296151326,
      "fairness_sensitivity": 9.482471474852388e-06,
      "trial_type_effect": 0.8379371518330506,
      "inequity_aversion": 3.3118472850064227,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -14.63314489755458,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9332661138564364,
      "fairness_sensitivity": 4.865149195316592,
      "trial_type_effect": 0.8734734452621195,
      "inequity_aversion": 2.729849225091766,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -13.396454319064626,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9395953972384902,
      "fairness_sensitivity": 0.14338981261347197,
      "trial_type_effect": -0.38522540102908676,
      "inequity_aversion": 2.188564337442275,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -15.925359100746737,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.6441937816584287,
      "fairness_sensitivity": 1.0121509369804207,
      "trial_type_effect": 0.9845250960609392,
      "inequity_aversion": 4.531374331610799,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -13.32489005744691,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -0.0016979545621920017,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.3378626695598963,
      "fairness_sensitivity": 3.717942481469985,
      "trial_type_effect": 0.014910147166952447,
      "inequity_aversion": 3.945522829706416,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -14.05168413364384,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -0.0016979545621920017,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.186618896805502,
      "fairness_sensitivity": 0.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 4.249384605897421,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -14.76788676325854,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9318986748046407,
      "fairness_sensitivity": 0.8000276896524637,
      "trial_type_effect": 1.976785270070214,
      "inequity_aversion": 4.966489695843734,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -11.853020857973474,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0752204501272171,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -4.756376402121809,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5391103663642973,
      "fairness_sensitivity": 1.848712498581085,
      "trial_type_effect": 1.655438066790268,
      "inequity_aversion": 2.4252475922728163,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -13.880032279902116,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -0.0016979545621920017,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -0.0016979545621920013,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -0.0016979545621920017,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -0.0016979545621920017,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 4.9343001981664845,
      "trial_type_effect": 1.0915738145320588,
      "inequity_aversion": 0.8334987697227894,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -2.9054914922551753,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.564457282805253,
      "fairness_sensitivity": 0.0,
      "trial_type_effect": 0.527826409939282,
      "inequity_aversion": 2.374390786604084,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -7.150805747591832,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -0.0016979545621920015,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4108570123323874,
      "fairness_sensitivity": 0.14586926201991424,
      "trial_type_effect": 0.6579305755252876,
      "inequity_aversion": 2.4116156616333866,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -16.84552851896803,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.384177730848442,
      "fairness_sensitivity": 0.0025044146126475026,
      "trial_type_effect": -1.407191462289007,
      "inequity_aversion": 2.749374463420152,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -8.882255262094729,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.6657298149880884,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -9.802439446902758,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.1801602441865664,
      "fairness_sensitivity": 1.3807724068564307,
      "trial_type_effect": 1.426963452994052,
      "inequity_aversion": 4.069877121229419,
      "loss_aversion": 4.529345987970799,
      "success": true,
      "log_likelihood": -15.202893787778821,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8716848204206713,
      "fairness_sensitivity": 2.654898554980595,
      "trial_type_effect": -0.06402734460664118,
      "inequity_aversion": 1.3347140181762847,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -17.955086921744957,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9460234079729435,
      "fairness_sensitivity": 0.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 3.8482530980796588,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -14.716017214327476,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.123964111902747,
      "fairness_sensitivity": 4.929083115618493,
      "trial_type_effect": 1.9856212141651846,
      "inequity_aversion": 3.2581226170703395,
      "loss_aversion": 4.259056437259183,
      "success": true,
      "log_likelihood": -9.524253161063376,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2688573268179555,
      "fairness_sensitivity": 2.7683684837732256,
      "trial_type_effect": 1.0818855626419732,
      "inequity_aversion": 4.438247246187066,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -10.757302823772225,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.3947176589730144,
      "fairness_sensitivity": 2.5811147242639216,
      "trial_type_effect": -0.5682415294006429,
      "inequity_aversion": 1.3108947933132067,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -9.673673411349622,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.653720605668811,
      "fairness_sensitivity": 2.931387448986957,
      "trial_type_effect": -0.8181280776523944,
      "inequity_aversion": 2.4161530411735823,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -15.6833068755723,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -0.0016979545621920017,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6764061425641527,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -1.9180671676110548,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9966357722100703,
      "fairness_sensitivity": 4.224055339715949,
      "trial_type_effect": 1.2837929357954594,
      "inequity_aversion": 4.58611032917909,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -14.074161180743785,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8368609981401984,
      "fairness_sensitivity": 1.6895204562219994,
      "trial_type_effect": 1.723261087116715,
      "inequity_aversion": 4.589136780586417,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -13.519135236495712,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -0.0016979545621920015,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 0.37233398651931476,
      "success": true,
      "log_likelihood": -7.218120781354502,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.595800654605445,
      "fairness_sensitivity": 2.5431318690210403,
      "trial_type_effect": 0.9863059030091318,
      "inequity_aversion": 4.673459826242265,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -9.91158850384592,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1976621654833675,
      "fairness_sensitivity": 0.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 2.4434063958971763,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -17.27990442900735,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4493762527151026,
      "fairness_sensitivity": 0.001321310256664318,
      "trial_type_effect": -1.2777574832506828,
      "inequity_aversion": 1.0144426244144251,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -18.271411851329034,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.5566533786068852,
      "fairness_sensitivity": 2.202427147496326,
      "trial_type_effect": 0.140081707207367,
      "inequity_aversion": 0.873846610460948,
      "loss_aversion": 2.030098901569117,
      "success": true,
      "log_likelihood": -9.057493941944612,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3801421852223588,
      "fairness_sensitivity": 1.8053910222687926,
      "trial_type_effect": -1.345614019465107,
      "inequity_aversion": 4.311553674449065,
      "loss_aversion": 4.639430781249595,
      "success": true,
      "log_likelihood": -19.48882237381943,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6193332650894199,
      "fairness_sensitivity": 0.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 4.848482372325283,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -12.787950045266422,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4064804670982631,
      "fairness_sensitivity": 2.900577715810673,
      "trial_type_effect": -1.6794597486279719,
      "inequity_aversion": 1.1163501642136264,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -5.334518923927018,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.899575313520749,
      "fairness_sensitivity": 3.682977056439767,
      "trial_type_effect": 1.6238384750621886,
      "inequity_aversion": 4.643110998848161,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -12.780821920309112,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.5819548288782763,
      "fairness_sensitivity": 2.0836439531853768,
      "trial_type_effect": 0.6407399491475463,
      "inequity_aversion": 4.404004581979301,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -13.493938538613008,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.179565692447089,
      "fairness_sensitivity": 2.06863348581211,
      "trial_type_effect": -0.29335173368276596,
      "inequity_aversion": 4.949064730916512,
      "loss_aversion": 0.8910472424490395,
      "success": true,
      "log_likelihood": -15.410934972916529,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.40386624444794,
      "fairness_sensitivity": 2.528276495611632,
      "trial_type_effect": 1.8833025694900105,
      "inequity_aversion": 3.8430728046961207,
      "loss_aversion": 1.7905454649643975,
      "success": true,
      "log_likelihood": -14.156158710438538,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2067530345923672,
      "fairness_sensitivity": 3.604739701585251,
      "trial_type_effect": 0.013304495944250727,
      "inequity_aversion": 4.266406860707211,
      "loss_aversion": 4.701923212086159,
      "success": true,
      "log_likelihood": -17.57806311549325,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6508694228360762,
      "fairness_sensitivity": 3.9461718781349155,
      "trial_type_effect": -0.6361644085709615,
      "inequity_aversion": 3.9140944226134753,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -17.267949942289754,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8684834841324252,
      "fairness_sensitivity": 1.5379876436156568,
      "trial_type_effect": 0.032318700001360424,
      "inequity_aversion": 3.0467302695248684,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -22.222798198024968,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6458486543421071,
      "fairness_sensitivity": 0.2432567369387764,
      "trial_type_effect": 0.47776489912015213,
      "inequity_aversion": 3.032041429215808,
      "loss_aversion": 1.2321496703854915,
      "success": true,
      "log_likelihood": -18.12844089094931,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.023769130069445,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -6.586068682723983,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.437579335770432,
      "fairness_sensitivity": 4.819726071132079,
      "trial_type_effect": 1.1245279683522769,
      "inequity_aversion": 3.84184616149392,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -14.239267368979245,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3610578254873487,
      "fairness_sensitivity": 4.021155429904461,
      "trial_type_effect": 0.28515077024040486,
      "inequity_aversion": 2.99095898166738,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -19.875222011869855,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1471961337949421,
      "fairness_sensitivity": 1.096137281361825,
      "trial_type_effect": -1.20918832046527,
      "inequity_aversion": 0.6058142308964347,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -18.808554901511993,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2651641817128358,
      "fairness_sensitivity": 1.5505194330208731,
      "trial_type_effect": 1.5579377382821178,
      "inequity_aversion": 3.3271559303898,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -12.436086396070626,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5059280367808434,
      "fairness_sensitivity": 4.559398451008555,
      "trial_type_effect": 0.9826668935058018,
      "inequity_aversion": 4.398502735128132,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -10.831517297357628,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8586796968819931,
      "fairness_sensitivity": 1.6706102695155234,
      "trial_type_effect": -2.0,
      "inequity_aversion": 0.9033849821532627,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -12.186850280157037,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.037686542045552,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 4.528000179378067,
      "success": true,
      "log_likelihood": -3.612453791719461,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.739928334236967,
      "fairness_sensitivity": 0.0713855257902273,
      "trial_type_effect": 0.33196598231309316,
      "inequity_aversion": 4.923201435572296,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -12.204752339346308,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.355078317271939,
      "fairness_sensitivity": 4.836338558784112,
      "trial_type_effect": 0.32048379392589993,
      "inequity_aversion": 1.5346510322135303,
      "loss_aversion": 1.8153210449918067,
      "success": true,
      "log_likelihood": -19.922791326602322,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4423501990700422,
      "fairness_sensitivity": 0.22511568855930802,
      "trial_type_effect": -0.9813645620019069,
      "inequity_aversion": 3.8003890131433486,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -16.91022773126864,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6764061444897815,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -1.9180671676110548,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5242203575803077,
      "fairness_sensitivity": 2.253890080169242,
      "trial_type_effect": 1.5793535539217596,
      "inequity_aversion": 3.2752810390945877,
      "loss_aversion": 0.5502398341813601,
      "success": true,
      "log_likelihood": -17.199266792604817,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.096131957765207,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -6.224295065931407,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.23696676118879,
      "fairness_sensitivity": 3.931212912043104,
      "trial_type_effect": 1.6801777545152836,
      "inequity_aversion": 2.718601912565237,
      "loss_aversion": 4.937900846486804,
      "success": true,
      "log_likelihood": -11.676518889993659,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.8915325675261943,
      "fairness_sensitivity": 2.7958943473452296,
      "trial_type_effect": 0.2340910609208615,
      "inequity_aversion": 2.314819645978144,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -8.511509220277203,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.3441395907108937,
      "fairness_sensitivity": 4.342076181796331,
      "trial_type_effect": -0.2871534635075383,
      "inequity_aversion": 1.5935157721174078,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -11.248309636927324,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.600898032289602,
      "fairness_sensitivity": 3.8515766673296583,
      "trial_type_effect": 1.8371441787820397,
      "inequity_aversion": 4.999939452560948,
      "loss_aversion": 1.3497390395070064,
      "success": true,
      "log_likelihood": -11.000223548825161,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5514861457184423,
      "fairness_sensitivity": 4.833682905252995,
      "trial_type_effect": -1.2374267576685418,
      "inequity_aversion": 2.705128934727451,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -9.685895269850423,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8924750568283532,
      "fairness_sensitivity": 4.0767983960833405,
      "trial_type_effect": 1.999998366839456,
      "inequity_aversion": 5.0,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -5.094488550034254,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0600105195174534,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -5.028017464083808,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.844321939437876,
      "fairness_sensitivity": 0.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 4.598078100831957,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -16.136462066748035,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.3678211217600773,
      "fairness_sensitivity": 3.0123453228655506,
      "trial_type_effect": 0.30052150312427495,
      "inequity_aversion": 3.026908064791249,
      "loss_aversion": 1.0514074171150256,
      "success": true,
      "log_likelihood": -13.916146873347447,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7302716129213382,
      "fairness_sensitivity": 0.07459882411572366,
      "trial_type_effect": 2.0,
      "inequity_aversion": 2.1735451924637457,
      "loss_aversion": 1.2416067178106884,
      "success": true,
      "log_likelihood": -22.652074837172545,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.454461750579782,
      "fairness_sensitivity": 2.4247393699977415,
      "trial_type_effect": 1.5462516164778184,
      "inequity_aversion": 2.0137308348385163,
      "loss_aversion": 1.63994874509844,
      "success": true,
      "log_likelihood": -14.828774517289244,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.231799011433655,
      "fairness_sensitivity": 4.121298811370139,
      "trial_type_effect": -0.8749931846088745,
      "inequity_aversion": 1.3247774126101863,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -6.6138164135908495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "fairness_sensitivity": 0.8193367278260995,
      "trial_type_effect": 1.9224266762387565,
      "inequity_aversion": 5.0,
      "loss_aversion": 4.291648236113461,
      "success": true,
      "log_likelihood": -8.419363074434319,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8435318075515772,
      "fairness_sensitivity": 0.08431024644971026,
      "trial_type_effect": 0.20604022777123143,
      "inequity_aversion": 1.1792978661905782,
      "loss_aversion": 1.3720843934107405,
      "success": true,
      "log_likelihood": -22.406902025771416,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6764061498143085,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -1.9180671676110548,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.869726706561688,
      "fairness_sensitivity": 0.19125634172887968,
      "trial_type_effect": -0.44447693817889666,
      "inequity_aversion": 3.3123954678973164,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -22.127264611161884,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.370204575991578,
      "fairness_sensitivity": 0.8135145137807186,
      "trial_type_effect": -1.8189528528947418,
      "inequity_aversion": 0.47984495789617454,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -10.750580998938549,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.45850925790742625,
      "fairness_sensitivity": 2.7613497425071363,
      "trial_type_effect": 1.8762170167727985,
      "inequity_aversion": 4.924074322070337,
      "loss_aversion": 3.190601105775049,
      "success": true,
      "log_likelihood": -7.719764967987934,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.435550084113718,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -4.963148357770383,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5000264719620545,
      "fairness_sensitivity": 1.2135935572247631,
      "trial_type_effect": -0.8346751463866925,
      "inequity_aversion": 0.33427527998617096,
      "loss_aversion": 2.9284460539467556,
      "success": true,
      "log_likelihood": -23.534864352645375,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0130775150540936,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -3.3233576985772335,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4520355634581184,
      "fairness_sensitivity": 2.889814122129618,
      "trial_type_effect": 0.06749008302131103,
      "inequity_aversion": 0.5046070808750578,
      "loss_aversion": 4.901491655860663,
      "success": true,
      "log_likelihood": -18.740670003702494,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1008732317309096,
      "fairness_sensitivity": 0.0,
      "trial_type_effect": 1.3630825211100785,
      "inequity_aversion": 1.734164075174972,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -14.811372058328715,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.948537428793862,
      "fairness_sensitivity": 1.0084172131833753,
      "trial_type_effect": -1.5620089318071742,
      "inequity_aversion": 0.8624646806947944,
      "loss_aversion": 5.0,
      "success": true,
      "log_likelihood": -5.585985798298358,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.125673485846708,
      "fairness_sensitivity": 5.0,
      "trial_type_effect": 2.0,
      "inequity_aversion": 5.0,
      "loss_aversion": 4.151268744723856,
      "success": true,
      "log_likelihood": -4.986300913406624,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.125188399175842,
      "fairness_sensitivity": 1.9645400646076665,
      "trial_type_effect": 0.11088046644928386,
      "inequity_aversion": 1.947014128328135,
      "loss_aversion": 0.0,
      "success": true,
      "log_likelihood": -20.58047450367452,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.9075337061877389,
        "p": 1.0538020390950414e-38
      },
      "fairness_sensitivity": {
        "r": 0.2771568291799468,
        "p": 0.005244283707342695
      },
      "trial_type_effect": {
        "r": 0.5326953254305893,
        "p": 1.1644654641455667e-08
      },
      "inequity_aversion": {
        "r": 0.11564042316798238,
        "p": 0.25191357102537754
      },
      "loss_aversion": {
        "r": 0.2690687763135993,
        "p": 0.006789978145720202
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 40.19873594360871,
  "bic_results": {
    "average_bic": 40.19873594360871,
    "individual_bics": [
      21.753729027502658,
      52.02683269836869,
      47.183884487389705,
      44.7105033304098,
      49.768312893774024,
      44.56737480717437,
      17.92099060140493,
      46.02096295956823,
      17.92099060140493,
      47.45336821879763,
      41.6236364082275,
      27.430347496524167,
      45.67765925208478,
      17.92099060140493,
      17.92099060140493,
      17.92099060140493,
      17.92099060140493,
      23.7285776767909,
      32.21920618746421,
      17.92099060140493,
      51.60865173021661,
      35.682105216470006,
      37.522473586086065,
      48.32338226783819,
      53.82776853577046,
      47.349629120935504,
      36.9661010144073,
      39.432200339825,
      37.264941514979796,
      49.28420844342515,
      17.92099060140493,
      21.753729027502658,
      46.06591705376812,
      44.95586516527197,
      17.92099060140493,
      32.35383625498955,
      37.740771699972385,
      52.47740355029525,
      54.460418394938614,
      36.03258257616977,
      56.89523943991941,
      43.49349478281339,
      28.586632540134584,
      43.479238532898776,
      44.90547176950656,
      48.7394646381136,
      46.229912113157624,
      53.07372092326705,
      52.45349457686005,
      62.36319108833048,
      54.17447647417917,
      31.089732057728515,
      46.39612943023904,
      57.66803871602026,
      55.53470449530454,
      42.7897674844218,
      39.5806292869958,
      42.29129525259462,
      25.142502275719472,
      42.327099370973166,
      57.76317734548519,
      51.73805015481783,
      21.753729027502658,
      52.31612827749018,
      30.36618482414336,
      41.27063247226786,
      34.940613132834955,
      40.41421396613519,
      39.91804178993087,
      37.2893852319814,
      28.106571792349058,
      27.973629620448165,
      50.19051882577662,
      45.74988843897545,
      63.22174436662564,
      47.57514372685904,
      31.145227519462246,
      34.75632084114919,
      62.73139874382338,
      21.753729027502658,
      62.17212391460431,
      39.41875669015765,
      33.357124628256415,
      27.843891407821317,
      64.9873233975713,
      24.564310089435015,
      55.39893469968554,
      47.540338808937975,
      29.089566288877265,
      27.8901965190938,
      59.07854369962959
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 5,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.908\n- fairness_sensitivity: r = 0.277\n- trial_type_effect: r = 0.533\n- inequity_aversion: r = 0.116\n- loss_aversion: r = 0.269",
  "previous_models": [
    "Specification: U = alpha * log(split_self + 1) - fairness_sensitivity * (split_opp - split_self) / (combined_earning + 1) * trial_type_effect - inequity_aversion * max(0, split_opp - split_self) / (combined_earning + 1) - loss_aversion * max(0, (token_self / (token_self + token_opp)) * combined_earning - split_self) / (combined_earning + 1)\nSummary: This model predicts responder behavior in a modified Ultimatum Game, considering self-interest (absolute monetary amount), fairness (relative split and contributions), inequity aversion, loss aversion and the effect of trial type. The model calculates a utility score based on these factors and predicts the probability of acceptance using a logistic function. The model incorporates diminishing returns for monetary gains using a logarithmic function. It also considers relative contributions and the impact of trial framing on fairness perceptions. Finally, loss aversion is incorporated by penalizing outcomes where the responder receives less than their proportional share of the total earnings. All fairness and loss aversion terms are scaled based on combined earnings, to reduce the impacts of outliers and large combined earnings.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 40.19873594360871\nOverall Accuracy: 0.7943\n\nParameter Recovery:\n- alpha: r = 0.908\n- fairness_sensitivity: r = 0.277\n- trial_type_effect: r = 0.533\n- inequity_aversion: r = 0.116\n- loss_aversion: r = 0.269"
  ]
}