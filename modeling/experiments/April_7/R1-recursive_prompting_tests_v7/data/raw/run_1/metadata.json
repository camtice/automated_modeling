{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). The model should predict the utility of accepting, which will then be converted to a probability via a logistic function.\n\nFirst, I need to understand the variables provided and how they relate to the responder's decision. The key variables here are the tokens found by self and opponent (token_self, token_opp), the combined earnings (combined_earning), and the proposed splits (split_self, split_opp). The trial_type indicates whether contributions are equal, opponent more, or participant more. \n\nThe responder's decision likely depends on how fair the proposed split is relative to their own contribution. Since each token is worth \u00a33, the participant's contribution is token_self * 3, and the opponent's is token_opp * 3. The combined_earning is the sum of both contributions. The proposed split (split_self and split_opp) might be compared to what's considered a fair split based on contributions.\n\nA common approach in such scenarios is to model fairness using inequity aversion. For example, a participant might feel negative utility if the split deviates from what they contributed. So, the fair split based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair value could be a key factor.\n\nLet me formalize this. The fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. The actual proposed split_self minus this fair share gives the deviation from fairness. If the proposed split is higher than fair, that's positive utility; if lower, negative. \n\nBut maybe participants have different sensitivities to overcompensation vs. undercompensation. So, perhaps we need two parameters: one for when the proposed split is more than fair (alpha) and another when it's less (beta). Alternatively, a single parameter could model the linear effect, but having separate parameters might capture asymmetric responses.\n\nAnother consideration is the absolute amount of the split_self. Even if the split is fair, the absolute amount might matter. For example, getting \u00a35 in a \u00a310 pot might feel different than \u00a350 in a \u00a3100 pot, even if both are 50%. But the problem statement mentions that two equal split_self variables don't represent the same fairness because combined_earning varies. So, the model should account for the proportion relative to contribution, not just absolute split.\n\nSo, the utility function could be based on the difference between the proposed split and the fair split, scaled by some parameters. Let's denote the fair split as S_fair = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference is D = split_self - S_fair. \n\nThe utility U could then be a function of D. If D is positive (split is better than fair), it adds positively to utility. If D is negative (split is worse than fair), it subtracts. But maybe people are more sensitive to being undercompensated than over. So, we can have two parameters: theta_plus (for D >=0) and theta_minus (for D <0). However, to keep the number of parameters low, perhaps a single parameter with a rectified linear function. Alternatively, use a single parameter that scales D, but allow for different weights for positive and negative D.\n\nWait, but the problem says the model should have parameters that can be learned, and we need to avoid high covariance. So, perhaps a linear model with two parameters: one for the effect of advantageous inequity (when D is positive) and one for disadvantageous inequity (when D is negative). This is similar to Fehr & Schmidt's inequity aversion model, but applied here to the split vs. contribution.\n\nAlternatively, maybe a single parameter that weights the difference D. But if the response is asymmetric, two parameters would be better. Let's think: U = alpha * max(D, 0) + beta * min(D, 0). Here, alpha and beta are learnable parameters. If alpha is positive, getting more than fair increases utility; if beta is negative, getting less than fair decreases utility. But perhaps both parameters are positive, and the utility is U = D * (alpha if D >=0 else beta). Wait, not sure. Let me think again.\n\nAlternatively, the utility could be U = split_self + gamma * (split_self - S_fair). Here, gamma is a parameter that scales how much deviations from fairness affect utility. If gamma is positive, then getting more than fair increases utility, but getting less decreases it. But maybe participants are only averse to disadvantageous splits, so gamma could be negative when D is negative. Hmm, this might complicate things.\n\nAlternatively, model the utility as U = split_self + theta * (split_self - S_fair). If theta is positive, then participants prefer getting more than fair, but if theta is negative, they dislike deviations from fairness. But this might not capture asymmetric responses. For example, someone might be very sensitive to getting less than fair but not care about getting more. So, perhaps split the term into two: U = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0). Here, theta_plus and theta_minus are separate parameters. If theta_minus is negative, then getting less than fair reduces utility, and theta_plus could be positive (liking overcompensation) or negative (disliking overcompensation, which might be less common).\n\nBut the problem states that the model should have a slight bias towards fewer parameters. So maybe using a single parameter for the effect of D, but allowing it to be different for positive and negative D. Wait, but that would require two parameters. Alternatively, use the absolute value of D with a sign. Hmm.\n\nAlternatively, use a reference point where the fair split is the reference, and the utility is the difference from that, scaled by a parameter. So U = split_self + lambda * (split_self - S_fair). But this might not capture asymmetry. Alternatively, U = split_self + lambda_pos * (split_self - S_fair) if split_self >= S_fair, else split_self + lambda_neg * (split_self - S_fair). Here, lambda_pos and lambda_neg are separate parameters. This allows different sensitivities to advantageous vs. disadvantageous inequity.\n\nBut to keep the number of parameters low, maybe use one parameter for the effect of D. Let's say U = split_self + alpha * (split_self - S_fair). Then, if alpha is positive, participants value getting more than fair; if alpha is negative, they dislike deviations from fairness (both over and under). But this doesn't capture asymmetry. However, given the instruction to have a slight bias towards fewer parameters, maybe this is acceptable. But perhaps the data requires asymmetry, so two parameters would be better. Let's see.\n\nAnother angle: the participant's decision might also depend on the percentage split (splitperc_self) compared to their contribution percentage. The contribution percentage is (token_self / (token_self + token_opp)) * 100. The split percentage (splitperc_self) is given. The difference between these percentages could be a factor. For example, if the participant contributed 40% but is offered 30%, that's a -10% difference. The utility could be a function of this percentage difference. Let's call this Delta_perc = splitperc_self - (token_self / (token_self + token_opp)) * 100. Then, U = beta * Delta_perc. But this is similar to the previous approach but in percentage terms.\n\nAlternatively, combine both absolute and percentage terms. But that might complicate the model. The problem states that the model should utilize variables appropriately but not necessarily all. Since combined_earning is variable, perhaps using the percentage difference is better because it's scale-invariant. But the split_self is in absolute terms. Hmm.\n\nWait, the problem mentions that two equal split_self variables don't represent the same fairness because combined_earning varies. So, the fairness is relative to the combined_earning and contributions. Therefore, the model should account for the proportion of the split relative to the contribution. So, the key is the difference between the proposed split percentage and the contribution percentage. For example, if the participant contributed 40% of the tokens, a split of 50% would be a +10% difference. This Delta_perc could be a key variable.\n\nSo, the utility could be modeled as U = alpha * (splitperc_self - contribution_percent) + beta * split_self. But split_self is in absolute terms. However, since combined_earning varies, the absolute amount might also matter. For example, getting \u00a35 in a \u00a310 pot (50%) vs. \u00a350 in a \u00a3100 pot (also 50%)\u2014if the model uses splitperc_self, both would be the same, but the problem states that equal split_self (in absolute terms) don't represent the same fairness. Wait, no: the problem says that two equal split_self variables (in absolute terms) don't represent the same fairness because combined_earning varies. So, split_self being the same in two trials but combined_earning different would mean different fairness. For example, split_self = \u00a35 in combined_earning of \u00a310 (fair if contribution is 50%) vs. split_self = \u00a35 in combined_earning of \u00a320 (contribution would need to be 25% to be fair). So, the fairness is relative to the contribution and combined_earning.\n\nTherefore, the key variable is how much the split deviates from the fair split, which is (token_self / (token_self + token_opp)) * combined_earning. The difference between split_self and this fair value is crucial. Let's call this difference D = split_self - S_fair. Then, the utility could be a function of D. If D is positive, the participant is getting more than fair; if negative, less. The utility could be U = D * theta, where theta is a learnable parameter. If theta is positive, participants prefer higher D (more than fair), and vice versa. But this is a linear model. However, people might have different sensitivities to gains vs. losses. So, perhaps a Prospect Theory approach where losses (D < 0) are weighted more heavily. So, U = theta_plus * max(D, 0) + theta_minus * min(D, 0). Here, theta_plus and theta_minus are separate parameters. If theta_minus is more negative than theta_plus is positive, losses loom larger.\n\nBut to keep the number of parameters low, maybe use a single parameter for the effect of D, but allow it to be different for positive and negative D. However, that would require two parameters. Alternatively, use a single parameter and model U = D * theta. But this assumes symmetric sensitivity. Given the instruction to have a slight bias towards fewer parameters, perhaps two parameters are acceptable if they are necessary. Let's proceed with two parameters: theta_plus and theta_minus. \n\nAdditionally, the absolute amount of split_self might influence utility. For example, even if the split is fair, a higher absolute amount might be preferred. So, the utility could also include split_self as a term. So, U = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0). Here, split_self is the direct monetary gain, and the theta terms capture the fairness effect. If theta_plus is positive, getting more than fair adds to utility; if theta_minus is negative, getting less subtracts. Alternatively, theta_plus could be a scaling factor for advantageous inequity, and theta_minus for disadvantageous. \n\nBut including split_self directly might lead to collinearity with D, since D = split_self - S_fair. Let's see: split_self = D + S_fair. So, including split_self and D would introduce multicollinearity. Therefore, perhaps it's better to model U as split_self plus the fairness terms. But since S_fair is a function of token_self and token_opp, which are known variables, maybe we can separate the terms. Alternatively, since S_fair is (token_self/(token_self + token_opp)) * combined_earning, which is a fixed value given the tokens, perhaps the model should focus on the difference D. \n\nAlternatively, the utility could be U = split_self + alpha * (split_self - S_fair). This combines the direct monetary gain with a fairness term. If alpha is positive, participants value getting more than fair; if alpha is negative, they dislike deviations from fair (both over and under). But this doesn't capture asymmetry. However, if we allow alpha to be different for positive and negative D, that would require two parameters. \n\nAnother approach: the participant's utility could depend on both their own payoff and the perceived fairness. So, U = split_self + beta * (split_self - S_fair). Here, beta is a parameter that determines how much deviations from fairness affect utility. If beta is positive, participants like getting more than fair; if beta is negative, they dislike deviations (both over and under). But this is a linear model and may not capture asymmetric responses. However, it's simple and has only one learnable parameter (beta), which is good for parameter recovery. \n\nAlternatively, use a piecewise function with two parameters for positive and negative D. For example:\n\nU = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\nHere, theta_plus and theta_minus are learnable parameters. This allows different effects for advantageous and disadvantageous inequity. This model has two parameters, which is manageable. \n\nBut the problem states that the model should have parameters that can be learned with 36 instances per participant. With two parameters, that's feasible. Also, parameter recovery is a priority, so having a model that's not too complex is better. \n\nAnother consideration: the participant's own contribution (token_self) might influence their sense of entitlement. For example, if they contributed more, they might expect a higher split. But this is already captured in S_fair, which is based on their contribution. So, including S_fair in the model through D should account for that. \n\nAdditionally, the trial_type variable indicates whether the contribution was equal, opponent more, or participant more. This might influence the responder's expectations. For example, in trials where the participant contributed more (trial_type=3), they might be more sensitive to receiving less than fair. However, including trial_type as a moderator would add more parameters. But since the model should use the provided variables and not necessarily all, perhaps trial_type can be omitted if the fairness term (D) already captures the necessary information. \n\nPutting this together, the model could be:\n\nU = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\nWhere D = split_self - S_fair, and S_fair = (token_self / (token_self + token_opp)) * combined_earning.\n\nThen, the probability of accepting is given by the logistic function:\n\nP(accept=1) = 1 / (1 + exp(-U))\n\nBut wait, the problem states that the utility should be converted to a probability using a logistic function with temperature 1. So, the standard logistic function. So, the model's utility U is passed through the logistic function to get the probability of acceptance. \n\nHowever, the problem also mentions that the utility variable may be unbounded. So, U can range from -inf to inf. \n\nNow, the parameters theta_plus and theta_minus need to be learnable. What are plausible ranges for these parameters? \n\nIf theta_plus is positive, it means that getting more than the fair share increases utility beyond the monetary value. If theta_plus is negative, it could mean that participants feel guilty about taking more than their fair share. Similarly, theta_minus being negative would mean that getting less than fair decreases utility (which is expected), but if it's positive, that would be counterintuitive. So, we can set bounds for theta_plus and theta_minus. For example, theta_plus could be in [-10, 10], and theta_minus in [-10, 10]. These are generous bounds but prevent parameters from going to infinity. \n\nAlternatively, given that the utility is split_self (which is in \u00a3) plus the theta terms multiplied by D (also in \u00a3), the parameters theta_plus and theta_minus are unitless (since D is \u00a3, theta is \u00a3/\u00a3). So, they could be any real number, but to prevent overfitting, setting bounds like [-5,5] might be reasonable. \n\nAnother consideration: the model should predict utility where less than 0 corresponds to less than 50% probability of acceptance. Since the logistic function maps U to probabilities, when U < 0, the probability is < 0.5. So, the utility function should be designed such that higher U increases the probability of acceptance. \n\nNow, let's check if all variables used in the model are present in the dataset. The variables needed are split_self, token_self, token_opp, and combined_earning. All of these are available. \n\nSo, the mathematical model would be:\n\nU = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\nWhere D = split_self - (token_self / (token_self + token_opp)) * combined_earning\n\nBut combined_earning is equal to (token_self + token_opp) * 3, since each token is \u00a33. Wait, combined_earning is given as a variable, but it's calculated as (token_self + token_opp) * 3. So, perhaps we can compute S_fair as (token_self / (token_self + token_opp)) * combined_earning, which simplifies to token_self * 3. Because combined_earning = (token_self + token_opp) * 3, so S_fair = (token_self / (token_self + token_opp)) * (token_self + token_opp) * 3 = token_self * 3. Wait, that's a key insight! \n\nWait, combined_earning is (token_self + token_opp) * 3. So, S_fair, which is the fair split based on contribution, is (token_self / (token_self + token_opp)) * combined_earning = (token_self / (token_self + token_opp)) * (token_self + token_opp) * 3 = token_self * 3. So, S_fair is simply 3 * token_self. Therefore, D = split_self - 3 * token_self. \n\nThis simplifies the model significantly. Because S_fair is just 3 * token_self, the difference D is split_self - 3 * token_self. So, the model can be rewritten as:\n\nU = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)\n\nBut split_self is the amount offered to the participant. So, if split_self is greater than 3 * token_self, the participant is getting more than their contribution. If it's less, they're getting less. \n\nThis simplifies the model because we don't need to compute combined_earning or token_opp for S_fair. Wait, but token_opp is part of combined_earning. However, since combined_earning is (token_self + token_opp)*3, and S_fair is 3*token_self, the opponent's contribution (token_opp) doesn't directly affect S_fair. Wait, that can't be right. Because S_fair is supposed to be the fair split based on the participant's contribution relative to the total. But according to this calculation, S_fair is 3*token_self, which is exactly the participant's contribution. But the total combined_earning is (token_self + token_opp)*3. So, the fair split would be the participant's contribution (3*token_self) plus half of the shared pot? Wait, no. Wait, in the problem description, the tokens found by each are their contributions to the group. The combined_earning is the total, which is split later. So, perhaps the fair split is based on the proportion of tokens contributed. For example, if the participant contributed token_self tokens and the opponent token_opp, then the fair split for the participant would be (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is (token_self + token_opp)*3. So, substituting, the fair split is (token_self / (token_self + token_opp)) * (token_self + token_opp)*3 = token_self *3. So, the fair split is exactly the participant's own contribution. Therefore, any split_self above 3*token_self is more than their contribution, and below is less. \n\nThis is a crucial point. So, the fair split is exactly what the participant contributed (3*token_self). Therefore, the difference D is split_self - 3*token_self. \n\nThis simplifies the model. So, the utility function becomes:\n\nU = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\nWhere D = split_self - 3 * token_self.\n\nBut split_self is part of the utility, so the model is combining the actual monetary offer with the fairness term. For example, if the offer is exactly 3*token_self, then D=0, and U = split_self. If the offer is more, then U increases by theta_plus * D. If less, U decreases by theta_minus * |D| (since min(D,0) is negative, and theta_minus would be negative, leading to a positive contribution when D is negative). Wait, let's clarify:\n\nIf D is positive (split_self > 3*token_self), then max(D,0) = D, min(D,0) =0. So, U = split_self + theta_plus * D.\n\nIf D is negative (split_self < 3*token_self), then max(D,0)=0, min(D,0)=D. So, U = split_self + theta_minus * D.\n\nBut theta_plus and theta_minus are parameters. If theta_plus is positive, then getting more than fair increases utility. If theta_minus is positive, then getting less than fair would decrease utility (since D is negative, theta_minus * D would be negative). Alternatively, if theta_minus is negative, then theta_minus * D (where D is negative) would be positive, which would increase utility when D is negative, which doesn't make sense. So, perhaps theta_plus and theta_minus should have different signs. \n\nWait, let's think: when D is positive (split_self > fair), the participant is getting more than their contribution. If theta_plus is positive, this adds to utility. If theta_plus is negative, this subtracts (maybe they feel guilty). \n\nWhen D is negative (split_self < fair), the participant is getting less. If theta_minus is positive, then theta_minus * D (negative) subtracts from utility. If theta_minus is negative, then theta_minus * D (negative) adds to utility (which is not desired). So, to model disadvantageous inequity aversion, theta_minus should be positive. Because when D is negative, theta_minus * D would be negative, reducing utility. \n\nWait, no: if theta_minus is positive, then for D negative (split_self < fair), theta_minus * D is negative, so U = split_self + (negative term), which reduces utility. That's correct for disadvantageous inequity aversion. So, theta_plus and theta_minus are both positive parameters. \n\nBut in the model, theta_plus scales the advantageous inequity (D positive), and theta_minus scales the disadvantageous inequity (D negative). So, the utility becomes:\n\nFor D >=0: U = split_self + theta_plus * D\n\nFor D <0: U = split_self + theta_minus * D\n\nBut since theta_plus and theta_minus are positive, in the case of D <0, theta_minus * D is negative, reducing utility. \n\nAlternatively, perhaps theta_plus and theta_minus can be any real numbers, but in practice, we expect theta_plus to be positive (liking advantageous inequity) and theta_minus to be positive (disliking disadvantageous inequity). However, the model allows them to be negative, which would have different interpretations. \n\nBut to set bounds, we can allow theta_plus and theta_minus to be within, say, [-5, 5], which is generous. \n\nSo, the model is:\n\nU = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)\n\nThis can be simplified as:\n\nU = split_self + theta_plus * (split_self - 3 * token_self) if split_self >= 3 * token_self else split_self + theta_minus * (split_self - 3 * token_self)\n\nBut mathematically, it's the same as the piecewise function.\n\nNow, the parameters theta_plus and theta_minus are learnable. The variables split_self and token_self come from the data. \n\nThis model uses only the variables split_self and token_self, which are present in the dataset. The combined_earning and token_opp are not directly used because S_fair simplifies to 3*token_self. \n\nBut wait, token_opp is part of the combined_earning, but in this model, it's not used because S_fair is calculated as 3*token_self. However, the problem states that the opponent's contribution (token_opp) is part of the shared pot. But according to the calculation, the fair split is based solely on the participant's contribution. Is that correct? \n\nYes, because the combined_earning is the sum of both contributions, and the fair split based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. But since combined_earning = 3*(token_self + token_opp), this simplifies to 3*token_self. So, the fair split is exactly the participant's own contribution. Therefore, the opponent's contribution doesn't affect the fair split beyond being part of the total. \n\nTherefore, the model correctly captures the fair split as the participant's contribution, and the difference D is split_self minus that. \n\nNow, the utility function is a combination of the actual split and the deviation from the fair split, with different parameters for advantageous and disadvantageous deviations. \n\nThis model has two learnable parameters: theta_plus and theta_minus. The variables split_self and token_self are from the data. \n\nThe target variable is 'accept', which is binary. The model's utility U is passed through a logistic function to get the probability of acceptance. \n\nNow, checking the requirements:\n\n1. Predict responder behavior (accept) - yes, only when trial_role is 1.\n\n2. Uses provided variables - split_self and token_self are used, which are in the dataset.\n\n3. Parameters have finite bounds - theta_plus and theta_minus can be bounded between, say, -10 and 10.\n\n4. Utility can be unbounded, which it is.\n\nThe model should also pass parameter recovery tests. Having two parameters is manageable with 36 trials per participant. \n\nAnother consideration: including the absolute split_self in the utility. For example, even if the split is fair, a higher split_self is better. The model already includes split_self as a term, so higher split_self directly increases utility. The theta terms modify this based on fairness. \n\nThis seems reasonable. For example, if two offers have the same D (deviation from fair), the one with higher split_self is preferred. \n\nIn summary, the model captures both the monetary value of the offer and the fairness relative to the participant's contribution, with separate parameters for advantageous and disadvantageous inequity.\n</think>\n\n<MODEL>\nU = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"theta_plus\": {\n      \"description\": \"Sensitivity to advantageous inequity (offer > fair share)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"theta_minus\": {\n      \"description\": \"Sensitivity to disadvantageous inequity (offer < fair share)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant (in \u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model combining direct monetary gain and asymmetric sensitivity to fairness deviations. The utility incorporates the proposed amount (split_self) and adjusts it by two parameters: theta_plus scales utility when the offer exceeds the participant's fair share (3 * token_self), while theta_minus scales utility when the offer falls short. This captures both self-interest and inequity aversion with separate parameters for advantageous and disadvantageous inequity.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['theta_plus', 'theta_minus']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"theta_plus\": {\n      \"description\": \"Sensitivity to advantageous inequity (offer > fair share)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"theta_minus\": {\n      \"description\": \"Sensitivity to disadvantageous inequity (offer < fair share)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant (in \\u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, theta_plus=0.0, theta_minus=0.0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0.0)\n        token_self = trial.get(\"token_self\", 0)\n\n        D = split_self - 3 * token_self\n        utility = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\":1,\"group\":\"group2\",\"trial_number\":1,\"trial_type\":1,\"trial_role\":1,\"token_opp\":2,\"token_self\":2,\"combined_earning\":12,\"split_opp\":6.0,\"split_self\":6.0,\"splitperc_opp\":50,\"splitperc_self\":50,\"accept\":1.0,\"accepted_amount\":6.0,\"accepted_perc\":50.0,\"proposed_perc\":50.0,\"proposed_amount\":6.0},{\"ID\":2,\"group\":\"group2\",\"trial_number\":2,\"trial_type\":1,\"trial_role\":1,\"token_opp\":2,\"token_self\":2,\"combined_earning\":12,\"split_opp\":9.0,\"split_self\":3.0,\"splitperc_opp\":75,\"splitperc_self\":25,\"accept\":0.0,\"accepted_amount\":null,\"accepted_perc\":null,\"proposed_perc\":25.0,\"proposed_amount\":3.0},{\"ID\":3,\"group\":\"group2\",\"trial_number\":3,\"trial_type\":1,\"trial_role\":1,\"token_opp\":2,\"token_self\":2,\"combined_earning\":12,\"split_opp\":3.0,\"split_self\":9.0,\"splitperc_opp\":25,\"splitperc_self\":75,\"accept\":1.0,\"accepted_amount\":9.0,\"accepted_perc\":75.0,\"proposed_perc\":75.0,\"proposed_amount\":9.0},{\"ID\":4,\"group\":\"group2\",\"trial_number\":4,\"trial_type\":1,\"trial_role\":1,\"token_opp\":2,\"token_self\":2,\"combined_earning\":12,\"split_opp\":6.0,\"split_self\":6.0,\"splitperc_opp\":50,\"splitperc_self\":50,\"accept\":1.0,\"accepted_amount\":6.0,\"accepted_perc\":50.0,\"proposed_perc\":50.0,\"proposed_amount\":6.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1744031546.724451
    }
  ],
  "learnable_parameters": {
    "theta_plus": {
      "description": "Sensitivity to advantageous inequity (offer > fair share)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "theta_minus": {
      "description": "Sensitivity to disadvantageous inequity (offer < fair share)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "theta_plus": {
      "description": "Sensitivity to advantageous inequity (offer > fair share)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "theta_minus": {
      "description": "Sensitivity to disadvantageous inequity (offer < fair share)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for the participant (in \u00a3)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model combining direct monetary gain and asymmetric sensitivity to fairness deviations. The utility incorporates the proposed amount (split_self) and adjusts it by two parameters: theta_plus scales utility when the offer exceeds the participant's fair share (3 * token_self), while theta_minus scales utility when the offer falls short. This captures both self-interest and inequity aversion with separate parameters for advantageous and disadvantageous inequity.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). The model should predict the utility of accepting, which will then be converted to a probability via a logistic function.\n\nFirst, I need to understand the variables provided and how they relate to the responder's decision. The key variables here are the tokens found by self and opponent (token_self, token_opp), the combined earnings (combined_earning), and the proposed splits (split_self, split_opp). The trial_type indicates whether contributions are equal, opponent more, or participant more. \n\nThe responder's decision likely depends on how fair the proposed split is relative to their own contribution. Since each token is worth \u00a33, the participant's contribution is token_self * 3, and the opponent's is token_opp * 3. The combined_earning is the sum of both contributions. The proposed split (split_self and split_opp) might be compared to what's considered a fair split based on contributions.\n\nA common approach in such scenarios is to model fairness using inequity aversion. For example, a participant might feel negative utility if the split deviates from what they contributed. So, the fair split based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair value could be a key factor.\n\nLet me formalize this. The fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. The actual proposed split_self minus this fair share gives the deviation from fairness. If the proposed split is higher than fair, that's positive utility; if lower, negative. \n\nBut maybe participants have different sensitivities to overcompensation vs. undercompensation. So, perhaps we need two parameters: one for when the proposed split is more than fair (alpha) and another when it's less (beta). Alternatively, a single parameter could model the linear effect, but having separate parameters might capture asymmetric responses.\n\nAnother consideration is the absolute amount of the split_self. Even if the split is fair, the absolute amount might matter. For example, getting \u00a35 in a \u00a310 pot might feel different than \u00a350 in a \u00a3100 pot, even if both are 50%. But the problem statement mentions that two equal split_self variables don't represent the same fairness because combined_earning varies. So, the model should account for the proportion relative to contribution, not just absolute split.\n\nSo, the utility function could be based on the difference between the proposed split and the fair split, scaled by some parameters. Let's denote the fair split as S_fair = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference is D = split_self - S_fair. \n\nThe utility U could then be a function of D. If D is positive (split is better than fair), it adds positively to utility. If D is negative (split is worse than fair), it subtracts. But maybe people are more sensitive to being undercompensated than over. So, we can have two parameters: theta_plus (for D >=0) and theta_minus (for D <0). However, to keep the number of parameters low, perhaps a single parameter with a rectified linear function. Alternatively, use a single parameter that scales D, but allow for different weights for positive and negative D.\n\nWait, but the problem says the model should have parameters that can be learned, and we need to avoid high covariance. So, perhaps a linear model with two parameters: one for the effect of advantageous inequity (when D is positive) and one for disadvantageous inequity (when D is negative). This is similar to Fehr & Schmidt's inequity aversion model, but applied here to the split vs. contribution.\n\nAlternatively, maybe a single parameter that weights the difference D. But if the response is asymmetric, two parameters would be better. Let's think: U = alpha * max(D, 0) + beta * min(D, 0). Here, alpha and beta are learnable parameters. If alpha is positive, getting more than fair increases utility; if beta is negative, getting less than fair decreases utility. But perhaps both parameters are positive, and the utility is U = D * (alpha if D >=0 else beta). Wait, not sure. Let me think again.\n\nAlternatively, the utility could be U = split_self + gamma * (split_self - S_fair). Here, gamma is a parameter that scales how much deviations from fairness affect utility. If gamma is positive, then getting more than fair increases utility, but getting less decreases it. But maybe participants are only averse to disadvantageous splits, so gamma could be negative when D is negative. Hmm, this might complicate things.\n\nAlternatively, model the utility as U = split_self + theta * (split_self - S_fair). If theta is positive, then participants prefer getting more than fair, but if theta is negative, they dislike deviations from fairness. But this might not capture asymmetric responses. For example, someone might be very sensitive to getting less than fair but not care about getting more. So, perhaps split the term into two: U = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0). Here, theta_plus and theta_minus are separate parameters. If theta_minus is negative, then getting less than fair reduces utility, and theta_plus could be positive (liking overcompensation) or negative (disliking overcompensation, which might be less common).\n\nBut the problem states that the model should have a slight bias towards fewer parameters. So maybe using a single parameter for the effect of D, but allowing it to be different for positive and negative D. Wait, but that would require two parameters. Alternatively, use the absolute value of D with a sign. Hmm.\n\nAlternatively, use a reference point where the fair split is the reference, and the utility is the difference from that, scaled by a parameter. So U = split_self + lambda * (split_self - S_fair). But this might not capture asymmetry. Alternatively, U = split_self + lambda_pos * (split_self - S_fair) if split_self >= S_fair, else split_self + lambda_neg * (split_self - S_fair). Here, lambda_pos and lambda_neg are separate parameters. This allows different sensitivities to advantageous vs. disadvantageous inequity.\n\nBut to keep the number of parameters low, maybe use one parameter for the effect of D. Let's say U = split_self + alpha * (split_self - S_fair). Then, if alpha is positive, participants value getting more than fair; if alpha is negative, they dislike deviations from fairness (both over and under). But this doesn't capture asymmetry. However, given the instruction to have a slight bias towards fewer parameters, maybe this is acceptable. But perhaps the data requires asymmetry, so two parameters would be better. Let's see.\n\nAnother angle: the participant's decision might also depend on the percentage split (splitperc_self) compared to their contribution percentage. The contribution percentage is (token_self / (token_self + token_opp)) * 100. The split percentage (splitperc_self) is given. The difference between these percentages could be a factor. For example, if the participant contributed 40% but is offered 30%, that's a -10% difference. The utility could be a function of this percentage difference. Let's call this Delta_perc = splitperc_self - (token_self / (token_self + token_opp)) * 100. Then, U = beta * Delta_perc. But this is similar to the previous approach but in percentage terms.\n\nAlternatively, combine both absolute and percentage terms. But that might complicate the model. The problem states that the model should utilize variables appropriately but not necessarily all. Since combined_earning is variable, perhaps using the percentage difference is better because it's scale-invariant. But the split_self is in absolute terms. Hmm.\n\nWait, the problem mentions that two equal split_self variables don't represent the same fairness because combined_earning varies. So, the fairness is relative to the combined_earning and contributions. Therefore, the model should account for the proportion of the split relative to the contribution. So, the key is the difference between the proposed split percentage and the contribution percentage. For example, if the participant contributed 40% of the tokens, a split of 50% would be a +10% difference. This Delta_perc could be a key variable.\n\nSo, the utility could be modeled as U = alpha * (splitperc_self - contribution_percent) + beta * split_self. But split_self is in absolute terms. However, since combined_earning varies, the absolute amount might also matter. For example, getting \u00a35 in a \u00a310 pot (50%) vs. \u00a350 in a \u00a3100 pot (also 50%)\u2014if the model uses splitperc_self, both would be the same, but the problem states that equal split_self (in absolute terms) don't represent the same fairness. Wait, no: the problem says that two equal split_self variables (in absolute terms) don't represent the same fairness because combined_earning varies. So, split_self being the same in two trials but combined_earning different would mean different fairness. For example, split_self = \u00a35 in combined_earning of \u00a310 (fair if contribution is 50%) vs. split_self = \u00a35 in combined_earning of \u00a320 (contribution would need to be 25% to be fair). So, the fairness is relative to the contribution and combined_earning.\n\nTherefore, the key variable is how much the split deviates from the fair split, which is (token_self / (token_self + token_opp)) * combined_earning. The difference between split_self and this fair value is crucial. Let's call this difference D = split_self - S_fair. Then, the utility could be a function of D. If D is positive, the participant is getting more than fair; if negative, less. The utility could be U = D * theta, where theta is a learnable parameter. If theta is positive, participants prefer higher D (more than fair), and vice versa. But this is a linear model. However, people might have different sensitivities to gains vs. losses. So, perhaps a Prospect Theory approach where losses (D < 0) are weighted more heavily. So, U = theta_plus * max(D, 0) + theta_minus * min(D, 0). Here, theta_plus and theta_minus are separate parameters. If theta_minus is more negative than theta_plus is positive, losses loom larger.\n\nBut to keep the number of parameters low, maybe use a single parameter for the effect of D, but allow it to be different for positive and negative D. However, that would require two parameters. Alternatively, use a single parameter and model U = D * theta. But this assumes symmetric sensitivity. Given the instruction to have a slight bias towards fewer parameters, perhaps two parameters are acceptable if they are necessary. Let's proceed with two parameters: theta_plus and theta_minus. \n\nAdditionally, the absolute amount of split_self might influence utility. For example, even if the split is fair, a higher absolute amount might be preferred. So, the utility could also include split_self as a term. So, U = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0). Here, split_self is the direct monetary gain, and the theta terms capture the fairness effect. If theta_plus is positive, getting more than fair adds to utility; if theta_minus is negative, getting less subtracts. Alternatively, theta_plus could be a scaling factor for advantageous inequity, and theta_minus for disadvantageous. \n\nBut including split_self directly might lead to collinearity with D, since D = split_self - S_fair. Let's see: split_self = D + S_fair. So, including split_self and D would introduce multicollinearity. Therefore, perhaps it's better to model U as split_self plus the fairness terms. But since S_fair is a function of token_self and token_opp, which are known variables, maybe we can separate the terms. Alternatively, since S_fair is (token_self/(token_self + token_opp)) * combined_earning, which is a fixed value given the tokens, perhaps the model should focus on the difference D. \n\nAlternatively, the utility could be U = split_self + alpha * (split_self - S_fair). This combines the direct monetary gain with a fairness term. If alpha is positive, participants value getting more than fair; if alpha is negative, they dislike deviations from fair (both over and under). But this doesn't capture asymmetry. However, if we allow alpha to be different for positive and negative D, that would require two parameters. \n\nAnother approach: the participant's utility could depend on both their own payoff and the perceived fairness. So, U = split_self + beta * (split_self - S_fair). Here, beta is a parameter that determines how much deviations from fairness affect utility. If beta is positive, participants like getting more than fair; if beta is negative, they dislike deviations (both over and under). But this is a linear model and may not capture asymmetric responses. However, it's simple and has only one learnable parameter (beta), which is good for parameter recovery. \n\nAlternatively, use a piecewise function with two parameters for positive and negative D. For example:\n\nU = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\nHere, theta_plus and theta_minus are learnable parameters. This allows different effects for advantageous and disadvantageous inequity. This model has two parameters, which is manageable. \n\nBut the problem states that the model should have parameters that can be learned with 36 instances per participant. With two parameters, that's feasible. Also, parameter recovery is a priority, so having a model that's not too complex is better. \n\nAnother consideration: the participant's own contribution (token_self) might influence their sense of entitlement. For example, if they contributed more, they might expect a higher split. But this is already captured in S_fair, which is based on their contribution. So, including S_fair in the model through D should account for that. \n\nAdditionally, the trial_type variable indicates whether the contribution was equal, opponent more, or participant more. This might influence the responder's expectations. For example, in trials where the participant contributed more (trial_type=3), they might be more sensitive to receiving less than fair. However, including trial_type as a moderator would add more parameters. But since the model should use the provided variables and not necessarily all, perhaps trial_type can be omitted if the fairness term (D) already captures the necessary information. \n\nPutting this together, the model could be:\n\nU = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\nWhere D = split_self - S_fair, and S_fair = (token_self / (token_self + token_opp)) * combined_earning.\n\nThen, the probability of accepting is given by the logistic function:\n\nP(accept=1) = 1 / (1 + exp(-U))\n\nBut wait, the problem states that the utility should be converted to a probability using a logistic function with temperature 1. So, the standard logistic function. So, the model's utility U is passed through the logistic function to get the probability of acceptance. \n\nHowever, the problem also mentions that the utility variable may be unbounded. So, U can range from -inf to inf. \n\nNow, the parameters theta_plus and theta_minus need to be learnable. What are plausible ranges for these parameters? \n\nIf theta_plus is positive, it means that getting more than the fair share increases utility beyond the monetary value. If theta_plus is negative, it could mean that participants feel guilty about taking more than their fair share. Similarly, theta_minus being negative would mean that getting less than fair decreases utility (which is expected), but if it's positive, that would be counterintuitive. So, we can set bounds for theta_plus and theta_minus. For example, theta_plus could be in [-10, 10], and theta_minus in [-10, 10]. These are generous bounds but prevent parameters from going to infinity. \n\nAlternatively, given that the utility is split_self (which is in \u00a3) plus the theta terms multiplied by D (also in \u00a3), the parameters theta_plus and theta_minus are unitless (since D is \u00a3, theta is \u00a3/\u00a3). So, they could be any real number, but to prevent overfitting, setting bounds like [-5,5] might be reasonable. \n\nAnother consideration: the model should predict utility where less than 0 corresponds to less than 50% probability of acceptance. Since the logistic function maps U to probabilities, when U < 0, the probability is < 0.5. So, the utility function should be designed such that higher U increases the probability of acceptance. \n\nNow, let's check if all variables used in the model are present in the dataset. The variables needed are split_self, token_self, token_opp, and combined_earning. All of these are available. \n\nSo, the mathematical model would be:\n\nU = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\nWhere D = split_self - (token_self / (token_self + token_opp)) * combined_earning\n\nBut combined_earning is equal to (token_self + token_opp) * 3, since each token is \u00a33. Wait, combined_earning is given as a variable, but it's calculated as (token_self + token_opp) * 3. So, perhaps we can compute S_fair as (token_self / (token_self + token_opp)) * combined_earning, which simplifies to token_self * 3. Because combined_earning = (token_self + token_opp) * 3, so S_fair = (token_self / (token_self + token_opp)) * (token_self + token_opp) * 3 = token_self * 3. Wait, that's a key insight! \n\nWait, combined_earning is (token_self + token_opp) * 3. So, S_fair, which is the fair split based on contribution, is (token_self / (token_self + token_opp)) * combined_earning = (token_self / (token_self + token_opp)) * (token_self + token_opp) * 3 = token_self * 3. So, S_fair is simply 3 * token_self. Therefore, D = split_self - 3 * token_self. \n\nThis simplifies the model significantly. Because S_fair is just 3 * token_self, the difference D is split_self - 3 * token_self. So, the model can be rewritten as:\n\nU = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)\n\nBut split_self is the amount offered to the participant. So, if split_self is greater than 3 * token_self, the participant is getting more than their contribution. If it's less, they're getting less. \n\nThis simplifies the model because we don't need to compute combined_earning or token_opp for S_fair. Wait, but token_opp is part of combined_earning. However, since combined_earning is (token_self + token_opp)*3, and S_fair is 3*token_self, the opponent's contribution (token_opp) doesn't directly affect S_fair. Wait, that can't be right. Because S_fair is supposed to be the fair split based on the participant's contribution relative to the total. But according to this calculation, S_fair is 3*token_self, which is exactly the participant's contribution. But the total combined_earning is (token_self + token_opp)*3. So, the fair split would be the participant's contribution (3*token_self) plus half of the shared pot? Wait, no. Wait, in the problem description, the tokens found by each are their contributions to the group. The combined_earning is the total, which is split later. So, perhaps the fair split is based on the proportion of tokens contributed. For example, if the participant contributed token_self tokens and the opponent token_opp, then the fair split for the participant would be (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is (token_self + token_opp)*3. So, substituting, the fair split is (token_self / (token_self + token_opp)) * (token_self + token_opp)*3 = token_self *3. So, the fair split is exactly the participant's own contribution. Therefore, any split_self above 3*token_self is more than their contribution, and below is less. \n\nThis is a crucial point. So, the fair split is exactly what the participant contributed (3*token_self). Therefore, the difference D is split_self - 3*token_self. \n\nThis simplifies the model. So, the utility function becomes:\n\nU = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\nWhere D = split_self - 3 * token_self.\n\nBut split_self is part of the utility, so the model is combining the actual monetary offer with the fairness term. For example, if the offer is exactly 3*token_self, then D=0, and U = split_self. If the offer is more, then U increases by theta_plus * D. If less, U decreases by theta_minus * |D| (since min(D,0) is negative, and theta_minus would be negative, leading to a positive contribution when D is negative). Wait, let's clarify:\n\nIf D is positive (split_self > 3*token_self), then max(D,0) = D, min(D,0) =0. So, U = split_self + theta_plus * D.\n\nIf D is negative (split_self < 3*token_self), then max(D,0)=0, min(D,0)=D. So, U = split_self + theta_minus * D.\n\nBut theta_plus and theta_minus are parameters. If theta_plus is positive, then getting more than fair increases utility. If theta_minus is positive, then getting less than fair would decrease utility (since D is negative, theta_minus * D would be negative). Alternatively, if theta_minus is negative, then theta_minus * D (where D is negative) would be positive, which would increase utility when D is negative, which doesn't make sense. So, perhaps theta_plus and theta_minus should have different signs. \n\nWait, let's think: when D is positive (split_self > fair), the participant is getting more than their contribution. If theta_plus is positive, this adds to utility. If theta_plus is negative, this subtracts (maybe they feel guilty). \n\nWhen D is negative (split_self < fair), the participant is getting less. If theta_minus is positive, then theta_minus * D (negative) subtracts from utility. If theta_minus is negative, then theta_minus * D (negative) adds to utility (which is not desired). So, to model disadvantageous inequity aversion, theta_minus should be positive. Because when D is negative, theta_minus * D would be negative, reducing utility. \n\nWait, no: if theta_minus is positive, then for D negative (split_self < fair), theta_minus * D is negative, so U = split_self + (negative term), which reduces utility. That's correct for disadvantageous inequity aversion. So, theta_plus and theta_minus are both positive parameters. \n\nBut in the model, theta_plus scales the advantageous inequity (D positive), and theta_minus scales the disadvantageous inequity (D negative). So, the utility becomes:\n\nFor D >=0: U = split_self + theta_plus * D\n\nFor D <0: U = split_self + theta_minus * D\n\nBut since theta_plus and theta_minus are positive, in the case of D <0, theta_minus * D is negative, reducing utility. \n\nAlternatively, perhaps theta_plus and theta_minus can be any real numbers, but in practice, we expect theta_plus to be positive (liking advantageous inequity) and theta_minus to be positive (disliking disadvantageous inequity). However, the model allows them to be negative, which would have different interpretations. \n\nBut to set bounds, we can allow theta_plus and theta_minus to be within, say, [-5, 5], which is generous. \n\nSo, the model is:\n\nU = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)\n\nThis can be simplified as:\n\nU = split_self + theta_plus * (split_self - 3 * token_self) if split_self >= 3 * token_self else split_self + theta_minus * (split_self - 3 * token_self)\n\nBut mathematically, it's the same as the piecewise function.\n\nNow, the parameters theta_plus and theta_minus are learnable. The variables split_self and token_self come from the data. \n\nThis model uses only the variables split_self and token_self, which are present in the dataset. The combined_earning and token_opp are not directly used because S_fair simplifies to 3*token_self. \n\nBut wait, token_opp is part of the combined_earning, but in this model, it's not used because S_fair is calculated as 3*token_self. However, the problem states that the opponent's contribution (token_opp) is part of the shared pot. But according to the calculation, the fair split is based solely on the participant's contribution. Is that correct? \n\nYes, because the combined_earning is the sum of both contributions, and the fair split based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. But since combined_earning = 3*(token_self + token_opp), this simplifies to 3*token_self. So, the fair split is exactly the participant's own contribution. Therefore, the opponent's contribution doesn't affect the fair split beyond being part of the total. \n\nTherefore, the model correctly captures the fair split as the participant's contribution, and the difference D is split_self minus that. \n\nNow, the utility function is a combination of the actual split and the deviation from the fair split, with different parameters for advantageous and disadvantageous deviations. \n\nThis model has two learnable parameters: theta_plus and theta_minus. The variables split_self and token_self are from the data. \n\nThe target variable is 'accept', which is binary. The model's utility U is passed through a logistic function to get the probability of acceptance. \n\nNow, checking the requirements:\n\n1. Predict responder behavior (accept) - yes, only when trial_role is 1.\n\n2. Uses provided variables - split_self and token_self are used, which are in the dataset.\n\n3. Parameters have finite bounds - theta_plus and theta_minus can be bounded between, say, -10 and 10.\n\n4. Utility can be unbounded, which it is.\n\nThe model should also pass parameter recovery tests. Having two parameters is manageable with 36 trials per participant. \n\nAnother consideration: including the absolute split_self in the utility. For example, even if the split is fair, a higher split_self is better. The model already includes split_self as a term, so higher split_self directly increases utility. The theta terms modify this based on fairness. \n\nThis seems reasonable. For example, if two offers have the same D (deviation from fair), the one with higher split_self is preferred. \n\nIn summary, the model captures both the monetary value of the offer and the fairness relative to the participant's contribution, with separate parameters for advantageous and disadvantageous inequity.\n</think>\n\n<MODEL>\nU = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"theta_plus\": {\n      \"description\": \"Sensitivity to advantageous inequity (offer > fair share)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"theta_minus\": {\n      \"description\": \"Sensitivity to disadvantageous inequity (offer < fair share)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant (in \u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model combining direct monetary gain and asymmetric sensitivity to fairness deviations. The utility incorporates the proposed amount (split_self) and adjusts it by two parameters: theta_plus scales utility when the offer exceeds the participant's fair share (3 * token_self), while theta_minus scales utility when the offer falls short. This captures both self-interest and inequity aversion with separate parameters for advantageous and disadvantageous inequity.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)\nSummary: A utility model combining direct monetary gain and asymmetric sensitivity to fairness deviations. The utility incorporates the proposed amount (split_self) and adjusts it by two parameters: theta_plus scales utility when the offer exceeds the participant's fair share (3 * token_self), while theta_minus scales utility when the offer falls short. This captures both self-interest and inequity aversion with separate parameters for advantageous and disadvantageous inequity.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, theta_plus=0.0, theta_minus=0.0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0.0)\n        token_self = trial.get(\"token_self\", 0)\n\n        D = split_self - 3 * token_self\n        utility = split_self + theta_plus * max(D, 0) + theta_minus * min(D, 0)\n\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.7948717948717948,
  "fitting_results": [
    {
      "theta_plus": -2.599013760875638,
      "theta_minus": 6.560668920463511,
      "success": true,
      "log_likelihood": -18.25271011487174,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.3918953376202226,
      "success": true,
      "log_likelihood": -17.556389488040637,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.4592502177400795,
      "success": true,
      "log_likelihood": -10.190946550345194,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.6437046685282415,
      "success": true,
      "log_likelihood": -14.10489778365494,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.9109604360437171,
      "success": true,
      "log_likelihood": -9.81077281770292,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.055899267110507,
      "success": true,
      "log_likelihood": -11.160250051146493,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -0.009638603825320122,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 5.665836167128491,
      "theta_minus": 1.0150757100375953,
      "success": true,
      "log_likelihood": -11.214146292020775,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -0.009638603825320124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 2.0196412446503142,
      "success": true,
      "log_likelihood": -11.364751675818107,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.8719668546674697,
      "success": true,
      "log_likelihood": -5.545597829082483,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -2.5990038227424646,
      "theta_minus": 5.263478773695338,
      "success": true,
      "log_likelihood": -17.403740486064503,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 2.0080157903072027,
      "success": true,
      "log_likelihood": -8.816823546846008,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -0.009638603825320124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -0.009638603825320124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -0.009638603825320124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -0.009638603825320122,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.38304464139439975,
      "success": true,
      "log_likelihood": -6.17698101403026,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.3621358736200347,
      "success": true,
      "log_likelihood": -7.618209720776814,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -0.009638603825320122,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.3282471945381789,
      "success": true,
      "log_likelihood": -15.804029257361682,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 9.913095949315798,
      "theta_minus": 0.43353705055329533,
      "success": true,
      "log_likelihood": -8.585803393678678,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.5892695051627916,
      "success": true,
      "log_likelihood": -23.244313860610227,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.2055553960970344,
      "success": true,
      "log_likelihood": -18.301182848259803,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.6366656445495387,
      "success": true,
      "log_likelihood": -14.50094479522103,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.1240947485502826,
      "success": true,
      "log_likelihood": -15.145025146628901,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.2580700758755192,
      "success": true,
      "log_likelihood": -11.61899326151582,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 8.686619403038085,
      "theta_minus": 3.6452830605472575,
      "success": true,
      "log_likelihood": -4.172789255075036,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.3350700994919095,
      "success": true,
      "log_likelihood": -9.438850623627122,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 4.6241028651335085,
      "theta_minus": -6.769375077385274,
      "success": true,
      "log_likelihood": -183.41631975333712,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -0.009638603825320124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -2.598991867734747,
      "theta_minus": 6.560686132578989,
      "success": true,
      "log_likelihood": -18.25271011466081,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.3131377599408591,
      "success": true,
      "log_likelihood": -28.84828601040687,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.7493693729381328,
      "success": true,
      "log_likelihood": -17.44446371826574,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -0.009638603825320124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.8323108290886244,
      "success": true,
      "log_likelihood": -13.336596218356902,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.8928408487585775,
      "success": true,
      "log_likelihood": -8.58750473360472,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.5313077873523582,
      "success": true,
      "log_likelihood": -14.700185030891635,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.9112479306681827,
      "success": true,
      "log_likelihood": -19.161017280442003,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -69.08719139394667,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 8.619935744439196,
      "theta_minus": 1.106199382177192,
      "success": true,
      "log_likelihood": -19.414339470751766,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 2.9461704668797313,
      "success": true,
      "log_likelihood": -18.475317053272814,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 8.931844824435933,
      "theta_minus": -0.17325325547942833,
      "success": true,
      "log_likelihood": -7.480101771999189,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.1417465061310292,
      "success": true,
      "log_likelihood": -16.865285193392737,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.0683923345786162,
      "success": true,
      "log_likelihood": -13.232293612688444,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.6065682663500183,
      "success": true,
      "log_likelihood": -16.36481906914045,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.3282783694954592,
      "success": true,
      "log_likelihood": -15.80402925540414,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 9.55677528087687,
      "theta_minus": 1.5189512110546954,
      "success": true,
      "log_likelihood": -24.45773876230051,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 2.093694975824326,
      "success": true,
      "log_likelihood": -11.614332843203938,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -2.598991800467078,
      "theta_minus": 0.8640518967118429,
      "success": true,
      "log_likelihood": -34.441237882791775,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.7286071551418861,
      "success": true,
      "log_likelihood": -18.61306830231947,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -2.598987047341611,
      "theta_minus": 3.027281709303541,
      "success": true,
      "log_likelihood": -28.87913846028269,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 8.512646042128123,
      "theta_minus": 1.0725878728414588,
      "success": true,
      "log_likelihood": -21.921818019132107,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -2.598999135397135,
      "theta_minus": 0.7679161788280705,
      "success": true,
      "log_likelihood": -27.066010350689726,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.6727159790715795,
      "success": true,
      "log_likelihood": -27.510017335322534,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 2.680167243439658,
      "success": true,
      "log_likelihood": -5.042905597191781,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 2.802914920173102,
      "success": true,
      "log_likelihood": -11.798884985236452,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -0.12180431704609311,
      "success": true,
      "log_likelihood": -15.839982995212827,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -2.5989813769349372,
      "theta_minus": 4.7231605880705905,
      "success": true,
      "log_likelihood": -20.095011570398373,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.39197689650581335,
      "success": true,
      "log_likelihood": -14.552600644826537,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.9372865213658205,
      "success": true,
      "log_likelihood": -23.140624488521727,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 5.894356423098752,
      "theta_minus": 1.5189369993092683,
      "success": true,
      "log_likelihood": -15.458447164672007,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -2.5989736896667606,
      "theta_minus": 6.560679568524497,
      "success": true,
      "log_likelihood": -18.2527101147705,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 6.193125437433026,
      "theta_minus": 1.2532084114775475,
      "success": true,
      "log_likelihood": -26.957936392740457,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 3.0271955057107736,
      "success": true,
      "log_likelihood": -14.279475577058047,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.1732476636919928,
      "success": true,
      "log_likelihood": -14.59998256765009,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 4.072111263548873,
      "theta_minus": 0.2023258565464354,
      "success": true,
      "log_likelihood": -8.441816711159392,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 6.745342645745398,
      "theta_minus": 0.38901435857082894,
      "success": true,
      "log_likelihood": -11.761583994895227,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.0233744587074112,
      "success": true,
      "log_likelihood": -16.60757146649202,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": -10.0,
      "success": true,
      "log_likelihood": -69.0871891735031,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 10.0,
      "success": true,
      "log_likelihood": -1.344831584348912,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -2.598992231048208,
      "theta_minus": 3.265576800093525,
      "success": true,
      "log_likelihood": -25.772838813427622,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.2777243973019226,
      "success": true,
      "log_likelihood": -16.258270640054185,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.4631182121584812,
      "success": true,
      "log_likelihood": -15.41332197670849,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -4.391568466886386,
      "theta_minus": 1.0726011906277557,
      "success": true,
      "log_likelihood": -28.36133920189513,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.9074740889462998,
      "success": true,
      "log_likelihood": -13.447416019980837,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.08488627840060975,
      "success": true,
      "log_likelihood": -7.768779387235217,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -10.0,
      "theta_minus": 5.97835531853414,
      "success": true,
      "log_likelihood": -20.918639390109533,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.6870688644614439,
      "success": true,
      "log_likelihood": -30.943189287011585,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -2.5989973118139065,
      "theta_minus": 6.56068882656806,
      "success": true,
      "log_likelihood": -18.252710114667753,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.0110804593804734,
      "success": true,
      "log_likelihood": -32.515260407464346,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 6.525377769497406,
      "theta_minus": 0.02248607259304598,
      "success": true,
      "log_likelihood": -12.825957864088387,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -10.0,
      "theta_minus": 4.573718733922798,
      "success": true,
      "log_likelihood": -15.429243893827906,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.318067544052155,
      "success": true,
      "log_likelihood": -10.500968728709758,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.7612941338993132,
      "success": true,
      "log_likelihood": -36.995728176095994,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 6.558051247171081,
      "success": true,
      "log_likelihood": -15.653048807494146,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.7913324564345727,
      "success": true,
      "log_likelihood": -19.597774924249247,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": -4.391361408538569,
      "theta_minus": 2.836604242692358,
      "success": true,
      "log_likelihood": -17.665556327986152,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 4.583443220719707,
      "theta_minus": 0.12228674892516862,
      "success": true,
      "log_likelihood": -6.280206602962541,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 1.2532035564952215,
      "success": true,
      "log_likelihood": -14.957338471328123,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta_plus": 10.0,
      "theta_minus": 0.631031617728826,
      "success": true,
      "log_likelihood": -26.42228126516735,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "theta_plus": {
        "r": 0.8553592504479895,
        "p": 9.521532490726714e-30
      },
      "theta_minus": {
        "r": 0.928368146389063,
        "p": 6.476141937742325e-44
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 42.7347333541112,
  "bic_results": {
    "average_bic": 42.7347333541112,
    "individual_bics": [
      43.6724581066557,
      42.27981685299349,
      27.54893097760261,
      35.3768334442221,
      26.788583512318063,
      29.487537979205207,
      7.18631508456286,
      29.595330460953768,
      7.18631508456286,
      29.896541228548436,
      18.25823353507719,
      41.974518849041225,
      24.800684970604237,
      7.18631508456286,
      7.18631508456286,
      7.18631508456286,
      7.18631508456286,
      19.52099990497274,
      22.403457318465847,
      7.18631508456286,
      38.775096391635586,
      24.338644664269573,
      53.65566559813267,
      43.76940357343182,
      36.16892746735428,
      37.457088170170024,
      30.405024399943862,
      15.512616387062291,
      26.044739124166462,
      373.99967738358646,
      7.18631508456286,
      43.67245810623384,
      64.86360989772596,
      42.0559653134437,
      7.18631508456286,
      33.84023031362602,
      24.34204734412166,
      36.56740793869549,
      45.489072437796224,
      145.34142066480555,
      45.99571681841575,
      44.117671983457846,
      22.1272414209106,
      40.89760826369769,
      33.63162510228911,
      39.896676015193115,
      38.7750963877205,
      56.08251540151324,
      30.395703563320097,
      76.04951364249577,
      44.393174481551156,
      64.9253147974776,
      51.01067391517643,
      61.29905857829167,
      62.187072547557285,
      17.25284907129578,
      30.76480784738512,
      38.847003867337875,
      47.357061017708965,
      36.27223916656529,
      53.44828685395567,
      38.083932206256236,
      43.67245810645322,
      61.08291066239313,
      35.72598903102831,
      36.3670030122124,
      24.050671299231006,
      30.690205866702676,
      40.382180809896255,
      145.34141622391843,
      9.856701045610045,
      58.71271550376746,
      39.68357915702059,
      37.9936818303292,
      63.88971628070248,
      34.061869916873896,
      22.70459665138265,
      49.004316657131284,
      69.05341645093539,
      43.672458106247724,
      72.19755869184091,
      32.81895360508899,
      38.02552566456803,
      28.168975334331734,
      81.1584942291042,
      38.473135491900514,
      46.36258772541071,
      42.49815053288452,
      19.7274510828373,
      37.08171481956847,
      60.01160040724692
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- theta_plus: r = 0.855\n- theta_minus: r = 0.928",
  "previous_models": [
    "Specification: U = split_self + theta_plus * max(split_self - 3 * token_self, 0) + theta_minus * min(split_self - 3 * token_self, 0)\nSummary: A utility model combining direct monetary gain and asymmetric sensitivity to fairness deviations. The utility incorporates the proposed amount (split_self) and adjusts it by two parameters: theta_plus scales utility when the offer exceeds the participant's fair share (3 * token_self), while theta_minus scales utility when the offer falls short. This captures both self-interest and inequity aversion with separate parameters for advantageous and disadvantageous inequity.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 42.7347333541112\nOverall Accuracy: 0.7949\n\nParameter Recovery:\n- theta_plus: r = 0.855\n- theta_minus: r = 0.928"
  ]
}