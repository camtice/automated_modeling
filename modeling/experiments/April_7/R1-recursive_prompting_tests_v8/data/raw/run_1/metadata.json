{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). \n\nFirst, I need to understand the variables provided and how they might influence the responder's decision. The key variables here are token_self (participant's contribution), token_opp (opponent's contribution), combined_earning (total money), split_self (proposed share for the participant), and split_opp (proposed share for the opponent). Also, trial_type indicates whether contributions are equal, opponent more, or participant more. \n\nThe model should predict the utility of accepting the proposal, which then translates into a probability via a logistic function. The utility should be a function of the fairness of the split relative to the contributions. \n\nI recall that in ultimatum game models, people often consider fairness. Here, the split might be compared to what's considered fair based on contributions. The fair split would be proportional to each person's contribution. For example, if the participant contributed more tokens, they might expect a larger share.\n\nSo, the utility could be based on the difference between the proposed split and the fair split. Let's define the fair split percentage for the participant as (token_self / (token_self + token_opp)) * 100. The proposed split percentage is splitperc_self. The difference between these could be a key factor. \n\nBut wait, the combined_earning is token_self * \u00a33 + token_opp * \u00a33. So each token is worth \u00a33. The fair split would be based on the proportion of tokens each contributed. So if the participant contributed 10 tokens and the opponent 20, the total is 30 tokens. The participant's fair share would be 10/30 = 33.33%, opponent 66.67%. \n\nThe proposed split (splitperc_self) might be compared to this fair percentage. The difference (splitperc_self - fair_percent) could be a parameter. If the proposed percentage is higher than fair, the responder is more likely to accept. If lower, they might reject. \n\nBut maybe participants have different sensitivities to unfairness. Some might be more tolerant, others less. So we can model this with a parameter that scales the difference. Let's call it alpha (\u03b1). The utility could be something like \u03b1*(proposed_percent - fair_percent). \n\nBut there's also the absolute amount. Even if the split is fair, if the total amount is low, maybe the responder is less likely to accept. Wait, but combined_earning is fixed per trial. Hmm. Alternatively, the utility might also depend on the actual monetary amount they receive. For example, split_self (the amount) could be a factor. But since combined_earning varies, maybe the relative split is more important. \n\nAlternatively, the utility could combine both the fairness in percentage terms and the absolute amount. But perhaps the fairness is the main driver. \n\nAnother consideration is the participant's own contribution. If they contributed more, they might feel entitled to a larger share. So the model should account for the difference between their contribution and the opponent's. \n\nWait, trial_type is 1 (equal), 2 (opponent more), 3 (participant more). This might capture the relative contributions. But maybe the model should directly use token_self and token_opp rather than trial_type, to get a continuous measure of contribution difference. \n\nSo, the fair percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference is (splitperc_self - fair_percent). Let's call this delta. \n\nThe utility U could be alpha * delta. If delta is positive (proposed is better than fair), utility increases, leading to higher acceptance probability. If delta is negative, utility decreases. \n\nBut maybe participants have a reference point. For example, they might accept if the proposed split is at least as much as their fair share minus some tolerance (beta). So, the utility could be alpha*(splitperc_self - (fair_percent - beta)). But beta could be a parameter representing the minimum acceptable percentage below the fair share. However, this might complicate things. \n\nAlternatively, the model could have a parameter that represents the responder's sensitivity to unfairness. For example, if the proposed split is less than the fair share, the responder experiences negative utility proportional to the difference. The utility function could be U = split_self - theta*(fair_self - split_self), where theta is a parameter indicating aversion to disadvantageous inequality. But this is similar to Fehr-Schmidt inequality aversion model. \n\nWait, the Fehr-Schmidt model is a common approach in such games. Let me recall: their utility function for player i is U_i = x_i - alpha * max(x_j - x_i, 0) - beta * max(x_i - x_j, 0), where x_i is i's payoff, x_j is the other's. But in this case, the responder is deciding to accept or reject, so their payoff is split_self if accepted, 0 if rejected. The opponent's payoff is split_opp if accepted, 0 if rejected. \n\nBut in the responder's decision, if they reject, both get zero. So the utility of accepting is U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0). The utility of rejecting is U_reject = 0. Then, the probability of accepting is logistic(U_accept - U_reject) = logistic(U_accept). \n\nBut in this case, alpha and beta are parameters for aversion to disadvantageous and advantageous inequality, respectively. However, in the responder's case, when they accept, their split_self is compared to split_opp. If split_self < split_opp, they experience negative utility from disadvantageous inequality. If split_self > split_opp, they might feel guilt (beta term). \n\nBut in the dataset, split_self and split_opp are given. So perhaps the model should incorporate these. However, the Fehr-Schmidt model might be a good starting point. \n\nBut the problem is that the model needs to use variables from the dataset. Let's see: split_self and split_opp are the proposed amounts. So, using those, the Fehr-Schmidt utility can be calculated. \n\nBut the model needs to predict the responder's decision. So the utility of accepting would be U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0). Then, the probability of accepting is 1 / (1 + exp(-U_accept)). Wait, but the logistic function converts a utility to a probability. However, in the Fehr-Schmidt model, the decision is deterministic if U_accept > 0. But here, we need a probabilistic model. So perhaps the utility is passed through a logistic function with a temperature parameter. \n\nAlternatively, the model could be probabilistic, where the probability of accepting is a logistic function of the utility. So, P(accept) = 1 / (1 + exp(-U_accept)). \n\nBut in this case, the utility U_accept is as per Fehr-Schmidt. However, the parameters alpha and beta would need to be learned. But the problem states that the model should have parameters that can be recovered and not have high covariance. \n\nBut Fehr-Schmidt model has two parameters (alpha and beta), which might be correlated. Maybe simplifying to a single parameter. For example, assuming that beta is zero (people don't care about advantageous inequality), which is sometimes done in simpler models. Then, the utility is split_self - alpha * max(split_opp - split_self, 0). \n\nAlternatively, another approach is to consider the ratio of the proposed split to the fair split. The fair split is (token_self / (token_self + token_opp)) * combined_earning. Wait, combined_earning is 3*(token_self + token_opp). So split_self should be compared to 3*token_self (since each token is \u00a33). Wait, no: the combined_earning is 3*(token_self + token_opp). The fair split for the participant would be 3*token_self, and the opponent's fair share is 3*token_opp. But the proposed split_self is part of the combined_earning. So, the fair split_self would be 3*token_self. But the proposed split_self could be more or less than that. \n\nWait, if the participant contributed token_self tokens, each worth \u00a33, their contribution to the pot is 3*token_self. The opponent contributed 3*token_opp. The total combined_earning is 3*(token_self + token_opp). The fair split would be each getting their own contribution. So split_self should be 3*token_self, split_opp 3*token_opp. But the proposer is offering split_self and split_opp, which sum to combined_earning. \n\nSo, the difference between the proposed split_self and the fair split_self (3*token_self) could be a key factor. Let's call this delta_self = split_self - 3*token_self. If delta_self is positive, the participant is getting more than their fair share; if negative, less. \n\nThe utility could then be modeled as U = delta_self * alpha. But maybe participants also care about the opponent's share relative to their own contribution. Alternatively, the ratio of split_self to fair_self. \n\nAlternatively, the model could use the ratio of the proposed split to the fair split. For example, (split_self / (3*token_self)) - 1. If this is positive, it's better than fair; if negative, worse. Then, multiply by a parameter alpha. \n\nBut perhaps participants have different sensitivities to under/over fair splits. For example, they might be more sensitive to getting less than their fair share (loss aversion). So the utility could be U = alpha * (split_self - 3*token_self) + beta * (split_self - 3*token_self) if split_self < 3*token_self, else gamma * (split_self - 3*token_self). But this complicates the model with multiple parameters. \n\nAlternatively, use a single parameter that scales the difference between proposed and fair. U = alpha * (split_self - 3*token_self). Then, the probability of accepting is logistic(U). \n\nBut maybe participants also consider the absolute amount. For example, even if the split is fair, if the total is small, they might be more likely to reject. But combined_earning is fixed per trial. However, the model should use variables from the dataset. \n\nAlternatively, the utility could be a function of both the absolute amount (split_self) and the fairness (split_self - fair_self). For example, U = alpha * split_self + beta * (split_self - fair_self). But this introduces two parameters. \n\nBut the problem states that parameter recovery is important, so we need to avoid too many parameters. Let's think: what's the key driver here? The fairness of the split relative to contributions. \n\nAnother angle: the participant's decision may depend on how the proposed split compares to their contribution and the opponent's. For example, if the participant contributed more, they might expect a higher split. \n\nSo, the fair split percentage is (token_self / (token_self + token_opp)) * 100. The proposed split percentage is splitperc_self. The difference between these is (splitperc_self - fair_percent). Let's call this delta_percent. \n\nThe utility could be U = alpha * delta_percent. If delta_percent is positive, higher utility. \n\nBut maybe participants also care about the absolute split percentage. For example, even if it's fair, a low percentage (like 40%) might be rejected. But in the Fehr-Schmidt model, the absolute amount is considered. \n\nAlternatively, combining both absolute and relative terms. But again, parameters may multiply. \n\nAlternatively, the utility could be U = split_self + alpha * (split_self - fair_self). Here, split_self is the absolute amount, and alpha scales the deviation from fair. If alpha is positive, getting more than fair increases utility. If alpha is negative, perhaps they dislike inequality. Wait, this might not capture the right direction. \n\nAlternatively, U = split_self - alpha * |split_self - fair_self|. This would penalize deviations from the fair split, whether positive or negative. But maybe participants only care about being below fair. \n\nHmm. Let's think about parameter count. The problem says to have a slight bias towards fewer parameters. So maybe a single parameter model. \n\nIf we use the Fehr-Schmidt approach with only alpha (disadvantageous inequality aversion), then the utility is split_self - alpha * max(split_opp - split_self, 0). Then, the probability of accept is logistic(U). \n\nBut split_opp is the opponent's proposed share, which is combined_earning - split_self. So split_opp = combined_earning - split_self. So max(split_opp - split_self, 0) = max(combined_earning - 2*split_self, 0). \n\nAlternatively, the model can be written in terms of split_self and combined_earning. But perhaps using split_opp directly is clearer. \n\nSo, the utility U = split_self - alpha * max(split_opp - split_self, 0). \n\nThis model has one parameter, alpha, which represents the aversion to disadvantageous inequality. If alpha is high, the participant is more likely to reject offers where the opponent gets more. \n\nBut in this case, when split_opp > split_self, the participant experiences a loss utility proportional to the difference. \n\nThis seems plausible. The probability of accepting would then be 1 / (1 + exp(-U)). But wait, the utility here is in monetary terms. The logistic function would convert this to a probability. However, the scale of U depends on the monetary amounts. For example, if split_self is \u00a310, and alpha is 0.5, then U could be 10 - 0.5*(split_opp - split_self). But split_opp could be higher, leading to a negative U. \n\nBut the logistic function's output is the probability of accepting. So if U is positive, the probability is above 0.5; if negative, below. \n\nBut the problem states that the utility should be such that less than 0 corresponds to less than 50% likelihood. Which aligns with the logistic function. \n\nSo this model could work. The parameter alpha is learnable, with bounds. Since alpha is a scaling factor on the inequality term, it should be non-negative (as aversion can't be negative). So alpha's range could be [0, some upper bound, like 5]. \n\nAlternatively, maybe alpha can be any positive number, but in practice, setting a reasonable upper bound. \n\nAnother consideration: the model should use variables available in the dataset. The variables split_self and split_opp are available. So the model can use those. \n\nSo the model would be:\n\nU = split_self - alpha * max(split_opp - split_self, 0)\n\nThen, P(accept) = 1 / (1 + exp(-U))\n\nBut wait, the utility here is in pounds. If split_self is \u00a310 and split_opp is \u00a320, then U = 10 - alpha*(10). If alpha is 0.5, U=10-5=5. Probability is ~0.993. If alpha is 2, U=10-20= -10, probability ~0.000045. \n\nBut in this case, higher alpha makes the participant more sensitive to disadvantageous inequality. \n\nThis model has one learnable parameter, alpha, which is bounded between 0 and, say, 5. \n\nBut maybe the model should also include a term for the participant's own contribution. For example, if the participant contributed more tokens, they might have a higher expectation. But how to incorporate that? \n\nAlternatively, the fair split is 3*token_self, so the difference between split_self and 3*token_self could be another term. \n\nSo U = split_self - alpha * max(split_opp - split_self, 0) + beta * (split_self - 3*token_self)\n\nBut this introduces another parameter, beta. However, the problem suggests a slight bias towards fewer parameters. So maybe stick with one parameter. \n\nAlternatively, combine both ideas. For example, the utility is based on the deviation from the fair split and the inequality. \n\nBut that would require two parameters. Let's see: \n\nU = (split_self - 3*token_self) - alpha * max(split_opp - split_self, 0)\n\nHere, the first term is the difference between the proposed split and the fair split (3*token_self). The second term is the inequality aversion. \n\nBut this model has two parameters: maybe a scaling factor on the first term and alpha. Wait, no. If (split_self - 3*token_self) is the term, then it's a linear term. But perhaps the model would have:\n\nU = beta*(split_self - 3*token_self) - alpha * max(split_opp - split_self, 0)\n\nHere, beta and alpha are parameters. But this introduces two parameters. However, the problem states that parameter recovery is a priority, so having two parameters might be okay if they are not highly correlated. \n\nBut maybe beta can be set to 1, assuming that participants value each pound equally. Then, the model reduces to U = (split_self - 3*token_self) - alpha * max(split_opp - split_self, 0). \n\nBut this assumes that the utility is directly the monetary gain over the fair share minus the inequality term. However, this might not capture the fact that participants might just care about the absolute split, not relative to their contribution. \n\nAlternatively, the participant's utility could be a combination of their monetary gain and a fairness term. \n\nBut given the dataset variables, perhaps the Fehr-Schmidt model with one parameter (alpha) is a good starting point. \n\nAnother angle: the participant's decision may depend on the ratio of their split to the opponent's. For example, if the split is 40-60, they might reject even if their absolute amount is okay. \n\nBut modeling that ratio would require a different approach. \n\nAlternatively, the model could use the difference in split percentages. For example, (splitperc_self - splitperc_opp) as a measure of fairness. But splitperc_opp is 100 - splitperc_self. So the difference is 2*splitperc_self - 100. \n\nBut this might not capture the contribution-based fairness. \n\nAlternatively, the fair split percentage is (token_self / (token_self + token_opp)) * 100. The proposed splitperc_self minus this fair percentage is the delta. The utility could be alpha * delta. \n\nSo U = alpha * (splitperc_self - fair_percent)\n\nThen, P(accept) = 1 / (1 + exp(-U))\n\nThis model has one parameter, alpha. If the proposed percentage is higher than fair, utility is positive, increasing acceptance probability. \n\nBut this ignores the absolute amount. For example, a 50% split when the fair is 50% is neutral, but if the total is small, maybe they still reject. But the model doesn't account for that. However, the combined_earning varies, but the split percentage is relative. \n\nAlternatively, the utility could be alpha * (splitperc_self - fair_percent) + beta * split_self. But again, two parameters. \n\nBut given the need for parameter recovery and fewer parameters, maybe the single parameter model based on the Fehr-Schmidt approach is better. \n\nSo, to recap, the two main options are:\n\n1. Fehr-Schmidt style model with one parameter (alpha) for disadvantageous inequality aversion.\n\nU = split_self - alpha * max(split_opp - split_self, 0)\n\n2. Proportional fairness model based on deviation from fair percentage.\n\nU = alpha * (splitperc_self - fair_percent)\n\nWhere fair_percent = (token_self / (token_self + token_opp)) * 100\n\nWhich model is better? \n\nThe Fehr-Schmidt model is established in the literature for ultimatum games and similar tasks. It directly models the aversion to receiving less than the other party. The proportional model focuses on whether the split is fair relative to contributions. \n\nIn the given task, the trial_type is based on who contributed more, which suggests that the participants' contributions (token_self and token_opp) are important. Therefore, the proportional model might better capture the effect of contributions on perceived fairness. \n\nBut the Fehr-Schmidt model doesn't take contributions into account; it only looks at the split amounts. However, in this experiment, the contributions (tokens) determine the fair split. So participants might compare the proposed split to their contribution-based fair share. \n\nTherefore, combining both aspects might be better. For example, the utility could be the difference between the proposed split and the fair split, minus a term for disadvantageous inequality. \n\nBut this would require two parameters. However, the problem allows for a few parameters as long as they are not highly correlated. \n\nAlternatively, a hybrid model:\n\nU = (split_self - fair_self) - alpha * max(split_opp - split_self, 0)\n\nWhere fair_self = 3 * token_self (since each token is \u00a33). \n\nThis model has one parameter, alpha. The first term is the monetary gain over the fair share, and the second term is the inequality aversion. \n\nBut if the proposed split_self is less than fair_self, the first term is negative. The second term is also negative if split_opp > split_self. So the utility would be even lower. \n\nThis model captures both the deviation from the fair share and the inequality. But maybe the two terms are correlated. For example, if split_self is less than fair_self, then split_opp is more than fair_opp (since fair_opp is 3*token_opp). So split_opp - split_self would be (3*token_opp + (split_self - fair_self)) - split_self = 3*token_opp - fair_self. Wait, maybe not. \n\nAlternatively, the model could be simplified to focus on the difference between the proposed split and the fair split, scaled by a parameter. \n\nU = alpha * (split_self - fair_self)\n\nWhere fair_self = 3 * token_self. \n\nThis model says that the utility is proportional to how much the proposed split exceeds the fair split. If it's higher, utility is positive; lower, negative. \n\nBut this ignores the opponent's split. For example, if the participant is offered more than their fair share, but the opponent is also getting more than their fair share, maybe the participant is still happy. But according to this model, only their own split matters. \n\nAlternatively, the model could compare the split to the fair split in percentage terms. \n\nU = alpha * (splitperc_self - fair_percent)\n\nWhere fair_percent is (token_self / (token_self + token_opp)) * 100. \n\nThis model captures whether the proposed percentage is higher or lower than the fair percentage. \n\nBut this might not account for the absolute amount. For example, a 5% increase over fair in a large total is more money than in a small total. But the model uses percentages, which are relative. \n\nHowever, the problem states that two equal split_self variables do not represent the same fairness because combined_earning varies. So the model must account for the total. \n\nWait, split_self is in \u00a3, so if combined_earning is higher, the same split_self could be a smaller percentage. For example, split_self=\u00a35 when combined_earning=\u00a310 (50%) vs. split_self=\u00a35 when combined_earning=\u00a320 (25%). So the same split_self can mean different fairness levels. \n\nTherefore, using splitperc_self (percentage) would account for the combined_earning. But the fair_percent is based on the tokens, which are contributions. \n\nSo, the model using splitperc_self and fair_percent would correctly adjust for the combined_earning. \n\nTherefore, the utility function U = alpha * (splitperc_self - fair_percent) seems appropriate. \n\nThis model has one parameter, alpha. If the proposed percentage is higher than the fair percentage, utility is positive, increasing acceptance probability. \n\nBut what if the proposed percentage is lower than fair? Then U is negative, leading to lower probability of acceptance. \n\nThis model directly addresses the fairness based on contributions and adjusts for the varying combined_earning via percentages. \n\nAnother consideration: the participant's own contribution (token_self) might influence their entitlement. For example, if they contributed more, they might have a higher expectation, so the same deviation from fair_percent might have a larger effect. But this would require an interaction term, which complicates the model. \n\nAlternatively, the model could have two parameters: one for the effect of the percentage difference and another for the effect of the absolute split_self. But again, parameter count is a concern. \n\nGiven the need for simplicity and parameter recovery, the single parameter model using the percentage difference seems reasonable. \n\nSo, the model would be:\n\nU = alpha * (splitperc_self - fair_percent)\n\nWhere fair_percent = (token_self / (token_self + token_opp)) * 100\n\nThen, P(accept) = 1 / (1 + exp(-U))\n\nThis model uses splitperc_self and token_self and token_opp from the dataset. \n\nThe parameter alpha is learnable. If alpha is positive, participants are more likely to accept when the proposed percentage exceeds the fair percentage. \n\nBut what if alpha is negative? That would imply that participants are more likely to accept when the proposed percentage is below the fair, which doesn't make sense. So alpha should be constrained to be non-negative. \n\nTherefore, the bounds for alpha would be [0, some upper limit]. Let's say 0 to 5, as a generous but reasonable range. \n\nNow, checking the variables in the dataset: splitperc_self is available, as well as token_self and token_opp. So fair_percent can be calculated as (token_self / (token_self + token_opp)) * 100. \n\nThis model meets the requirements: it uses available variables, predicts responder behavior (accept), has one learnable parameter with bounds, and the utility can be passed through a logistic function. \n\nAnother possible consideration: participants might have a non-linear sensitivity to the percentage difference. For example, they might be more sensitive to small deviations when the fair percentage is around 50%. But adding such non-linearity would require more parameters, which we want to avoid. \n\nTherefore, the linear model is appropriate. \n\nIn summary, the model uses the difference between the proposed percentage and the fair percentage (based on contributions) scaled by a learnable parameter alpha. The utility is this product, and the probability of acceptance is the logistic function of the utility. \n\nNow, let's formalize this into the required tags. \n\nThe mathematical model would be:\n\nU = \u03b1 * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nBut wait, splitperc_self is already a percentage. So the fair_percent is (token_self / (token_self + token_opp)) * 100. So the difference is splitperc_self - fair_percent. \n\nSo the model is:\n\nU = \u03b1 * (splitperc_self - fair_percent)\n\nWhere fair_percent is calculated as (token_self / (token_self + token_opp)) * 100. \n\nBut in the <MODEL> tags, we need to write the formula using the variable names from the dataset. \n\nIn the dataset, splitperc_self is available, and token_self and token_opp are available. So fair_percent can be computed as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the model can be written as:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nBut in mathematical notation, using LaTeX:\n\nU = \\alpha \\left( \\text{splitperc\\_self} - \\left( \\frac{\\text{token\\_self}}{\\text{token\\_self} + \\text{token\\_opp}} \\times 100 \\right) \\right )\n\nBut in the <MODEL> tag, we need to use variable names as they are. So:\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n</MODEL>\n\nThe variables involved are:\n\n- alpha: learnable parameter, range [0, 5]\n- splitperc_self: from data\n- token_self: from data\n- token_opp: from data\n\nThe target variable is 'accept'. \n\nNow, the variable descriptions in JSON:\n\nFor alpha:\n\n{\n  \"alpha\": {\n    \"description\": \"Sensitivity to deviation from fair percentage split\",\n    \"range\": {\n      \"min\": 0,\n      \"max\": 5,\n      \"inclusive_min\": true,\n      \"inclusive_max\": true\n    },\n    \"learnable\": true,\n    \"source\": \"learnable\"\n  },\n  \"splitperc_self\": {\n    \"description\": \"Proposed percentage share for the participant\",\n    \"range\": {\n      \"min\": 0,\n      \"max\": 100,\n      \"inclusive_min\": true,\n      \"inclusive_max\": true\n    },\n    \"learnable\": false,\n    \"source\": \"data\"\n  },\n  \"token_self\": {\n    \"description\": \"Number of tokens found by the participant\",\n    \"range\": {\n      \"min\": 0,\n      \"max\": \"inf\",\n      \"inclusive_min\": true,\n      \"inclusive_max\": false\n    },\n    \"learnable\": false,\n    \"source\": \"data\"\n  },\n  \"token_opp\": {\n    \"description\": \"Number of tokens found by the opponent\",\n    \"range\": {\n      \"min\": 0,\n      \"max\": \"inf\",\n      \"inclusive_min\": true,\n      \"inclusive_max\": false\n    },\n    \"learnable\": false,\n    \"source\": \"data\"\n  }\n}\n\nThe utility U is calculated, so it's not learnable. \n\nThe summary should explain that the model captures the responder's sensitivity to how much the proposed percentage deviates from their contribution-based fair share. \n\nSo the summary would be:\n\n<SUMMARY>\nA utility model where the participant's acceptance probability depends on the deviation of the proposed percentage split from their contribution-based fair share. The model uses a single learnable parameter (\u03b1) representing sensitivity to unfairness, with higher values indicating greater aversion to offers below the fair percentage.\n</SUMMARY>\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair percentage. Positive values increase rejection likelihood when the proposed split is below the fair percentage.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant (0-100%)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant (self-contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent (opponent's contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA fairness-driven utility model where acceptance probability depends on the deviation between the proposed percentage split and the contribution-based fair share. The single learnable parameter (\u03b1) quantifies sensitivity to unfair offers, with higher values increasing rejection likelihood when the proposal is below the fair percentage. The model accounts for varying total earnings by using percentage terms relative to contributions.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                tags, we need to write the formula using the variable names from the dataset. \n\nIn the dataset, splitperc_self is available, and token_self and token_opp are available. So fair_percent can be computed as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the model can be written as:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nBut in mathematical notation, using LaTeX:\n\nU = \\alpha \\left( \\text{splitperc\\_self} - \\left( \\frac{\\text{token\\_self}}{\\text{token\\_self} + \\text{token\\_opp}} \\times 100 \\right) \\right )\n\nBut in the <MODEL> tag, we need to use variable names as they are. So:\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair percentage. Positive values increase rejection likelihood when the proposed split is below the fair percentage.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant (0-100%)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant (self-contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent (opponent's contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        fair_percent = (token_self / (token_self + token_opp)) * 100 if (token_self + token_opp) > 0 else 50  # avoid divide by zero\n\n        U = alpha * (splitperc_self - fair_percent)\n\n        model_predictions.append(U)  # directly append utility\n\n    return model_predictions\n```",
      "timestamp": 1744033258.5696619
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair percentage. Positive values increase rejection likelihood when the proposed split is below the fair percentage.",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "tags, we need to write the formula using the variable names from the dataset. \n\nIn the dataset, splitperc_self is available, and token_self and token_opp are available. So fair_percent can be computed as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the model can be written as:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nBut in mathematical notation, using LaTeX:\n\nU = \\alpha \\left( \\text{splitperc\\_self} - \\left( \\frac{\\text{token\\_self}}{\\text{token\\_self} + \\text{token\\_opp}} \\times 100 \\right) \\right )\n\nBut in the <MODEL> tag, we need to use variable names as they are. So:\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)",
  "variable_descriptions": {
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair percentage. Positive values increase rejection likelihood when the proposed split is below the fair percentage.",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "splitperc_self": {
      "description": "Proposed percentage share for the participant (0-100%)",
      "range": {
        "min": 0,
        "max": 100,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant (self-contribution)",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent (opponent's contribution)",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model where the participant's acceptance probability depends on the deviation of the proposed percentage split from their contribution-based fair share. The model uses a single learnable parameter (\u03b1) representing sensitivity to unfairness, with higher values indicating greater aversion to offers below the fair percentage.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). \n\nFirst, I need to understand the variables provided and how they might influence the responder's decision. The key variables here are token_self (participant's contribution), token_opp (opponent's contribution), combined_earning (total money), split_self (proposed share for the participant), and split_opp (proposed share for the opponent). Also, trial_type indicates whether contributions are equal, opponent more, or participant more. \n\nThe model should predict the utility of accepting the proposal, which then translates into a probability via a logistic function. The utility should be a function of the fairness of the split relative to the contributions. \n\nI recall that in ultimatum game models, people often consider fairness. Here, the split might be compared to what's considered fair based on contributions. The fair split would be proportional to each person's contribution. For example, if the participant contributed more tokens, they might expect a larger share.\n\nSo, the utility could be based on the difference between the proposed split and the fair split. Let's define the fair split percentage for the participant as (token_self / (token_self + token_opp)) * 100. The proposed split percentage is splitperc_self. The difference between these could be a key factor. \n\nBut wait, the combined_earning is token_self * \u00a33 + token_opp * \u00a33. So each token is worth \u00a33. The fair split would be based on the proportion of tokens each contributed. So if the participant contributed 10 tokens and the opponent 20, the total is 30 tokens. The participant's fair share would be 10/30 = 33.33%, opponent 66.67%. \n\nThe proposed split (splitperc_self) might be compared to this fair percentage. The difference (splitperc_self - fair_percent) could be a parameter. If the proposed percentage is higher than fair, the responder is more likely to accept. If lower, they might reject. \n\nBut maybe participants have different sensitivities to unfairness. Some might be more tolerant, others less. So we can model this with a parameter that scales the difference. Let's call it alpha (\u03b1). The utility could be something like \u03b1*(proposed_percent - fair_percent). \n\nBut there's also the absolute amount. Even if the split is fair, if the total amount is low, maybe the responder is less likely to accept. Wait, but combined_earning is fixed per trial. Hmm. Alternatively, the utility might also depend on the actual monetary amount they receive. For example, split_self (the amount) could be a factor. But since combined_earning varies, maybe the relative split is more important. \n\nAlternatively, the utility could combine both the fairness in percentage terms and the absolute amount. But perhaps the fairness is the main driver. \n\nAnother consideration is the participant's own contribution. If they contributed more, they might feel entitled to a larger share. So the model should account for the difference between their contribution and the opponent's. \n\nWait, trial_type is 1 (equal), 2 (opponent more), 3 (participant more). This might capture the relative contributions. But maybe the model should directly use token_self and token_opp rather than trial_type, to get a continuous measure of contribution difference. \n\nSo, the fair percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference is (splitperc_self - fair_percent). Let's call this delta. \n\nThe utility U could be alpha * delta. If delta is positive (proposed is better than fair), utility increases, leading to higher acceptance probability. If delta is negative, utility decreases. \n\nBut maybe participants have a reference point. For example, they might accept if the proposed split is at least as much as their fair share minus some tolerance (beta). So, the utility could be alpha*(splitperc_self - (fair_percent - beta)). But beta could be a parameter representing the minimum acceptable percentage below the fair share. However, this might complicate things. \n\nAlternatively, the model could have a parameter that represents the responder's sensitivity to unfairness. For example, if the proposed split is less than the fair share, the responder experiences negative utility proportional to the difference. The utility function could be U = split_self - theta*(fair_self - split_self), where theta is a parameter indicating aversion to disadvantageous inequality. But this is similar to Fehr-Schmidt inequality aversion model. \n\nWait, the Fehr-Schmidt model is a common approach in such games. Let me recall: their utility function for player i is U_i = x_i - alpha * max(x_j - x_i, 0) - beta * max(x_i - x_j, 0), where x_i is i's payoff, x_j is the other's. But in this case, the responder is deciding to accept or reject, so their payoff is split_self if accepted, 0 if rejected. The opponent's payoff is split_opp if accepted, 0 if rejected. \n\nBut in the responder's decision, if they reject, both get zero. So the utility of accepting is U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0). The utility of rejecting is U_reject = 0. Then, the probability of accepting is logistic(U_accept - U_reject) = logistic(U_accept). \n\nBut in this case, alpha and beta are parameters for aversion to disadvantageous and advantageous inequality, respectively. However, in the responder's case, when they accept, their split_self is compared to split_opp. If split_self < split_opp, they experience negative utility from disadvantageous inequality. If split_self > split_opp, they might feel guilt (beta term). \n\nBut in the dataset, split_self and split_opp are given. So perhaps the model should incorporate these. However, the Fehr-Schmidt model might be a good starting point. \n\nBut the problem is that the model needs to use variables from the dataset. Let's see: split_self and split_opp are the proposed amounts. So, using those, the Fehr-Schmidt utility can be calculated. \n\nBut the model needs to predict the responder's decision. So the utility of accepting would be U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0). Then, the probability of accepting is 1 / (1 + exp(-U_accept)). Wait, but the logistic function converts a utility to a probability. However, in the Fehr-Schmidt model, the decision is deterministic if U_accept > 0. But here, we need a probabilistic model. So perhaps the utility is passed through a logistic function with a temperature parameter. \n\nAlternatively, the model could be probabilistic, where the probability of accepting is a logistic function of the utility. So, P(accept) = 1 / (1 + exp(-U_accept)). \n\nBut in this case, the utility U_accept is as per Fehr-Schmidt. However, the parameters alpha and beta would need to be learned. But the problem states that the model should have parameters that can be recovered and not have high covariance. \n\nBut Fehr-Schmidt model has two parameters (alpha and beta), which might be correlated. Maybe simplifying to a single parameter. For example, assuming that beta is zero (people don't care about advantageous inequality), which is sometimes done in simpler models. Then, the utility is split_self - alpha * max(split_opp - split_self, 0). \n\nAlternatively, another approach is to consider the ratio of the proposed split to the fair split. The fair split is (token_self / (token_self + token_opp)) * combined_earning. Wait, combined_earning is 3*(token_self + token_opp). So split_self should be compared to 3*token_self (since each token is \u00a33). Wait, no: the combined_earning is 3*(token_self + token_opp). The fair split for the participant would be 3*token_self, and the opponent's fair share is 3*token_opp. But the proposed split_self is part of the combined_earning. So, the fair split_self would be 3*token_self. But the proposed split_self could be more or less than that. \n\nWait, if the participant contributed token_self tokens, each worth \u00a33, their contribution to the pot is 3*token_self. The opponent contributed 3*token_opp. The total combined_earning is 3*(token_self + token_opp). The fair split would be each getting their own contribution. So split_self should be 3*token_self, split_opp 3*token_opp. But the proposer is offering split_self and split_opp, which sum to combined_earning. \n\nSo, the difference between the proposed split_self and the fair split_self (3*token_self) could be a key factor. Let's call this delta_self = split_self - 3*token_self. If delta_self is positive, the participant is getting more than their fair share; if negative, less. \n\nThe utility could then be modeled as U = delta_self * alpha. But maybe participants also care about the opponent's share relative to their own contribution. Alternatively, the ratio of split_self to fair_self. \n\nAlternatively, the model could use the ratio of the proposed split to the fair split. For example, (split_self / (3*token_self)) - 1. If this is positive, it's better than fair; if negative, worse. Then, multiply by a parameter alpha. \n\nBut perhaps participants have different sensitivities to under/over fair splits. For example, they might be more sensitive to getting less than their fair share (loss aversion). So the utility could be U = alpha * (split_self - 3*token_self) + beta * (split_self - 3*token_self) if split_self < 3*token_self, else gamma * (split_self - 3*token_self). But this complicates the model with multiple parameters. \n\nAlternatively, use a single parameter that scales the difference between proposed and fair. U = alpha * (split_self - 3*token_self). Then, the probability of accepting is logistic(U). \n\nBut maybe participants also consider the absolute amount. For example, even if the split is fair, if the total is small, they might be more likely to reject. But combined_earning is fixed per trial. However, the model should use variables from the dataset. \n\nAlternatively, the utility could be a function of both the absolute amount (split_self) and the fairness (split_self - fair_self). For example, U = alpha * split_self + beta * (split_self - fair_self). But this introduces two parameters. \n\nBut the problem states that parameter recovery is important, so we need to avoid too many parameters. Let's think: what's the key driver here? The fairness of the split relative to contributions. \n\nAnother angle: the participant's decision may depend on how the proposed split compares to their contribution and the opponent's. For example, if the participant contributed more, they might expect a higher split. \n\nSo, the fair split percentage is (token_self / (token_self + token_opp)) * 100. The proposed split percentage is splitperc_self. The difference between these is (splitperc_self - fair_percent). Let's call this delta_percent. \n\nThe utility could be U = alpha * delta_percent. If delta_percent is positive, higher utility. \n\nBut maybe participants also care about the absolute split percentage. For example, even if it's fair, a low percentage (like 40%) might be rejected. But in the Fehr-Schmidt model, the absolute amount is considered. \n\nAlternatively, combining both absolute and relative terms. But again, parameters may multiply. \n\nAlternatively, the utility could be U = split_self + alpha * (split_self - fair_self). Here, split_self is the absolute amount, and alpha scales the deviation from fair. If alpha is positive, getting more than fair increases utility. If alpha is negative, perhaps they dislike inequality. Wait, this might not capture the right direction. \n\nAlternatively, U = split_self - alpha * |split_self - fair_self|. This would penalize deviations from the fair split, whether positive or negative. But maybe participants only care about being below fair. \n\nHmm. Let's think about parameter count. The problem says to have a slight bias towards fewer parameters. So maybe a single parameter model. \n\nIf we use the Fehr-Schmidt approach with only alpha (disadvantageous inequality aversion), then the utility is split_self - alpha * max(split_opp - split_self, 0). Then, the probability of accept is logistic(U). \n\nBut split_opp is the opponent's proposed share, which is combined_earning - split_self. So split_opp = combined_earning - split_self. So max(split_opp - split_self, 0) = max(combined_earning - 2*split_self, 0). \n\nAlternatively, the model can be written in terms of split_self and combined_earning. But perhaps using split_opp directly is clearer. \n\nSo, the utility U = split_self - alpha * max(split_opp - split_self, 0). \n\nThis model has one parameter, alpha, which represents the aversion to disadvantageous inequality. If alpha is high, the participant is more likely to reject offers where the opponent gets more. \n\nBut in this case, when split_opp > split_self, the participant experiences a loss utility proportional to the difference. \n\nThis seems plausible. The probability of accepting would then be 1 / (1 + exp(-U)). But wait, the utility here is in monetary terms. The logistic function would convert this to a probability. However, the scale of U depends on the monetary amounts. For example, if split_self is \u00a310, and alpha is 0.5, then U could be 10 - 0.5*(split_opp - split_self). But split_opp could be higher, leading to a negative U. \n\nBut the logistic function's output is the probability of accepting. So if U is positive, the probability is above 0.5; if negative, below. \n\nBut the problem states that the utility should be such that less than 0 corresponds to less than 50% likelihood. Which aligns with the logistic function. \n\nSo this model could work. The parameter alpha is learnable, with bounds. Since alpha is a scaling factor on the inequality term, it should be non-negative (as aversion can't be negative). So alpha's range could be [0, some upper bound, like 5]. \n\nAlternatively, maybe alpha can be any positive number, but in practice, setting a reasonable upper bound. \n\nAnother consideration: the model should use variables available in the dataset. The variables split_self and split_opp are available. So the model can use those. \n\nSo the model would be:\n\nU = split_self - alpha * max(split_opp - split_self, 0)\n\nThen, P(accept) = 1 / (1 + exp(-U))\n\nBut wait, the utility here is in pounds. If split_self is \u00a310 and split_opp is \u00a320, then U = 10 - alpha*(10). If alpha is 0.5, U=10-5=5. Probability is ~0.993. If alpha is 2, U=10-20= -10, probability ~0.000045. \n\nBut in this case, higher alpha makes the participant more sensitive to disadvantageous inequality. \n\nThis model has one learnable parameter, alpha, which is bounded between 0 and, say, 5. \n\nBut maybe the model should also include a term for the participant's own contribution. For example, if the participant contributed more tokens, they might have a higher expectation. But how to incorporate that? \n\nAlternatively, the fair split is 3*token_self, so the difference between split_self and 3*token_self could be another term. \n\nSo U = split_self - alpha * max(split_opp - split_self, 0) + beta * (split_self - 3*token_self)\n\nBut this introduces another parameter, beta. However, the problem suggests a slight bias towards fewer parameters. So maybe stick with one parameter. \n\nAlternatively, combine both ideas. For example, the utility is based on the deviation from the fair split and the inequality. \n\nBut that would require two parameters. Let's see: \n\nU = (split_self - 3*token_self) - alpha * max(split_opp - split_self, 0)\n\nHere, the first term is the difference between the proposed split and the fair split (3*token_self). The second term is the inequality aversion. \n\nBut this model has two parameters: maybe a scaling factor on the first term and alpha. Wait, no. If (split_self - 3*token_self) is the term, then it's a linear term. But perhaps the model would have:\n\nU = beta*(split_self - 3*token_self) - alpha * max(split_opp - split_self, 0)\n\nHere, beta and alpha are parameters. But this introduces two parameters. However, the problem states that parameter recovery is a priority, so having two parameters might be okay if they are not highly correlated. \n\nBut maybe beta can be set to 1, assuming that participants value each pound equally. Then, the model reduces to U = (split_self - 3*token_self) - alpha * max(split_opp - split_self, 0). \n\nBut this assumes that the utility is directly the monetary gain over the fair share minus the inequality term. However, this might not capture the fact that participants might just care about the absolute split, not relative to their contribution. \n\nAlternatively, the participant's utility could be a combination of their monetary gain and a fairness term. \n\nBut given the dataset variables, perhaps the Fehr-Schmidt model with one parameter (alpha) is a good starting point. \n\nAnother angle: the participant's decision may depend on the ratio of their split to the opponent's. For example, if the split is 40-60, they might reject even if their absolute amount is okay. \n\nBut modeling that ratio would require a different approach. \n\nAlternatively, the model could use the difference in split percentages. For example, (splitperc_self - splitperc_opp) as a measure of fairness. But splitperc_opp is 100 - splitperc_self. So the difference is 2*splitperc_self - 100. \n\nBut this might not capture the contribution-based fairness. \n\nAlternatively, the fair split percentage is (token_self / (token_self + token_opp)) * 100. The proposed splitperc_self minus this fair percentage is the delta. The utility could be alpha * delta. \n\nSo U = alpha * (splitperc_self - fair_percent)\n\nThen, P(accept) = 1 / (1 + exp(-U))\n\nThis model has one parameter, alpha. If the proposed percentage is higher than fair, utility is positive, increasing acceptance probability. \n\nBut this ignores the absolute amount. For example, a 50% split when the fair is 50% is neutral, but if the total is small, maybe they still reject. But the model doesn't account for that. However, the combined_earning varies, but the split percentage is relative. \n\nAlternatively, the utility could be alpha * (splitperc_self - fair_percent) + beta * split_self. But again, two parameters. \n\nBut given the need for parameter recovery and fewer parameters, maybe the single parameter model based on the Fehr-Schmidt approach is better. \n\nSo, to recap, the two main options are:\n\n1. Fehr-Schmidt style model with one parameter (alpha) for disadvantageous inequality aversion.\n\nU = split_self - alpha * max(split_opp - split_self, 0)\n\n2. Proportional fairness model based on deviation from fair percentage.\n\nU = alpha * (splitperc_self - fair_percent)\n\nWhere fair_percent = (token_self / (token_self + token_opp)) * 100\n\nWhich model is better? \n\nThe Fehr-Schmidt model is established in the literature for ultimatum games and similar tasks. It directly models the aversion to receiving less than the other party. The proportional model focuses on whether the split is fair relative to contributions. \n\nIn the given task, the trial_type is based on who contributed more, which suggests that the participants' contributions (token_self and token_opp) are important. Therefore, the proportional model might better capture the effect of contributions on perceived fairness. \n\nBut the Fehr-Schmidt model doesn't take contributions into account; it only looks at the split amounts. However, in this experiment, the contributions (tokens) determine the fair split. So participants might compare the proposed split to their contribution-based fair share. \n\nTherefore, combining both aspects might be better. For example, the utility could be the difference between the proposed split and the fair split, minus a term for disadvantageous inequality. \n\nBut this would require two parameters. However, the problem allows for a few parameters as long as they are not highly correlated. \n\nAlternatively, a hybrid model:\n\nU = (split_self - fair_self) - alpha * max(split_opp - split_self, 0)\n\nWhere fair_self = 3 * token_self (since each token is \u00a33). \n\nThis model has one parameter, alpha. The first term is the monetary gain over the fair share, and the second term is the inequality aversion. \n\nBut if the proposed split_self is less than fair_self, the first term is negative. The second term is also negative if split_opp > split_self. So the utility would be even lower. \n\nThis model captures both the deviation from the fair share and the inequality. But maybe the two terms are correlated. For example, if split_self is less than fair_self, then split_opp is more than fair_opp (since fair_opp is 3*token_opp). So split_opp - split_self would be (3*token_opp + (split_self - fair_self)) - split_self = 3*token_opp - fair_self. Wait, maybe not. \n\nAlternatively, the model could be simplified to focus on the difference between the proposed split and the fair split, scaled by a parameter. \n\nU = alpha * (split_self - fair_self)\n\nWhere fair_self = 3 * token_self. \n\nThis model says that the utility is proportional to how much the proposed split exceeds the fair split. If it's higher, utility is positive; lower, negative. \n\nBut this ignores the opponent's split. For example, if the participant is offered more than their fair share, but the opponent is also getting more than their fair share, maybe the participant is still happy. But according to this model, only their own split matters. \n\nAlternatively, the model could compare the split to the fair split in percentage terms. \n\nU = alpha * (splitperc_self - fair_percent)\n\nWhere fair_percent is (token_self / (token_self + token_opp)) * 100. \n\nThis model captures whether the proposed percentage is higher or lower than the fair percentage. \n\nBut this might not account for the absolute amount. For example, a 5% increase over fair in a large total is more money than in a small total. But the model uses percentages, which are relative. \n\nHowever, the problem states that two equal split_self variables do not represent the same fairness because combined_earning varies. So the model must account for the total. \n\nWait, split_self is in \u00a3, so if combined_earning is higher, the same split_self could be a smaller percentage. For example, split_self=\u00a35 when combined_earning=\u00a310 (50%) vs. split_self=\u00a35 when combined_earning=\u00a320 (25%). So the same split_self can mean different fairness levels. \n\nTherefore, using splitperc_self (percentage) would account for the combined_earning. But the fair_percent is based on the tokens, which are contributions. \n\nSo, the model using splitperc_self and fair_percent would correctly adjust for the combined_earning. \n\nTherefore, the utility function U = alpha * (splitperc_self - fair_percent) seems appropriate. \n\nThis model has one parameter, alpha. If the proposed percentage is higher than the fair percentage, utility is positive, increasing acceptance probability. \n\nBut what if the proposed percentage is lower than fair? Then U is negative, leading to lower probability of acceptance. \n\nThis model directly addresses the fairness based on contributions and adjusts for the varying combined_earning via percentages. \n\nAnother consideration: the participant's own contribution (token_self) might influence their entitlement. For example, if they contributed more, they might have a higher expectation, so the same deviation from fair_percent might have a larger effect. But this would require an interaction term, which complicates the model. \n\nAlternatively, the model could have two parameters: one for the effect of the percentage difference and another for the effect of the absolute split_self. But again, parameter count is a concern. \n\nGiven the need for simplicity and parameter recovery, the single parameter model using the percentage difference seems reasonable. \n\nSo, the model would be:\n\nU = alpha * (splitperc_self - fair_percent)\n\nWhere fair_percent = (token_self / (token_self + token_opp)) * 100\n\nThen, P(accept) = 1 / (1 + exp(-U))\n\nThis model uses splitperc_self and token_self and token_opp from the dataset. \n\nThe parameter alpha is learnable. If alpha is positive, participants are more likely to accept when the proposed percentage exceeds the fair percentage. \n\nBut what if alpha is negative? That would imply that participants are more likely to accept when the proposed percentage is below the fair, which doesn't make sense. So alpha should be constrained to be non-negative. \n\nTherefore, the bounds for alpha would be [0, some upper limit]. Let's say 0 to 5, as a generous but reasonable range. \n\nNow, checking the variables in the dataset: splitperc_self is available, as well as token_self and token_opp. So fair_percent can be calculated as (token_self / (token_self + token_opp)) * 100. \n\nThis model meets the requirements: it uses available variables, predicts responder behavior (accept), has one learnable parameter with bounds, and the utility can be passed through a logistic function. \n\nAnother possible consideration: participants might have a non-linear sensitivity to the percentage difference. For example, they might be more sensitive to small deviations when the fair percentage is around 50%. But adding such non-linearity would require more parameters, which we want to avoid. \n\nTherefore, the linear model is appropriate. \n\nIn summary, the model uses the difference between the proposed percentage and the fair percentage (based on contributions) scaled by a learnable parameter alpha. The utility is this product, and the probability of acceptance is the logistic function of the utility. \n\nNow, let's formalize this into the required tags. \n\nThe mathematical model would be:\n\nU = \u03b1 * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nBut wait, splitperc_self is already a percentage. So the fair_percent is (token_self / (token_self + token_opp)) * 100. So the difference is splitperc_self - fair_percent. \n\nSo the model is:\n\nU = \u03b1 * (splitperc_self - fair_percent)\n\nWhere fair_percent is calculated as (token_self / (token_self + token_opp)) * 100. \n\nBut in the <MODEL> tags, we need to write the formula using the variable names from the dataset. \n\nIn the dataset, splitperc_self is available, and token_self and token_opp are available. So fair_percent can be computed as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the model can be written as:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nBut in mathematical notation, using LaTeX:\n\nU = \\alpha \\left( \\text{splitperc\\_self} - \\left( \\frac{\\text{token\\_self}}{\\text{token\\_self} + \\text{token\\_opp}} \\times 100 \\right) \\right )\n\nBut in the <MODEL> tag, we need to use variable names as they are. So:\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n</MODEL>\n\nThe variables involved are:\n\n- alpha: learnable parameter, range [0, 5]\n- splitperc_self: from data\n- token_self: from data\n- token_opp: from data\n\nThe target variable is 'accept'. \n\nNow, the variable descriptions in JSON:\n\nFor alpha:\n\n{\n  \"alpha\": {\n    \"description\": \"Sensitivity to deviation from fair percentage split\",\n    \"range\": {\n      \"min\": 0,\n      \"max\": 5,\n      \"inclusive_min\": true,\n      \"inclusive_max\": true\n    },\n    \"learnable\": true,\n    \"source\": \"learnable\"\n  },\n  \"splitperc_self\": {\n    \"description\": \"Proposed percentage share for the participant\",\n    \"range\": {\n      \"min\": 0,\n      \"max\": 100,\n      \"inclusive_min\": true,\n      \"inclusive_max\": true\n    },\n    \"learnable\": false,\n    \"source\": \"data\"\n  },\n  \"token_self\": {\n    \"description\": \"Number of tokens found by the participant\",\n    \"range\": {\n      \"min\": 0,\n      \"max\": \"inf\",\n      \"inclusive_min\": true,\n      \"inclusive_max\": false\n    },\n    \"learnable\": false,\n    \"source\": \"data\"\n  },\n  \"token_opp\": {\n    \"description\": \"Number of tokens found by the opponent\",\n    \"range\": {\n      \"min\": 0,\n      \"max\": \"inf\",\n      \"inclusive_min\": true,\n      \"inclusive_max\": false\n    },\n    \"learnable\": false,\n    \"source\": \"data\"\n  }\n}\n\nThe utility U is calculated, so it's not learnable. \n\nThe summary should explain that the model captures the responder's sensitivity to how much the proposed percentage deviates from their contribution-based fair share. \n\nSo the summary would be:\n\n<SUMMARY>\nA utility model where the participant's acceptance probability depends on the deviation of the proposed percentage split from their contribution-based fair share. The model uses a single learnable parameter (\u03b1) representing sensitivity to unfairness, with higher values indicating greater aversion to offers below the fair percentage.\n</SUMMARY>\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair percentage. Positive values increase rejection likelihood when the proposed split is below the fair percentage.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant (0-100%)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant (self-contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent (opponent's contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA fairness-driven utility model where acceptance probability depends on the deviation between the proposed percentage split and the contribution-based fair share. The single learnable parameter (\u03b1) quantifies sensitivity to unfair offers, with higher values increasing rejection likelihood when the proposal is below the fair percentage. The model accounts for varying total earnings by using percentage terms relative to contributions.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: tags, we need to write the formula using the variable names from the dataset. \n\nIn the dataset, splitperc_self is available, and token_self and token_opp are available. So fair_percent can be computed as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the model can be written as:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nBut in mathematical notation, using LaTeX:\n\nU = \\alpha \\left( \\text{splitperc\\_self} - \\left( \\frac{\\text{token\\_self}}{\\text{token\\_self} + \\text{token\\_opp}} \\times 100 \\right) \\right )\n\nBut in the <MODEL> tag, we need to use variable names as they are. So:\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: A utility model where the participant's acceptance probability depends on the deviation of the proposed percentage split from their contribution-based fair share. The model uses a single learnable parameter (\u03b1) representing sensitivity to unfairness, with higher values indicating greater aversion to offers below the fair percentage.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        fair_percent = (token_self / (token_self + token_opp)) * 100 if (token_self + token_opp) > 0 else 50  # avoid divide by zero\n\n        U = alpha * (splitperc_self - fair_percent)\n\n        model_predictions.append(U)  # directly append utility\n\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.5915750915750916,
  "fitting_results": [
    {
      "alpha": 0.1699318084457721,
      "success": true,
      "log_likelihood": -8.460896738708,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.031933833190427495,
      "success": true,
      "log_likelihood": -22.44975161167134,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.039049514846133386,
      "success": true,
      "log_likelihood": -21.446297695970614,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04942326921554804,
      "success": true,
      "log_likelihood": -19.904280461791977,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06052927170389921,
      "success": true,
      "log_likelihood": -18.261585638384602,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.011246102910797966,
      "success": true,
      "log_likelihood": -24.5995817467774,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.007639164551329151,
      "success": true,
      "log_likelihood": -24.788303958944994,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0672767543368995,
      "success": true,
      "log_likelihood": -17.304291939493346,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.059837083935513596,
      "success": true,
      "log_likelihood": -18.361889142177088,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.15400773262877973,
      "success": true,
      "log_likelihood": -9.268950857393095,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05915560077829367,
      "success": true,
      "log_likelihood": -18.46104829091991,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.027752478505234997,
      "success": true,
      "log_likelihood": -22.99644540542717,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04675004364621683,
      "success": true,
      "log_likelihood": -20.304900371969826,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.023477048283857885,
      "success": true,
      "log_likelihood": -23.50829943727021,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03995137486279401,
      "success": true,
      "log_likelihood": -21.314634304459997,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.022447072155222,
      "success": true,
      "log_likelihood": -23.62310382511491,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.022447071504272994,
      "success": true,
      "log_likelihood": -23.623103825106696,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12288069945488982,
      "success": true,
      "log_likelihood": -11.323306103358872,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16993185560895,
      "success": true,
      "log_likelihood": -8.460896738709625,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.02961826657939844,
      "success": true,
      "log_likelihood": -22.757438579447648,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.053435402650021266,
      "success": true,
      "log_likelihood": -19.304600760625895,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.005271297002972896,
      "success": true,
      "log_likelihood": -24.874347839989607,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.004976825695302339,
      "success": true,
      "log_likelihood": -24.882887911229624,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.036860056515003604,
      "success": true,
      "log_likelihood": -21.762525496321377,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.002337146206183981,
      "success": true,
      "log_likelihood": -24.93772210250632,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.009132644768695351,
      "success": true,
      "log_likelihood": -24.718429697397777,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11790020046209772,
      "success": true,
      "log_likelihood": -11.724471975995861,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.017165759104903314,
      "success": true,
      "log_likelihood": -24.150542964971482,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.014948107964388465,
      "success": true,
      "log_likelihood": -24.337826758644443,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.02313232943444709,
      "success": true,
      "log_likelihood": -23.547140195289327,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04323701560267146,
      "success": true,
      "log_likelihood": -20.829578866854508,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06052887477352815,
      "success": true,
      "log_likelihood": -18.261585638014346,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.005861144690299712,
      "success": true,
      "log_likelihood": -24.85579410032477,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10358219012467984,
      "success": true,
      "log_likelihood": -13.012403413764797,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.018456096342848068,
      "success": true,
      "log_likelihood": -24.03181341648202,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05526512194458617,
      "success": true,
      "log_likelihood": -19.03287932065706,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09545273792563067,
      "success": true,
      "log_likelihood": -13.840750462667707,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09697093933501758,
      "success": true,
      "log_likelihood": -13.680404198600833,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.14154252067662473,
      "success": true,
      "log_likelihood": -10.006664541906344,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.004682633578181514,
      "success": true,
      "log_likelihood": -24.890937432078726,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03393487682362586,
      "success": true,
      "log_likelihood": -22.175346582185412,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16993171212114666,
      "success": true,
      "log_likelihood": -8.460896738706586,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.02999808419796472,
      "success": true,
      "log_likelihood": -22.707758549548426,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11124826803147624,
      "success": true,
      "log_likelihood": -12.296972867021982,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.021768103881504842,
      "success": true,
      "log_likelihood": -23.696794128314075,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0174865317125545,
      "success": true,
      "log_likelihood": -24.121666085453015,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.9878427175006514,
      "success": true,
      "log_likelihood": -1.3862944776833366,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10184161321116497,
      "success": true,
      "log_likelihood": -13.183581049697114,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.025938668046729407,
      "success": true,
      "log_likelihood": -23.220124257634424,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.011550502413378138,
      "success": true,
      "log_likelihood": -24.580584661343025,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10921579235726762,
      "success": true,
      "log_likelihood": -12.480681162638838,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16993166807785826,
      "success": true,
      "log_likelihood": -8.460896738706786,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.008533804283065476,
      "success": true,
      "log_likelihood": -24.747873236690737,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10726434850806302,
      "success": true,
      "log_likelihood": -12.66107040831481,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03352908986887557,
      "success": true,
      "log_likelihood": -22.231566111502918,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3333333333333333,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.093300401771899,
      "success": true,
      "log_likelihood": -24.412147663344744,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09545273855012662,
      "success": true,
      "log_likelihood": -13.840750462205554,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03193383219337259,
      "success": true,
      "log_likelihood": -22.449751611750024,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.5648477422860672,
        "p": 9.268554300808495e-10
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 45.96760511049515,
  "bic_results": {
    "average_bic": 45.96760511049515,
    "individual_bics": [
      20.50531241587211,
      48.483022161798786,
      46.47611433039734,
      43.39207986204006,
      40.10669021522531,
      52.78268243201091,
      53.49011592437213,
      53.160126856346096,
      53.49011592437213,
      38.1921028174428,
      40.307297222810284,
      22.1214206532423,
      40.505615520295926,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      49.57640974931045,
      53.49011592437213,
      44.19331968239576,
      50.60011781299653,
      46.2127875473761,
      50.82972658868593,
      50.8297265886695,
      26.230131145173853,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      20.50531241587536,
      49.098396097351404,
      42.1927204597079,
      53.49011592437213,
      53.33221461843532,
      53.349294760915356,
      47.10856993109886,
      53.45896314346875,
      53.49011592437213,
      53.020378333251664,
      27.03246289044783,
      53.49011592437213,
      51.88460486839907,
      52.259172455744995,
      53.49011592437213,
      50.67779932903476,
      45.242676672165125,
      40.1066902144848,
      53.295107139105646,
      53.49011592437213,
      29.608325765985704,
      51.64714577142015,
      53.49011592437213,
      41.64927757977023,
      31.265019863791522,
      30.944327335657775,
      53.49011592437213,
      23.596848022268798,
      53.49011592437213,
      53.36539380261356,
      47.93421210282693,
      20.50531241586928,
      48.99903603755296,
      28.177464672500072,
      50.97710719508426,
      53.49011592437213,
      53.49011592437213,
      51.82685110936214,
      53.49011592437213,
      6.356107893822783,
      29.950681037850337,
      50.02376745372496,
      53.49011592437213,
      52.74468826114216,
      53.49011592437213,
      53.49011592437213,
      28.544881263733785,
      53.49011592437213,
      20.505312415869682,
      53.07926541183758,
      53.49011592437213,
      28.90565975508573,
      48.046651161461945,
      53.49011592437213,
      52.407814265145596,
      53.49011592437213,
      31.265019862867216,
      53.49011592437213,
      48.48302216195616,
      53.49011592437213
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.565",
  "previous_models": [
    "Specification: tags, we need to write the formula using the variable names from the dataset. \n\nIn the dataset, splitperc_self is available, and token_self and token_opp are available. So fair_percent can be computed as (token_self / (token_self + token_opp)) * 100. \n\nTherefore, the model can be written as:\n\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nBut in mathematical notation, using LaTeX:\n\nU = \\alpha \\left( \\text{splitperc\\_self} - \\left( \\frac{\\text{token\\_self}}{\\text{token\\_self} + \\text{token\\_opp}} \\times 100 \\right) \\right )\n\nBut in the <MODEL> tag, we need to use variable names as they are. So:\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: A utility model where the participant's acceptance probability depends on the deviation of the proposed percentage split from their contribution-based fair share. The model uses a single learnable parameter (\u03b1) representing sensitivity to unfairness, with higher values indicating greater aversion to offers below the fair percentage.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 45.96760511049515\nOverall Accuracy: 0.5916\n\nParameter Recovery:\n- alpha: r = 0.565"
  ]
}