{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\nOkay, let's analyze the previous results and think about how to improve the instructions. The previous model had a BIC of around 46 and an accuracy of about 59%, which isn't terrible but there's room for improvement. The parameter recovery for alpha was a correlation of 0.565, which is moderate but could be better. \n\nFirst, the model used a single parameter (alpha) to capture sensitivity to unfairness based on the deviation from the fair percentage. The parameter recovery here might be low because the model is too simplistic. Maybe adding another parameter could help capture more variance in the data, but we need to avoid high covariance. Alternatively, the model might not be considering other factors that influence decisions, like the absolute amount offered or the context of the trial type (e.g., whether the participant contributed more or less than the opponent).\n\nThe previous instructions emphasized minimizing the number of parameters to avoid covariance. However, the current model might be underfitting. Perhaps allowing a slightly more complex model with two parameters could improve recovery and accuracy without introducing too much covariance. For example, adding a parameter that accounts for the participant's sensitivity to advantageous inequality (when they get more than fair) versus disadvantageous inequality (when they get less).\n\nAnother angle is the functional form of the utility. The linear model might not capture non-linear responses. For instance, participants might be highly sensitive to small deviations below the fair share but less so for larger deviations. Using a non-linear transformation (like a sigmoid) could help, but that might complicate the model. Alternatively, using a piecewise function with different alphas for positive and negative deviations.\n\nAlso, the previous model didn't consider the trial_type variable, which indicates whether contributions were equal, opponent more, or participant more. Maybe participants behave differently in these scenarios. For example, when they contributed more, they might be more sensitive to unfair splits. Incorporating interaction terms with trial_type could improve the model.\n\nThe instructions should encourage exploring these possibilities. Suggesting models that include trial_type interactions, non-linear terms, or additional parameters for different inequality types. However, the instructions need to balance complexity and parameter recovery. Maybe allowing up to two parameters if they are orthogonal in their effects.\n\nAdditionally, the model's utility directly translates to the probability via a logistic function. If the utility's scale is off, the probabilities might not be well-calibrated. Including a temperature parameter (beta) to scale the utility before applying the logistic function could help. For example, U = beta * (alpha * delta), where beta controls the steepness of the probability curve. This adds a parameter but might improve fit.\n\nAnother consideration is the reference point. The fair percentage is based on contributions, but participants might have different reference points, like expecting an equal split regardless of contributions. Including a parameter that weights contribution-based fairness versus equal split could capture individual differences.\n\nThe new instructions should encourage testing these variations while still being cautious about parameter covariance. Emphasize exploring both linear and non-linear relationships, considering trial context, and possibly introducing a second parameter if justified. Also, suggest checking if including absolute terms (like split_self) alongside relative terms improves the model.\n\nFinally, the previous model's accuracy was around 59%, which is better than chance but leaves room for improvement. Encouraging the model to think beyond simple fairness models, like incorporating risk aversion or social preferences, might help. For example, participants might reject low offers not just due to fairness but to punish the proposer, which could be modeled with a different mechanism.\n</think>\n\n**Revised Instructions for Run 2/5:**\n\n1. **Expand Psychological Mechanisms**: Explore models that account for both *disadvantageous inequality* (rejecting low offers) and *advantageous inequality* (guilt effects for high offers). Consider piecewise utility functions or dual parameters (e.g., \u03b1\u2081 for below-fair splits, \u03b1\u2082 for above-fair splits) to capture asymmetric sensitivity.\n\n2. **Contextual Sensitivity**: Incorporate trial context (e.g., `trial_type` indicating contribution asymmetry) via interaction terms. Example:  \n   `U = \u03b1 * (splitperc_self \u2212 fair_percent) * trial_type_weight`  \n   where `trial_type_weight` modulates sensitivity based on who contributed more.\n\n3. **Non-Linear Transformations**: Test non-linear responses to unfairness (e.g., logistic/scaling parameters for diminishing sensitivity):  \n   `U = \u03b1 * tanh(splitperc_self \u2212 fair_percent)`  \n   or threshold-based models where small deviations are ignored.\n\n4. **Absolute vs. Relative Utility**: Hybrid models combining percentage fairness *and* absolute monetary amounts (e.g., `split_self`). Example:  \n   `U = \u03b1 * (splitperc_self \u2212 fair_percent) + \u03b2 * split_self`.\n\n5. **Parameter Constraints**: Allow up to **2 learnable parameters** if they target orthogonal effects (e.g., fairness sensitivity + baseline acceptance bias). Use bounded ranges (e.g., [\u22122, 5] for asymmetric effects) to guide identifiability.\n\n6. **Temperature Parameter**: Introduce a scaling parameter to the logistic function (e.g., `P(accept) = 1 / (1 + exp(\u2212\u03b2 * U))`) to better calibrate probability slopes.\n\n**Key Focus**: Prioritize models with interpretable parameters that map to distinct psychological processes. Test whether trial-by-trial variation in contribution asymmetry (`token_self` vs. `token_opp`) predicts residuals in the previous model.",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: <think>\nOkay, let's analyze the previous results and think about how to improve the instructions. The previous model had a BIC of around 46 and an accuracy of about 59%, which isn't terrible but there's room for improvement. The parameter recovery for alpha was a correlation of 0.565, which is moderate but could be better. \n\nFirst, the model used a single parameter (alpha) to capture sensitivity to unfairness based on the deviation from the fair percentage. The parameter recovery here might be low because the model is too simplistic. Maybe adding another parameter could help capture more variance in the data, but we need to avoid high covariance. Alternatively, the model might not be considering other factors that influence decisions, like the absolute amount offered or the context of the trial type (e.g., whether the participant contributed more or less than the opponent).\n\nThe previous instructions emphasized minimizing the number of parameters to avoid covariance. However, the current model might be underfitting. Perhaps allowing a slightly more complex model with two parameters could improve recovery and accuracy without introducing too much covariance. For example, adding a parameter that accounts for the participant's sensitivity to advantageous inequality (when they get more than fair) versus disadvantageous inequality (when they get less).\n\nAnother angle is the functional form of the utility. The linear model might not capture non-linear responses. For instance, participants might be highly sensitive to small deviations below the fair share but less so for larger deviations. Using a non-linear transformation (like a sigmoid) could help, but that might complicate the model. Alternatively, using a piecewise function with different alphas for positive and negative deviations.\n\nAlso, the previous model didn't consider the trial_type variable, which indicates whether contributions were equal, opponent more, or participant more. Maybe participants behave differently in these scenarios. For example, when they contributed more, they might be more sensitive to unfair splits. Incorporating interaction terms with trial_type could improve the model.\n\nThe instructions should encourage exploring these possibilities. Suggesting models that include trial_type interactions, non-linear terms, or additional parameters for different inequality types. However, the instructions need to balance complexity and parameter recovery. Maybe allowing up to two parameters if they are orthogonal in their effects.\n\nAdditionally, the model's utility directly translates to the probability via a logistic function. If the utility's scale is off, the probabilities might not be well-calibrated. Including a temperature parameter (beta) to scale the utility before applying the logistic function could help. For example, U = beta * (alpha * delta), where beta controls the steepness of the probability curve. This adds a parameter but might improve fit.\n\nAnother consideration is the reference point. The fair percentage is based on contributions, but participants might have different reference points, like expecting an equal split regardless of contributions. Including a parameter that weights contribution-based fairness versus equal split could capture individual differences.\n\nThe new instructions should encourage testing these variations while still being cautious about parameter covariance. Emphasize exploring both linear and non-linear relationships, considering trial context, and possibly introducing a second parameter if justified. Also, suggest checking if including absolute terms (like split_self) alongside relative terms improves the model.\n\nFinally, the previous model's accuracy was around 59%, which is better than chance but leaves room for improvement. Encouraging the model to think beyond simple fairness models, like incorporating risk aversion or social preferences, might help. For example, participants might reject low offers not just due to fairness but to punish the proposer, which could be modeled with a different mechanism.\n</think>\n\n**Revised Instructions for Run 2/5:**\n\n1. **Expand Psychological Mechanisms**: Explore models that account for both *disadvantageous inequality* (rejecting low offers) and *advantageous inequality* (guilt effects for high offers). Consider piecewise utility functions or dual parameters (e.g., \u03b1\u2081 for below-fair splits, \u03b1\u2082 for above-fair splits) to capture asymmetric sensitivity.\n\n2. **Contextual Sensitivity**: Incorporate trial context (e.g., `trial_type` indicating contribution asymmetry) via interaction terms. Example:  \n   `U = \u03b1 * (splitperc_self \u2212 fair_percent) * trial_type_weight`  \n   where `trial_type_weight` modulates sensitivity based on who contributed more.\n\n3. **Non-Linear Transformations**: Test non-linear responses to unfairness (e.g., logistic/scaling parameters for diminishing sensitivity):  \n   `U = \u03b1 * tanh(splitperc_self \u2212 fair_percent)`  \n   or threshold-based models where small deviations are ignored.\n\n4. **Absolute vs. Relative Utility**: Hybrid models combining percentage fairness *and* absolute monetary amounts (e.g., `split_self`). Example:  \n   `U = \u03b1 * (splitperc_self \u2212 fair_percent) + \u03b2 * split_self`.\n\n5. **Parameter Constraints**: Allow up to **2 learnable parameters** if they target orthogonal effects (e.g., fairness sensitivity + baseline acceptance bias). Use bounded ranges (e.g., [\u22122, 5] for asymmetric effects) to guide identifiability.\n\n6. **Temperature Parameter**: Introduce a scaling parameter to the logistic function (e.g., `P(accept) = 1 / (1 + exp(\u2212\u03b2 * U))`) to better calibrate probability slopes.\n\n**Key Focus**: Prioritize models with interpretable parameters that map to distinct psychological processes. Test whether trial-by-trial variation in contribution asymmetry (`token_self` vs. `token_opp`) predicts residuals in the previous model.\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal. The model needs to focus on responder behavior, use the given variables appropriately, and ensure parameter recovery is feasible.\n\nFirst, I need to identify which variables are relevant for the responder's decision. The target variable is 'accept', which is binary. The key factors influencing this decision likely involve fairness considerations based on the participants' and opponents' contributions, as well as the proposed splits.\n\nThe dataset includes variables like token_self and token_opp, which determine each party's contribution to the combined earnings. The combined_earning is the total, which is \u00a33 times the sum of tokens. The split_self and split_opp are the proposed amounts, and their percentages (splitperc_self and splitperc_opp) might be critical. \n\nFairness here isn't just about equal splits but proportional to contributions. So, the fair percentage for the participant should be token_self / (token_self + token_opp) * 100. The deviation from this fair percentage in the proposed split (splitperc_self) could be a key driver. For example, if the participant contributed 60% of the tokens, a 50% split would be unfair, whereas a 60% split would be fair.\n\nThe model should calculate utility based on this deviation. A common approach is to use a utility function that includes a term for the deviation from fairness, weighted by a parameter (e.g., \u03b1) that captures sensitivity to unfairness. If the utility is negative, the participant is more likely to reject the offer.\n\nSo, the utility (U) could be defined as the proposed percentage minus the fair percentage, multiplied by \u03b1. If U is positive, acceptance is more likely; if negative, rejection. The probability of accepting can then be modeled with a logistic function, translating utility into a probability.\n\nBut wait, the problem states that the utility should predict a probability less than 50% when utility is less than 0. The logistic function naturally does this because when U < 0, the probability is < 0.5. So, using a standard logistic link function makes sense here.\n\nNow, considering parameter recovery, the model should have learnable parameters with finite bounds. The key parameter here is \u03b1, which represents the sensitivity to unfairness. If \u03b1 is positive, it means the participant is more likely to accept when the offer is fairer. But participants might have different sensitivities, so \u03b1 needs to be learnable.\n\nAnother consideration is whether to include other variables. For example, the absolute amount offered (split_self) might matter\u2014people might accept slightly unfair offers if the absolute amount is high. However, the problem states to use the variables appropriately but not necessarily all. Since the main focus is fairness relative to contribution, maybe the percentage terms are sufficient.\n\nBut let's check the variables again. The trial_type indicates whether contributions are equal, opponent more, or participant more. This might affect the responder's perception. For example, if the participant contributed more, they might be more sensitive to unfair splits. However, including trial_type might complicate the model. Since the fair percentage already accounts for the contribution ratio, maybe trial_type is redundant. Alternatively, trial_type could interact with the fairness deviation. But to keep the model simple and ensure parameter recovery, perhaps it's better to stick with the core fairness deviation.\n\nSo, the model will focus on the difference between the proposed percentage (splitperc_self) and the fair percentage (calculated as token_self / (token_self + token_opp) * 100). The utility is this difference multiplied by \u03b1. The probability of acceptance is then the logistic of this utility.\n\nNow, the parameters. \u03b1 needs to be bounded. Since \u03b1 scales the effect of the fairness deviation, it's reasonable to set bounds that allow for both positive and negative sensitivities, but in reality, we'd expect \u03b1 to be positive (higher fairness leading to higher acceptance). However, to be safe, maybe set a range from -5 to 5. But considering that a negative \u03b1 would imply that more unfair offers (lower than fair) are more likely to be accepted, which doesn't make sense. So perhaps \u03b1 should be constrained to be non-negative. Wait, but the problem says bounds should be generous. So maybe set \u03b1 from -2 to 5, allowing for some flexibility but avoiding extreme values that could hinder parameter recovery.\n\nWait, but if \u03b1 is negative, the model would predict that as the offer becomes more unfair (lower than fair), the utility increases, leading to higher acceptance. That's counterintuitive. So perhaps \u03b1 should be constrained to be positive. But the problem says bounds should be generous. Maybe set \u03b1 from 0 to 5. That way, \u03b1 can't be negative, ensuring that higher fairness leads to higher utility. But the problem allows for the model to have parameters with generous bounds, not necessarily strictly positive. However, given the context, a negative \u03b1 doesn't make sense here. So setting \u03b1's min to 0 makes sense.\n\nAlternatively, maybe some participants have a negative \u03b1, but that's unlikely. To allow for the model to capture any possible data pattern, even if theoretically unexpected, perhaps set \u03b1 from -2 to 5. But given that the task is about responders accepting or rejecting based on fairness, a positive \u03b1 is expected. However, for parameter recovery, it's better to have a reasonable range. Let's set \u03b1 from 0 to 5, inclusive.\n\nAnother consideration: the fair percentage is calculated as (token_self / (token_self + token_opp)) * 100. But if both tokens are zero, that's a problem. However, in the dataset, token_self and token_opp are the number of tokens found, which are non-negative integers. So combined_earning is 3*(token_self + token_opp), which implies that token_self + token_opp is at least 1 (since combined_earning is an integer). So division by zero isn't an issue.\n\nNow, the utility function would be:\n\nU = \u03b1 * (splitperc_self - fair_percent)\n\nWhere fair_percent = (token_self / (token_self + token_opp)) * 100\n\nThen, the probability of acceptance is P(accept=1) = 1 / (1 + exp(-U))\n\nThis ensures that when the proposed percentage is higher than the fair percentage (splitperc_self > fair_percent), U is positive, leading to higher probability of acceptance. When it's lower, U is negative, lower probability.\n\nBut wait, the problem states that the utility variable may be unbounded, but the parameters must have finite bounds. So \u03b1 is bounded, but U can be anything.\n\nNow, the variables in the model:\n\n- \u03b1: learnable parameter, sensitivity to fairness deviation. Range [0,5], inclusive.\n\n- splitperc_self: from data, the proposed percentage to the participant.\n\n- fair_percent: calculated as (token_self / (token_self + token_opp)) * 100.\n\nSo, the model formula is:\n\nU = \u03b1 * (splitperc_self - fair_percent)\n\nThen, P(accept=1) = logistic(U)\n\nThis model uses only the necessary variables: splitperc_self, token_self, token_opp. All others are either calculated (fair_percent) or parameters (\u03b1).\n\nNow, checking the dataset variables: token_self and token_opp are available, as well as splitperc_self. So the model is feasible.\n\nNow, variable descriptions:\n\n- \u03b1: learnable, range [0,5], inclusive.\n\n- splitperc_self: data variable, from the dataset.\n\n- token_self and token_opp: data variables used to calculate fair_percent.\n\n- fair_percent: calculated variable.\n\nThe target variable is 'accept'.\n\nThis model should be able to predict accept/reject decisions based on the fairness of the offer relative to contributions. The parameter \u03b1 can be recovered through model fitting, and the bounds are set to 0-5 to allow for varying sensitivity levels without being too restrictive.\n\nAnother consideration: maybe the model should include a term for the absolute amount, but the problem states to focus on the variables provided. Since splitperc_self and the token counts are available, the relative fairness is captured. Including absolute terms might complicate the model and add more parameters, which could affect parameter recovery. So keeping it simple with one parameter is better.\n\nThus, the final model is as described.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp) * 100))\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-proportional fairness\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA fairness utility model where responders evaluate offers based on deviation from a contribution-proportional fair split. The utility is determined by the difference between the proposed percentage for the participant and their token-based entitlement, scaled by a sensitivity parameter (alpha). Negative utility (unfair offers) reduces acceptance likelihood via logistic mapping.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = alpha * (splitperc_self - (token_self / (token_self + token_opp) * 100))\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-proportional fairness\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        fair_percent = (token_self / (token_self + token_opp)) * 100 if (token_self + token_opp) > 0 else 50\n\n        U = alpha * (splitperc_self - fair_percent)\n\n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 0, \"token_opp\": 6, \"token_self\": 6, \"combined_earning\": 36, \"split_opp\": 18.0, \"split_self\": 18.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 18.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 18.0}, {\"ID\": 1, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 0, \"token_opp\": 6, \"token_self\": 6, \"combined_earning\": 36, \"split_opp\": 18.0, \"split_self\": 18.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 18.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 18.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1744033335.843746
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to deviation from contribution-proportional fairness",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = alpha * (splitperc_self - (token_self / (token_self + token_opp) * 100))",
  "variable_descriptions": {
    "alpha": {
      "description": "Sensitivity to deviation from contribution-proportional fairness",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "splitperc_self": {
      "description": "Proposed percentage share for the participant",
      "range": {
        "min": 0,
        "max": 100,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    }
  },
  "model_summary": "A fairness utility model where responders evaluate offers based on deviation from a contribution-proportional fair split. The utility is determined by the difference between the proposed percentage for the participant and their token-based entitlement, scaled by a sensitivity parameter (alpha). Negative utility (unfair offers) reduces acceptance likelihood via logistic mapping.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal. The model needs to focus on responder behavior, use the given variables appropriately, and ensure parameter recovery is feasible.\n\nFirst, I need to identify which variables are relevant for the responder's decision. The target variable is 'accept', which is binary. The key factors influencing this decision likely involve fairness considerations based on the participants' and opponents' contributions, as well as the proposed splits.\n\nThe dataset includes variables like token_self and token_opp, which determine each party's contribution to the combined earnings. The combined_earning is the total, which is \u00a33 times the sum of tokens. The split_self and split_opp are the proposed amounts, and their percentages (splitperc_self and splitperc_opp) might be critical. \n\nFairness here isn't just about equal splits but proportional to contributions. So, the fair percentage for the participant should be token_self / (token_self + token_opp) * 100. The deviation from this fair percentage in the proposed split (splitperc_self) could be a key driver. For example, if the participant contributed 60% of the tokens, a 50% split would be unfair, whereas a 60% split would be fair.\n\nThe model should calculate utility based on this deviation. A common approach is to use a utility function that includes a term for the deviation from fairness, weighted by a parameter (e.g., \u03b1) that captures sensitivity to unfairness. If the utility is negative, the participant is more likely to reject the offer.\n\nSo, the utility (U) could be defined as the proposed percentage minus the fair percentage, multiplied by \u03b1. If U is positive, acceptance is more likely; if negative, rejection. The probability of accepting can then be modeled with a logistic function, translating utility into a probability.\n\nBut wait, the problem states that the utility should predict a probability less than 50% when utility is less than 0. The logistic function naturally does this because when U < 0, the probability is < 0.5. So, using a standard logistic link function makes sense here.\n\nNow, considering parameter recovery, the model should have learnable parameters with finite bounds. The key parameter here is \u03b1, which represents the sensitivity to unfairness. If \u03b1 is positive, it means the participant is more likely to accept when the offer is fairer. But participants might have different sensitivities, so \u03b1 needs to be learnable.\n\nAnother consideration is whether to include other variables. For example, the absolute amount offered (split_self) might matter\u2014people might accept slightly unfair offers if the absolute amount is high. However, the problem states to use the variables appropriately but not necessarily all. Since the main focus is fairness relative to contribution, maybe the percentage terms are sufficient.\n\nBut let's check the variables again. The trial_type indicates whether contributions are equal, opponent more, or participant more. This might affect the responder's perception. For example, if the participant contributed more, they might be more sensitive to unfair splits. However, including trial_type might complicate the model. Since the fair percentage already accounts for the contribution ratio, maybe trial_type is redundant. Alternatively, trial_type could interact with the fairness deviation. But to keep the model simple and ensure parameter recovery, perhaps it's better to stick with the core fairness deviation.\n\nSo, the model will focus on the difference between the proposed percentage (splitperc_self) and the fair percentage (calculated as token_self / (token_self + token_opp) * 100). The utility is this difference multiplied by \u03b1. The probability of acceptance is then the logistic of this utility.\n\nNow, the parameters. \u03b1 needs to be bounded. Since \u03b1 scales the effect of the fairness deviation, it's reasonable to set bounds that allow for both positive and negative sensitivities, but in reality, we'd expect \u03b1 to be positive (higher fairness leading to higher acceptance). However, to be safe, maybe set a range from -5 to 5. But considering that a negative \u03b1 would imply that more unfair offers (lower than fair) are more likely to be accepted, which doesn't make sense. So perhaps \u03b1 should be constrained to be non-negative. Wait, but the problem says bounds should be generous. So maybe set \u03b1 from -2 to 5, allowing for some flexibility but avoiding extreme values that could hinder parameter recovery.\n\nWait, but if \u03b1 is negative, the model would predict that as the offer becomes more unfair (lower than fair), the utility increases, leading to higher acceptance. That's counterintuitive. So perhaps \u03b1 should be constrained to be positive. But the problem says bounds should be generous. Maybe set \u03b1 from 0 to 5. That way, \u03b1 can't be negative, ensuring that higher fairness leads to higher utility. But the problem allows for the model to have parameters with generous bounds, not necessarily strictly positive. However, given the context, a negative \u03b1 doesn't make sense here. So setting \u03b1's min to 0 makes sense.\n\nAlternatively, maybe some participants have a negative \u03b1, but that's unlikely. To allow for the model to capture any possible data pattern, even if theoretically unexpected, perhaps set \u03b1 from -2 to 5. But given that the task is about responders accepting or rejecting based on fairness, a positive \u03b1 is expected. However, for parameter recovery, it's better to have a reasonable range. Let's set \u03b1 from 0 to 5, inclusive.\n\nAnother consideration: the fair percentage is calculated as (token_self / (token_self + token_opp)) * 100. But if both tokens are zero, that's a problem. However, in the dataset, token_self and token_opp are the number of tokens found, which are non-negative integers. So combined_earning is 3*(token_self + token_opp), which implies that token_self + token_opp is at least 1 (since combined_earning is an integer). So division by zero isn't an issue.\n\nNow, the utility function would be:\n\nU = \u03b1 * (splitperc_self - fair_percent)\n\nWhere fair_percent = (token_self / (token_self + token_opp)) * 100\n\nThen, the probability of acceptance is P(accept=1) = 1 / (1 + exp(-U))\n\nThis ensures that when the proposed percentage is higher than the fair percentage (splitperc_self > fair_percent), U is positive, leading to higher probability of acceptance. When it's lower, U is negative, lower probability.\n\nBut wait, the problem states that the utility variable may be unbounded, but the parameters must have finite bounds. So \u03b1 is bounded, but U can be anything.\n\nNow, the variables in the model:\n\n- \u03b1: learnable parameter, sensitivity to fairness deviation. Range [0,5], inclusive.\n\n- splitperc_self: from data, the proposed percentage to the participant.\n\n- fair_percent: calculated as (token_self / (token_self + token_opp)) * 100.\n\nSo, the model formula is:\n\nU = \u03b1 * (splitperc_self - fair_percent)\n\nThen, P(accept=1) = logistic(U)\n\nThis model uses only the necessary variables: splitperc_self, token_self, token_opp. All others are either calculated (fair_percent) or parameters (\u03b1).\n\nNow, checking the dataset variables: token_self and token_opp are available, as well as splitperc_self. So the model is feasible.\n\nNow, variable descriptions:\n\n- \u03b1: learnable, range [0,5], inclusive.\n\n- splitperc_self: data variable, from the dataset.\n\n- token_self and token_opp: data variables used to calculate fair_percent.\n\n- fair_percent: calculated variable.\n\nThe target variable is 'accept'.\n\nThis model should be able to predict accept/reject decisions based on the fairness of the offer relative to contributions. The parameter \u03b1 can be recovered through model fitting, and the bounds are set to 0-5 to allow for varying sensitivity levels without being too restrictive.\n\nAnother consideration: maybe the model should include a term for the absolute amount, but the problem states to focus on the variables provided. Since splitperc_self and the token counts are available, the relative fairness is captured. Including absolute terms might complicate the model and add more parameters, which could affect parameter recovery. So keeping it simple with one parameter is better.\n\nThus, the final model is as described.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp) * 100))\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-proportional fairness\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Proposed percentage share for the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA fairness utility model where responders evaluate offers based on deviation from a contribution-proportional fair split. The utility is determined by the difference between the proposed percentage for the participant and their token-based entitlement, scaled by a sensitivity parameter (alpha). Negative utility (unfair offers) reduces acceptance likelihood via logistic mapping.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = alpha * (splitperc_self - (token_self / (token_self + token_opp) * 100))\nSummary: A fairness utility model where responders evaluate offers based on deviation from a contribution-proportional fair split. The utility is determined by the difference between the proposed percentage for the participant and their token-based entitlement, scaled by a sensitivity parameter (alpha). Negative utility (unfair offers) reduces acceptance likelihood via logistic mapping.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        fair_percent = (token_self / (token_self + token_opp)) * 100 if (token_self + token_opp) > 0 else 50\n\n        U = alpha * (splitperc_self - fair_percent)\n\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.5873015873015873,
  "fitting_results": [
    {
      "alpha": 0.1699319173765632,
      "success": true,
      "log_likelihood": -8.46089673871266,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.031933817205461366,
      "success": true,
      "log_likelihood": -22.44975161167207,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03904952395240526,
      "success": true,
      "log_likelihood": -21.44629769597155,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04942328911834915,
      "success": true,
      "log_likelihood": -19.90428046179077,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06052892909524712,
      "success": true,
      "log_likelihood": -18.26158563818436,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.01124611693322951,
      "success": true,
      "log_likelihood": -24.599581746777613,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.007639147529832025,
      "success": true,
      "log_likelihood": -24.78830395894632,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06727669141560838,
      "success": true,
      "log_likelihood": -17.30429193948927,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.059837190381040864,
      "success": true,
      "log_likelihood": -18.361889142190293,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.15400447784304844,
      "success": true,
      "log_likelihood": -9.268950855491479,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.059155626056029566,
      "success": true,
      "log_likelihood": -18.461048290922246,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.027752480891412204,
      "success": true,
      "log_likelihood": -22.996445405427142,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.046752516190313675,
      "success": true,
      "log_likelihood": -20.304900381839477,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.02347708295561217,
      "success": true,
      "log_likelihood": -23.508299437274964,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03995111455507734,
      "success": true,
      "log_likelihood": -21.314634304338142,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.022447072502066328,
      "success": true,
      "log_likelihood": -23.623103825114903,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.022444794825931744,
      "success": true,
      "log_likelihood": -23.623103837811218,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12288047578729862,
      "success": true,
      "log_likelihood": -11.323306103376494,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.203650530093562,
      "success": true,
      "log_likelihood": -47.431272591699845,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.029618268052283302,
      "success": true,
      "log_likelihood": -22.75743857944762,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05343531717968356,
      "success": true,
      "log_likelihood": -19.30460076063597,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.2777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.005271293410275579,
      "success": true,
      "log_likelihood": -24.874347839989756,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.004976826036014543,
      "success": true,
      "log_likelihood": -24.882887911229616,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.036860050938047006,
      "success": true,
      "log_likelihood": -21.762525496321402,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.002337146015790173,
      "success": true,
      "log_likelihood": -24.937722102506324,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.00913264571817188,
      "success": true,
      "log_likelihood": -24.71842969739775,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11790021950112846,
      "success": true,
      "log_likelihood": -11.724471975995737,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.017165788728941478,
      "success": true,
      "log_likelihood": -24.150542964973152,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.014948101004892524,
      "success": true,
      "log_likelihood": -24.337826758644866,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.02313213588406635,
      "success": true,
      "log_likelihood": -23.547140195202175,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04323625330697611,
      "success": true,
      "log_likelihood": -20.829578865844194,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06052874958689289,
      "success": true,
      "log_likelihood": -18.26158563802745,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.005861145037361931,
      "success": true,
      "log_likelihood": -24.855794100324772,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1035823502765835,
      "success": true,
      "log_likelihood": -13.012403413778227,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.01845609624411587,
      "success": true,
      "log_likelihood": -24.03181341648202,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.055265001137524104,
      "success": true,
      "log_likelihood": -19.032879320666396,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09545275023764307,
      "success": true,
      "log_likelihood": -13.840750462667886,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0969709375042737,
      "success": true,
      "log_likelihood": -13.680404198600833,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.2777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.14154237497993588,
      "success": true,
      "log_likelihood": -10.006664541904932,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.00468263081041033,
      "success": true,
      "log_likelihood": -24.89093743207882,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03393483802854921,
      "success": true,
      "log_likelihood": -22.175346582182403,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16993183121515398,
      "success": true,
      "log_likelihood": -8.46089673870871,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.029998148218387675,
      "success": true,
      "log_likelihood": -22.707758549538227,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11124823103249143,
      "success": true,
      "log_likelihood": -12.296972867021282,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.021768110992430166,
      "success": true,
      "log_likelihood": -23.696794128314295,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.017486669270336593,
      "success": true,
      "log_likelihood": -24.121666085400975,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.914300028844185,
      "success": true,
      "log_likelihood": -1.3862945111255969,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1018415978388436,
      "success": true,
      "log_likelihood": -13.183581049695674,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.025938653926196736,
      "success": true,
      "log_likelihood": -23.220124257634662,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.011550504059391209,
      "success": true,
      "log_likelihood": -24.580584661343,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10921583771587101,
      "success": true,
      "log_likelihood": -12.480681162641094,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16993168941472414,
      "success": true,
      "log_likelihood": -8.460896738706625,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.008533807139366125,
      "success": true,
      "log_likelihood": -24.747873236690655,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10726435086971974,
      "success": true,
      "log_likelihood": -12.661070408314819,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03352907076241013,
      "success": true,
      "log_likelihood": -22.231566111502502,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.642269475735776,
      "success": true,
      "log_likelihood": -24.41214566829113,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09545273161342299,
      "success": true,
      "log_likelihood": -13.840750462205525,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03193383186803539,
      "success": true,
      "log_likelihood": -22.449751611750035,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.36002479260566317,
        "p": 0.00023400388261749026
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 46.82409684479528,
  "bic_results": {
    "average_bic": 46.82409684479528,
    "individual_bics": [
      20.50531241588143,
      48.48302216180025,
      46.476114330399206,
      43.39207986203765,
      40.10669021482483,
      52.782682432011335,
      53.49011592437213,
      53.16012685634875,
      53.49011592437213,
      38.19210281743465,
      40.307297222836695,
      22.121420649439067,
      40.5056155203006,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      49.57640974931039,
      53.49011592437213,
      44.193319702135064,
      50.60011781300604,
      46.21278754713239,
      50.829726588685915,
      50.829726614078545,
      26.230131145209096,
      53.49011592437213,
      53.49011592437213,
      53.49011592437213,
      98.4460641218558,
      49.09839609735135,
      42.19272045972805,
      53.49011592437213,
      53.33221461843562,
      53.34929476091534,
      47.10856993109891,
      53.45896314346876,
      53.49011592437213,
      53.02037833325161,
      27.032462890447583,
      53.49011592437213,
      51.88460486840241,
      52.25917245574584,
      53.49011592437213,
      50.67779932886046,
      45.2426766701445,
      40.10669021451101,
      53.29510713910565,
      53.49011592437213,
      29.608325766012562,
      51.64714577142015,
      53.49011592437213,
      41.6492775797889,
      31.26501986379188,
      30.944327335657775,
      53.49011592437213,
      23.596848022265974,
      53.49011592437213,
      53.36539380261375,
      47.934212102820915,
      20.50531241587353,
      48.99903603753256,
      28.177464672498672,
      50.9771071950847,
      53.49011592437213,
      53.49011592437213,
      51.82685110925806,
      53.49011592437213,
      6.356107960707304,
      29.950681037847456,
      50.02376745372543,
      53.49011592437213,
      52.74468826114211,
      53.49011592437213,
      53.49011592437213,
      28.544881263738297,
      53.49011592437213,
      20.50531241586936,
      53.07926541183742,
      53.49011592437213,
      28.905659755085747,
      48.046651161461114,
      53.49011592437213,
      52.40781027503837,
      53.49011592437213,
      31.26501986286716,
      53.49011592437213,
      48.48302216195618,
      53.49011592437213
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.360",
  "previous_models": [
    "Specification: U = alpha * (splitperc_self - (token_self / (token_self + token_opp) * 100))\nSummary: A fairness utility model where responders evaluate offers based on deviation from a contribution-proportional fair split. The utility is determined by the difference between the proposed percentage for the participant and their token-based entitlement, scaled by a sensitivity parameter (alpha). Negative utility (unfair offers) reduces acceptance likelihood via logistic mapping.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 46.82409684479528\nOverall Accuracy: 0.5873\n\nParameter Recovery:\n- alpha: r = 0.360"
  ]
}