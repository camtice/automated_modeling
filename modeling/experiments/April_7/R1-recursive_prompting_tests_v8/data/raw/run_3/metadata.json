{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\n\nOkay, let's see. The previous model had a parameter recovery of 0.36 for alpha, which is pretty low. The BIC was around 46.8, and accuracy was about 58.7%. So, the model isn't capturing enough variance, and the parameters aren't being recovered well. The previous instructions suggested adding a second parameter, non-linear transformations, and considering trial context. But maybe the model needs more specific guidance.\n\nFirst, the low parameter recovery suggests that alpha isn't being estimated well. Maybe the model is too simple, or there's noise in the data that's not accounted for. The previous run tried a linear model with alpha, but perhaps participants have different sensitivities depending on the situation. For example, when they contributed more (trial_type 3), they might be more sensitive to unfair splits. So, adding an interaction term with trial_type could help. Let's think about how to structure that.\n\nAlso, the model might benefit from distinguishing between advantageous and disadvantageous inequality. Right now, the utility is linear, but maybe people react more strongly to being below the fair share than above. So, a piecewise function where alpha is different for positive and negative deviations. That would add a second parameter, but if they're orthogonal, recovery might improve.\n\nAnother angle: the utility function could include a non-linear component, like a quadratic term for larger deviations, or a sigmoid to cap the utility. But that might complicate things. Alternatively, using a temperature parameter to scale the utility before applying the logistic function. This could help with the probability calibration, making the model more sensitive to changes in utility.\n\nThe previous instructions allowed up to two parameters, but maybe the model needs to explicitly suggest using two parameters, like alpha for disadvantageous and beta for advantageous inequality. Or alpha for sensitivity and beta as a baseline acceptance bias. That way, the model can capture more individual differences.\n\nAlso, considering the absolute amount offered (split_self) alongside the percentage might help. Some participants might care more about the absolute amount, especially if it's a large sum, even if the percentage is unfair. Adding a term for split_self could improve accuracy.\n\nTrial_type could be used as a weight. For example, in trials where the participant contributed more (trial_type 3), the sensitivity alpha is multiplied by a factor. This factor could be a fixed value based on trial_type or a learnable parameter. For example, U = alpha * (splitperc_self - fair_percent) * (1 + gamma * trial_type), where gamma is a learnable parameter for each trial_type. But that might add too many parameters.\n\nAlternatively, using trial_type to split the data into different models, but that's not feasible here. Instead, interaction terms that modulate alpha based on trial_type. For example, alpha = alpha_base + alpha_delta * (trial_type == 3). But this would require two parameters: alpha_base and alpha_delta.\n\nAnother thought: the fair percentage calculation assumes that participants compute it exactly, but maybe some participants have a different perception. Adding a noise term or a parameter that adjusts the fair percentage (like a bias term) could account for this. For example, fair_percent = (token_self / (token_self + token_opp)) * 100 + delta, where delta is a learnable parameter. But this adds another parameter and might complicate recovery.\n\nThe temperature parameter idea: right now, the utility is directly passed through a logistic function. If the utility is scaled by a temperature parameter beta, P(accept) = 1 / (1 + exp(-beta * U)). This beta could control the steepness of the logistic curve. A higher beta means more deterministic choices. This could help if participants vary in how deterministically they choose based on utility.\n\nSo, the new instructions should encourage models that:\n\n1. Use two parameters: one for disadvantageous inequality and one for advantageous.\n2. Include interaction terms with trial_type to modulate sensitivity.\n3. Add a temperature parameter to scale the utility.\n4. Consider non-linear transformations of the fairness deviation.\n5. Incorporate absolute terms like split_self.\n\nBut need to balance complexity. Maybe allow up to two parameters, with clear guidance on how they should capture different effects. Also, suggest testing non-linearities like tanh or piecewise functions.\n</think>\n\n**Revised Instructions for Run 3/5:**  \n\n1. **Asymmetric Inequality Sensitivity**: Design models with *separate parameters* for advantageous (offers above fair share) and disadvantageous inequality (offers below). Example:  \n   `U = \u03b1\u2081 * max(fair_percent \u2212 splitperc_self, 0) + \u03b1\u2082 * max(splitperc_self \u2212 fair_percent, 0)`  \n   Ensure \u03b1\u2081 and \u03b1\u2082 have distinct bounds (e.g., \u03b1\u2081 \u2208 [0, 10] for rejection sensitivity, \u03b1\u2082 \u2208 [\u22125, 5] for guilt effects).  \n\n2. **Trial-Type Modulated Utility**: Use `trial_type` to weight fairness sensitivity. For example:  \n   `U = \u03b1 * (splitperc_self \u2212 fair_percent) * (1 + \u03b3 * I_{trial_type=3})`  \n   where `I_{trial_type=3}` is 1 when the participant contributed more, and \u03b3 \u2208 [\u22120.5, 0.5] modulates sensitivity in this context.  \n\n3. **Non-Linear Utility Scaling**: Replace linear fairness terms with sigmoidal or hyperbolic tangent transformations to capture diminishing sensitivity:  \n   `U = \u03b1 * tanh(\u03b2 * (splitperc_self \u2212 fair_percent))`  \n   where \u03b2 \u2208 [0.01, 0.1] controls the steepness of the response curve.  \n\n4. **Hybrid Absolute-Relative Utility**: Combine percentage fairness with absolute monetary gains:  \n   `U = \u03b1 * (splitperc_self \u2212 fair_percent) + \u03b8 * split_self`  \n   where \u03b8 \u2208 [\u22120.1, 0.1] captures marginal utility of \u00a31 gains.  \n\n5. **Logistic Temperature**: Introduce a temperature parameter (\u03c4) to the logistic mapping:  \n   `P(accept) = 1 / (1 + exp(\u2212\u03c4 * U))` with \u03c4 \u2208 [0.1, 5].  \n\n**Critical Constraints**:  \n- Maximum of **2 learnable parameters** to avoid overfitting.  \n- Parameters must target *distinct cognitive processes* (e.g., one for fairness, one for gain sensitivity).  \n- Use non-overlapping bounds (e.g., \u03b1 \u2208 [0, 5], \u03c4 \u2208 [0.1, 3]) to improve identifiability.  \n\n**Key Focus**: Prioritize models where parameters have clear psychological interpretations (e.g., \"rejection sensitivity\" vs. \"guilt aversion\") and test whether non-linear transformations reduce residuals in high-disparity trials.",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: <think>\n\nOkay, let's see. The previous model had a parameter recovery of 0.36 for alpha, which is pretty low. The BIC was around 46.8, and accuracy was about 58.7%. So, the model isn't capturing enough variance, and the parameters aren't being recovered well. The previous instructions suggested adding a second parameter, non-linear transformations, and considering trial context. But maybe the model needs more specific guidance.\n\nFirst, the low parameter recovery suggests that alpha isn't being estimated well. Maybe the model is too simple, or there's noise in the data that's not accounted for. The previous run tried a linear model with alpha, but perhaps participants have different sensitivities depending on the situation. For example, when they contributed more (trial_type 3), they might be more sensitive to unfair splits. So, adding an interaction term with trial_type could help. Let's think about how to structure that.\n\nAlso, the model might benefit from distinguishing between advantageous and disadvantageous inequality. Right now, the utility is linear, but maybe people react more strongly to being below the fair share than above. So, a piecewise function where alpha is different for positive and negative deviations. That would add a second parameter, but if they're orthogonal, recovery might improve.\n\nAnother angle: the utility function could include a non-linear component, like a quadratic term for larger deviations, or a sigmoid to cap the utility. But that might complicate things. Alternatively, using a temperature parameter to scale the utility before applying the logistic function. This could help with the probability calibration, making the model more sensitive to changes in utility.\n\nThe previous instructions allowed up to two parameters, but maybe the model needs to explicitly suggest using two parameters, like alpha for disadvantageous and beta for advantageous inequality. Or alpha for sensitivity and beta as a baseline acceptance bias. That way, the model can capture more individual differences.\n\nAlso, considering the absolute amount offered (split_self) alongside the percentage might help. Some participants might care more about the absolute amount, especially if it's a large sum, even if the percentage is unfair. Adding a term for split_self could improve accuracy.\n\nTrial_type could be used as a weight. For example, in trials where the participant contributed more (trial_type 3), the sensitivity alpha is multiplied by a factor. This factor could be a fixed value based on trial_type or a learnable parameter. For example, U = alpha * (splitperc_self - fair_percent) * (1 + gamma * trial_type), where gamma is a learnable parameter for each trial_type. But that might add too many parameters.\n\nAlternatively, using trial_type to split the data into different models, but that's not feasible here. Instead, interaction terms that modulate alpha based on trial_type. For example, alpha = alpha_base + alpha_delta * (trial_type == 3). But this would require two parameters: alpha_base and alpha_delta.\n\nAnother thought: the fair percentage calculation assumes that participants compute it exactly, but maybe some participants have a different perception. Adding a noise term or a parameter that adjusts the fair percentage (like a bias term) could account for this. For example, fair_percent = (token_self / (token_self + token_opp)) * 100 + delta, where delta is a learnable parameter. But this adds another parameter and might complicate recovery.\n\nThe temperature parameter idea: right now, the utility is directly passed through a logistic function. If the utility is scaled by a temperature parameter beta, P(accept) = 1 / (1 + exp(-beta * U)). This beta could control the steepness of the logistic curve. A higher beta means more deterministic choices. This could help if participants vary in how deterministically they choose based on utility.\n\nSo, the new instructions should encourage models that:\n\n1. Use two parameters: one for disadvantageous inequality and one for advantageous.\n2. Include interaction terms with trial_type to modulate sensitivity.\n3. Add a temperature parameter to scale the utility.\n4. Consider non-linear transformations of the fairness deviation.\n5. Incorporate absolute terms like split_self.\n\nBut need to balance complexity. Maybe allow up to two parameters, with clear guidance on how they should capture different effects. Also, suggest testing non-linearities like tanh or piecewise functions.\n</think>\n\n**Revised Instructions for Run 3/5:**  \n\n1. **Asymmetric Inequality Sensitivity**: Design models with *separate parameters* for advantageous (offers above fair share) and disadvantageous inequality (offers below). Example:  \n   `U = \u03b1\u2081 * max(fair_percent \u2212 splitperc_self, 0) + \u03b1\u2082 * max(splitperc_self \u2212 fair_percent, 0)`  \n   Ensure \u03b1\u2081 and \u03b1\u2082 have distinct bounds (e.g., \u03b1\u2081 \u2208 [0, 10] for rejection sensitivity, \u03b1\u2082 \u2208 [\u22125, 5] for guilt effects).  \n\n2. **Trial-Type Modulated Utility**: Use `trial_type` to weight fairness sensitivity. For example:  \n   `U = \u03b1 * (splitperc_self \u2212 fair_percent) * (1 + \u03b3 * I_{trial_type=3})`  \n   where `I_{trial_type=3}` is 1 when the participant contributed more, and \u03b3 \u2208 [\u22120.5, 0.5] modulates sensitivity in this context.  \n\n3. **Non-Linear Utility Scaling**: Replace linear fairness terms with sigmoidal or hyperbolic tangent transformations to capture diminishing sensitivity:  \n   `U = \u03b1 * tanh(\u03b2 * (splitperc_self \u2212 fair_percent))`  \n   where \u03b2 \u2208 [0.01, 0.1] controls the steepness of the response curve.  \n\n4. **Hybrid Absolute-Relative Utility**: Combine percentage fairness with absolute monetary gains:  \n   `U = \u03b1 * (splitperc_self \u2212 fair_percent) + \u03b8 * split_self`  \n   where \u03b8 \u2208 [\u22120.1, 0.1] captures marginal utility of \u00a31 gains.  \n\n5. **Logistic Temperature**: Introduce a temperature parameter (\u03c4) to the logistic mapping:  \n   `P(accept) = 1 / (1 + exp(\u2212\u03c4 * U))` with \u03c4 \u2208 [0.1, 5].  \n\n**Critical Constraints**:  \n- Maximum of **2 learnable parameters** to avoid overfitting.  \n- Parameters must target *distinct cognitive processes* (e.g., one for fairness, one for gain sensitivity).  \n- Use non-overlapping bounds (e.g., \u03b1 \u2208 [0, 5], \u03c4 \u2208 [0.1, 3]) to improve identifiability.  \n\n**Key Focus**: Prioritize models where parameters have clear psychological interpretations (e.g., \"rejection sensitivity\" vs. \"guilt aversion\") and test whether non-linear transformations reduce residuals in high-disparity trials.\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant accepts or rejects a proposal, focusing on responder behavior. The model needs to use the given variables appropriately, have learnable parameters with finite bounds, and pass parameter recovery tests while maintaining accuracy.\n\nFirst, I need to identify which variables are relevant. The target is 'accept', which is binary (1 or 0). The model should predict this based on the utility of the offer. Since the task involves fairness considerations, variables like splitperc_self (the percentage offered to the participant) and the fair percentage based on tokens found (token_self and token_opp) are likely important.\n\nThe fair percentage would be the participant's contribution relative to the total. So, fair_percent = (token_self / (token_self + token_opp)) * 100. The difference between the offered percentage (splitperc_self) and the fair percentage could drive the utility. If the offer is below the fair percentage, that's disadvantageous inequality; if above, advantageous.\n\nThe previous example used a linear model with a single parameter (alpha) for fairness sensitivity. But the revised instructions suggest using two parameters for asymmetric sensitivity. So, let's split alpha into alpha_advantageous and alpha_disadvantageous. This way, the model can capture different sensitivities when the offer is above or below the fair share.\n\nAlso, including a temperature parameter (tau) in the logistic function might help. The utility U would be scaled by tau before converting to a probability via the logistic function. This adds a second learnable parameter, which controls the steepness of the decision boundary.\n\nSo, the utility function becomes:\nU = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)\n\nThen, the probability of accepting is P(accept) = 1 / (1 + exp(-tau * U)).\n\nThis model has two learnable parameters: alpha_disadvantageous and alpha_advantageous, and tau. Wait, but the instructions say maximum of two parameters. Hmm. Maybe combine the temperature with one of the alphas? Or perhaps the temperature is necessary. Let me check the constraints again.\n\nThe critical constraints say maximum of 2 learnable parameters. So, I need to adjust. Maybe keep the two alphas and drop the temperature, or include tau as a third parameter. But the user's latest instructions for Run 3/5 allow up to two parameters. Wait, looking back: \"Maximum of 2 learnable parameters to avoid overfitting.\" So, I need to stick to two.\n\nSo, perhaps drop the temperature and keep the two alphas. Alternatively, use one alpha and a temperature. But the asymmetric sensitivity is important. Let's proceed with two alphas as the two parameters.\n\nWait, but the example in the previous response had two parameters (alpha and tau). But according to the latest instructions, maximum two. So, in this case, the model would have alpha_disadvantageous and alpha_advantageous as two parameters. That fits.\n\nNow, the variable descriptions. The fair_percent is calculated from token_self and token_opp. The splitperc_self is from the data. The max functions create terms for disadvantageous and advantageous inequality.\n\nEach alpha should have bounds. For disadvantageous (when offer is below fair), alpha_disadvantageous is positive, as a higher negative deviation reduces utility. For advantageous, maybe alpha_advantageous could be negative (if people feel guilty having more) or positive (if they like it). But to allow flexibility, set bounds that include both positive and negative, but perhaps with different ranges.\n\nFor alpha_disadvantageous, since offers below fair are likely to decrease acceptance, this parameter should be positive. So, set a min of 0 and a max of, say, 10. For alpha_advantageous, it could be negative or positive. Maybe set a wider range, like -5 to 5.\n\nWait, but in the utility function, if alpha_advantageous is positive, then offers above fair increase utility, making acceptance more likely. If negative, offers above fair decrease utility (maybe due to guilt), making acceptance less likely. So, the bounds should allow both possibilities.\n\nSo, variables:\n\n- alpha_disadvantageous: learnable, range [0, 10]\n- alpha_advantageous: learnable, range [-5, 5]\n\nThe utility U is the sum of the two terms. Then, the probability of accept is logistic(U). But the user's note says \"Predict utility, where less than 0 is less than 50% likely.\" So, the utility U is passed through a logistic function with temperature 1. So, P(accept) = 1 / (1 + exp(-U)).\n\nWait, but the model type says the utility will be converted to a probability using a logistic function with temperature 1. So, no need for a separate tau parameter here. Thus, the model has two learnable parameters: alpha_disadvantageous and alpha_advantageous.\n\nBut the example in the previous response included a temperature parameter (tau). However, the user's latest instruction says to have maximum two parameters. So, perhaps in this case, we proceed without tau, using the two alphas as the parameters.\n\nThus, the model would be:\n\nU = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)\n\nThen, P(accept) = 1 / (1 + exp(-U)).\n\nThis uses two parameters, which fits the constraint. The variables involved are calculated (fair_percent) and from the data (splitperc_self).\n\nNow, variable descriptions:\n\n- alpha_disadvantageous: learnable, range [0, 10], inclusive.\n- alpha_advantageous: learnable, range [-5, 5], inclusive.\n- fair_percent: calculated from token_self and token_opp.\n- splitperc_self: from data.\n- U: calculated utility.\n\nThis model allows different sensitivities to offers below and above the fair share. For example, if alpha_disadvantageous is high, even small negative deviations lead to rejection. If alpha_advantageous is negative, offers above fair reduce utility (perhaps due to guilt), making acceptance less likely, which could explain some rejections even when the offer is favorable.\n\nThis should help with parameter recovery as each parameter captures a distinct psychological factor. The bounds are set to be generous but finite. Testing this model should show if participants have asymmetric responses to inequality.\n</think>\n\n<MODEL>\nU = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha_disadvantageous\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (offer below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha_advantageous\": {\n      \"description\": \"Sensitivity to advantageous inequality (offer above fair share). Negative values indicate guilt aversion.\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"fair_percent\": {\n      \"description\": \"Fair share percentage based on token contributions: (token_self / (token_self + token_opp)) * 100\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage of total offered to participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-parameter fairness utility model that separately quantifies sensitivity to disadvantageous (offers below equitable share) and advantageous inequality (offers above equitable share). The fair share is computed from relative token contributions. Learnable parameters capture distinct psychological processes: rejection sensitivity for unfavorable offers and guilt aversion for favorable ones. Utility is converted to acceptance probability via logistic function.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha_disadvantageous', 'alpha_advantageous']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha_disadvantageous\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (offer below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha_advantageous\": {\n      \"description\": \"Sensitivity to advantageous inequality (offer above fair share). Negative values indicate guilt aversion.\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"fair_percent\": {\n      \"description\": \"Fair share percentage based on token contributions: (token_self / (token_self + token_opp)) * 100\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage of total offered to participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha_disadvantageous=1, alpha_advantageous=1):\n    model_predictions = []\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n\n        total_tokens = token_self + token_opp\n        if total_tokens == 0:\n            fair_percent = 50  # Default to 50 if no tokens\n        else:\n            fair_percent = (token_self / total_tokens) * 100\n\n        disadvantageous_inequality = max(fair_percent - splitperc_self, 0)\n        advantageous_inequality = max(splitperc_self - fair_percent, 0)\n\n        U = alpha_disadvantageous * disadvantageous_inequality + alpha_advantageous * advantageous_inequality\n\n        model_predictions.append(U)\n    return model_predictions\n```",
      "timestamp": 1744033401.7558882
    }
  ],
  "learnable_parameters": {
    "alpha_disadvantageous": {
      "description": "Sensitivity to disadvantageous inequality (offer below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha_advantageous": {
      "description": "Sensitivity to advantageous inequality (offer above fair share). Negative values indicate guilt aversion.",
      "range": {
        "min": -5,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the offer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha_disadvantageous": {
      "description": "Sensitivity to disadvantageous inequality (offer below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha_advantageous": {
      "description": "Sensitivity to advantageous inequality (offer above fair share). Negative values indicate guilt aversion.",
      "range": {
        "min": -5,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "fair_percent": {
      "description": "Fair share percentage based on token contributions: (token_self / (token_self + token_opp)) * 100",
      "range": {
        "min": 0,
        "max": 100,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "splitperc_self": {
      "description": "Percentage of total offered to participant",
      "range": {
        "min": 0,
        "max": 100,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A dual-parameter fairness utility model that separately quantifies sensitivity to disadvantageous (offers below equitable share) and advantageous inequality (offers above equitable share). The fair share is computed from relative token contributions. Learnable parameters capture distinct psychological processes: rejection sensitivity for unfavorable offers and guilt aversion for favorable ones. Utility is converted to acceptance probability via logistic function.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant accepts or rejects a proposal, focusing on responder behavior. The model needs to use the given variables appropriately, have learnable parameters with finite bounds, and pass parameter recovery tests while maintaining accuracy.\n\nFirst, I need to identify which variables are relevant. The target is 'accept', which is binary (1 or 0). The model should predict this based on the utility of the offer. Since the task involves fairness considerations, variables like splitperc_self (the percentage offered to the participant) and the fair percentage based on tokens found (token_self and token_opp) are likely important.\n\nThe fair percentage would be the participant's contribution relative to the total. So, fair_percent = (token_self / (token_self + token_opp)) * 100. The difference between the offered percentage (splitperc_self) and the fair percentage could drive the utility. If the offer is below the fair percentage, that's disadvantageous inequality; if above, advantageous.\n\nThe previous example used a linear model with a single parameter (alpha) for fairness sensitivity. But the revised instructions suggest using two parameters for asymmetric sensitivity. So, let's split alpha into alpha_advantageous and alpha_disadvantageous. This way, the model can capture different sensitivities when the offer is above or below the fair share.\n\nAlso, including a temperature parameter (tau) in the logistic function might help. The utility U would be scaled by tau before converting to a probability via the logistic function. This adds a second learnable parameter, which controls the steepness of the decision boundary.\n\nSo, the utility function becomes:\nU = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)\n\nThen, the probability of accepting is P(accept) = 1 / (1 + exp(-tau * U)).\n\nThis model has two learnable parameters: alpha_disadvantageous and alpha_advantageous, and tau. Wait, but the instructions say maximum of two parameters. Hmm. Maybe combine the temperature with one of the alphas? Or perhaps the temperature is necessary. Let me check the constraints again.\n\nThe critical constraints say maximum of 2 learnable parameters. So, I need to adjust. Maybe keep the two alphas and drop the temperature, or include tau as a third parameter. But the user's latest instructions for Run 3/5 allow up to two parameters. Wait, looking back: \"Maximum of 2 learnable parameters to avoid overfitting.\" So, I need to stick to two.\n\nSo, perhaps drop the temperature and keep the two alphas. Alternatively, use one alpha and a temperature. But the asymmetric sensitivity is important. Let's proceed with two alphas as the two parameters.\n\nWait, but the example in the previous response had two parameters (alpha and tau). But according to the latest instructions, maximum two. So, in this case, the model would have alpha_disadvantageous and alpha_advantageous as two parameters. That fits.\n\nNow, the variable descriptions. The fair_percent is calculated from token_self and token_opp. The splitperc_self is from the data. The max functions create terms for disadvantageous and advantageous inequality.\n\nEach alpha should have bounds. For disadvantageous (when offer is below fair), alpha_disadvantageous is positive, as a higher negative deviation reduces utility. For advantageous, maybe alpha_advantageous could be negative (if people feel guilty having more) or positive (if they like it). But to allow flexibility, set bounds that include both positive and negative, but perhaps with different ranges.\n\nFor alpha_disadvantageous, since offers below fair are likely to decrease acceptance, this parameter should be positive. So, set a min of 0 and a max of, say, 10. For alpha_advantageous, it could be negative or positive. Maybe set a wider range, like -5 to 5.\n\nWait, but in the utility function, if alpha_advantageous is positive, then offers above fair increase utility, making acceptance more likely. If negative, offers above fair decrease utility (maybe due to guilt), making acceptance less likely. So, the bounds should allow both possibilities.\n\nSo, variables:\n\n- alpha_disadvantageous: learnable, range [0, 10]\n- alpha_advantageous: learnable, range [-5, 5]\n\nThe utility U is the sum of the two terms. Then, the probability of accept is logistic(U). But the user's note says \"Predict utility, where less than 0 is less than 50% likely.\" So, the utility U is passed through a logistic function with temperature 1. So, P(accept) = 1 / (1 + exp(-U)).\n\nWait, but the model type says the utility will be converted to a probability using a logistic function with temperature 1. So, no need for a separate tau parameter here. Thus, the model has two learnable parameters: alpha_disadvantageous and alpha_advantageous.\n\nBut the example in the previous response included a temperature parameter (tau). However, the user's latest instruction says to have maximum two parameters. So, perhaps in this case, we proceed without tau, using the two alphas as the parameters.\n\nThus, the model would be:\n\nU = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)\n\nThen, P(accept) = 1 / (1 + exp(-U)).\n\nThis uses two parameters, which fits the constraint. The variables involved are calculated (fair_percent) and from the data (splitperc_self).\n\nNow, variable descriptions:\n\n- alpha_disadvantageous: learnable, range [0, 10], inclusive.\n- alpha_advantageous: learnable, range [-5, 5], inclusive.\n- fair_percent: calculated from token_self and token_opp.\n- splitperc_self: from data.\n- U: calculated utility.\n\nThis model allows different sensitivities to offers below and above the fair share. For example, if alpha_disadvantageous is high, even small negative deviations lead to rejection. If alpha_advantageous is negative, offers above fair reduce utility (perhaps due to guilt), making acceptance less likely, which could explain some rejections even when the offer is favorable.\n\nThis should help with parameter recovery as each parameter captures a distinct psychological factor. The bounds are set to be generous but finite. Testing this model should show if participants have asymmetric responses to inequality.\n</think>\n\n<MODEL>\nU = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha_disadvantageous\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (offer below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha_advantageous\": {\n      \"description\": \"Sensitivity to advantageous inequality (offer above fair share). Negative values indicate guilt aversion.\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"fair_percent\": {\n      \"description\": \"Fair share percentage based on token contributions: (token_self / (token_self + token_opp)) * 100\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage of total offered to participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-parameter fairness utility model that separately quantifies sensitivity to disadvantageous (offers below equitable share) and advantageous inequality (offers above equitable share). The fair share is computed from relative token contributions. Learnable parameters capture distinct psychological processes: rejection sensitivity for unfavorable offers and guilt aversion for favorable ones. Utility is converted to acceptance probability via logistic function.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)\nSummary: A dual-parameter fairness utility model that separately quantifies sensitivity to disadvantageous (offers below equitable share) and advantageous inequality (offers above equitable share). The fair share is computed from relative token contributions. Learnable parameters capture distinct psychological processes: rejection sensitivity for unfavorable offers and guilt aversion for favorable ones. Utility is converted to acceptance probability via logistic function.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha_disadvantageous=1, alpha_advantageous=1):\n    model_predictions = []\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n\n        total_tokens = token_self + token_opp\n        if total_tokens == 0:\n            fair_percent = 50  # Default to 50 if no tokens\n        else:\n            fair_percent = (token_self / total_tokens) * 100\n\n        disadvantageous_inequality = max(fair_percent - splitperc_self, 0)\n        advantageous_inequality = max(splitperc_self - fair_percent, 0)\n\n        U = alpha_disadvantageous * disadvantageous_inequality + alpha_advantageous * advantageous_inequality\n\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.6294261294261294,
  "fitting_results": [
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -46.59282175594248,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 6.403122906859032,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -415.8516110991237,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 7.681991255386147,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -392.82576016802295,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.224696684397198,
      "success": true,
      "log_likelihood": -23.56700413203871,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 4.685658008807037,
      "alpha_advantageous": 3.4384418770133944,
      "success": true,
      "log_likelihood": -461.9033132878661,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 8.746028559127453,
      "alpha_advantageous": 2.1450919620713833,
      "success": true,
      "log_likelihood": -300.7223570634803,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 4.233070036970141,
      "alpha_advantageous": 3.61804322355335,
      "success": true,
      "log_likelihood": -1.3862958481140533,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 10.0,
      "alpha_advantageous": 4.915740411289704,
      "success": true,
      "log_likelihood": -1.3862943573199027,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.914270190902863,
      "success": true,
      "log_likelihood": -23.56700413203813,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": -2.451695359471364,
      "success": true,
      "log_likelihood": -46.59285512385844,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 6.246988261989774,
      "alpha_advantageous": 3.6648136068481607,
      "success": true,
      "log_likelihood": -484.92916389000385,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 7.116706179406529,
      "alpha_advantageous": 3.9188450108968365,
      "success": true,
      "log_likelihood": -1.3862943574240623,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 10.0,
      "alpha_advantageous": 2.5240816705925107,
      "success": true,
      "log_likelihood": -1.3862944065278213,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 5.388229482207654,
      "alpha_advantageous": 2.588101338966772,
      "success": true,
      "log_likelihood": -1.3862944211155601,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 4.101084015261112,
      "alpha_advantageous": 3.614901243186223,
      "success": true,
      "log_likelihood": -1.3862966724516785,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 7.873002909103109,
      "alpha_advantageous": 1.7706834658536164,
      "success": true,
      "log_likelihood": -116.51555647794405,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 8.389617910180041,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -93.48969807748311,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 4.192951084303843,
      "alpha_advantageous": 3.093633426185999,
      "success": true,
      "log_likelihood": -1.386296062590158,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 9.234569287735834,
      "alpha_advantageous": 4.7860072360819945,
      "success": true,
      "log_likelihood": -116.51554900752224,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.478789700283184,
      "success": true,
      "log_likelihood": -23.56700413203823,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 2.0352228470310063,
      "success": true,
      "log_likelihood": -23.567005412675115,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 1.6173362842084027,
      "success": true,
      "log_likelihood": -23.567024896820055,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 5.343986117352171,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -93.48969811419947,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 9.80292720410327,
      "alpha_advantageous": 3.848081698016623,
      "success": true,
      "log_likelihood": -185.59310179765086,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 8.32237890162817,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -1.386294357321685,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 3.633464369346818,
      "success": true,
      "log_likelihood": -46.328934262241766,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.992460585729956,
      "success": true,
      "log_likelihood": -23.56700413203813,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 1.6201248236386816,
      "success": true,
      "log_likelihood": -23.567024514367148,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 9.999165965892686,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -1.3862943573199003,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 5.6025903380562925,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -254.67065460327066,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 2.2712475519603927,
      "success": true,
      "log_likelihood": -23.56700439753826,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 9.529348199527876,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -277.6965055178056,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.06810444430222842,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -15.967084729194159,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 9.560094373903977,
      "alpha_advantageous": 3.3958724369607225,
      "success": true,
      "log_likelihood": -323.74820737803367,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.263398252540286,
      "success": true,
      "log_likelihood": -23.567004132038576,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 6.548765872901308,
      "alpha_advantageous": 3.8881660579170543,
      "success": true,
      "log_likelihood": -24.412145288027702,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.076129977885097,
      "success": true,
      "log_likelihood": -23.567004132039703,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 1.6721687198917876,
      "success": true,
      "log_likelihood": -23.567018539005616,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.015496595267261031,
      "alpha_advantageous": 3.7362605414928547,
      "success": true,
      "log_likelihood": -22.91685437194126,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 5.436900707764705,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -392.82576019494553,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.56700413203813,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.56700413203813,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.7927464509166615,
      "success": true,
      "log_likelihood": -46.59272184418951,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.011102741765958053,
      "alpha_advantageous": 4.600534313288283,
      "success": true,
      "log_likelihood": -23.226889476488903,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": -4.933220374224307,
      "success": true,
      "log_likelihood": -46.59285506207858,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 8.794090002047154,
      "alpha_advantageous": 4.365167788652599,
      "success": true,
      "log_likelihood": -369.799909237968,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0020740230763470224,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -46.58072610715871,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.497990145257695,
      "success": true,
      "log_likelihood": -23.567004132038218,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.11670502129157494,
      "alpha_advantageous": 3.777746847747963,
      "success": true,
      "log_likelihood": -10.82436382799425,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": -3.9676997176741464,
      "success": true,
      "log_likelihood": -46.59285506208181,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 6.037213463918334,
      "alpha_advantageous": 3.9132937300987587,
      "success": true,
      "log_likelihood": -162.56725087124937,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.56700413203813,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 7.792783632426815,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -415.85161109805875,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": -3.5872321549845374,
      "success": true,
      "log_likelihood": -46.59285506211965,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 2.7651378919525547,
      "success": true,
      "log_likelihood": -23.567004141903322,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 3.4199509274662283,
      "success": true,
      "log_likelihood": -23.567004132163504,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.033387501240869,
      "success": true,
      "log_likelihood": -23.56700413204022,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0689623208599422,
      "alpha_advantageous": 1.3377936101218852,
      "success": true,
      "log_likelihood": -15.85300037816129,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 8.75539219928066,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -116.51554900752261,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.071021854855653,
      "success": true,
      "log_likelihood": -23.567004132039756,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 8.93085576140614,
      "alpha_advantageous": 4.951315725709623,
      "success": true,
      "log_likelihood": -70.4638471474415,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 3.5156500024089006,
      "success": true,
      "log_likelihood": -46.084565258456124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.567004132038125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.02458260248945063,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -22.018803468926627,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.4692646582722855,
      "success": true,
      "log_likelihood": -46.592855062078684,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 5.97364015578012,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -254.67065459226617,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 9.019990314111588,
      "alpha_advantageous": 3.509928759842577,
      "success": true,
      "log_likelihood": -47.437996217469774,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": -4.855628928067518,
      "success": true,
      "log_likelihood": -23.567004132038132,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.005347392908972028,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -23.486918463680897,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 5.0,
      "success": true,
      "log_likelihood": -46.592821755942495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 1.6216341894684045,
      "success": true,
      "log_likelihood": -23.56702431030167,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.10384014872414143,
      "alpha_advantageous": 1.6665477834310636,
      "success": true,
      "log_likelihood": -11.924380784909589,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": -4.883456012812085,
      "success": true,
      "log_likelihood": -23.567004132038132,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.587118682207091,
      "success": true,
      "log_likelihood": -23.567004132038175,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.00504814629640189,
      "alpha_advantageous": -4.182692134870181,
      "success": true,
      "log_likelihood": -69.53955352604372,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 4.999999999999998,
      "success": true,
      "log_likelihood": -23.56700413203813,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 7.220575053499948,
      "alpha_advantageous": 4.76160063985381,
      "success": true,
      "log_likelihood": -254.67065458783554,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 2.8889539582863217,
      "success": true,
      "log_likelihood": -46.59285506639991,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 9.822358501512138,
      "alpha_advantageous": 4.405792368583559,
      "success": true,
      "log_likelihood": -47.437996217400965,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.0,
      "alpha_advantageous": 3.4577138295893235,
      "success": true,
      "log_likelihood": -23.567004132135597,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha_disadvantageous": 0.01710874875766303,
      "alpha_advantageous": 1.8839676389805229,
      "success": true,
      "log_likelihood": -22.781020600870693,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha_disadvantageous": {
        "r": 0.17923647706430876,
        "p": 0.07437337442263368
      },
      "alpha_advantageous": {
        "r": 0.8511156071656301,
        "p": 3.5273331718713494e-29
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 168.31785156648328,
  "bic_results": {
    "average_bic": 168.31785156648328,
    "individual_bics": [
      100.35268138879718,
      838.8702600751596,
      792.8185582129581,
      54.30104614098964,
      930.9736644526445,
      608.6117520038728,
      9.939629573140326,
      54.30104614098847,
      9.939626591552026,
      54.30104614098847,
      54.301046140988475,
      100.3527481246291,
      977.0253656569199,
      9.939626591760344,
      9.939626689967863,
      9.939626719143341,
      9.939631221815578,
      240.1981508328003,
      194.14643403187844,
      9.939630002092535,
      54.30104614098847,
      240.1981358919567,
      54.30104614098868,
      54.30104614098847,
      54.30104870226245,
      54.30104614098847,
      54.30108767055233,
      54.30104614098847,
      194.14643410531116,
      378.35324147221394,
      9.93962659155559,
      99.82490640139575,
      54.301046140988475,
      54.301086905646514,
      9.93962659155202,
      54.30104614098847,
      516.5083470834536,
      54.301046671988736,
      562.5600489125234,
      39.101207335300536,
      654.6634526329796,
      54.30104614098937,
      55.99132845296762,
      54.30104614099162,
      54.30107495492345,
      53.000746620794736,
      792.8185582668033,
      54.301046140988475,
      54.301046140988475,
      100.35248156529124,
      53.620816829890025,
      100.35274800106937,
      746.7668563528482,
      100.32849009122964,
      54.30104614098847,
      54.30104614098865,
      54.30104614098847,
      28.81576553290072,
      100.35274800107584,
      332.30153961941096,
      54.301046140988475,
      838.8702600730297,
      100.35274800115151,
      54.30104616071886,
      54.301046141239226,
      54.30104614099266,
      38.8730386332348,
      240.19813589195743,
      54.30104614099173,
      148.09473217179521,
      54.30104614098847,
      99.33616839382447,
      54.30104614098847,
      51.20464481476547,
      100.35274800106959,
      516.5083470614446,
      102.04303031185177,
      54.30104614098848,
      54.14087480427401,
      100.35268138879721,
      54.301086497515556,
      31.0157994467314,
      54.30104614098848,
      54.30104614098857,
      146.24614492899966,
      54.301046140988475,
      516.5083470525833,
      100.35274800971204,
      102.04303031171415,
      54.30104614118341,
      52.729079078653605
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha_disadvantageous: r = 0.179\n- alpha_advantageous: r = 0.851",
  "previous_models": [
    "Specification: U = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * max(splitperc_self - fair_percent, 0)\nSummary: A dual-parameter fairness utility model that separately quantifies sensitivity to disadvantageous (offers below equitable share) and advantageous inequality (offers above equitable share). The fair share is computed from relative token contributions. Learnable parameters capture distinct psychological processes: rejection sensitivity for unfavorable offers and guilt aversion for favorable ones. Utility is converted to acceptance probability via logistic function.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 168.31785156648328\nOverall Accuracy: 0.6294\n\nParameter Recovery:\n- alpha_disadvantageous: r = 0.179\n- alpha_advantageous: r = 0.851"
  ]
}