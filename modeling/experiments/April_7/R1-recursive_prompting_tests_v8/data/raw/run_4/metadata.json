{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\n\nOkay, let's analyze the previous run results. The model had a BIC of 168.32, which is higher than the previous runs, indicating worse model fit. However, the accuracy improved to 62.9%, which is better. The parameter recovery for alpha_advantageous was good (r=0.851), but alpha_disadvantageous was poor (r=0.179). This suggests that the model's structure might be capturing advantageous inequality well but not disadvantageous.\n\nThe high BIC suggests that adding two parameters (alpha_disadvantageous and alpha_advantageous) might have overfit the data, especially since the previous single-parameter models had lower BIC. However, the accuracy improved, so there's a trade-off. The poor recovery for alpha_disadvantageous indicates that this parameter isn't being estimated reliably, possibly due to collinearity with other terms or insufficient data in certain conditions.\n\nTo improve, the next model should aim to maintain or increase accuracy while reducing BIC and improving parameter recovery. Since the previous model used two parameters but had mixed recovery, maybe introducing a non-linear transformation or interaction term could help. For example, using a sigmoid function for disadvantageous inequality to capture threshold effects, where participants only react strongly beyond a certain unfairness level.\n\nAnother angle: the model could incorporate trial_type as a moderator. For instance, in trials where the participant contributed more (trial_type=3), their sensitivity to disadvantageous inequality might be higher. Adding an interaction term between trial_type and the inequality terms could help. However, this would add more parameters, which might not be ideal given the BIC. Alternatively, use trial_type to split the data into subgroups, but that's not feasible here.\n\nAlternatively, consider a hybrid model that combines absolute and relative terms. For example, include both the percentage deviation and the absolute amount offered (split_self). This could capture participants who care about both fairness and the actual monetary gain. However, this adds another parameter, which might not be allowed due to the two-parameter constraint.\n\nWait, the previous instructions allowed up to two parameters. So, perhaps one parameter for disadvantageous and another that combines advantageous with an absolute term. For example: U = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * (splitperc_self - fair_percent + theta * split_self). But theta would be a third parameter, which isn't allowed. Alternatively, fix theta to a small value, but that's not learnable.\n\nAnother idea: use a non-linear function for disadvantageous inequality, like a quadratic term, to capture increasing sensitivity as offers get more unfair. For example: U = alpha_disadvantageous * (max(fair_percent - splitperc_self, 0))^2 + alpha_advantageous * max(splitperc_self - fair_percent, 0). This might better capture the psychology where small unfairness is tolerated, but larger deviations are punished more harshly. This keeps two parameters but introduces non-linearity.\n\nAlternatively, use a piecewise linear function with a breakpoint. For example, different alphas for different ranges of disadvantageous inequality. But that would require more parameters.\n\nThe temperature parameter (tau) could be reintroduced to scale the utility before the logistic function. This adds a third parameter, but if the constraint is strict, maybe combine it with one of the existing parameters. However, the previous instructions said max two parameters, so this might not be feasible.\n\nGiven the constraints, the best approach might be to keep two parameters but modify the model structure to better capture disadvantageous inequality. For example, using a non-linear transformation for the disadvantageous term while keeping the advantageous term linear. This could improve parameter recovery for alpha_disadvantageous by making the model more sensitive to variations in that term.\n\nAnother possibility is to include a baseline acceptance bias. For example, U = alpha_disadvantageous * max(...) + alpha_advantageous * max(...) + beta. Here, beta is a constant bias term. But this adds a third parameter. Alternatively, fix beta to zero, but that's not helpful.\n\nWait, the previous model didn't include a bias term. Maybe participants have a general tendency to accept or reject offers regardless of fairness. Adding a bias term (beta) could capture this, but again, it's a third parameter. If the constraint is strict, this isn't allowed. So, perhaps the model needs to find a way to capture this within two parameters.\n\nAlternatively, model the utility as U = alpha * (splitperc_self - fair_percent) + gamma * split_self. Here, alpha captures fairness sensitivity, and gamma captures sensitivity to absolute gains. This uses two parameters. However, this abandons the asymmetric approach but might improve BIC if it's a better fit.\n\nConsidering the poor recovery of alpha_disadvantageous, perhaps the model should focus on a different functional form for disadvantageous inequality. For example, using a logarithmic function to represent diminishing sensitivity: U = alpha_disadvantageous * log(1 + max(fair_percent - splitperc_self, 0)) + alpha_advantageous * max(...). This could help in cases where small deviations are less impactful, improving parameter estimation.\n\nIn summary, the new instructions should encourage models that:\n\n1. Introduce non-linear transformations for disadvantageous inequality to improve parameter recovery.\n2. Consider hybrid terms combining relative and absolute fairness.\n3. Use trial_type as a moderator for sensitivity parameters without adding extra learnable parameters.\n4. Experiment with different functional forms (e.g., quadratic, logarithmic) for inequality terms.\n5. Maintain a maximum of two learnable parameters to avoid overfitting.\n\nSpecific suggestions could include using a sigmoid for disadvantageous inequality, incorporating absolute terms with one parameter, or using trial_type to adjust sensitivity without additional parameters.\n</think>\n\n**Revised Instructions for Run 4/5:**  \n\n1. **Non-Linear Disadvantageous Sensitivity**: Replace linear terms for disadvantageous inequality with *saturating functions* (e.g., `log(1 + x)`, `sqrt(x)`) to model diminishing sensitivity to extreme unfairness. Example:  \n   `U = \u03b1\u2081 * log(1 + max(fair_percent \u2212 splitperc_self, 0)) + \u03b1\u2082 * max(splitperc_self \u2212 fair_percent, 0)`  \n\n2. **Trial-Type Contextualization**: Use `trial_type` to *non-parametrically* scale sensitivity:  \n   - For `trial_type=3` (participant contributed more), multiply disadvantageous sensitivity by 2x (no new parameters).  \n   - Example: `U = \u03b1\u2081 * [1 + I_{trial_type=3}] * max(fair_percent \u2212 splitperc_self, 0) + \u03b1\u2082 * max(...)`  \n\n3. **Absolute Fairness Threshold**: Add a fixed threshold (e.g., 10%) where disadvantageous sensitivity *only activates* below it:  \n   `U = \u03b1\u2081 * max(fair_percent \u2212 splitperc_self \u2212 10, 0) + \u03b1\u2082 * max(...)`  \n\n4. **Hybrid Logistic-Nonlinear**: Apply logistic transformation *directly to fairness deviation* before scaling:  \n   `U = \u03b1\u2081 * (1 / (1 + exp(\u2212(fair_percent \u2212 splitperc_self)))) + \u03b1\u2082 * max(...)`  \n\n5. **Dynamic Reference Points**: Let the fair percentage include a *learnable bias* (\u03b4) shared across parameters:  \n   `fair_percent_adj = fair_percent + \u03b4` (\u03b4 \u2208 [\u221210, 10], counts as 1 of 2 parameters).  \n\n**Critical Constraints**:  \n- Retain **2 learnable parameters** with *orthogonal roles* (e.g., one for nonlinear fairness, one for absolute gains).  \n- Use *non-overlapping bounds* (e.g., \u03b1\u2081 \u2208 [0, 5], \u03b1\u2082 \u2208 [\u22122, 2]) to prevent identifiability issues.  \n\n**Key Focus**: Radically simplify the disadvantageous term\u2019s structure while testing whether explicit trial-type logic (without new parameters) improves recovery. Prioritize models where \u03b1\u2081 recovery exceeds r > 0.6.",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: <think>\n\nOkay, let's analyze the previous run results. The model had a BIC of 168.32, which is higher than the previous runs, indicating worse model fit. However, the accuracy improved to 62.9%, which is better. The parameter recovery for alpha_advantageous was good (r=0.851), but alpha_disadvantageous was poor (r=0.179). This suggests that the model's structure might be capturing advantageous inequality well but not disadvantageous.\n\nThe high BIC suggests that adding two parameters (alpha_disadvantageous and alpha_advantageous) might have overfit the data, especially since the previous single-parameter models had lower BIC. However, the accuracy improved, so there's a trade-off. The poor recovery for alpha_disadvantageous indicates that this parameter isn't being estimated reliably, possibly due to collinearity with other terms or insufficient data in certain conditions.\n\nTo improve, the next model should aim to maintain or increase accuracy while reducing BIC and improving parameter recovery. Since the previous model used two parameters but had mixed recovery, maybe introducing a non-linear transformation or interaction term could help. For example, using a sigmoid function for disadvantageous inequality to capture threshold effects, where participants only react strongly beyond a certain unfairness level.\n\nAnother angle: the model could incorporate trial_type as a moderator. For instance, in trials where the participant contributed more (trial_type=3), their sensitivity to disadvantageous inequality might be higher. Adding an interaction term between trial_type and the inequality terms could help. However, this would add more parameters, which might not be ideal given the BIC. Alternatively, use trial_type to split the data into subgroups, but that's not feasible here.\n\nAlternatively, consider a hybrid model that combines absolute and relative terms. For example, include both the percentage deviation and the absolute amount offered (split_self). This could capture participants who care about both fairness and the actual monetary gain. However, this adds another parameter, which might not be allowed due to the two-parameter constraint.\n\nWait, the previous instructions allowed up to two parameters. So, perhaps one parameter for disadvantageous and another that combines advantageous with an absolute term. For example: U = alpha_disadvantageous * max(fair_percent - splitperc_self, 0) + alpha_advantageous * (splitperc_self - fair_percent + theta * split_self). But theta would be a third parameter, which isn't allowed. Alternatively, fix theta to a small value, but that's not learnable.\n\nAnother idea: use a non-linear function for disadvantageous inequality, like a quadratic term, to capture increasing sensitivity as offers get more unfair. For example: U = alpha_disadvantageous * (max(fair_percent - splitperc_self, 0))^2 + alpha_advantageous * max(splitperc_self - fair_percent, 0). This might better capture the psychology where small unfairness is tolerated, but larger deviations are punished more harshly. This keeps two parameters but introduces non-linearity.\n\nAlternatively, use a piecewise linear function with a breakpoint. For example, different alphas for different ranges of disadvantageous inequality. But that would require more parameters.\n\nThe temperature parameter (tau) could be reintroduced to scale the utility before the logistic function. This adds a third parameter, but if the constraint is strict, maybe combine it with one of the existing parameters. However, the previous instructions said max two parameters, so this might not be feasible.\n\nGiven the constraints, the best approach might be to keep two parameters but modify the model structure to better capture disadvantageous inequality. For example, using a non-linear transformation for the disadvantageous term while keeping the advantageous term linear. This could improve parameter recovery for alpha_disadvantageous by making the model more sensitive to variations in that term.\n\nAnother possibility is to include a baseline acceptance bias. For example, U = alpha_disadvantageous * max(...) + alpha_advantageous * max(...) + beta. Here, beta is a constant bias term. But this adds a third parameter. Alternatively, fix beta to zero, but that's not helpful.\n\nWait, the previous model didn't include a bias term. Maybe participants have a general tendency to accept or reject offers regardless of fairness. Adding a bias term (beta) could capture this, but again, it's a third parameter. If the constraint is strict, this isn't allowed. So, perhaps the model needs to find a way to capture this within two parameters.\n\nAlternatively, model the utility as U = alpha * (splitperc_self - fair_percent) + gamma * split_self. Here, alpha captures fairness sensitivity, and gamma captures sensitivity to absolute gains. This uses two parameters. However, this abandons the asymmetric approach but might improve BIC if it's a better fit.\n\nConsidering the poor recovery of alpha_disadvantageous, perhaps the model should focus on a different functional form for disadvantageous inequality. For example, using a logarithmic function to represent diminishing sensitivity: U = alpha_disadvantageous * log(1 + max(fair_percent - splitperc_self, 0)) + alpha_advantageous * max(...). This could help in cases where small deviations are less impactful, improving parameter estimation.\n\nIn summary, the new instructions should encourage models that:\n\n1. Introduce non-linear transformations for disadvantageous inequality to improve parameter recovery.\n2. Consider hybrid terms combining relative and absolute fairness.\n3. Use trial_type as a moderator for sensitivity parameters without adding extra learnable parameters.\n4. Experiment with different functional forms (e.g., quadratic, logarithmic) for inequality terms.\n5. Maintain a maximum of two learnable parameters to avoid overfitting.\n\nSpecific suggestions could include using a sigmoid for disadvantageous inequality, incorporating absolute terms with one parameter, or using trial_type to adjust sensitivity without additional parameters.\n</think>\n\n**Revised Instructions for Run 4/5:**  \n\n1. **Non-Linear Disadvantageous Sensitivity**: Replace linear terms for disadvantageous inequality with *saturating functions* (e.g., `log(1 + x)`, `sqrt(x)`) to model diminishing sensitivity to extreme unfairness. Example:  \n   `U = \u03b1\u2081 * log(1 + max(fair_percent \u2212 splitperc_self, 0)) + \u03b1\u2082 * max(splitperc_self \u2212 fair_percent, 0)`  \n\n2. **Trial-Type Contextualization**: Use `trial_type` to *non-parametrically* scale sensitivity:  \n   - For `trial_type=3` (participant contributed more), multiply disadvantageous sensitivity by 2x (no new parameters).  \n   - Example: `U = \u03b1\u2081 * [1 + I_{trial_type=3}] * max(fair_percent \u2212 splitperc_self, 0) + \u03b1\u2082 * max(...)`  \n\n3. **Absolute Fairness Threshold**: Add a fixed threshold (e.g., 10%) where disadvantageous sensitivity *only activates* below it:  \n   `U = \u03b1\u2081 * max(fair_percent \u2212 splitperc_self \u2212 10, 0) + \u03b1\u2082 * max(...)`  \n\n4. **Hybrid Logistic-Nonlinear**: Apply logistic transformation *directly to fairness deviation* before scaling:  \n   `U = \u03b1\u2081 * (1 / (1 + exp(\u2212(fair_percent \u2212 splitperc_self)))) + \u03b1\u2082 * max(...)`  \n\n5. **Dynamic Reference Points**: Let the fair percentage include a *learnable bias* (\u03b4) shared across parameters:  \n   `fair_percent_adj = fair_percent + \u03b4` (\u03b4 \u2208 [\u221210, 10], counts as 1 of 2 parameters).  \n\n**Critical Constraints**:  \n- Retain **2 learnable parameters** with *orthogonal roles* (e.g., one for nonlinear fairness, one for absolute gains).  \n- Use *non-overlapping bounds* (e.g., \u03b1\u2081 \u2208 [0, 5], \u03b1\u2082 \u2208 [\u22122, 2]) to prevent identifiability issues.  \n\n**Key Focus**: Radically simplify the disadvantageous term\u2019s structure while testing whether explicit trial-type logic (without new parameters) improves recovery. Prioritize models where \u03b1\u2081 recovery exceeds r > 0.6.\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant accepts or rejects a proposal, focusing on responder behavior. The model needs to use the given variables appropriately, pass parameter recovery tests, and have learnable parameters with finite bounds.\n\nFirst, I need to identify which variables are relevant for predicting 'accept'. Since we're focusing on responder behavior, we should look at trials where trial_role is 1 (responder). The key factors here are likely the fairness of the split and the participant's own contribution.\n\nThe variables token_self and token_opp determine the combined_earning (each token is \u00a33, so combined_earning = 3*(token_self + token_opp)). The split_self and split_opp are the proposed shares. However, the problem states that equal split percentages don't represent the same fairness because the combined earning varies. So, the model should consider the participant's contribution relative to the opponent's.\n\nA common approach in such scenarios is to model fairness using inequity aversion. Participants might dislike unequal outcomes, so their utility depends on both their own share and the difference from what's considered fair. The fair percentage for the participant would be their contribution (token_self) divided by the total tokens (token_self + token_opp), multiplied by 100. If the proposed percentage (splitperc_self) is less than this fair percentage, it's disadvantageous inequity. If it's more, it's advantageous.\n\nThe utility function could then be based on the difference between the proposed percentage and the fair percentage, with separate parameters for disadvantageous and advantageous inequity. For example:\n\nUtility = \u03b1 * max(fair_percent - splitperc_self, 0) + \u03b2 * max(splitperc_self - fair_percent, 0)\n\nHere, \u03b1 represents sensitivity to disadvantageous inequity (getting less than fair), and \u03b2 for advantageous (getting more). However, since the task mentions that the model should predict utility where less than 0 is less than 50% likely, we can pass this utility through a logistic function to get the probability of acceptance.\n\nBut wait, the model needs to predict the binary 'accept' variable. So the utility would be transformed into a probability using a logistic function. The parameters \u03b1 and \u03b2 would be learned, with bounds to keep them within a reasonable range. For example, \u03b1 (disadvantageous) might be negative because getting less than fair decreases utility, leading to rejection. \u03b2 could be positive, as getting more might increase acceptance likelihood, but maybe participants don't care as much about advantageous inequity.\n\nAlternatively, maybe only disadvantageous inequity matters. Let me think. If the proposed share is less than fair, the participant is more likely to reject. If it's equal or more, they accept. So perhaps the model should focus on the disadvantageous term. But the problem states that two equal split_self variables don't represent the same fairness, so the fair percentage must be calculated based on tokens.\n\nAnother consideration: the accepted_amount is the amount the participant gets if they accept. So the utility could also depend on the absolute amount, not just the percentage. But the model should use the variables appropriately. Since split_self is in \u00a3, maybe the utility combines both the absolute amount and the fairness.\n\nWait, the model needs to use the provided variables. The splitperc_self is the proposed percentage for the participant. The fair percentage would be (token_self / (token_self + token_opp)) * 100. The difference between splitperc_self and fair_percent indicates how fair the offer is. So the utility could be a function of this difference, with parameters for how much the participant cares about being below or above the fair percentage.\n\nSo, the utility function might look like:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * split_self\n\nBut split_self is in \u00a3, which is 3 * token_self * (splitperc_self / 100). Wait, no. split_self is the proposed amount in \u00a3, which is (splitperc_self / 100) * combined_earning. Combined earning is 3*(token_self + token_opp). So split_self = (splitperc_self / 100) * 3*(token_self + token_opp). But maybe participants care about the absolute amount they receive, not just the percentage. So including split_self as a separate term could capture that.\n\nAlternatively, the utility could be based on the difference between the proposed percentage and the fair percentage, scaled by some parameters. For example:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * (split_self - fair_amount)\n\nWhere fair_amount is (token_self / (token_self + token_opp)) * combined_earning. But this might be redundant since split_self is already derived from the percentage. Hmm.\n\nAlternatively, the model could focus on the difference between the proposed percentage and the fair percentage, with separate parameters for when the proposal is disadvantageous (splitperc_self < fair_percent) and advantageous (splitperc_self > fair_percent). This is similar to inequity aversion models where individuals have different sensitivities to being ahead or behind.\n\nSo, the utility could be:\n\nU = \u03b1 * max(fair_percent - splitperc_self, 0) + \u03b2 * max(splitperc_self - fair_percent, 0)\n\nHere, \u03b1 is the sensitivity to disadvantageous inequity (negative impact on utility), and \u03b2 is sensitivity to advantageous (positive impact). However, typically in such models, \u03b1 is negative (disutility when getting less than fair) and \u03b2 might be positive or negative. But since the model needs to predict acceptance, which is more likely when utility is higher, the parameters should be set such that higher U increases acceptance probability.\n\nWait, the utility is passed through a logistic function to get the probability of acceptance. So if U is the utility, then P(accept) = 1 / (1 + exp(-U)). Therefore, if U is positive, the probability is above 50%; if negative, below.\n\nSo, for disadvantageous inequity (splitperc_self < fair_percent), the term max(fair_percent - splitperc_self, 0) is positive. If \u03b1 is negative, this would decrease U, making rejection more likely. For advantageous, if \u03b2 is positive, it increases U, making acceptance more likely.\n\nBut the problem states that the model should predict utility where less than 0 is less than 50% likely. So the utility directly maps to the log-odds of acceptance.\n\nTherefore, the model should calculate U based on the fairness and other factors, then P(accept) = logistic(U).\n\nNow, considering the parameters. The learnable parameters are \u03b1 and \u03b2. They need finite bounds. For \u03b1 (disadvantageous), we can set a negative range, say between -10 and 0, as a higher negative value means more sensitivity to being shortchanged. For \u03b2 (advantageous), maybe between -5 and 5, but if participants don't care about being overpaid, \u03b2 could be near zero. Alternatively, maybe \u03b2 is positive, so getting more than fair increases utility.\n\nBut perhaps the advantageous term isn't necessary. Maybe participants only care about disadvantageous inequity. So the model could have only \u03b1 as a parameter. However, the problem says the model should pass parameter recovery tests, which might require at least two parameters to capture different aspects.\n\nAlternatively, include a term for the absolute offer (split_self) in addition to the fairness term. For example:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * split_self\n\nHere, \u03b1 captures fairness sensitivity, and \u03b2 captures the linear effect of the absolute amount. But since split_self is correlated with splitperc_self (as split_self = combined_earning * splitperc_self /100), there might be multicollinearity. However, the model might still work if both terms are important.\n\nBut given that the combined earning varies, the same split percentage can lead to different absolute amounts. So participants might care about both the percentage (fairness) and the absolute amount. For example, a 40% offer might be unfair, but if the absolute amount is high enough, they might still accept.\n\nSo, including both terms could make sense. The parameters \u03b1 and \u03b2 would then represent the trade-off between fairness and absolute gain.\n\nAlternatively, use the difference between split_self and the fair amount (fair_amount = (token_self / (token_self + token_opp)) * combined_earning). So:\n\nU = \u03b1 * (split_self - fair_amount) + \u03b2 * split_self\n\nBut this might be similar to the percentage difference, since split_self - fair_amount = combined_earning * (splitperc_self - fair_percent)/100.\n\nHmm. So perhaps using the percentage difference is sufficient, but including the absolute amount as a separate term could add explanatory power.\n\nAnother angle: the utility could be based on the ratio of the proposed share to the fair share. For example:\n\nU = \u03b1 * (split_self / fair_amount - 1) + \u03b2 * split_self\n\nBut this complicates the model. Maybe stick to the percentage difference.\n\nConsidering the need for parameter recovery, having two parameters (e.g., \u03b1 and \u03b2) that are orthogonal would help. If the model has parameters that are not correlated, recovery is easier.\n\nPutting this all together, the model could be:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * split_self\n\nWhere fair_percent is calculated as (token_self / (token_self + token_opp)) * 100.\n\nThis way, \u03b1 captures the sensitivity to fairness (positive \u03b1 means higher utility when the offer is more than fair), and \u03b2 captures the sensitivity to the absolute amount received. Both parameters are learnable.\n\nThe probability of acceptance is then logistic(U). The parameters \u03b1 and \u03b2 need bounds. For \u03b1, perhaps between -10 and 10, and \u03b2 between 0 and 1 (assuming higher absolute amounts increase acceptance likelihood). But \u03b2 could also be negative if higher amounts sometimes lead to rejection, though that's unlikely.\n\nAlternatively, since split_self is in \u00a3, and higher amounts are better, \u03b2 should be positive. So \u03b2's range could be [0, 5], and \u03b1 could be [-10, 10].\n\nBut the problem states that the utility variable may be unbounded, but learnable parameters must have finite bounds. So \u03b1 and \u03b2 need to have specified ranges.\n\nAnother consideration: the fair_percent is a calculated variable based on token_self and token_opp. Since these are provided in the dataset, the model can compute it.\n\nSo, the mathematical model would be:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * split_self\n\nWhere fair_percent = (token_self / (token_self + token_opp)) * 100\n\nThis uses the variables splitperc_self, token_self, token_opp, and split_self from the dataset.\n\nNow, the variables section needs to describe each variable in the model. The learnable parameters are \u03b1 and \u03b2. The other variables (splitperc_self, token_self, token_opp, split_self) are from the data.\n\nFor the parameters:\n\n- \u03b1: sensitivity to fairness difference. Range could be [-10, 10], inclusive.\n- \u03b2: sensitivity to absolute amount. Range [0, 5], inclusive.\n\nThese ranges are generous but finite.\n\nThe target variable is 'accept', which is binary. The model's utility U is used in a logistic function to predict the probability of acceptance.\n\nSo, the final model is encapsulated as:\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta * split_self\n</MODEL>\n\nVariables:\n\n- alpha: learnable parameter, range [-10, 10]\n- beta: learnable parameter, range [0, 5]\n- splitperc_self: from data\n- token_self: from data\n- token_opp: from data\n- split_self: from data\n\nThe utility U is calculated, then passed through a logistic function to get P(accept=1).\n\nNow, structuring the variables in JSON:\n\nFor alpha and beta, specify their ranges and descriptions. The other variables are data or calculated.\n\n\"variables\": {\n  \"alpha\": {\n    \"description\": \"Sensitivity to fairness deviation (positive values increase utility when offer exceeds fair percentage)\",\n    \"range\": { \"min\": -10, \"max\": 10, \"inclusive_min\": true, \"inclusive_max\": true },\n    \"learnable\": true,\n    \"source\": \"learnable\"\n  },\n  \"beta\": {\n    \"description\": \"Sensitivity to absolute offer amount in \u00a3 (positive values increase utility with higher split_self)\",\n    \"range\": { \"min\": 0, \"max\": 5, \"inclusive_min\": true, \"inclusive_max\": true },\n    \"learnable\": true,\n    \"source\": \"learnable\"\n  },\n  \"splitperc_self\": {\n    \"description\": \"Proposed percentage for the participant\",\n    \"source\": \"data\",\n    \"learnable\": false\n  },\n  \"token_self\": {\n    \"description\": \"Number of tokens found by the participant\",\n    \"source\": \"data\",\n    \"learnable\": false\n  },\n  \"token_opp\": {\n    \"description\": \"Number of tokens found by the opponent\",\n    \"source\": \"data\",\n    \"learnable\": false\n  },\n  \"split_self\": {\n    \"description\": \"Proposed share for the participant in \u00a3\",\n    \"source\": \"data\",\n    \"learnable\": false\n  }\n}\n\nThe target variable is 'accept'.\n\nThe summary should concisely describe the model: it combines fairness deviation and absolute offer amount to predict acceptance, with two learnable parameters for sensitivity to each factor.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta * split_self\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to percentage deviation from fair split (positive values increase utility when offered percentage exceeds contribution-based fair share)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute monetary amount offered (\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage of total pot offered to participant\",\n      \"source\": \"data\",\n      \"learnable\": false\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution to shared pot\",\n      \"source\": \"data\",\n      \"learnable\": false\n    },\n    \"token_opp\": {\n      \"description\": \"Opponent's token contribution to shared pot\",\n      \"source\": \"data\",\n      \"learnable\": false\n    },\n    \"split_self\": {\n      \"description\": \"Absolute \u00a3 amount offered to participant\",\n      \"source\": \"data\",\n      \"learnable\": false\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-component utility model combining relative fairness evaluation (percentage offered vs contribution-based fair share) and absolute monetary value. Two learnable parameters capture 1) sensitivity to equitable distribution and 2) preference for higher absolute payoffs, enabling separate recovery of fairness vs self-interest motives.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta * split_self\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to percentage deviation from fair split (positive values increase utility when offered percentage exceeds contribution-based fair share)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute monetary amount offered (\\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage of total pot offered to participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution to shared pot\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Opponent's token contribution to shared pot\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_self\": {\n      \"description\": \"Absolute \\u00a3 amount offered to participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        split_self = trial.get(\"split_self\", 0)\n\n        # Calculate fair percentage\n        fair_percent = (token_self / (token_self + token_opp)) * 100 if (token_self + token_opp) > 0 else 50\n\n        # Calculate utility\n        utility = alpha * (splitperc_self - fair_percent) + beta * split_self\n\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 11, \"token_self\": 11, \"combined_earning\": 66, \"split_opp\": 19.8, \"split_self\": 46.2, \"splitperc_opp\": 30, \"splitperc_self\": 70, \"accept\": 1.0, \"accepted_amount\": 46.2, \"accepted_perc\": 70.0, \"proposed_perc\": 70.0, \"proposed_amount\": 46.2}, {\"ID\": 1, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 11, \"token_self\": 11, \"combined_earning\": 66, \"split_opp\": 33.0, \"split_self\": 33.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 33.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 33.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1744033487.216074
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to percentage deviation from fair split (positive values increase utility when offered percentage exceeds contribution-based fair share)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to absolute monetary amount offered (\u00a3)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta * split_self",
  "variable_descriptions": {
    "alpha": {
      "description": "Sensitivity to percentage deviation from fair split (positive values increase utility when offered percentage exceeds contribution-based fair share)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to absolute monetary amount offered (\u00a3)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "splitperc_self": {
      "description": "Percentage of total pot offered to participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Participant's token contribution to shared pot",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Opponent's token contribution to shared pot",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_self": {
      "description": "Absolute \u00a3 amount offered to participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A dual-component utility model combining relative fairness evaluation (percentage offered vs contribution-based fair share) and absolute monetary value. Two learnable parameters capture 1) sensitivity to equitable distribution and 2) preference for higher absolute payoffs, enabling separate recovery of fairness vs self-interest motives.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant accepts or rejects a proposal, focusing on responder behavior. The model needs to use the given variables appropriately, pass parameter recovery tests, and have learnable parameters with finite bounds.\n\nFirst, I need to identify which variables are relevant for predicting 'accept'. Since we're focusing on responder behavior, we should look at trials where trial_role is 1 (responder). The key factors here are likely the fairness of the split and the participant's own contribution.\n\nThe variables token_self and token_opp determine the combined_earning (each token is \u00a33, so combined_earning = 3*(token_self + token_opp)). The split_self and split_opp are the proposed shares. However, the problem states that equal split percentages don't represent the same fairness because the combined earning varies. So, the model should consider the participant's contribution relative to the opponent's.\n\nA common approach in such scenarios is to model fairness using inequity aversion. Participants might dislike unequal outcomes, so their utility depends on both their own share and the difference from what's considered fair. The fair percentage for the participant would be their contribution (token_self) divided by the total tokens (token_self + token_opp), multiplied by 100. If the proposed percentage (splitperc_self) is less than this fair percentage, it's disadvantageous inequity. If it's more, it's advantageous.\n\nThe utility function could then be based on the difference between the proposed percentage and the fair percentage, with separate parameters for disadvantageous and advantageous inequity. For example:\n\nUtility = \u03b1 * max(fair_percent - splitperc_self, 0) + \u03b2 * max(splitperc_self - fair_percent, 0)\n\nHere, \u03b1 represents sensitivity to disadvantageous inequity (getting less than fair), and \u03b2 for advantageous (getting more). However, since the task mentions that the model should predict utility where less than 0 is less than 50% likely, we can pass this utility through a logistic function to get the probability of acceptance.\n\nBut wait, the model needs to predict the binary 'accept' variable. So the utility would be transformed into a probability using a logistic function. The parameters \u03b1 and \u03b2 would be learned, with bounds to keep them within a reasonable range. For example, \u03b1 (disadvantageous) might be negative because getting less than fair decreases utility, leading to rejection. \u03b2 could be positive, as getting more might increase acceptance likelihood, but maybe participants don't care as much about advantageous inequity.\n\nAlternatively, maybe only disadvantageous inequity matters. Let me think. If the proposed share is less than fair, the participant is more likely to reject. If it's equal or more, they accept. So perhaps the model should focus on the disadvantageous term. But the problem states that two equal split_self variables don't represent the same fairness, so the fair percentage must be calculated based on tokens.\n\nAnother consideration: the accepted_amount is the amount the participant gets if they accept. So the utility could also depend on the absolute amount, not just the percentage. But the model should use the variables appropriately. Since split_self is in \u00a3, maybe the utility combines both the absolute amount and the fairness.\n\nWait, the model needs to use the provided variables. The splitperc_self is the proposed percentage for the participant. The fair percentage would be (token_self / (token_self + token_opp)) * 100. The difference between splitperc_self and fair_percent indicates how fair the offer is. So the utility could be a function of this difference, with parameters for how much the participant cares about being below or above the fair percentage.\n\nSo, the utility function might look like:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * split_self\n\nBut split_self is in \u00a3, which is 3 * token_self * (splitperc_self / 100). Wait, no. split_self is the proposed amount in \u00a3, which is (splitperc_self / 100) * combined_earning. Combined earning is 3*(token_self + token_opp). So split_self = (splitperc_self / 100) * 3*(token_self + token_opp). But maybe participants care about the absolute amount they receive, not just the percentage. So including split_self as a separate term could capture that.\n\nAlternatively, the utility could be based on the difference between the proposed percentage and the fair percentage, scaled by some parameters. For example:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * (split_self - fair_amount)\n\nWhere fair_amount is (token_self / (token_self + token_opp)) * combined_earning. But this might be redundant since split_self is already derived from the percentage. Hmm.\n\nAlternatively, the model could focus on the difference between the proposed percentage and the fair percentage, with separate parameters for when the proposal is disadvantageous (splitperc_self < fair_percent) and advantageous (splitperc_self > fair_percent). This is similar to inequity aversion models where individuals have different sensitivities to being ahead or behind.\n\nSo, the utility could be:\n\nU = \u03b1 * max(fair_percent - splitperc_self, 0) + \u03b2 * max(splitperc_self - fair_percent, 0)\n\nHere, \u03b1 is the sensitivity to disadvantageous inequity (negative impact on utility), and \u03b2 is sensitivity to advantageous (positive impact). However, typically in such models, \u03b1 is negative (disutility when getting less than fair) and \u03b2 might be positive or negative. But since the model needs to predict acceptance, which is more likely when utility is higher, the parameters should be set such that higher U increases acceptance probability.\n\nWait, the utility is passed through a logistic function to get the probability of acceptance. So if U is the utility, then P(accept) = 1 / (1 + exp(-U)). Therefore, if U is positive, the probability is above 50%; if negative, below.\n\nSo, for disadvantageous inequity (splitperc_self < fair_percent), the term max(fair_percent - splitperc_self, 0) is positive. If \u03b1 is negative, this would decrease U, making rejection more likely. For advantageous, if \u03b2 is positive, it increases U, making acceptance more likely.\n\nBut the problem states that the model should predict utility where less than 0 is less than 50% likely. So the utility directly maps to the log-odds of acceptance.\n\nTherefore, the model should calculate U based on the fairness and other factors, then P(accept) = logistic(U).\n\nNow, considering the parameters. The learnable parameters are \u03b1 and \u03b2. They need finite bounds. For \u03b1 (disadvantageous), we can set a negative range, say between -10 and 0, as a higher negative value means more sensitivity to being shortchanged. For \u03b2 (advantageous), maybe between -5 and 5, but if participants don't care about being overpaid, \u03b2 could be near zero. Alternatively, maybe \u03b2 is positive, so getting more than fair increases utility.\n\nBut perhaps the advantageous term isn't necessary. Maybe participants only care about disadvantageous inequity. So the model could have only \u03b1 as a parameter. However, the problem says the model should pass parameter recovery tests, which might require at least two parameters to capture different aspects.\n\nAlternatively, include a term for the absolute offer (split_self) in addition to the fairness term. For example:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * split_self\n\nHere, \u03b1 captures fairness sensitivity, and \u03b2 captures the linear effect of the absolute amount. But since split_self is correlated with splitperc_self (as split_self = combined_earning * splitperc_self /100), there might be multicollinearity. However, the model might still work if both terms are important.\n\nBut given that the combined earning varies, the same split percentage can lead to different absolute amounts. So participants might care about both the percentage (fairness) and the absolute amount. For example, a 40% offer might be unfair, but if the absolute amount is high enough, they might still accept.\n\nSo, including both terms could make sense. The parameters \u03b1 and \u03b2 would then represent the trade-off between fairness and absolute gain.\n\nAlternatively, use the difference between split_self and the fair amount (fair_amount = (token_self / (token_self + token_opp)) * combined_earning). So:\n\nU = \u03b1 * (split_self - fair_amount) + \u03b2 * split_self\n\nBut this might be similar to the percentage difference, since split_self - fair_amount = combined_earning * (splitperc_self - fair_percent)/100.\n\nHmm. So perhaps using the percentage difference is sufficient, but including the absolute amount as a separate term could add explanatory power.\n\nAnother angle: the utility could be based on the ratio of the proposed share to the fair share. For example:\n\nU = \u03b1 * (split_self / fair_amount - 1) + \u03b2 * split_self\n\nBut this complicates the model. Maybe stick to the percentage difference.\n\nConsidering the need for parameter recovery, having two parameters (e.g., \u03b1 and \u03b2) that are orthogonal would help. If the model has parameters that are not correlated, recovery is easier.\n\nPutting this all together, the model could be:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * split_self\n\nWhere fair_percent is calculated as (token_self / (token_self + token_opp)) * 100.\n\nThis way, \u03b1 captures the sensitivity to fairness (positive \u03b1 means higher utility when the offer is more than fair), and \u03b2 captures the sensitivity to the absolute amount received. Both parameters are learnable.\n\nThe probability of acceptance is then logistic(U). The parameters \u03b1 and \u03b2 need bounds. For \u03b1, perhaps between -10 and 10, and \u03b2 between 0 and 1 (assuming higher absolute amounts increase acceptance likelihood). But \u03b2 could also be negative if higher amounts sometimes lead to rejection, though that's unlikely.\n\nAlternatively, since split_self is in \u00a3, and higher amounts are better, \u03b2 should be positive. So \u03b2's range could be [0, 5], and \u03b1 could be [-10, 10].\n\nBut the problem states that the utility variable may be unbounded, but learnable parameters must have finite bounds. So \u03b1 and \u03b2 need to have specified ranges.\n\nAnother consideration: the fair_percent is a calculated variable based on token_self and token_opp. Since these are provided in the dataset, the model can compute it.\n\nSo, the mathematical model would be:\n\nU = \u03b1 * (splitperc_self - fair_percent) + \u03b2 * split_self\n\nWhere fair_percent = (token_self / (token_self + token_opp)) * 100\n\nThis uses the variables splitperc_self, token_self, token_opp, and split_self from the dataset.\n\nNow, the variables section needs to describe each variable in the model. The learnable parameters are \u03b1 and \u03b2. The other variables (splitperc_self, token_self, token_opp, split_self) are from the data.\n\nFor the parameters:\n\n- \u03b1: sensitivity to fairness difference. Range could be [-10, 10], inclusive.\n- \u03b2: sensitivity to absolute amount. Range [0, 5], inclusive.\n\nThese ranges are generous but finite.\n\nThe target variable is 'accept', which is binary. The model's utility U is used in a logistic function to predict the probability of acceptance.\n\nSo, the final model is encapsulated as:\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta * split_self\n</MODEL>\n\nVariables:\n\n- alpha: learnable parameter, range [-10, 10]\n- beta: learnable parameter, range [0, 5]\n- splitperc_self: from data\n- token_self: from data\n- token_opp: from data\n- split_self: from data\n\nThe utility U is calculated, then passed through a logistic function to get P(accept=1).\n\nNow, structuring the variables in JSON:\n\nFor alpha and beta, specify their ranges and descriptions. The other variables are data or calculated.\n\n\"variables\": {\n  \"alpha\": {\n    \"description\": \"Sensitivity to fairness deviation (positive values increase utility when offer exceeds fair percentage)\",\n    \"range\": { \"min\": -10, \"max\": 10, \"inclusive_min\": true, \"inclusive_max\": true },\n    \"learnable\": true,\n    \"source\": \"learnable\"\n  },\n  \"beta\": {\n    \"description\": \"Sensitivity to absolute offer amount in \u00a3 (positive values increase utility with higher split_self)\",\n    \"range\": { \"min\": 0, \"max\": 5, \"inclusive_min\": true, \"inclusive_max\": true },\n    \"learnable\": true,\n    \"source\": \"learnable\"\n  },\n  \"splitperc_self\": {\n    \"description\": \"Proposed percentage for the participant\",\n    \"source\": \"data\",\n    \"learnable\": false\n  },\n  \"token_self\": {\n    \"description\": \"Number of tokens found by the participant\",\n    \"source\": \"data\",\n    \"learnable\": false\n  },\n  \"token_opp\": {\n    \"description\": \"Number of tokens found by the opponent\",\n    \"source\": \"data\",\n    \"learnable\": false\n  },\n  \"split_self\": {\n    \"description\": \"Proposed share for the participant in \u00a3\",\n    \"source\": \"data\",\n    \"learnable\": false\n  }\n}\n\nThe target variable is 'accept'.\n\nThe summary should concisely describe the model: it combines fairness deviation and absolute offer amount to predict acceptance, with two learnable parameters for sensitivity to each factor.\n</think>\n\n<MODEL>\nU = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta * split_self\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to percentage deviation from fair split (positive values increase utility when offered percentage exceeds contribution-based fair share)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute monetary amount offered (\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage of total pot offered to participant\",\n      \"source\": \"data\",\n      \"learnable\": false\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution to shared pot\",\n      \"source\": \"data\",\n      \"learnable\": false\n    },\n    \"token_opp\": {\n      \"description\": \"Opponent's token contribution to shared pot\",\n      \"source\": \"data\",\n      \"learnable\": false\n    },\n    \"split_self\": {\n      \"description\": \"Absolute \u00a3 amount offered to participant\",\n      \"source\": \"data\",\n      \"learnable\": false\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-component utility model combining relative fairness evaluation (percentage offered vs contribution-based fair share) and absolute monetary value. Two learnable parameters capture 1) sensitivity to equitable distribution and 2) preference for higher absolute payoffs, enabling separate recovery of fairness vs self-interest motives.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta * split_self\nSummary: A dual-component utility model combining relative fairness evaluation (percentage offered vs contribution-based fair share) and absolute monetary value. Two learnable parameters capture 1) sensitivity to equitable distribution and 2) preference for higher absolute payoffs, enabling separate recovery of fairness vs self-interest motives.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        split_self = trial.get(\"split_self\", 0)\n\n        # Calculate fair percentage\n        fair_percent = (token_self / (token_self + token_opp)) * 100 if (token_self + token_opp) > 0 else 50\n\n        # Calculate utility\n        utility = alpha * (splitperc_self - fair_percent) + beta * split_self\n\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.77991452991453,
  "fitting_results": [
    {
      "alpha": 10.0,
      "beta": 0.052042891106504036,
      "success": true,
      "log_likelihood": -47.39924687744662,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08620345173451914,
      "beta": 0.4538158688233116,
      "success": true,
      "log_likelihood": -17.29618048702495,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1874260028156265,
      "beta": 0.9926637680841295,
      "success": true,
      "log_likelihood": -10.667416276312762,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1447291993562907,
      "beta": 0.6290069720073941,
      "success": true,
      "log_likelihood": -13.375243144911014,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.19608535946179806,
      "beta": 0.7720061435523852,
      "success": true,
      "log_likelihood": -11.25736704914857,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08624743935812994,
      "beta": 0.7401730065535648,
      "success": true,
      "log_likelihood": -14.735358003864032,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.42802723622253686,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -2.025307500294265e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0770884698699214,
      "beta": 0.7106542615784736,
      "success": true,
      "log_likelihood": -15.279150826307943,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.472917595805468,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.911931859887311e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.18419153828383392,
      "beta": 0.655299352286333,
      "success": true,
      "log_likelihood": -11.613547771905683,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8195488841078036,
      "beta": 3.386872024115848,
      "success": true,
      "log_likelihood": -4.27400218442029,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.05199372177209563,
      "success": true,
      "log_likelihood": -70.42509785937922,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.15451644290846508,
      "beta": 0.5852162994168911,
      "success": true,
      "log_likelihood": -12.939873558228058,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.46100897297801124,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.9025168703074717e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.5127043628859402,
      "beta": 4.981579663618882,
      "success": true,
      "log_likelihood": -2.3290798296096565e-06,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.434403414779305,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.9847749164606793e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.39814632142568024,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -2.328797052129026e-06,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.24165636811488636,
      "beta": 4.585756332944059,
      "success": true,
      "log_likelihood": -1.4681340602197235,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07157080196713682,
      "beta": 1.5983047294475878,
      "success": true,
      "log_likelihood": -7.674662027133727,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.42211501658431344,
      "beta": 4.85835245099737,
      "success": true,
      "log_likelihood": -3.0432260859423716e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08723705983003266,
      "beta": 0.5104821728785405,
      "success": true,
      "log_likelihood": -16.862528656925974,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.25517437940391197,
      "beta": 4.999999999999102,
      "success": true,
      "log_likelihood": -19.900522579135064,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 2.3448997771715043,
      "success": true,
      "log_likelihood": -207.2326591432296,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07928365090110526,
      "beta": 0.5007567368945784,
      "success": true,
      "log_likelihood": -17.38855204058001,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09189907471102089,
      "beta": 0.4086134185800432,
      "success": true,
      "log_likelihood": -17.064254731505144,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10841782057095305,
      "beta": 0.7473098963433471,
      "success": true,
      "log_likelihood": -13.994473176742318,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.21815103987060805,
      "beta": 1.6656755311802451,
      "success": true,
      "log_likelihood": -20.853930049838066,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.32959958900070657,
      "beta": 0.8231237219326691,
      "success": true,
      "log_likelihood": -6.597328550838984,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03690099140759953,
      "beta": 1.047304641082394,
      "success": true,
      "log_likelihood": -10.182986600975465,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -6.872074547584797,
      "beta": 3.5048545986344384,
      "success": true,
      "log_likelihood": -230.25847101520534,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.4926527517245659,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -1.9962355603573842e-06,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.6090340408059345,
      "beta": 1.5228697304612407e-07,
      "success": true,
      "log_likelihood": -47.13387517456332,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05575936150297176,
      "beta": 0.23905039857306365,
      "success": true,
      "log_likelihood": -20.758046723091194,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11291359280276984,
      "beta": 0.41287382729237426,
      "success": true,
      "log_likelihood": -15.421582742077748,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -9.043131640603182,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -46.051701852168264,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09521298406654391,
      "beta": 0.9419259480735725,
      "success": true,
      "log_likelihood": -12.623447838190895,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12219192584548329,
      "beta": 1.2386536097978582,
      "success": true,
      "log_likelihood": -10.062060404159137,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09576237203691104,
      "beta": 0.46722791662666485,
      "success": true,
      "log_likelihood": -16.583783070360674,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04455578392543928,
      "beta": 0.45056944692902334,
      "success": true,
      "log_likelihood": -19.345499077344076,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.008948443758379538,
      "beta": 0.8597209517515559,
      "success": true,
      "log_likelihood": -9.741287085133477,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04365157398103956,
      "beta": 0.35532985639138215,
      "success": true,
      "log_likelihood": -20.725735761991064,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16062440473103567,
      "beta": 0.21691582729893288,
      "success": true,
      "log_likelihood": -10.692767868065452,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.7065452131495241,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -23.025874934465673,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07038243896418346,
      "beta": 0.5063910928764989,
      "success": true,
      "log_likelihood": -17.791498803691287,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09056011564565375,
      "beta": 0.7186285446974694,
      "success": true,
      "log_likelihood": -14.827357259594487,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04333074563309483,
      "beta": 0.6596515229566232,
      "success": true,
      "log_likelihood": -15.938638460920988,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06992369050632921,
      "beta": 0.4279759766184766,
      "success": true,
      "log_likelihood": -18.55484631161562,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08165738139951449,
      "beta": 0.30623392293347534,
      "success": true,
      "log_likelihood": -18.08099571943141,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11952293617666103,
      "beta": 0.38995770986843104,
      "success": true,
      "log_likelihood": -14.857877027695485,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.025039743882306842,
      "beta": 0.20307698827226353,
      "success": true,
      "log_likelihood": -23.23636529142777,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03484824030437889,
      "beta": 0.49243627393303435,
      "success": true,
      "log_likelihood": -18.53714495546083,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10358051640771934,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -13.012403415067475,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06661591037018147,
      "beta": 0.4565377170173497,
      "success": true,
      "log_likelihood": -18.489435284207055,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.028464631676638125,
      "beta": 0.3211473402612583,
      "success": true,
      "log_likelihood": -21.500029375810307,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08720644102436562,
      "beta": 0.2386297820118501,
      "success": true,
      "log_likelihood": -17.318008379978426,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6533660703153994,
      "beta": 2.208430488631695,
      "success": true,
      "log_likelihood": -5.303238595723984,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16707991556798146,
      "beta": 0.3657109231657258,
      "success": true,
      "log_likelihood": -11.2587202933765,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -6.7288326323092535,
      "beta": 3.6656100381561676,
      "success": true,
      "log_likelihood": -138.15492814541048,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.643823196746216,
      "beta": 0.05204032282040346,
      "success": true,
      "log_likelihood": -70.4248007490822,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.029924376366663532,
      "beta": 0.7328321649804476,
      "success": true,
      "log_likelihood": -13.936235008916555,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03885740462651094,
      "beta": 0.3607129995861078,
      "success": true,
      "log_likelihood": -20.790038470209875,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08299226303910956,
      "beta": 0.4083565945228285,
      "success": true,
      "log_likelihood": -17.754451632155597,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0520561865011281,
      "success": true,
      "log_likelihood": -47.39924687502602,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06643232680653906,
      "beta": 0.3230649757635806,
      "success": true,
      "log_likelihood": -19.48274871907277,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.16794388962445445,
      "beta": 0.28555986058637056,
      "success": true,
      "log_likelihood": -10.68714494114409,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0953719315387812,
      "beta": 0.6539979716285882,
      "success": true,
      "log_likelihood": -15.276697043467944,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.014683168527187823,
      "beta": 0.970228257311566,
      "success": true,
      "log_likelihood": -8.939722466175477,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -10.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -161.18095650667595,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09440334149506696,
      "beta": 0.7114541833188437,
      "success": true,
      "log_likelihood": -14.774252396736387,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.007475935558815899,
      "beta": 0.6269592475040677,
      "success": true,
      "log_likelihood": -11.497029743024516,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 2.309391869351685,
      "success": true,
      "log_likelihood": -9.57325608174883e-07,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.052054960558568654,
      "success": true,
      "log_likelihood": -93.4509487351233,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08621130855875832,
      "beta": 0.5250602557968522,
      "success": true,
      "log_likelihood": -16.80858152498195,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03915939175240312,
      "beta": 0.7419268928308695,
      "success": true,
      "log_likelihood": -14.438958715205398,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.028775282486639228,
      "beta": 0.17863863735673047,
      "success": true,
      "log_likelihood": -23.300446744423315,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04762029297576851,
      "beta": 0.5533650632326989,
      "success": true,
      "log_likelihood": -17.72587562568077,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.36886787666381915,
      "beta": 4.198021938788052,
      "success": true,
      "log_likelihood": -45.88707837015047,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.052054909572743634,
      "success": true,
      "log_likelihood": -70.42509780508436,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.024127314417677605,
      "beta": 0.2980861676981119,
      "success": true,
      "log_likelihood": -21.85302224456694,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.1253311283913465,
      "beta": 0.05205547823256668,
      "success": true,
      "log_likelihood": -47.3992468781547,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.026379792738732265,
      "beta": 0.18727248815982184,
      "success": true,
      "log_likelihood": -23.34836000099399,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.040842085554822674,
      "beta": 0.4541695459484057,
      "success": true,
      "log_likelihood": -10.646691325043301,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.819433656535613,
      "beta": 0.05202363720537557,
      "success": true,
      "log_likelihood": -116.47679954214736,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.20303685437399838,
      "beta": 1.180492419541316,
      "success": true,
      "log_likelihood": -9.372343129076272,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.012492959689987707,
      "beta": 0.16712759675676286,
      "success": true,
      "log_likelihood": -23.782035158926572,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.20066390980792365,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.233690109864338,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.036045997624965284,
      "beta": 0.4540821795856956,
      "success": true,
      "log_likelihood": -19.25890005430388,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10940530422024212,
      "beta": 0.08803431225528087,
      "success": true,
      "log_likelihood": -13.611084971995249,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -8.179236406101356,
      "beta": 3.458575306718691,
      "success": true,
      "log_likelihood": -92.10340371221258,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.14184129395578407,
      "beta": 0.8341357458602062,
      "success": true,
      "log_likelihood": -12.411837312340337,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.013398428030166028,
      "beta": 0.3123630862852306,
      "success": true,
      "log_likelihood": -21.119542593089438,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.9227664967437768,
        "p": 2.260762138707119e-42
      },
      "beta": {
        "r": 0.2391573915984051,
        "p": 0.016557405613927406
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 62.78386038474846,
  "bic_results": {
    "average_bic": 62.78386038474846,
    "individual_bics": [
      101.96553163180546,
      41.75939885096212,
      28.501870429537746,
      33.917524166734246,
      29.681771975209358,
      36.63775388464028,
      7.167041927527221,
      37.72533952952811,
      7.16704170077594,
      30.394133420723584,
      15.7150422457528,
      148.01723359567066,
      33.046784993368334,
      7.16704168194596,
      7.167042535071879,
      7.167041846462053,
      7.167042534506324,
      10.103305997351667,
      22.516361931179674,
      7.167043963364391,
      40.89209519076417,
      46.968083035182346,
      421.6323561633714,
      41.94414195807224,
      41.29554733992251,
      35.155984230396854,
      48.87489797658835,
      20.361694978590187,
      27.533011078863147,
      467.6839799073229,
      7.167041869383341,
      101.43478822603886,
      48.68313132309461,
      38.01020336106772,
      99.27044158124875,
      32.41393355329401,
      27.291158685230492,
      40.334604017633566,
      45.85803603160037,
      26.649612047179176,
      48.61850940089435,
      28.552573613043123,
      53.218787745843564,
      42.75003548429479,
      36.821752396101196,
      39.04431479875419,
      44.27673050014346,
      43.32902931577504,
      36.88279193230319,
      53.63976845976776,
      44.241327787833875,
      33.19184470704717,
      44.14590844532633,
      50.16709662853283,
      41.80305463686907,
      17.77351506836019,
      29.684478463665222,
      283.4768941677332,
      148.0166393750766,
      35.03950789474533,
      48.74711481733197,
      42.67594114122341,
      101.96553162696426,
      46.13253531505776,
      28.5413277592004,
      37.72043196384811,
      25.046482809263175,
      329.5289508902641,
      36.71554267038499,
      30.16109736296125,
      7.167039791563436,
      194.06893534715883,
      40.78420092687612,
      36.044955307323015,
      53.76793136575885,
      42.61878912827376,
      98.94119461721316,
      148.01723348708094,
      50.8730823660461,
      101.96553163322162,
      53.863757878900195,
      28.460420526998824,
      240.12063696120694,
      25.911724135064766,
      54.73110819476536,
      21.634418096640896,
      45.68483798551998,
      34.38920782090272,
      191.37384530133738,
      31.990712501592895,
      49.406123063091094
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.923\n- beta: r = 0.239",
  "previous_models": [
    "Specification: U = alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100) + beta * split_self\nSummary: A dual-component utility model combining relative fairness evaluation (percentage offered vs contribution-based fair share) and absolute monetary value. Two learnable parameters capture 1) sensitivity to equitable distribution and 2) preference for higher absolute payoffs, enabling separate recovery of fairness vs self-interest motives.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 62.78386038474846\nOverall Accuracy: 0.7799\n\nParameter Recovery:\n- alpha: r = 0.923\n- beta: r = 0.239"
  ]
}