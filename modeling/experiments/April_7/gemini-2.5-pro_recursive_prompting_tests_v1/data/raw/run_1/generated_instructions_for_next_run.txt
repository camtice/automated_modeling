Okay, let's refine the modeling approach. The previous model provided a baseline, but we need to significantly improve **parameter recovery** (aiming for correlations well above 0.7) while also seeking better **BIC** and **accuracy**. High covariance between parameters seemed to be an issue.

For this next run:

1.  **Focus on Parameter Identifiability:** Think critically about how the chosen mathematical form impacts the ability to uniquely estimate each learnable parameter. Are there alternative ways to mathematically represent the core psychological concepts (like fairness or inequity aversion) that might lead to less parameter trade-off? For instance, how might the *scale* of the inequity matter relative to the potential reward?
2.  **Explore Fairness Reference Points:** Don't assume the proportional share based *only* on tokens is the sole driver of fairness judgments. Consider alternative or mixed reference points. Could a simple 50/50 split expectation play a role, perhaps weighted against the contribution-based share? Could the relevant reference point shift based on the `trial_type` or other contextual factors? How might you model a blend of fairness norms?
3.  **Consider Decision Noise:** Introduce stochasticity into the decision process. A common way is to add a 'temperature' parameter (`tau`) to the logistic function: `P(accept) = 1 / (1 + exp(-U / tau))`. This parameter captures variability/noise in choices not explained by the deterministic utility. Think about appropriate bounds for `tau` (must be positive).
4.  **Re-evaluate Functional Forms:** Is the standard linear penalty for inequity (`alpha * max(...)`) the best? Explore non-linear forms or ways to scale the inequity penalty (e.g., relative to the total stakes or the fair share itself) that might better capture behavior *and* improve parameter distinction. Could utility depend non-linearly on the offered amount (`split_self`) itself?
5.  **Parameter Count:** While aiming for parsimony is good, prioritize identifiability. Adding a well-motivated parameter (like decision temperature) is acceptable if it improves recovery and overall model performance. Avoid parameters that might capture very similar variance.
6.  **Novelty Encouraged:** Move beyond the most standard implementations if you have a psychologically plausible alternative that might better fit the goals. Think about heuristics or simpler comparisons participants might use.

Reason step by step about:
*   Key psychological mechanisms (revisiting fairness, self-interest, consistency/noise, potential reference point shifts).
*   How these might interact *differently* than the previous model.
*   Mathematical forms that capture these interactions *with better parameter identifiability*.
*   Necessary parameters and their justification (including potential new ones like temperature).
*   Mapping to observable behavior.