{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Okay, the previous run implemented a standard Fehr-Schmidt model with a temperature parameter (`tau`). While this significantly improved the BIC compared to the very first run, the parameter recovery was extremely poor (correlations near zero). This indicates a critical issue with parameter identifiability \u2013 the model structure, despite being theoretically grounded, doesn't allow us to reliably estimate the contribution of each parameter (`alpha`, `beta`, `tau`) from the data.\n\nFor the next run, the **primary, overriding goal is to achieve good parameter recovery (aim for r > 0.7 for all parameters)**. Improving BIC and accuracy remains important, but not at the expense of identifiable parameters. We need to fundamentally rethink the model structure to address the likely trade-offs between parameters observed previously.\n\nHere are specific directions and suggestions:\n\n1.  **Challenge the Fehr-Schmidt Functional Form:** The linear penalty (`alpha * difference`, `beta * difference`) combined with `split_self` and `tau` seems problematic for recovery in this context.\n    *   **Consider Relative Inequity:** Instead of penalizing the *absolute* difference from the fair share, consider penalizing the *relative* difference. For example:\n        *   Inequity term = `alpha * max(0, (fair_share_self - split_self) / fair_share_self)` (handle `fair_share_self = 0` cases).\n        *   Or, Inequity term = `alpha * max(0, fair_share_self - split_self) / combined_earning`.\n        *   This makes the inequity sensitivity potentially independent of the overall stakes (`combined_earning`), which might disentangle it from the direct effect of `split_self` and the noise parameter `tau`.\n    *   **Simplify Inequity Aversion:** Is the distinction between `alpha` and `beta` recoverable here? Try a model with a single inequity aversion parameter (`k`) that penalizes *any* deviation (advantageous or disadvantageous) from the fair share.\n        *   Example: `U = split_self - k * abs(split_self - fair_share_self)`\n        *   You could combine this with the relative difference idea: `U = split_self - k * abs(split_self - fair_share_self) / fair_share_self`\n        *   This reduces the number of potentially correlated inequity parameters.\n\n2.  **Re-examine the Reference Point:** While proportional fairness based on tokens is intuitive, perhaps it's not the sole reference.\n    *   **Model Mixed References:** Explicitly model the reference point as a weighted average of proportional fairness and a simple 50/50 split.\n        *   `reference = w * (combined_earning * token_self / (token_self + token_opp)) + (1 - w) * (combined_earning / 2)`\n        *   `U = split_self - ... penalty based on (split_self - reference) ...`\n        *   Here `w` would be a learnable parameter [0, 1]. Be mindful that adding parameters can hurt recovery, so only do this if you have a strong rationale that this mixed reference is key *and* you structure the rest of the model for identifiability (e.g., using a simplified inequity term as above).\n\n3.  **Utility Scaling and Noise:** How does the scale of `U` interact with `tau`? If `U` varies greatly depending on `combined_earning`, the effect of `tau` might become unstable.\n    *   Consider normalizing the utility *before* the logistic function, perhaps by dividing `U` by `combined_earning` or some other scaling factor related to the stakes of the trial.\n    *   Example: `P_accept = 1 / (1 + exp(-(U / combined_earning) / tau))`\n\n4.  **Think Heuristically:** Move away from complex utility calculations. What simpler rule might participants use?\n    *   Could acceptance depend on whether `split_self` exceeds the fair share by a certain threshold? `P_accept = logistic( k * (split_self - fair_share_self) )` ? (This resembles the previous model but collapses inequity).\n    *   Could it be a comparison of percentages? `P_accept = logistic( k * (splitperc_self - (token_self*100 / (token_self + token_opp))) )`?\n\n**Priority:** Focus on structural changes that reduce parameter trade-offs. Simplifying the inequity term (e.g., single parameter `k`) and/or using relative inequity measures seems like a promising avenue to explore first, potentially combined with `tau` for noise. Be bold in departing from the exact Fehr-Schmidt formulation that failed recovery last time. Justify your parameter choices in terms of how they are expected to be identifiable from the behavioral data.",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: Okay, the previous run implemented a standard Fehr-Schmidt model with a temperature parameter (`tau`). While this significantly improved the BIC compared to the very first run, the parameter recovery was extremely poor (correlations near zero). This indicates a critical issue with parameter identifiability \u2013 the model structure, despite being theoretically grounded, doesn't allow us to reliably estimate the contribution of each parameter (`alpha`, `beta`, `tau`) from the data.\n\nFor the next run, the **primary, overriding goal is to achieve good parameter recovery (aim for r > 0.7 for all parameters)**. Improving BIC and accuracy remains important, but not at the expense of identifiable parameters. We need to fundamentally rethink the model structure to address the likely trade-offs between parameters observed previously.\n\nHere are specific directions and suggestions:\n\n1.  **Challenge the Fehr-Schmidt Functional Form:** The linear penalty (`alpha * difference`, `beta * difference`) combined with `split_self` and `tau` seems problematic for recovery in this context.\n    *   **Consider Relative Inequity:** Instead of penalizing the *absolute* difference from the fair share, consider penalizing the *relative* difference. For example:\n        *   Inequity term = `alpha * max(0, (fair_share_self - split_self) / fair_share_self)` (handle `fair_share_self = 0` cases).\n        *   Or, Inequity term = `alpha * max(0, fair_share_self - split_self) / combined_earning`.\n        *   This makes the inequity sensitivity potentially independent of the overall stakes (`combined_earning`), which might disentangle it from the direct effect of `split_self` and the noise parameter `tau`.\n    *   **Simplify Inequity Aversion:** Is the distinction between `alpha` and `beta` recoverable here? Try a model with a single inequity aversion parameter (`k`) that penalizes *any* deviation (advantageous or disadvantageous) from the fair share.\n        *   Example: `U = split_self - k * abs(split_self - fair_share_self)`\n        *   You could combine this with the relative difference idea: `U = split_self - k * abs(split_self - fair_share_self) / fair_share_self`\n        *   This reduces the number of potentially correlated inequity parameters.\n\n2.  **Re-examine the Reference Point:** While proportional fairness based on tokens is intuitive, perhaps it's not the sole reference.\n    *   **Model Mixed References:** Explicitly model the reference point as a weighted average of proportional fairness and a simple 50/50 split.\n        *   `reference = w * (combined_earning * token_self / (token_self + token_opp)) + (1 - w) * (combined_earning / 2)`\n        *   `U = split_self - ... penalty based on (split_self - reference) ...`\n        *   Here `w` would be a learnable parameter [0, 1]. Be mindful that adding parameters can hurt recovery, so only do this if you have a strong rationale that this mixed reference is key *and* you structure the rest of the model for identifiability (e.g., using a simplified inequity term as above).\n\n3.  **Utility Scaling and Noise:** How does the scale of `U` interact with `tau`? If `U` varies greatly depending on `combined_earning`, the effect of `tau` might become unstable.\n    *   Consider normalizing the utility *before* the logistic function, perhaps by dividing `U` by `combined_earning` or some other scaling factor related to the stakes of the trial.\n    *   Example: `P_accept = 1 / (1 + exp(-(U / combined_earning) / tau))`\n\n4.  **Think Heuristically:** Move away from complex utility calculations. What simpler rule might participants use?\n    *   Could acceptance depend on whether `split_self` exceeds the fair share by a certain threshold? `P_accept = logistic( k * (split_self - fair_share_self) )` ? (This resembles the previous model but collapses inequity).\n    *   Could it be a comparison of percentages? `P_accept = logistic( k * (splitperc_self - (token_self*100 / (token_self + token_opp))) )`?\n\n**Priority:** Focus on structural changes that reduce parameter trade-offs. Simplifying the inequity term (e.g., single parameter `k`) and/or using relative inequity measures seems like a promising avenue to explore first, potentially combined with `tau` for noise. Be bold in departing from the exact Fehr-Schmidt formulation that failed recovery last time. Justify your parameter choices in terms of how they are expected to be identifiable from the behavioral data.\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "Okay, let's design a model focusing explicitly on parameter recovery by simplifying the structure and using relative measures.\n\n**Step-by-step Reasoning:**\n\n1.  **Goal:** Predict responder acceptance (`accept`) with high parameter recovery (`r > 0.7`).\n2.  **Problem with Previous Models:** Fehr-Schmidt (`alpha`, `beta`) with absolute differences and temperature (`tau`) likely led to parameter trade-offs and poor recovery.\n3.  **Simplify Inequity:** Replace separate `alpha` and `beta` with a single inequity aversion parameter (`k`) penalizing any deviation from fairness.\n4.  **Use Relative Measures:** Calculate fairness and offers as proportions of the total `combined_earning` to mitigate scaling issues and potentially disentangle parameters.\n5.  **Define Fair Share:** The contribution-based fair share proportion for the self is `fair_perc = token_self / (token_self + token_opp)`. Handle the edge case where `token_self + token_opp = 0` by setting `fair_perc = 0.5`.\n6.  **Define Offer Proportion:** The proportion of the total pot offered to the self is `split_perc = split_self / combined_earning`. Handle the edge case `combined_earning = 0` by setting `split_perc = 0` (though this case implies zero tokens found, aligning with the `fair_perc` edge case). We assume `combined_earning > 0` for meaningful trials.\n7.  **Formulate Utility:** Combine the direct gain (represented by `split_perc`) and the fairness consideration. A simple, interpretable form is: `U = split_perc - k * abs(split_perc - fair_perc)`.\n    *   This means the utility starts with the proportion of the pot offered.\n    *   It is then penalized based on the absolute difference between the offered proportion and the fair proportion.\n    *   The parameter `k` controls the strength of this penalty. If `k=0`, only the offered proportion matters. If `k > 0`, deviations from fairness reduce utility.\n8.  **Probability Mapping:** Use a logistic function with temperature 1, as specified: `P(accept) = 1 / (1 + exp(-U))`.\n9.  **Parameter Bounds:** The single learnable parameter is `k`. It represents aversion to inequity, so it should be non-negative (`k >= 0`). A large value would imply extreme sensitivity to fairness. A generous upper bound like 10 seems reasonable, allowing the fairness term to potentially outweigh the direct gain term significantly. So, `k` range: `[0, 10]`.\n10. **Variable Check:** The model uses `token_self`, `token_opp`, `split_self`, `combined_earning` from the dataset to calculate `fair_perc` and `split_perc`, and the learnable parameter `k`. It predicts `accept`. All necessary inputs are available.\n11. **Recovery Rationale:** This model simplifies the inequity structure significantly (one parameter `k` instead of `alpha` and `beta`). By using proportions (`split_perc`, `fair_perc`), it scales the gain and fairness deviation relative to the total stakes, potentially making the influence of `k` more consistent across trials with different `combined_earning` values and less likely to trade off against simple sensitivity to `split_self` magnitude or a noise parameter. This focus on relative comparisons and reduced parameter count aims directly at improving identifiability.\n\n<MODEL>\nfair_perc = (token_self / (token_self + token_opp)) if (token_self + token_opp) > 0 else 0.5\nsplit_perc = (split_self / combined_earning) if combined_earning > 0 else 0\nU = split_perc - k * abs(split_perc - fair_perc)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal.\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"k\": {\n      \"description\": \"Inequity aversion parameter. Scales the negative utility associated with deviation from the proportional fair share.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant.\",\n      \"range\": {\n        \"min\": 0, \n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent.\",\n       \"range\": {\n        \"min\": 0, \n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_self\": {\n      \"description\": \"The proposed share for the participant (in \u00a3).\",\n      \"range\": {\n        \"min\": 0, \n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Amount of money earned in total for the trial (in \u00a3).\",\n       \"range\": {\n        \"min\": 0, \n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n     \"fair_perc\": {\n      \"description\": \"Proportionally fair share for the participant based on token contribution.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_perc\": {\n      \"description\": \"Proportion of the combined earning offered to the participant.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance utility is based on the proportion of the total earnings offered (`split_perc`), penalized by the absolute deviation from the contribution-based proportional fair share (`fair_perc`). A single learnable parameter (`k`) controls the weight of this inequity penalty. Utility is calculated as `U = split_perc - k * abs(split_perc - fair_perc)`. The probability of acceptance is derived using a logistic function with temperature 1.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['k']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                fair_perc = (token_self / (token_self + token_opp)) if (token_self + token_opp) > 0 else 0.5\nsplit_perc = (split_self / combined_earning) if combined_earning > 0 else 0\nU = split_perc - k * abs(split_perc - fair_perc)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal.\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"k\": {\n      \"description\": \"Inequity aversion parameter. Scales the negative utility associated with deviation from the proportional fair share.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_self\": {\n      \"description\": \"The proposed share for the participant (in \\u00a3).\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Amount of money earned in total for the trial (in \\u00a3).\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"fair_perc\": {\n      \"description\": \"Proportionally fair share for the participant based on token contribution.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_perc\": {\n      \"description\": \"Proportion of the combined earning offered to the participant.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, k=1):\n    model_predictions = []\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        split_self = trial.get(\"split_self\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n\n        # Calculate fair proportion\n        if (token_self + token_opp) > 0:\n            fair_perc = token_self / (token_self + token_opp)\n        else:\n            fair_perc = 0.5\n\n        # Calculate split proportion\n        if combined_earning > 0:\n            split_perc = split_self / combined_earning\n        else:\n            split_perc = 0\n\n        # Calculate utility\n        U = split_perc - k * abs(split_perc - fair_perc)\n        \n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"group1\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 5, \"token_self\": 5, \"combined_earning\": 10, \"split_opp\": 5.0, \"split_self\": 5.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 5.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 5.0}, {\"ID\": 1, \"group\": \"group1\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 8, \"token_self\": 2, \"combined_earning\": 10, \"split_opp\": 2.0, \"split_self\": 8.0, \"splitperc_opp\": 20, \"splitperc_self\": 80, \"accept\": 1.0, \"accepted_amount\": 8.0, \"accepted_perc\": 80.0, \"proposed_perc\": 80.0, \"proposed_amount\": 8.0}]'\n\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1744035129.4938428
    }
  ],
  "learnable_parameters": {
    "k": {
      "description": "Inequity aversion parameter. Scales the negative utility associated with deviation from the proportional fair share.",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "fair_perc = (token_self / (token_self + token_opp)) if (token_self + token_opp) > 0 else 0.5\nsplit_perc = (split_self / combined_earning) if combined_earning > 0 else 0\nU = split_perc - k * abs(split_perc - fair_perc)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal.",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "k": {
      "description": "Inequity aversion parameter. Scales the negative utility associated with deviation from the proportional fair share.",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "token_self": {
      "description": "Number of tokens found by the participant.",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent.",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_self": {
      "description": "The proposed share for the participant (in \u00a3).",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "combined_earning": {
      "description": "Amount of money earned in total for the trial (in \u00a3).",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "fair_perc": {
      "description": "Proportionally fair share for the participant based on token contribution.",
      "range": {
        "min": 0,
        "max": 1,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "split_perc": {
      "description": "Proportion of the combined earning offered to the participant.",
      "range": {
        "min": 0,
        "max": 1,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    }
  },
  "model_summary": "A utility model where acceptance utility is based on the proportion of the total earnings offered (`split_perc`), penalized by the absolute deviation from the contribution-based proportional fair share (`fair_perc`). A single learnable parameter (`k`) controls the weight of this inequity penalty. Utility is calculated as `U = split_perc - k * abs(split_perc - fair_perc)`. The probability of acceptance is derived using a logistic function with temperature 1.",
  "target_variable": "accept",
  "full_reasoning": "Okay, let's design a model focusing explicitly on parameter recovery by simplifying the structure and using relative measures.\n\n**Step-by-step Reasoning:**\n\n1.  **Goal:** Predict responder acceptance (`accept`) with high parameter recovery (`r > 0.7`).\n2.  **Problem with Previous Models:** Fehr-Schmidt (`alpha`, `beta`) with absolute differences and temperature (`tau`) likely led to parameter trade-offs and poor recovery.\n3.  **Simplify Inequity:** Replace separate `alpha` and `beta` with a single inequity aversion parameter (`k`) penalizing any deviation from fairness.\n4.  **Use Relative Measures:** Calculate fairness and offers as proportions of the total `combined_earning` to mitigate scaling issues and potentially disentangle parameters.\n5.  **Define Fair Share:** The contribution-based fair share proportion for the self is `fair_perc = token_self / (token_self + token_opp)`. Handle the edge case where `token_self + token_opp = 0` by setting `fair_perc = 0.5`.\n6.  **Define Offer Proportion:** The proportion of the total pot offered to the self is `split_perc = split_self / combined_earning`. Handle the edge case `combined_earning = 0` by setting `split_perc = 0` (though this case implies zero tokens found, aligning with the `fair_perc` edge case). We assume `combined_earning > 0` for meaningful trials.\n7.  **Formulate Utility:** Combine the direct gain (represented by `split_perc`) and the fairness consideration. A simple, interpretable form is: `U = split_perc - k * abs(split_perc - fair_perc)`.\n    *   This means the utility starts with the proportion of the pot offered.\n    *   It is then penalized based on the absolute difference between the offered proportion and the fair proportion.\n    *   The parameter `k` controls the strength of this penalty. If `k=0`, only the offered proportion matters. If `k > 0`, deviations from fairness reduce utility.\n8.  **Probability Mapping:** Use a logistic function with temperature 1, as specified: `P(accept) = 1 / (1 + exp(-U))`.\n9.  **Parameter Bounds:** The single learnable parameter is `k`. It represents aversion to inequity, so it should be non-negative (`k >= 0`). A large value would imply extreme sensitivity to fairness. A generous upper bound like 10 seems reasonable, allowing the fairness term to potentially outweigh the direct gain term significantly. So, `k` range: `[0, 10]`.\n10. **Variable Check:** The model uses `token_self`, `token_opp`, `split_self`, `combined_earning` from the dataset to calculate `fair_perc` and `split_perc`, and the learnable parameter `k`. It predicts `accept`. All necessary inputs are available.\n11. **Recovery Rationale:** This model simplifies the inequity structure significantly (one parameter `k` instead of `alpha` and `beta`). By using proportions (`split_perc`, `fair_perc`), it scales the gain and fairness deviation relative to the total stakes, potentially making the influence of `k` more consistent across trials with different `combined_earning` values and less likely to trade off against simple sensitivity to `split_self` magnitude or a noise parameter. This focus on relative comparisons and reduced parameter count aims directly at improving identifiability.\n\n<MODEL>\nfair_perc = (token_self / (token_self + token_opp)) if (token_self + token_opp) > 0 else 0.5\nsplit_perc = (split_self / combined_earning) if combined_earning > 0 else 0\nU = split_perc - k * abs(split_perc - fair_perc)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal.\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"k\": {\n      \"description\": \"Inequity aversion parameter. Scales the negative utility associated with deviation from the proportional fair share.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant.\",\n      \"range\": {\n        \"min\": 0, \n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent.\",\n       \"range\": {\n        \"min\": 0, \n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_self\": {\n      \"description\": \"The proposed share for the participant (in \u00a3).\",\n      \"range\": {\n        \"min\": 0, \n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Amount of money earned in total for the trial (in \u00a3).\",\n       \"range\": {\n        \"min\": 0, \n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n     \"fair_perc\": {\n      \"description\": \"Proportionally fair share for the participant based on token contribution.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_perc\": {\n      \"description\": \"Proportion of the combined earning offered to the participant.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance utility is based on the proportion of the total earnings offered (`split_perc`), penalized by the absolute deviation from the contribution-based proportional fair share (`fair_perc`). A single learnable parameter (`k`) controls the weight of this inequity penalty. Utility is calculated as `U = split_perc - k * abs(split_perc - fair_perc)`. The probability of acceptance is derived using a logistic function with temperature 1.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: fair_perc = (token_self / (token_self + token_opp)) if (token_self + token_opp) > 0 else 0.5\nsplit_perc = (split_self / combined_earning) if combined_earning > 0 else 0\nU = split_perc - k * abs(split_perc - fair_perc)\nSummary: A utility model where acceptance utility is based on the proportion of the total earnings offered (`split_perc`), penalized by the absolute deviation from the contribution-based proportional fair share (`fair_perc`). A single learnable parameter (`k`) controls the weight of this inequity penalty. Utility is calculated as `U = split_perc - k * abs(split_perc - fair_perc)`. The probability of acceptance is derived using a logistic function with temperature 1.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, k=1):\n    model_predictions = []\n    for trial in trial_data:\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        split_self = trial.get(\"split_self\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n\n        # Calculate fair proportion\n        if (token_self + token_opp) > 0:\n            fair_perc = token_self / (token_self + token_opp)\n        else:\n            fair_perc = 0.5\n\n        # Calculate split proportion\n        if combined_earning > 0:\n            split_perc = split_self / combined_earning\n        else:\n            split_perc = 0\n\n        # Calculate utility\n        U = split_perc - k * abs(split_perc - fair_perc)\n        \n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.5995115995115995,
  "fitting_results": [
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -11.376148022262969,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 3.6271028499198366,
      "success": true,
      "log_likelihood": -21.038047866133464,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 4.314079988919103,
      "success": true,
      "log_likelihood": -19.66488691339448,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 5.295918590061425,
      "success": true,
      "log_likelihood": -19.038306426705535,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 6.314839644446076,
      "success": true,
      "log_likelihood": -17.30181623699109,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 1.5866648195255761,
      "success": true,
      "log_likelihood": -21.99826944785348,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.004406575765763,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 1.2263571764546275,
      "success": true,
      "log_likelihood": -22.07945851776071,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.004406575765756,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 6.914820337135832,
      "success": true,
      "log_likelihood": -16.76050459843425,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 6.252492847775329,
      "success": true,
      "log_likelihood": -17.056543212561785,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -11.526148022232857,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 6.190906633945155,
      "success": true,
      "log_likelihood": -17.510237191639213,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.004406575765763,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.004406575765756,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.004406575765756,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.00440657576576,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.704406575625125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.6544065756349,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.004406575765753,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 3.219222015829498,
      "success": true,
      "log_likelihood": -21.115250576939463,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.854406575594233,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 5.04542299471366,
      "success": true,
      "log_likelihood": -19.91911257975667,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 2.7994522471363625,
      "success": true,
      "log_likelihood": -21.81672737508037,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 4.400407832510106,
      "success": true,
      "log_likelihood": -20.019649020393985,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 2.6979597904986616,
      "success": true,
      "log_likelihood": -21.504157395887965,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 2.6979596229514353,
      "success": true,
      "log_likelihood": -21.404157395911895,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -12.209481355423593,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.804406575604283,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -21.80440657540167,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.004406575765756,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -11.376148022262967,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 3.401575672844365,
      "success": true,
      "log_likelihood": -22.08941743200102,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 5.668236225216293,
      "success": true,
      "log_likelihood": -18.648993085563124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.00440657576576,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.9892861044801746,
      "success": true,
      "log_likelihood": -22.327146523117523,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.9597704894545477,
      "success": true,
      "log_likelihood": -21.89338860598508,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 4.1037512024853875,
      "success": true,
      "log_likelihood": -20.365577233880554,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.6949478848388942,
      "success": true,
      "log_likelihood": -22.667477927223853,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.554406575654937,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 1.375670606759027,
      "success": true,
      "log_likelihood": -22.721047795011515,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -13.442814688103475,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.304406575704853,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 2.1755743841762234,
      "success": true,
      "log_likelihood": -22.153307890384035,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 1.9553369427975043,
      "success": true,
      "log_likelihood": -21.944235519247627,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -22.20440657532012,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3333333333333333,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 2.765480377376989,
      "success": true,
      "log_likelihood": -21.71310160293108,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 4.713425517941896,
      "success": true,
      "log_likelihood": -20.53817971428533,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 6.3148655454196945,
      "success": true,
      "log_likelihood": -17.701816236659358,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 1.285960062564663,
      "success": true,
      "log_likelihood": -23.937586971082926,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -22.504406575259466,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -14.126148021288802,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 2.3034858082273977,
      "success": true,
      "log_likelihood": -22.404014684159176,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.5775034274402631,
      "success": true,
      "log_likelihood": -23.609892544773352,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 5.836497620075253,
      "success": true,
      "log_likelihood": -19.311398177917745,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 9.231365432187998,
      "success": true,
      "log_likelihood": -13.901763059508177,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 9.34666554547183,
      "success": true,
      "log_likelihood": -14.29694986685795,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -21.30440657550046,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -11.976148022135641,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -21.35440657549319,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.9302806244919465,
      "success": true,
      "log_likelihood": -23.059139010264875,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 3.82125448712554,
      "success": true,
      "log_likelihood": -20.6777410490143,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -11.376148022262969,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 3.438643291969308,
      "success": true,
      "log_likelihood": -21.88241599829982,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -13.59281468832526,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 2.6309818518830865,
      "success": true,
      "log_likelihood": -21.59297161262102,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.504406575665193,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -21.004406575563614,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 2.2074038577370985,
      "success": true,
      "log_likelihood": -22.166783212148992,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.804406575603075,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -10.076148022581137,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -14.242814687847956,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 3.041454479002524,
      "success": true,
      "log_likelihood": -21.526081780900668,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -21.654406575432688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 2.2393268434287,
      "success": true,
      "log_likelihood": -23.379727228906066,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.19531458404938118,
      "success": true,
      "log_likelihood": -22.143645287259965,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.454406575674966,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -11.842814687871224,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.1362937703336226,
      "success": true,
      "log_likelihood": -23.749172339182085,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -11.376148022262965,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 1.3158204716116262,
      "success": true,
      "log_likelihood": -23.865905510808595,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.804406575603682,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -10.959481355728848,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 3.781949181283811,
      "success": true,
      "log_likelihood": -20.39110068084325,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.16581384142887257,
      "success": true,
      "log_likelihood": -23.896654733294717,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -11.642814688875669,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -22.55440657524969,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 10.0,
      "success": true,
      "log_likelihood": -12.409481355078508,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -20.404406575684817,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 3.627103235745132,
      "success": true,
      "log_likelihood": -20.93804786624163,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "k": 0.0,
      "success": true,
      "log_likelihood": -22.65440657523027,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "k": {
        "r": 0.8545512049014172,
        "p": 1.2257163062695815e-29
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 42.203327692582654,
  "bic_results": {
    "average_bic": 42.203327692582654,
    "individual_bics": [
      26.335814982982047,
      45.65961467072304,
      42.91329276524507,
      41.66013179186718,
      38.18715141243829,
      47.58005783416307,
      43.592332089987636,
      47.74243597397753,
      43.59233208998762,
      37.10452813532461,
      37.69660536357968,
      26.635814982921822,
      38.603993321734535,
      43.592332089987636,
      43.59233208998762,
      43.59233208998762,
      43.59233208998763,
      44.99233208970636,
      44.89233208972591,
      43.592332089987615,
      45.814020092335035,
      45.292332089644574,
      43.42174409796945,
      47.21697368861685,
      43.62281697924408,
      46.59183373023204,
      46.3918337302799,
      28.002481649303295,
      45.192332089664674,
      47.19233208925945,
      43.59233208998762,
      26.335814982982043,
      47.76235380245815,
      40.881505109582356,
      43.59233208998763,
      48.237811984691156,
      47.37029615042627,
      44.31467340621722,
      48.918474792903815,
      44.69233208976598,
      49.02561452847914,
      30.46914831466306,
      44.192332089865815,
      47.89013471922418,
      47.47198997695136,
      47.99233208909635,
      47.00972214431827,
      44.65987836702677,
      38.987151411774825,
      51.45869288062196,
      48.59233208897504,
      31.835814981033714,
      48.39154830677446,
      50.80330402800281,
      42.2063152942916,
      31.387045057472463,
      32.17741867217201,
      46.19233208945703,
      27.53581498272739,
      46.29233208944249,
      49.70179695898586,
      44.93900103648471,
      26.335814982982047,
      47.34835093505575,
      30.76914831510663,
      46.76946216369815,
      44.592332089786495,
      45.59233208958334,
      47.917085362754094,
      45.19233208966226,
      23.735814983618383,
      32.069148314152024,
      46.635682500257445,
      46.892332089321485,
      50.34297339626824,
      47.87080951297604,
      44.49233208980604,
      27.269148314198556,
      51.08186361682028,
      26.33581498298204,
      51.3153299600733,
      45.19233208966347,
      25.502481649913804,
      44.365720300142605,
      51.37682840504554,
      26.869148316207447,
      48.69233208895549,
      28.402481648613126,
      44.39233208982574,
      45.45961467093937,
      48.892332088916646
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- k: r = 0.855",
  "previous_models": [
    "Specification: fair_perc = (token_self / (token_self + token_opp)) if (token_self + token_opp) > 0 else 0.5\nsplit_perc = (split_self / combined_earning) if combined_earning > 0 else 0\nU = split_perc - k * abs(split_perc - fair_perc)\nSummary: A utility model where acceptance utility is based on the proportion of the total earnings offered (`split_perc`), penalized by the absolute deviation from the contribution-based proportional fair share (`fair_perc`). A single learnable parameter (`k`) controls the weight of this inequity penalty. Utility is calculated as `U = split_perc - k * abs(split_perc - fair_perc)`. The probability of acceptance is derived using a logistic function with temperature 1.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 42.203327692582654\nOverall Accuracy: 0.5995\n\nParameter Recovery:\n- k: r = 0.855"
  ]
}