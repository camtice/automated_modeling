Okay, here are the improved instructions for the next run (Run 54), building on the results of the previous runs.

Instructions for Run 54 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 53) implemented a piecewise linear model based on fairness difference (`U = perc_difference >= 0 ? beta_gain_scale * split_perc_self : beta_loss_penalty * abs_perc_difference`). Despite having a piecewise structure, it unfortunately failed the parameter recovery test for *both* `beta_gain_scale` (r = 0.058) and `beta_loss_penalty` (r = 0.302). Its overall performance (BIC 76.69, Acc 0.636) was also poorer than several previous models.

Analysis of Run 53's failure: The structure assigned the parameters to scale `split_perc_self` in the gain regime and `abs_perc_difference` in the loss regime. While piecewise, these terms might still be too correlated or capture insufficiently distinct aspects of the offer value within their respective regimes for the parameters to be uniquely identified. Scaling linear terms, even in different parts of the input space, might not provide the necessary functional separation for robust estimation.

Recall the relative success of Run 1 (BIC 40.53, Acc 0.671, Recovery >= 0.727 for both parameters), which used a piecewise structure on the *ratio* (`R_val`) and applied *different functional forms* (log vs. linear difference) scaled by the two parameters (`beta_gain_log` and `beta_loss_linear_diff`) in distinct gain vs. loss regimes. This structure seems to have created more distinct influences for the two parameters.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** such that *both* parameters have truly distinct and identifiable influences on responder decisions (`trial_role == 1`). This requires making their contributions to utility as mathematically separable as possible across the data.

Based on this analysis, focus your exploration for Run 54 on model structures where the two learnable parameters scale terms that are mathematically **maximally distinct** from each other, or whose influence is primarily expressed through **different, non-linearly related aspects** of the utility calculation. Simply scaling different linear terms in different regimes (as in Run 53) appears insufficient.

Consider these strategies to enhance identifiability:

*   **Piecewise Structures with Orthogonal, Non-Linear Parameter Roles per Regime:** Continue exploring piecewise functions based on a fairness split (e.g., `perc_difference >= 0` or `R_val >= 1`). The key is to assign `beta1` to scale a term or function `f1(Inputs)` *exclusively or primarily* in one regime (favorable offers) and `beta2` to scale a *different* term or function `f2(Inputs)` *exclusively or primarily* in the other regime (unfavorable offers). Crucially, ensure that `f1` and `f2` are mathematically diverse and non-linearly related transformations of the inputs.
    *   *Structural Principle:* Design the model as `U = IF(Condition based on fairness, beta1 * f1(Inputs), beta2 * f2(Inputs))`, where `f1` and `f2` are distinct non-linear functions of the relevant inputs (like `split_perc_self`, `fair_perc_self_calc`, `R_val`, `perc_difference`). Avoid `f1` and `f2` being simple linear terms or linearly related transforms of the same base input.
    *   *Example Ideas (building on Run 1's success with ratio/log):*
        *   `U = IF(R_val >= 1.0, beta_gain_sensitivity * log(R_val + 1e-6), beta_loss_slope * (R_val - 1.0))` (Revisit Run 1 structure, perhaps refine bounds/inputs).
        *   `U = IF(perc_difference >= 0, beta_pos_gain * sqrt(split_perc_self), beta_neg_fairness * exp(perc_difference))` (Using non-linear functions like sqrt and exp on relevant terms).
        *   `U = IF(R_val >= 1.0, beta_gain_alpha * R_val^power_alpha, beta_loss_beta * (R_val - 1.0)^power_beta)` (Introducing exponents; note: this would require 4 parameters, maybe fix exponents or combine parameters differently). A better version: `U = IF(R_val >= 1.0, beta_gain_alpha * (R_val - 1.0)^2, beta_loss_beta * (1.0 - R_val)^2)` (Scaling squared deviation from 1 in each direction).

*   **Single Highly Non-Linear Function with Orthogonal Parameter Roles:** If not using a piecewise structure, design a single, complex non-linear function of the inputs where `beta1` and `beta2` clearly influence distinct mathematical properties of the function's output. For instance, one parameter could control a general sensitivity or intercept, while the other specifically modulates a non-linearity, asymmetry, or threshold effect around the fairness point *within the unified formula*. This requires a more sophisticated function design than simple additive linear models. Think about functions that capture diminishing sensitivity to increasing gains or increasing sensitivity to losses relative to a reference point.

*   **Avoid:** Any model structure where the terms scaled by `beta1` and `beta2` are linearly dependent or highly correlated across the dataset, or where one parameter's influence can be easily absorbed into the other. Linear scaling of simple input terms, even in piecewise models, appears insufficient for identifiability with this dataset, based on previous results.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens = token_self + token_opp` and `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)`. It is highly recommended to also calculate intermediate variables like ratios (`R_val = split_perc_self / fair_perc_self_calc`) and differences (`perc_difference = split_perc_self - fair_perc_self_calc`) and use non-linear transformations (log, power, exp, sqrt, abs) of these or the raw inputs as needed to create separable parameter influences.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-20.0, 20.0] or wider/narrower if appropriate for the variable scale). Consider whether parameters are expected to be positive or negative and set bounds accordingly, but keep them generous.

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, any intermediate terms like ratios/differences/transformed values, and `U`) are included in your <VARIABLES> descriptions with `source: "calculated"`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: "data"`. Ensure the description for `fair_perc_self_calc` includes the `sum_tokens > 0` condition.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the proposed percentage share for self.

*   **Target Variable:** Specify "accept" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain how its specific mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters (e.g., by having them scale distinct *non-linear* terms in different regimes, or by influencing separate, non-linearly related aspects of the utility calculation).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This requires making their contributions to utility as distinct and non-overlapping as possible, often by having them scale different, non-linearly related terms or influence different regimes of the decision space. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation. Do not simply repeat structures that have failed recovery; propose a genuinely different way for the two parameters to influence the decision process.