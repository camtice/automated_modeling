Okay, here are the improved instructions for the next run, focusing intensely on designing a model structure that ensures parameter identifiability for *both* learnable parameters while building on the successes and addressing the failures of the previous attempts.

Instructions for Run 63 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 62) implemented a model (`U = beta_value * split_perc_self * exp(-beta_unfairness * max_deviation_positive)`) which attempted to use a **multiplicative, non-linear modulation** of the value term based on unfairness. This model critically failed to achieve sufficient parameter recovery for *both* `beta_value` (r = 0.571) and `beta_unfairness` (r = -0.073). Furthermore, it resulted in significantly worse BIC (84.03) and accuracy (0.544) compared to previous attempts, including the cubic penalty model from Run 61. This indicates that the specific multiplicative exponential structure was not effective for capturing the decision process in this dataset *or* for ensuring parameter identifiability.

The cubic penalty model (Run 61: `U = beta_value * split_perc_self - beta_unfairness * max(0, fair_perc_self_calc - split_perc_self)^3`) achieved a much lower BIC (44.74) and good recovery for `beta_value` (r = 0.917), but still failed on `beta_unfairness` recovery (r = -0.172). This highlights that while a non-linear penalty term can improve fit and isolate one parameter in an *additive* structure, it may not provide sufficient identifiability for the penalty parameter itself.

The challenge for this run is to design a model where the influence of the two learnable parameters is mathematically distinct and sufficiently strong across the data space to allow for reliable estimation of *both*. We know that simple additive/subtractive penalty structures often struggle with the second parameter's recovery, and the specific multiplicative exponential structure attempted last failed completely.

We need to find a mathematical structure for utility, still using exactly **2 learnable parameters**, where their influence on responder decisions (`trial_role == 1`) is profoundly separable and distinguishable. Given the failures so far, consider fundamentally different ways the parameters can interact with `split_perc_self` and `fair_perc_self_calc`.

Think **"Out of the Box"**: Avoid simply scaling a fairness deviation and adding/subtracting it from a value term. The multiplicative modulation attempt in Run 62 also failed. The problem might lie in the *form* of the function linking the second parameter to fairness and value. Explore structures where the parameters' influence is not just additive or a simple multiplicative decay, but integrated in a novel way.

Consider alternative strategies focusing on creating mathematically distinct parameter influences:

*   **Different Non-linear Combinations:** Can utility be a combination of terms where *each* term involves a different parameter and uses a different non-linear function of the relevant variables (`split_perc_self`, `fair_perc_self_calc`, or derived deviations/ratios)? For example, `U = f(split_perc_self, beta_value) + g(fair_perc_self_calc, split_perc_self, beta_unfairness)`, where `f` and `g` are distinct non-linear functions (e.g., logarithmic, power, sigmoidal, exponential, but used differently than the failed Run 62 attempt).
*   **Ratio-based Utility (Revisited):** Structure utility as a ratio of components influenced by the parameters. E.g., `U = (ValueTerm(split_perc_self, beta_value)) / (PenaltyTerm(fair_perc_self_calc, split_perc_self, beta_unfairness))` or similar, being careful to handle potential divisions by zero (use epsilon).
*   **Interaction beyond Simple Modulation:** Explore structures where the parameters' influence is truly interactive, perhaps in a way that changes the *shape* of the utility function as a function of `split_perc_self` differently depending on the fairness and the second parameter's value. This might involve polynomials or other functions where coefficients or exponents are determined by the parameters and fairness.
*   ** piecewise Utility with Parameter-Controlled Breakpoints or Slopes:** While conceptually complex, a piecewise function where the transition point or the slope/curvature around the fairness point is controlled by the second parameter could create distinct influences.

The goal is to design a utility calculation `U` involving `split_perc_self`, `fair_perc_self_calc` (derived from `token_self`, `token_opp`), and exactly two learnable parameters (`beta_X`, `beta_Y`) such that the influence of `beta_X` on `U` is as mathematically independent from the influence of `beta_Y` as possible across the range of `split_perc_self` and `fair_perc_self_calc` values in the data, particularly focusing on making the second parameter's effect pronounced and non-linear specifically in the unfair regime, but in a way that *is* recoverable. Avoid structures where the parameters are simply scaling additive components.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and any other necessary intermediate variables. Ensure `fair_perc_self_calc` handles `sum_tokens == 0` correctly (defaulting to 0.5). Use a small epsilon (`1e-6` or similar) for numerical stability if needed (e.g., denominators, logarithms).
*   **Model Formula:** Define `U`. Include calculation steps for derived variables *before* the formula for `U`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds (e.g., [-10.0, 10.0], [-50.0, 50.0], [0.0, 100.0], etc. appropriate for the scale of their influence).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including U) with `source: "calculated"`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: "data"`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify "accept" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific non-linear, asymmetric, or interactive mathematical structure is fundamentally different from simple additive/subtractive penalties and the failed multiplicative exponential, and how this difference is intended to ensure robust parameter identifiability for *both* learnable parameters*. Explicitly explain why this new structure should allow for better recovery of the second parameter compared to previous attempts.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative and explore structures that move beyond simple additive penalties and simple multiplicative modulation.