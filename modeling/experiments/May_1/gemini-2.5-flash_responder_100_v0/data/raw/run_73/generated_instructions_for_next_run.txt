Okay, here are the improved instructions for the next run (Run 74 of 100), based on the results of recent attempts.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery.

The model tested in the previous run, an additive utility model with an exponential penalty term (`U_accept = beta_value * split_perc_self - beta_unfairness * exp(loss_below_fair)`), performed extremely poorly on all metrics (Average BIC: 950.97, Accuracy: 0.431). Crucially, parameter recovery was abysmal (beta_value: r=0.180, beta_unfairness: r=0.000). This demonstrates that this specific functional form, despite the hypothesis that the exponential term would create separability, failed entirely to capture the underlying behavior or yield identifiable parameters.

Previous attempts using simple additive structures (`U_accept = beta_value * split_perc_self - beta_unfairness * penalty`) with linear, squared, or ratio-based penalties have also consistently shown poor parameter identifiability (recovery well below 0.7 for at least one parameter, often both). Even the reference-dependent additive structure (`U_accept = beta_value * gain_above_fair - beta_unfairness * loss_below_fair`) in a recent run failed dramatically (recovery r=0.131 and r=0.015).

These repeated failures across various additive structures (simple linear, squared, ratio penalty, exponential penalty, reference-dependent gain/loss) strongly suggest that **a purely additive combination of value and unfairness terms might inherently lead to parameter confounding** in this task, regardless of the specific functional form of the penalty term. The influences of 'value' and 'unfairness' simply added together might be too correlated or indistinguishable across the observed data range.

For Run 74, we must fundamentally reconsider the *structural principle* by which value and unfairness combine to influence utility. Instead of adding or subtracting components, let's explore a **multiplicative or interactive structure**. The goal is to design a model where the influence of one parameter (e.g., unfairness sensitivity) *modulates* or *scales* the influence of the other (e.g., value sensitivity) in a non-additive way. This interaction is hypothesized to create distinct parameter dependencies on the input variables, thereby improving identifiability.

Proposed Model Structure: A multiplicative utility model where the value derived from the offer is discounted or scaled down based on its unfairness using an exponential decay.
`U_accept = beta_value * split_perc_self * exp(-beta_unfairness * loss_below_fair)`

Here:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0).
Calculate `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
Finally, `U_accept = beta_value * split_perc_self * exp(-beta_unfairness * loss_below_fair)`. Note the multiplication and the negative exponential.

In this structure, `beta_value` scales the potential utility from the offered percentage (`split_perc_self`) linearly. `beta_unfairness` scales the *rate of exponential decay* of this potential utility as the offer falls below the fair share (`loss_below_fair`). The term `exp(-beta_unfairness * loss_below_fair)` is a multiplicative factor between 0 and 1 (assuming `beta_unfairness >= 0` and `loss_below_fair >= 0`). A higher `beta_unfairness` or a larger `loss_below_fair` leads to a smaller multiplicative factor, thus reducing the overall utility. The key hypothesis for improved identifiability is that this **multiplicative interaction**, specifically the exponential decay applied to the linear value term, creates a fundamentally different and more separable influence landscape for `beta_value` and `beta_unfairness` compared to all tested additive structures.

Design the model using this multiplicative structure with exponential decay: `U_accept = beta_value * split_perc_self * exp(-beta_unfairness * loss_below_fair)`, where `loss_below_fair` is calculated as specified above using `fair_perc_self_calc` as the reference.

Focus on clearly defining the `loss_below_fair` term and articulating in the summary how this specific structure, by combining a linear value term with an *exponential multiplicative penalty* based on deviation from fair share, is intended to achieve robust parameter identifiability by creating mathematically distinct and interacting influence profiles for `beta_value` and `beta_unfairness` across different offers, addressing the persistent identifiability failures seen in purely additive structures.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include any other necessary intermediate variables.
*   **Model Formula:** Define `U_accept` using the multiplicative structure with exponential decay: `sum_tokens = token_self + token_opp`, `fair_perc_self_calc = (token_self / sum_tokens) * 100.0` (handle `sum_tokens == 0`), `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`, `U_accept = beta_value * split_perc_self * exp(-beta_unfairness * loss_below_fair)`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (value sensitivity) and `beta_unfairness` (unfairness sensitivity decay rate). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and may aid recovery. Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: "calculated"`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: "data"`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify "accept" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model\'s core idea (multiplicative structure, linear value term scaled by exponential unfairness penalty based on deviation from fair share). Crucially, explain *how* this structure, by using a *multiplicative* exponential decay function for the penalty term applied to the linear value term, is specifically intended to improve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles for `beta_value` and `beta_unfairness`, addressing the limitations and failures of previous purely additive structures.
*   **Think Outside the Box:** Recognize that previous attempts with standard additive structures and simple non-linear forms have consistently failed parameter recovery. The proposed multiplicative exponential decay structure is a deliberate departure, explicitly attempting to create functional separability through interaction. Continue to consider non-obvious ways parameters might interact or influence decisions beyond simple additive combinations, as finding a structure that truly shines in parameter recovery often requires exploring less standard structural principles like multiplication, gating, or thresholds.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters\' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.