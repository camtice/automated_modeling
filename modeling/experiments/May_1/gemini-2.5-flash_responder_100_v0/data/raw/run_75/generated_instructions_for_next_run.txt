Okay, here are the improved instructions for the next run (Run 76 of 100), based on the results of recent attempts.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. The model tested in the previous run (Run 75), the multiplicative structure `U_accept = split_perc_self * (beta_value - beta_unfairness * loss_below_fair)`, achieved a notably low BIC (57.07), suggesting it might capture some aspects of the data well. However, it catastrophically failed parameter recovery for *both* parameters (r = 0.100 and r = -0.043). This reinforces that achieving a good fit metric like BIC is insufficient if the underlying parameters cannot be reliably estimated. The previous multiplicative formulation, despite its promising BIC, resulted in highly confounded parameter influences.

Previous attempts across various additive and multiplicative structures have consistently struggled with parameter identifiability. This suggests that simply combining value and unfairness terms additively or with simple linear multiplication does not create sufficiently distinct influence profiles for the parameters across the range of possible offers in this task.

For Run 76, we will continue exploring non-additive structures, given the relatively good BIC performance of the previous multiplicative attempt. However, we need a structure where the parameters' influences are mathematically more separable to ensure robust identifiability. The failure of the previous multiplicative model suggests that multiplying the sensitivity factor directly by `split_perc_self` may have contributed to confounding, particularly for low offers.

Proposed Model Structure: A divisive utility model where the core value derived from the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, is *reduced* by a factor based on unfairness (`loss_below_fair`), controlled by `beta_unfairness`.

`U_accept = (beta_value * split_perc_self) / (1 + beta_unfairness * loss_below_fair)`

Here:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
The utility `U_accept` is calculated as the product of `beta_value` and `split_perc_self`, divided by `(1 + beta_unfairness * loss_below_fair)`.

In this structure, `beta_value` acts as a primary scaling factor for the 'value' component (`split_perc_self`), similar to previous models. `beta_unfairness`, however, specifically influences the *denominator* of the utility calculation. For offers that are fair or better (`loss_below_fair = 0`), the denominator is `(1 + 0) = 1`, and utility is simply `beta_value * split_perc_self`. For unfair offers (`loss_below_fair > 0`), the denominator increases proportionally to `loss_below_fair`, controlled by `beta_unfairness`, thus *reducing* the utility via division. This separation, where `beta_value` scales the numerator and `beta_unfairness` scales an additive term in the denominator, is hypothesized to create distinct and non-linearly interacting influence profiles for the parameters, potentially improving parameter identifiability compared to simple additive or the previous multiplicative forms.

Design the model using this divisive structure: `U_accept = (beta_value * split_perc_self) / (1 + beta_unfairness * loss_below_fair)`, where `loss_below_fair` and `fair_perc_self_calc` are calculated as specified above using `token_self`, `token_opp`, and `split_perc_self`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific divisive structure, by placing `beta_unfairness` in the denominator affecting the scaling of the entire `beta_value * split_perc_self` term, is intended to achieve robust parameter identifiability by creating mathematically distinct and interactive influence profiles, addressing the persistent identifiability failures seen in previous models. `beta_value` represents a base sensitivity to the offer percentage, and `beta_unfairness` represents how strongly increasing unfairness causes this sensitivity to be *divided down*.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (base value sensitivity) and `beta_unfairness` (unfairness divisor sensitivity). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and likely necessary for parameter recovery in this structure (e.g., `beta_unfairness` should be non-negative so the denominator increases with `loss_below_fair`, and `beta_value` should be non-negative for utility to increase with `split_perc_self` on fair offers). Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: "calculated"`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: "data"`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify "accept" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (divisive structure where utility from `split_perc_self` is reduced by unfairness). Crucially, explain *how* this specific divisive structure, where `beta_unfairness` influences the denominator, is specifically intended to improve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles for `beta_value` and `beta_unfairness`, addressing the limitations and failures of previous structures.
*   **Think Outside the Box:** Recognize that previous attempts with standard additive, squared additive, exponential additive, and linear multiplicative structures have consistently failed parameter recovery. The proposed divisive structure is another attempt at a non-additive interaction principle. Success in parameter recovery often requires exploring less standard structural principles. Continue to consider how parameters might interact via division, exponents, thresholds, or other non-linear functions that haven't been tried yet, focusing on creating distinctly shaped influences for each parameter across the range of possible offers.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.