Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: "accept").
2.  **Achieve excellent parameter recovery (>= 0.7 correlation) for ALL learnable parameters.** This remains the *single most important* objective. Previous attempts, including the most recent one, have consistently failed here. A model is unusable if any learnable parameter cannot be reliably recovered, regardless of other metrics (like BIC or accuracy). Prioritize model structures that inherently enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables available during responder trials (trial_role == 1).
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

**Analysis of Previous Attempt (Run 8):** The most recent model (`beta_value * split_perc_self - gamma_fairness * max(0, proportional_share - split_perc_self) - delta_inequity * max(0, 50 - split_perc_self)`) showed some improvements in BIC (393.86) and accuracy (0.769) compared to some earlier models, but it **failed parameter recovery spectacularly** (`beta_value: r=0.592`, `gamma_fairness: r=0.296`, `delta_inequity: r=0.237`). All three parameters were significantly below the critical 0.7 threshold. This confirms the diagnosis from earlier runs: simple additive components scaled by learnable parameters, even in a piecewise linear form distinguishing between the 50% and proportional norms, still results in highly correlated or substitutable parameters given the data structure. The effects of increasing the general value of the offer (`beta_value`) and decreasing the penalties (`gamma_fairness`, `delta_inequity`) are too similar, and the effects of the two different penalties also overlap too much, especially when offers are low.

**Guidance for the Next Model (Radically Prioritizing Parameter Recovery - Continued):** We *must* move beyond simple linear, additive structures if we are to achieve parameter identifiability. The challenge is to design parameters that influence the utility function in ways that are *qualitatively* different from each other, particularly around the critical reference points (50% and the proportional share, `fair_perc_self`).

Consider how parameters could control:
*   **Non-linear penalties/bonuses:** Instead of linear penalties scaled by parameters, could parameters control the *exponent* of a deviation term (e.g., `deviation^parameter_exponent`)? Or could penalties be quadratic or exponential functions controlled by parameters? For example, `gamma_50 * max(0, 50 - split_perc_self)^exponent_50` and `gamma_prop * max(0, fair_perc_self - split_perc_self)^exponent_prop`. Parameters controlling exponents (`exponent_50`, `exponent_prop`) or scaling non-linear terms (`gamma_50`, `gamma_prop`) might have more distinct effects.
*   **Switching slopes/curvatures:** Design a model where the sensitivity to `split_perc_self` is different depending on whether the offer is, say, below the proportional share, between the proportional share and 50%, or above 50%. Parameters could define the *slopes* or *curvatures* in each of these ranges. This creates piecewise functions, but the parameters control the *shape of the segments*, not just scale an additive penalty.
*   **Relative influence via non-additive means:** Can a parameter mediate the *interaction* between the offer value and deviation from norms? For example, `Utility = split_perc_self * (1 - beta_aversion * total_deviation)`. Or could a parameter affect *how* the 50% norm and the proportional norm combine their influence, perhaps in a multiplicative or non-linear way?
*   **Thresholds:** Could a parameter define a *threshold* below which acceptance probability drops sharply, independent of the specific penalty calculation? (e.g., if `split_perc_self < beta_rejection_threshold`, Utility is very low). This might be challenging to make identifiable alongside other parameters, but it's a structural departure.

Focus on defining parameters whose effect on utility cannot be replicated by simply scaling other terms or shifting the function linearly. Parameters should ideally affect the utility function's slope, curvature, or point of inflection in a way that is unique to that parameter.

Continue to build the model using percentage-based values (`split_perc_self`, `fair_perc_self`). Aim for the minimum number of learnable parameters necessary *only if* their effects are clearly separable and identifiable. A model with fewer, identifiable parameters is vastly preferred over one with many, unidentifiable ones, regardless of seemingly good BIC or accuracy scores on validation data.

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded (-inf to inf).

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1) and use variables available in that context. The `fair_perc_self` variable (calculated as `(token_self / (token_self + token_opp)) * 100`) is useful for fairness calculations.

For run 9 of 100, please think through this step by step, focusing intently on how the *specific mathematical structure* of the model guarantees parameter identifiability and distinguishes the influence of each parameter, minimizing covariance based on the *specific* data characteristics. How can you define learnable parameters that control fundamentally different aspects of the utility function (e.g., changing slopes in ranges, exponents, multiplicative interactions, switching points based on norms) rather than just scaling additive components? Think creatively about non-linear and piecewise structures relative to *both* the 50% and proportional norms. Then provide your model specification, variable descriptions, target variable, and summary.