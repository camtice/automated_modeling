run_number,average_bic,bic_control,bic_cocaine,overall_accuracy,model_specification,model_summary,version,instructions_used,beta_zero_recovery,beta_one_recovery,beta_two_recovery,beta_three_recovery,penalty_weight_recovery,fifty_fifty_weight_recovery,beta_self_interest_recovery,beta_50_aversion_recovery,beta_proportional_aversion_recovery,beta_base_recovery,beta_split_recovery,beta_unfairness_recovery,alpha_recovery,beta_gain_recovery,gamma_50_recovery,gamma_prop_recovery,gamma_unfairness_recovery,beta_value_recovery,gamma_fairness_recovery,delta_inequity_recovery,beta_recovery,delta_prop_recovery,delta_50_recovery,gamma_proportional_aversion_recovery,gamma_recovery,beta_step_recovery,alpha_fair_param_recovery,alpha_unfair_param_recovery,alpha_sensitivity_recovery,beta_unfair_penalty_recovery,gamma_generous_bonus_recovery,sensitivity_generous_recovery,penalty_slope_unfair_recovery,sensitivity_offer_recovery,penalty_magnitude_unfairness_recovery,beta_bias_recovery,penalty_unfair_flat_recovery,sensitivity_base_recovery,sensitivity_unfair_recovery,slope_unfair_deviation_recovery,slope_generous_deviation_recovery,acceptance_bias_recovery,penalty_unfair_deviation_recovery,penalty_unfair_deviation_sq_recovery,beta_amount_recovery,beta_fair_gain_recovery,beta_fair_loss_recovery,beta_0_recovery,beta_1_recovery,beta_2_recovery,gamma_param_recovery,fairness_threshold_perc_recovery,beta_utility_below_threshold_recovery,beta_loss_recovery,alpha_param_recovery,lambda_param_recovery,beta_param_recovery,beta_utility_below_recovery,beta_utility_fair_recovery,beta_gain_slope_recovery,beta_loss_slope_recovery,beta_utility_at_fair_below_recovery,beta_slope_below_recovery,beta_utility_at_fair_above_recovery,beta_slope_above_recovery,beta_intercept_recovery,beta_split_perc_recovery,beta_fair_perc_recovery,beta_unfair_recovery,kappa_recovery,mu_recovery,beta_loss_penalty_recovery,beta_linear_relative_recovery,beta_extra_loss_recovery,beta_ratio_sens_recovery,beta_baseline_recovery,beta_split_sens_recovery,beta_fair_sens_recovery,beta_gain_ratio_recovery,beta_loss_ratio_recovery,beta_gain_log_sens_recovery,beta_loss_linear_sens_recovery,beta_loss_log_sens_recovery,beta_gain_linear_diff_recovery,beta_loss_linear_diff_recovery,beta_gain_log_recovery,beta_proposed_share_recovery,beta_fair_share_recovery,beta_fairness_deviation_recovery,beta_fairness_penalty_recovery,beta_unfairness_penalty_recovery,beta_gain_scale_recovery,beta_gain_sensitivity_recovery,beta_loss_sensitivity_recovery,beta_relative_fairness_recovery,beta_interaction_recovery,beta_unfairness_effect_recovery,beta_unfairness_penalty_sensitivity_recovery,beta_fairness_interaction_recovery,beta_base_sensitivity_recovery,beta_penalty_recovery,beta_sensitivity_recovery,parameter_perc_fairness_recovery,parameter_context_intercept_recovery,beta_offer_sensitivity_recovery,beta_fairness_deviation_sensitivity_recovery,sensitivity_to_offer_recovery,aversion_strength_recovery,deviation_sensitivity_recovery,below_fair_impact_recovery,self_more_contrib_intercept_shift_recovery,base_accept_utility_recovery,total_earning_sensitivity_recovery
42,31.191667715359205,24.423964838201183,37.25440154281327,0.797924297924298,"S_tokens = token_self + token_opp
F_perc_self = if S_tokens > 0 then (token_self / S_tokens) * 100.0 else 50.0
R = if F_perc_self > 0 then split_perc_self / F_perc_self else if split_perc_self == 0 then 1 else 1000
U = beta_ratio_sens * R + beta_baseline","A computational model predicting responder acceptance decisions based on the utility of the proposed offer. Utility is modeled as a linear function of the ratio between the proposed percentage share and the participant's fair percentage share (based on token contribution), incorporating handling for cases where the fair share is zero. The model includes two learnable parameters: a sensitivity parameter (beta_ratio_sens) scaling the impact of the ratio, and a baseline parameter (beta_baseline) representing general acceptance propensity. This ratio-based structure is hypothesized to improve parameter identifiability compared to models based on differences or piecewise deviations from fairness. The utility (U) is converted to a probability of acceptance using a logistic function.",v0,"Instructions for Run 42 of 100:

The unwavering primary objective is achieving parameter recovery >= 0.7 for *all* learnable parameters. This remains the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery.

The previous run (Run 41) explored a piecewise linear model (`U = beta_gain * gain_above_fair - beta_loss * loss_below_fair`) hypothesizing that separating sensitivity to gains and losses would enhance identifiability. Although this model achieved a relatively low BIC (187.15), indicating good fit, it completely failed parameter recovery for both `beta_gain` (r = -0.083) and `beta_loss` (r = 0.012). This demonstrates that despite theoretical separation, the linear structure applied to piecewise deviations did not allow for independent estimation of these parameters from the available data.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`). Linear combinations or simple piecewise applications of deviation terms have repeatedly shown issues with simultaneous parameter recovery.

For this run, let's pivot to a different structural approach that relates the proposed share to the fair share using a **ratio** instead of a difference. This departs from the previous models and might offer a novel path to parameter identifiability.

Design a computational model that predicts the utility of accepting an offer based on the *ratio* of the proposed percentage share to the fair percentage share.

*   **Promising Structural Direction (Ratio-based):** Consider using the ratio `split_perc_self / fair_perc_self` as a key input driver for utility. This frame treats fairness not as an anchor point for deviations, but as a reference denominator. Structure the utility (`U`) as a linear function of this ratio, plus a baseline utility.
    *   Calculate the necessary intermediate variables:
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Note: this calculation already handles the case where `sum_tokens` is 0, setting `fair_perc_self` to 50.0, thus avoiding division by zero in the ratio if `fair_perc_self` is in the denominator).
    *   Structure the utility (`U`) based on the ratio and your two learnable parameters. A promising form is:
        `U = beta_ratio_sens * (split_perc_self / fair_perc_self) + beta_baseline`
    *   This structure models utility as increasing linearly with the ratio of the proposed share to the fair share (scaled by `beta_ratio_sens`), offset by a constant baseline utility (`beta_baseline`). This attempts to separate the sensitivity to the *relative proportion* of the offer (`beta_ratio_sens`) from the general propensity to accept offers (`beta_baseline`).

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens` and `fair_perc_self` as described above.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.

*   **Model Formula:** Define the utility (`U`) based on `split_perc_self`, `fair_perc_self`, and your 2 learnable parameters using the ratio structure suggested. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   `beta_ratio_sens`: Sensitivity to the `split_perc_self / fair_perc_self` ratio. Should likely be positive (higher ratio -> higher utility). Suggest generous bounds like [0.0, 20.0].
    *   `beta_baseline`: Baseline utility (intercept). Can be positive or negative. Represents utility when the ratio is zero (e.g., offering 0%). Suggest generous bounds like [-20.0, 20.0].
    Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds for both.

*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self`, etc.) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`, etc.).

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by the *ratio* of the proposed share to the fair share, and how this structure aims to improve parameter identifiability compared to models based on the *difference* or piecewise deviations from fair.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This ratio-based approach is a significant departure from previous attempts and might offer the necessary structural separation. If this structure also fails parameter recovery, be prepared to think even further outside the box for the next iteration – perhaps utility is not linear in the ratio or difference, perhaps fairness acts as a categorical modifier or a non-linear threshold, or maybe different inputs are needed entirely.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.39288594114164543,0.9157406211344098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5,35.785469482359346,31.835783555524124,39.32372979181589,0.8025030525030524,"fair_perc_self = token_self / (token_self + token_opp)
reference_perc = alpha * 0.5 + (1 - alpha) * fair_perc_self
deviation_below_reference = max(0, reference_perc - split_perc_self)
utility = beta_base + beta_split * split_perc_self - beta_unfairness * deviation_below_reference^2","A reference-dependent utility model for responder decisions, using only percentage-based values for fairness and offer evaluation. Utility is a function of a baseline parameter, the linear gain from the proposed percentage share, and a non-linear (quadratic) penalty for receiving less than a dynamic reference percentage. The reference percentage is a weighted average of the 50% equal split norm and the participant's proportional share based on contributions, with the weighting governed by a learnable parameter 'alpha'. This non-linear structure and dynamic reference point are designed to improve parameter identifiability. The model has four learnable parameters: a base utility (beta_base), sensitivity to the proposed split (beta_split), sensitivity to unfairness (beta_unfairness), and the weight for the reference point (alpha).",v0,"<new_instructions>Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Achieve excellent parameter recovery (>= 0.7 correlation) for ALL learnable parameters.** This is the *single most important* objective. A model is unusable if any learnable parameter cannot be reliably recovered. Models that do not meet this threshold will be rejected regardless of other performance metrics (BIC, accuracy). Prioritize model structures that inherently enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables available during responder trials (trial_role == 1).
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

**Addressing Past Failures:** Previous model attempts, particularly those using simple linear combinations of features or mixing absolute (£) and percentage (%) values in the utility function, have consistently failed to achieve adequate parameter recovery. The parameters' influences were likely too correlated or not distinct enough given the data structure.

**Guidance for the Next Model (Prioritizing Parameter Recovery):**

*   **Consistent Scaling:** Given the variable `combined_earning` across trials, using percentages (`split_perc_self`) is likely a more robust approach for calculating utility related to the proposed offer and fairness deviations. **Strongly consider calculating *all* components of the utility function that relate to the offer amount and fairness deviations using percentages** (e.g., basing the 'self-interest' term on `split_perc_self` rather than `split_self`). This provides a consistent scale and may improve parameter identifiability.
*   **Break Away from Linearity:** Standard linear additive models (e.g., `U = beta1*X1 + beta2*X2 + ...`) have proven insufficient for parameter recovery here. **Design a model with non-linear functional forms or non-additive interactions between components.** For example:
    *   Instead of linear penalties for fairness deviations, explore quadratic penalties (e.g., `beta * max(0, norm - split_perc_self)^2`).
    *   Consider parameters that modulate the strength or effect of other terms or variables in a non-linear way.
    *   Explore reference-dependent utility where utility is explicitly a function of deviations from norms, but perhaps the sensitivity to positive vs. negative deviations is different, or the deviation itself is transformed non-linearly.
    *   Think about how parameters could interact in a multiplicative or gating fashion, *if* this interaction helps make their individual influences distinct (be cautious of introducing more collinearity).
*   **Ensure Distinct Parameter Influences:** Carefully define how each learnable parameter contributes to the overall utility. The contribution of one parameter should not be easily explained by a combination of other parameters. This is key to achieving parameter recovery.
*   **Minimum Necessary Parameters (for identifiability):** While the structure is more important than just the count, a model with fewer, well-identified parameters is preferable to one with many poorly-identified parameters. Strive for parsimony *while ensuring identifiability*.

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded (-inf to inf).

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1).

First, reason step by step about:
*   The key psychological mechanisms involved in responder decisions (self-interest, aversion to 50% split unfairness, aversion to proportional split unfairness).
*   **Crucial Design Choice:** How to consistently scale the offer and fairness terms. **Strongly advocate for and implement a model that uses *only* percentages (`split_perc_self`)** for all offer-related components in the utility function to improve consistency and potential identifiability across varying `combined_earning`.
*   **Mandatory Non-linear Structure:** How to incorporate non-linear functional forms or interactions into the utility calculation (e.g., quadratic penalties, non-linear parameter interactions) to ensure parameter identifiability and break away from failed linear approaches.
*   How the learnable parameters are defined to have maximally distinct and identifiable effects on utility, minimizing covariance.
*   What is the *minimum necessary* set of learnable parameters that can robustly capture the essential aspects of the behavior while prioritizing parameter identifiability (>= 0.7 recovery)?
*   How the model variables map to observable behavior (""accept"") via the logistic function.

Please think through this step by step, focusing intently on how the model structure guarantees parameter identifiability, then provide your model specification, variable descriptions, target variable, and summary.
</new_instructions>",,,,,,,,,,0.926497635638048,0.382879459741262,0.21932950901298373,0.11481742047563356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27,37.406551686743725,33.31448336616275,41.07236289059752,0.778998778998779,"U_accept = beta_0 + beta_1 * split_self - beta_2 * max(0.0, (fair_perc_self - split_perc_self)/100.0)**gamma_param","A utility model for responder acceptance decisions incorporating a linear effect of the received amount and a non-linear penalty for offers falling below the fair share. The fairness penalty is calculated based on the scaled percentage deviation from the fair share (derived from token contributions), is only active for negative deviations, and its magnitude is shaped by a power-law exponent parameter. This structure aims to improve parameter identifiability by separating the influence of absolute gain from the non-linear psychological cost of unfairness relative to entitlement.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions:
Previous attempts modeling responder behavior have consistently struggled with parameter recovery. Specifically, recent models, including those based on linear or quadratic effects of percentage deviations from fairness and a model combining absolute amount with percentage deviations, have failed to achieve acceptable parameter recovery (>= 0.7 for *all* learnable parameters). This indicates a significant issue with parameter identifiability – the data does not provide enough unique information to reliably estimate all parameters in these specific structural forms.

For this run (27 of 100), the **absolute highest priority is achieving parameter recovery >= 0.7 for *all* learnable parameters**. While good BIC and accuracy are desired, a model is unusable if its parameters cannot be reliably estimated. This means you must design a model where each learnable parameter contributes uniquely to the predicted utility across different scenarios in the dataset, making it possible to distinguish the influence of one parameter from another during fitting.

Given the repeated failure of previous structural approaches (linear/quadratic effects of percentage/amount + deviation terms, including separate gain/loss terms), it is clear that a fundamentally *different* modeling structure is required to improve parameter identifiability in this dataset. Simple linear combinations of standard variables (`split_perc_self`, `unfair_deviation`, `split_self`) seem insufficient.

**Guidance for the Next Model (Focusing on Identifiability):**

Design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer, prioritizing parameter identifiability above all else. This requires creative thinking beyond standard linear models of offers and deviations.

*   **Parameter Identifiability:** Design the model structure such that changing the value of *one* learnable parameter has a distinct and non-overlapping effect on the predicted utility compared to changing another parameter. This is the key to good recovery.
*   **Explore Novel Structures:** Previous attempts indicate that simple linear or quadratic functions of `split_perc_self`, `unfair_deviation`, or `split_self` in combination do not yield identifiable parameters. Consider:
    *   **Non-linearities:** Explore non-linear effects beyond quadratic. Perhaps threshold models (utility changes sharply above/below a certain offer/deviation level), exponential decay of negative utility with increasing unfairness, or sigmoidal effects.
    *   **Contextual Effects:** How does `trial_type` (contribution structure) or `combined_earning` (total pot size) influence preferences? Could these variables modulate the effect of other terms (e.g., sensitivity to unfairness depends on the pot size, or the baseline acceptance bias depends on `trial_type`)? Interaction terms or separate parameters per context type might be viable approaches here.
    *   **Ratio-based Fairness:** Instead of percentage differences (`unfair_deviation`), model utility based on the ratio of received share to fair share (`split_perc_self / fair_perc_self`). This might capture proportional fairness differently and potentially be more identifiable.
    *   **Alternative Utility Formulations:** Are there psychological theories of fairness or decision-making that suggest a utility structure fundamentally different from linear combinations? (e.g., models based on comparing offers to a reference point beyond just the fair share, or models with multiplicative components).
    *   **Calculated Variables:** You will likely need `fair_perc_self` calculated as `(token_self / (token_self + token_opp)) * 100` (handle the `token_self + token_opp == 0` case by setting `fair_perc_self` to 50). Include *all* calculated variables used in the model in your descriptions. Use percentage terms (`split_perc_self`) for fairness comparisons to potentially reduce collinearity with absolute amount (`split_self`) unless you have a specific, justified structure using both.
*   **Parameter Constraints:** Define clear, generous, finite numerical bounds for *all* learnable parameters. Consider carefully if parameters should be non-negative or unbounded based on their psychological interpretation and their role in making them identifiable.
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags. Use clear notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include bounds and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure, its key components, and how it relates to the decision process, with a focus on the *novelty* or structural changes aimed at identifiability.

Design a model that you believe has the best chance of achieving parameter recovery >= 0.7 for all its learnable parameters by adopting a structural form that avoids the apparent identifiability issues of previous linear/quadratic approaches, while still fitting the data well (lower BIC, higher accuracy). Think critically about how each parameter influences decisions and how this influence can be uniquely captured by the model structure given the characteristics of the dataset.

Please think through this step by step, focusing intently on designing a model structure that improves parameter identifiability, and ensure all output requirements are met. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5533521676027743,0.4208910149886495,0.017657181244694245,0.16445578023328422,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44,38.673503761676976,35.23930386818747,41.74997449959466,0.710927960927961,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)
epsilon_val = 1e-6
R_val = (split_perc_self + epsilon_val) / (fair_perc_self_calc + epsilon_val)
U = IF(R_val >= 1.0, beta_gain_ratio * R_val, beta_loss_ratio * R_val)","This model predicts the utility of accepting an offer using a piecewise linear function of the ratio between the proposed percentage share and the participant's fair share based on token contributions. The utility is calculated as the ratio multiplied by one of two sensitivity parameters: 'beta_gain_ratio' if the ratio is 1 or greater (fair or favorable offers), and 'beta_loss_ratio' if the ratio is less than 1 (unfair offers). This piecewise structure aims to separate the influence of favorable versus unfavorable offer ratios on the decision utility, using exactly two learnable parameters.",v0,"Okay, here are the improved instructions for the next run (Run 44), building on the results of Run 43.

Instructions for Run 44 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery.

The previous run (Run 43) explored a model based on the weighted sum of the logarithms of the proposed share and the fair share (`U = beta_split_sens * log(split) + beta_fair_sens * log(fair)`). This structure resulted in moderate BIC (87.24) and accuracy (0.780). Critically, parameter recovery was mixed: `beta_fair_sens` recovered acceptably (r = 0.771), but `beta_split_sens` failed significantly (r = 0.399). This indicates that simply taking the logarithm of the inputs and adding them with separate weights was not sufficient to structurally decouple the influences of the proposed share and the fair share in a way that allowed both sensitivity parameters to be reliably estimated from the data. The influences of `log(split_perc_self)` and `log(fair_perc_self)` may be too correlated or their effect too similar within this additive structure.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`). Linear combinations, piecewise deviations based on absolute difference, linear relationships with derived ratios, and additive logarithmic relationships have all shown issues with simultaneous parameter recovery for *both* parameters.

For this run, let's revisit the *ratio* concept but apply the parameters in a piecewise manner, attempting to separate the sensitivity to favorable ratios (at or above fair) from the sensitivity to unfavorable ratios (below fair). This departs from the linear ratio model (Run 42) and the additive logarithmic model (Run 43), offering a new structural form.

Design a computational model that predicts the utility of accepting an offer based on a **piecewise linear function of the ratio** between the proposed percentage share and the participant's fair percentage share.

*   **Structural Direction (Piecewise Ratio):** Consider modeling utility (`U`) as proportional to the ratio of the proposed share to the fair share, but with different proportionality constants (sensitivities) depending on whether the ratio is at least 1 (offer is fair or generous) or less than 1 (offer is unfair).
    *   Calculate the necessary intermediate variables:
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensures `fair_perc_self` is >= 0).
        *   `epsilon_val = 1e-6` (Small constant to handle potential division by zero if `fair_perc_self` is 0, although the IF statement handles the sum_tokens=0 case, split_perc_self could still be 0).
        *   `R_val = (split_perc_self + epsilon_val) / (fair_perc_self + epsilon_val)` (Calculate the ratio, adding epsilon to numerator and denominator for robustness).
    *   Structure the utility (`U`) using a conditional statement based on the value of `R_val`:
        *   If `R_val >= 1.0`, `U = beta_gain_ratio * R_val`
        *   If `R_val < 1.0`, `U = beta_loss_ratio * R_val`
    *   This structure uses two distinct learnable parameters (`beta_gain_ratio` and `beta_loss_ratio`) to scale the utility based on whether the offer ratio is favorable or unfavorable relative to fairness. This separation of influence for gain/loss scenarios using the ratio as the metric is a new structural attempt to achieve parameter identifiability.

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens`, `fair_perc_self`, `epsilon_val`, `R_val`, and `U`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.

*   **Model Formula:** Define the utility (`U`) based on the conditional logic applied to `R_val` and your 2 learnable parameters. Include the calculation steps for necessary derived variables (`sum_tokens`, `fair_perc_self`, `epsilon_val`, `R_val`) before the final formula for `U`. Use clear mathematical notation (IF/ELSE or similar conditional structure).

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   `beta_gain_ratio`: Sensitivity parameter for the ratio when it is greater than or equal to 1 (fair or favorable offers). Suggest generous bounds like [-10.0, 10.0].
    *   `beta_loss_ratio`: Sensitivity parameter for the ratio when it is less than 1 (unfair offers). Suggest generous bounds like [-10.0, 10.0].
    Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds for both.

*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self`, `epsilon_val`, `R_val`, `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`, etc.).

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Be mindful of the range for `R_val` given its calculation.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by a piecewise linear function of the ratio between proposed and fair shares, with separate sensitivity parameters (`beta_gain_ratio`, `beta_loss_ratio`) for ratios at or above 1 and ratios below 1. Explain how this piecewise structure on the ratio aims to improve parameter identifiability compared to previous additive or linear models.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This piecewise ratio approach is another significant departure from previous attempts and might offer the necessary structural separation. If this structure also fails parameter recovery for either parameter, be prepared to think even further outside the box for the next iteration – consider entirely different inputs, different ways of combining inputs (e.g., multiplicative interactions, exponential terms), or conceptualizing fairness's role differently (e.g., as a non-linear modulator of sensitivity, or a threshold in a non-standard way). The key is finding a structure where the two parameters' contributions to utility are as independent as possible across the dataset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6975694176645031,0.9776755111946818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46,40.45120575355156,37.720684333865314,42.89729785868716,0.6617826617826618,"sum_tokens = token_self + token_opp
epsilon_val = 1e-6
fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)
R_val = (split_perc_self + epsilon_val) / (fair_perc_self_calc + epsilon_val)
U = IF(R_val >= 1.0, beta_gain_log_sens * log(R_val), beta_loss_log_sens * log(R_val))","A utility model for responder decisions based on a piecewise logarithmic function of the ratio between the proposed share and the fair share. Utility is calculated as the logarithm of this ratio scaled by one of two learnable parameters: 'beta_gain_log_sens' when the ratio is 1 or greater (fair or favorable offers), and 'beta_loss_log_sens' when the ratio is less than 1 (unfair offers). This structure aims to capture different sensitivities to advantageous versus disadvantageous offers while maintaining a consistent logarithmic functional form.",v0,"Okay, here are the improved instructions for the next run (Run 46), building on the results of the previous run.

Instructions for Run 46 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 45) explored a piecewise *log-linear* model based on the ratio of proposed share to fair share. Specifically, it used `U = beta_gain_log_sens * log(R_val)` for ratios >= 1 and `U = beta_loss_linear_sens * R_val` for ratios < 1. This model achieved parameter recovery values of 0.711 for `beta_gain_log_sens` and 0.964 for `beta_loss_linear_sens`. This is a significant success, as it is the first model that meets the parameter recovery threshold (>= 0.7) for *both* learnable parameters simultaneously, making it the current best *recoverable* model. Its BIC (41.09) is good, though not the absolute lowest seen so far, and its accuracy (0.680) is moderate.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`). The piecewise structure based on the ratio `R_val` and the split at 1.0 appears promising for parameter identifiability, as demonstrated by the success of the log-linear model in the previous run. The difference in functional form (log vs linear) across the piecewise split seemed to help separate parameter influences.

For this run, let's continue to explore the *piecewise ratio* concept and the split at `R_val = 1.0`, maintaining exactly **2 learnable parameters**, but try applying the *same functional form* (logarithm) to the ratio in *both* regions. This creates a piecewise *log-log* model structure based on the ratio.

Design a computational model that predicts the utility of accepting an offer based on a **piecewise log-log function of the ratio** between the proposed percentage share and the participant's fair percentage share.

*   **Structural Direction (Piecewise Log-Log Ratio):** Model utility (`U`) based on the logarithm of the ratio of the proposed share to the fair share, using different scaling parameters depending on whether the ratio is at least 1 (fair or generous) or less than 1 (unfair).
    *   Calculate the necessary intermediate variables as before:
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensures `fair_perc_self_calc` is >= 0).
        *   `epsilon_val = 1e-6` (Small constant for numerical stability).
        *   `R_val = (split_perc_self + epsilon_val) / (fair_perc_self_calc + epsilon_val)` (Calculate the ratio, ensuring `R_val` is always positive).
    *   Structure the utility (`U`) using a conditional statement based on the value of `R_val`:\n
        *   If `R_val >= 1.0`, `U = beta_gain_log_sens * log(R_val)` (Logarithmic scaling for gain/favorable offers, same as previous run).
        *   If `R_val < 1.0`, `U = beta_loss_log_sens * log(R_val)` (Logarithmic scaling for loss/unfair offers, *different* from previous run). Note that `log(R_val)` will be negative for `R_val < 1`.
    *   This structure uses two distinct learnable parameters (`beta_gain_log_sens` and `beta_loss_log_sens`) that scale the logarithm of the ratio depending on whether the offer is favorable or unfavorable. This modification from the previous linear scaling in the loss region (`R_val < 1`) is an attempt to see if a consistent logarithmic functional form across the piecewise split can maintain or improve parameter identifiability and potentially improve fit (BIC/Accuracy).

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, and `U`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.

*   **Model Formula:** Define the utility (`U`) based on the conditional logic applied to `R_val` and your 2 learnable parameters. Include the calculation steps for necessary derived variables (`sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`) before the final formula for `U`. Use clear mathematical notation (IF/ELSE or similar conditional structure) and the standard mathematical logarithm function `log()`.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   `beta_gain_log_sens`: Sensitivity parameter scaling the *log* of the ratio when it is greater than or equal to 1 (fair or favorable offers). Suggest generous bounds like [-10.0, 10.0].
    *   `beta_loss_log_sens`: Sensitivity parameter scaling the *log* of the ratio when it is less than 1 (unfair offers). Suggest generous bounds like [-10.0, 10.0].
    Choose descriptive names suitable for Python code that reflect the functional form they scale (log) and the ratio range (gain vs loss). Define clear, generous, finite numerical bounds for both.

*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`). Be mindful of the range for `R_val` given its calculation (it will be >= `epsilon_val` / (100 + `epsilon_val`)).

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by a piecewise function of the ratio between proposed and fair shares, with a *logarithmic* relationship scaled by `beta_gain_log_sens` for ratios at or above 1, and a *logarithmic* relationship scaled by `beta_loss_log_sens` for ratios below 1. Explain how this consistent logarithmic piecewise structure on the ratio is the next attempt to improve fit (BIC/Accuracy) while maintaining the parameter identifiability achieved by the previous piecewise log-linear ratio model.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This piecewise log-log ratio approach is a direct evolution of the successful log-linear model. If this structure also meets recovery but doesn't improve BIC/Accuracy, or if it fails recovery for either parameter, be prepared to think even further outside the box for the next iteration – consider entirely different inputs, different ways of combining inputs (e.g., non-ratio based interactions, exponential terms), or conceptualizing fairness's role differently (e.g., as a non-linear modulator of sensitivity, or a threshold in a non-standard way). The key is finding a structure where the two parameters’ contributions to utility are as independent as possible across the dataset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7950317653605782,,0.9616432608101623,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
47,40.534449251706036,38.01918652621858,42.78770544328854,0.6715506715506716,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)
epsilon_val = 1e-6
R_val = (split_perc_self + epsilon_val) / (fair_perc_self_calc + epsilon_val)
R_diff = R_val - 1.0
U = IF(R_diff >= 0.0, beta_gain_linear_diff * R_diff, beta_loss_linear_diff * R_diff)","The model predicts the utility of accepting an offer based on a piecewise linear function of the difference between the proposed percentage share ratio (relative to the calculated fair share based on token contributions) and 1.0 (the fair reference point). Specifically, the utility is calculated as beta_gain_linear_diff multiplied by this difference if the proposed ratio is greater than or equal to 1.0 (representing a gain or fair scenario), and as beta_loss_linear_diff multiplied by this difference if the proposed ratio is less than 1.0 (representing a loss or unfair scenario). This structure uses two learnable parameters, one for gains and one for losses relative to fairness, attempting to capture differential sensitivity to favorable versus unfavorable deviations.",v0,"Okay, here are the improved instructions for the next run (Run 47), building on the results of the previous runs.

Instructions for Run 47 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

One of the recently tested models, the piecewise log-log ratio model (where utility was `beta_gain_log_sens * log(R_val)` for `R_val >= 1` and `beta_loss_log_sens * log(R_val)` for `R_val < 1`), successfully achieved parameter recovery values above the 0.7 threshold for *both* of its learnable parameters (`beta_gain_log_sens: r = 0.795`, `beta_loss_log_sens: r = 0.962`). This makes it the current best *recoverable* model found so far. Its BIC (40.45) and accuracy (0.662) are acceptable but indicate room for improvement.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`). The piecewise structure, particularly splitting based on the ratio `R_val` being >= or < 1.0, appears key to the parameter identifiability success seen in recent models.

For this run, let's continue to explore the *piecewise structure* based on the ratio `R_val` split at 1.0, maintaining exactly **2 learnable parameters**, but try a new functional form: **piecewise linear based on the *difference* from the fair ratio (i.e., `R_val - 1.0`)**. This approach aligns with behavioral economic concepts of deviations from a reference point (fairness) and may offer a better fit to the data compared to the log function, while preserving parameter identifiability through the piecewise split and scaling of deviations.

Design a computational model that predicts the utility of accepting an offer based on a **piecewise linear function of the *difference* between the ratio of the proposed percentage share to the participant's fair percentage share, and 1.0**.

*   **Structural Direction (Piecewise Linear Difference):** Model utility (`U`) based on the difference `R_val - 1.0`, using different linear scaling parameters depending on whether this difference is non-negative (fair or generous offers) or negative (unfair offers).
    *   Calculate the necessary intermediate variables as before:
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensures `fair_perc_self_calc` is >= 0).
        *   `epsilon_val = 1e-6` (Small constant for numerical stability).
        *   `R_val = (split_perc_self + epsilon_val) / (fair_perc_self_calc + epsilon_val)` (Calculate the ratio, ensuring `R_val` is always positive).
    *   Calculate the difference from the fair ratio: `R_diff = R_val - 1.0`.
    *   Structure the utility (`U`) using a conditional statement based on the value of `R_diff` (or equivalently, `R_val`):\n
        *   If `R_diff >= 0.0` (i.e., `R_val >= 1.0`), `U = beta_gain_linear_diff * R_diff` (Linear scaling for gain/favorable offers).
        *   If `R_diff < 0.0` (i.e., `R_val < 1.0`), `U = beta_loss_linear_diff * R_diff` (Linear scaling for loss/unfair offers). Note that `R_diff` will be negative for `R_val < 1`.
    *   This structure uses two distinct learnable parameters (`beta_gain_linear_diff` and `beta_loss_linear_diff`) that scale the positive or negative deviation from the fair ratio (1.0). This modification explores whether a linear relationship around the reference point provides a better fit and if scaling the difference (which is positive for gains and negative for losses) further aids parameter identifiability compared to scaling the positive ratio itself (as in the previous piecewise linear ratio model).

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, `R_diff`, and `U`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.

*   **Model Formula:** Define the utility (`U`) based on the conditional logic applied to `R_val` (or `R_diff`) and your 2 learnable parameters. Include the calculation steps for necessary derived variables (`sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, `R_diff`) before the final formula for `U`. Use clear mathematical notation (IF/ELSE or similar conditional structure).

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   `beta_gain_linear_diff`: Sensitivity parameter scaling the *difference* (`R_val - 1.0`) when the ratio is greater than or equal to 1 (fair or favorable offers). Suggest generous bounds like [-10.0, 10.0].
    *   `beta_loss_linear_diff`: Sensitivity parameter scaling the *difference* (`R_val - 1.0`) when the ratio is less than 1 (unfair offers). Suggest generous bounds like [-10.0, 10.0].
    Choose descriptive names suitable for Python code that reflect the functional form (linear) and the value they scale (the difference) and the ratio range (gain vs loss). Define clear, generous, finite numerical bounds for both.

*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, `R_diff`, `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`). Be mindful of the range for `R_diff` given its calculation (it can be negative).

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by a piecewise linear function of the *difference* between the proposed ratio and 1.0 (the fair reference point), scaled by `beta_gain_linear_diff` for differences >= 0 (gain/favorable offers) and by `beta_loss_linear_diff` for differences < 0 (loss/unfair offers). Explain how this piecewise linear structure on the difference from the reference point is the next attempt to improve fit (BIC/Accuracy) while building on the success of the piecewise approach for parameter identifiability.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This piecewise linear difference approach is a promising alternative to the piecewise ratio models, potentially capturing deviations from fairness in a more direct way. If this structure also meets recovery but doesn't significantly improve BIC/Accuracy, or if it fails recovery for either parameter, be prepared to think even further outside the box for the next iteration – consider entirely different inputs, different ways of combining inputs (e.g., non-ratio based interactions, exponential terms, incorporating absolute values or amounts), or conceptualizing fairness\'s role differently (e.g., as a non-linear modulator of sensitivity, or a threshold in a non-standard way). The key is finding a structure where the two parameters’ contributions to utility are as independent as possible across the dataset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7269760705863475,0.9659907491127714,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48,40.60060666209781,38.08552090978607,42.85370431521041,0.6874236874236874,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)
epsilon_val = 1e-6
R_val = (split_perc_self + epsilon_val) / (fair_perc_self_calc + epsilon_val)
R_diff = R_val - 1.0
U = IF(R_val >= 1.0, beta_gain_log * log(R_val), beta_loss_linear_diff * R_diff)","The model predicts the utility of accepting an offer for the responder using a piecewise function based on the ratio of the proposed percentage share to the participant's calculated fair percentage share (R_val). For offers where R_val is greater than or equal to 1.0 (fair or favorable), utility is determined by the logarithm of R_val scaled by the learnable parameter beta_gain_log. For offers where R_val is less than 1.0 (unfair), utility is determined by the linear difference (R_val - 1.0) scaled by the learnable parameter beta_loss_linear_diff. This piecewise hybrid structure aims to capture distinct sensitivities to gains and losses relative to the fairness reference point using exactly two identifiable learnable parameters.",v0,"Okay, here are the improved instructions for the next run (Run 48), building on the results of the previous runs.

Instructions for Run 48 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 47) successfully achieved parameter recovery above the 0.7 threshold for *both* of its learnable parameters (`beta_gain_linear_diff: r = 0.727`, `beta_loss_linear_diff: r = 0.966`), using a piecewise linear function of the *difference* from the fair ratio (`R_val - 1.0`). This reinforces that the piecewise structure, splitting based on the fair ratio (`R_val` vs 1.0), seems crucial for parameter identifiability with exactly two learnable parameters. However, the BIC (40.53) and accuracy (0.672) did not significantly improve compared to the current best *recoverable* model (the piecewise log-log ratio model, BIC 40.45, accuracy 0.662). This suggests the linear functional form applied to the difference might not provide the best fit to the data.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`) while also improving model fit (lower BIC, higher accuracy).

For this run, let's continue to leverage the successful *piecewise structure* based on the ratio `R_val` split at 1.0, maintaining exactly **2 learnable parameters**. We will explore a *hybrid* functional form, combining elements from previously successful models: **use a logarithmic function for the gain side (`R_val >= 1.0`) and a linear function of the difference for the loss side (`R_val < 1.0`)**. This attempts to combine the fit characteristics of the logarithmic form seen in the best recoverable model for favorable offers with the linear difference form that showed good recovery in the last run for unfavorable offers.

Design a computational model that predicts the utility of accepting an offer based on a **piecewise function of the ratio of the proposed percentage share to the participant's fair percentage share (`R_val`)**. For offers where `R_val >= 1.0`, utility should be a logarithmic function of `R_val` scaled by one learnable parameter. For offers where `R_val < 1.0`, utility should be a linear function of the *difference* (`R_val - 1.0`) scaled by a second learnable parameter.

*   **Structural Direction (Piecewise Hybrid Function):** Model utility (`U`) based on `R_val`, using different functional forms and scaling parameters depending on whether `R_val` is non-negative (fair or generous offers) or negative (unfair offers).
    *   Calculate the necessary intermediate variables as before:
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensures `fair_perc_self_calc` is >= 0).
        *   `epsilon_val = 1e-6` (Small constant for numerical stability).
        *   `R_val = (split_perc_self + epsilon_val) / (fair_perc_self_calc + epsilon_val)` (Calculate the ratio, ensuring `R_val` is always positive).
        *   `R_diff = R_val - 1.0` (Calculate the difference from the fair ratio).
    *   Structure the utility (`U`) using a conditional statement based on the value of `R_val`:
        *   If `R_val >= 1.0`: `U = beta_gain_log * log(R_val)` (Logarithmic scaling for gain/favorable offers, using `log(R_val)` where `R_val >= 1`).
        *   If `R_val < 1.0`: `U = beta_loss_linear_diff * R_diff` (Linear scaling for loss/unfair offers, using the *difference* `R_diff = R_val - 1.0`, which will be negative here).
    *   This structure uses two distinct learnable parameters (`beta_gain_log` and `beta_loss_linear_diff`) that scale the outcome of different functional forms applied based on whether the offer ratio is a gain or a loss relative to the fair point. This is a direct attempt to combine the piecewise identifiability success with potentially better functional forms.

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, `R_diff`, and `U`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.

*   **Model Formula:** Define the utility (`U`) based on the conditional logic applied to `R_val` and your 2 learnable parameters. Include the calculation steps for necessary derived variables (`sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, `R_diff`) before the final formula for `U`. Use clear mathematical notation (IF/ELSE or similar conditional structure), making sure to use `log()` for the logarithm.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   `beta_gain_log`: Sensitivity parameter scaling `log(R_val)` when `R_val >= 1.0` (fair or favorable offers). Suggest generous bounds like [-10.0, 10.0].
    *   `beta_loss_linear_diff`: Sensitivity parameter scaling the *difference* (`R_val - 1.0`) when `R_val < 1.0` (unfair offers). Suggest generous bounds like [-10.0, 10.0].
    Choose descriptive names suitable for Python code that reflect the functional form (log vs linear) and the value they scale (ratio vs difference) and the ratio range (gain vs loss). Define clear, generous, finite numerical bounds for both.

*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, `R_diff`, `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`). Be mindful of the range for `R_diff` given its calculation (it can be negative).

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by a piecewise function based on the ratio (`R_val`), using `log(R_val)` scaled by `beta_gain_log` for gain/favorable offers (`R_val >= 1.0`) and the linear difference (`R_val - 1.0`) scaled by `beta_loss_linear_diff` for loss/unfair offers (`R_val < 1.0`). Explain how this hybrid piecewise structure is designed to improve fit (BIC/Accuracy) by incorporating the log function for gains, while building on the successful piecewise structure and linear difference form for losses to maintain parameter identifiability.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This piecewise hybrid approach is a direct attempt to combine successful elements from previous models. If this structure also meets recovery but doesn't significantly improve BIC/Accuracy, or if it fails recovery for either parameter, be prepared to think even further outside the box for the next iteration – consider alternative conceptualizations of fairness's influence (e.g., is it just a reference point, or does it modulate sensitivity in a non-linear way?), different ways of incorporating the absolute amounts or tokens themselves rather than just percentages, or exploring fundamentally different model structures (e.g., additive utility components instead of just scaled ratios/differences). The key is finding a structure where the two parameters’ contributions to utility are as independent as possible across the dataset, while also capturing the behavioral patterns better than previous attempts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.9703758582916148,0.6988921612051353,,,,,,,,,,,,,,,,,,,,,,,,,,,
45,41.0893120716175,38.01468993149763,43.84366107214155,0.6804029304029304,"sum_tokens = token_self + token_opp
epsilon_val = 1e-6
fair_perc_self_calc = IF sum_tokens > 0 THEN (token_self / sum_tokens) * 100.0 ELSE 50.0 END IF
R_val = (split_perc_self + epsilon_val) / (fair_perc_self_calc + epsilon_val)
U = IF R_val >= 1.0 THEN beta_gain_log_sens * log(R_val) ELSE beta_loss_linear_sens * R_val END IF","A computational model predicting responder acceptance utility based on the ratio of the proposed share to the participant's fair share derived from token contributions. Utility is calculated using a piecewise function of this ratio: for ratios at or above 1, utility is the logarithm of the ratio scaled by a learnable parameter (beta_gain_log_sens); for ratios below 1, utility is the linear ratio scaled by a separate learnable parameter (beta_loss_linear_sens). This log-linear piecewise structure aims to improve parameter identifiability compared to previous linear models by allowing distinct sensitivities to favorable (gain) and unfavorable (loss) deviations from the fair point, mediated through different functional forms.",v0,"Okay, here are the improved instructions for the next run (Run 45), building on the results of the previous run.

Instructions for Run 45 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery.

The previous run explored a piecewise linear model based on the ratio of proposed share to fair share. It used `U = beta_gain_ratio * R_val` for ratios >= 1 and `U = beta_loss_ratio * R_val` for ratios < 1. This structure yielded good BIC (38.67) and moderate accuracy (0.711). Parameter recovery was mixed: `beta_loss_ratio` recovered well (r = 0.978), but `beta_gain_ratio` recovered just below the threshold (r = 0.698). This suggests the piecewise *linear* application of the ratio was successful in decoupling the ""loss"" sensitivity, but not quite the ""gain"" sensitivity, indicating the functional form (`beta * R_val`) might still not be optimal for the favorable range (`R_val >= 1`). The influence of the parameter scaling the linear ratio in the gain region might not be distinct enough from the baseline or other factors in the data.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`). Linear combinations, piecewise deviations, linear ratio models, and additive logarithmic models have all shown issues with simultaneous parameter recovery for *both* parameters. The previous piecewise *linear* ratio model got close but didn't pass the threshold for one parameter.

For this run, let's maintain the *piecewise ratio* concept and the split at `R_val = 1.0`, but differentiate the *functional form* applied to the ratio in the two regions. Let's keep the successful linear scaling for the unfavorable range (`R_val < 1.0`) and introduce a *logarithmic* scaling for the favorable range (`R_val >= 1.0`). This creates a piecewise *log-linear* model structure.

Design a computational model that predicts the utility of accepting an offer based on a **piecewise log-linear function of the ratio** between the proposed percentage share and the participant's fair percentage share.

*   **Structural Direction (Piecewise Log-Linear Ratio):** Model utility (`U`) based on the ratio of the proposed share to the fair share, using different functional forms and scaling parameters depending on whether the ratio is at least 1 (fair or generous) or less than 1 (unfair).
    *   Calculate the necessary intermediate variables as before:
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensures `fair_perc_self_calc` is >= 0).
        *   `epsilon_val = 1e-6` (Small constant for numerical stability).
        *   `R_val = (split_perc_self + epsilon_val) / (fair_perc_self_calc + epsilon_val)` (Calculate the ratio, ensuring `R_val` is always positive).
    *   Structure the utility (`U`) using a conditional statement based on the value of `R_val`:
        *   If `R_val >= 1.0`, `U = beta_gain_log_sens * log(R_val)` (Logarithmic scaling for gain/favorable offers). Note that `log(1.0)` is 0, so utility is 0 at the exact fair point on the gain side.
        *   If `R_val < 1.0`, `U = beta_loss_linear_sens * R_val` (Linear scaling for loss/unfavorable offers), preserving the structure that showed good recovery in the previous run for this range.
    *   This structure uses two distinct learnable parameters (`beta_gain_log_sens` and `beta_loss_linear_sens`) that scale different functional forms of the ratio depending on whether the offer is favorable or unfavorable. This change in functional form for the gain region is a direct attempt to improve the identifiability of the ""gain"" sensitivity parameter compared to the previous linear approach.

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, and `U`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.

*   **Model Formula:** Define the utility (`U`) based on the conditional logic applied to `R_val` and your 2 learnable parameters. Include the calculation steps for necessary derived variables (`sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`) before the final formula for `U`. Use clear mathematical notation (IF/ELSE or similar conditional structure) and the standard mathematical logarithm function `log()`.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   `beta_gain_log_sens`: Sensitivity parameter scaling the *log* of the ratio when it is greater than or equal to 1 (fair or favorable offers). Suggest generous bounds like [-10.0, 10.0].
    *   `beta_loss_linear_sens`: Sensitivity parameter scaling the *linear* ratio when it is less than 1 (unfair offers). Suggest generous bounds like [-10.0, 10.0].
    Choose descriptive names suitable for Python code that reflect the functional form they scale (log vs linear) and the ratio range (gain vs loss). Define clear, generous, finite numerical bounds for both.

*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self_calc`, `epsilon_val`, `R_val`, `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`). Be mindful of the range for `R_val` given its calculation (it will be >= `epsilon_val` / (100 + `epsilon_val`)).

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by a piecewise function of the ratio between proposed and fair shares, with a *logarithmic* relationship scaled by `beta_gain_log_sens` for ratios at or above 1, and a *linear* relationship scaled by `beta_loss_linear_sens` for ratios below 1. Explain how this combined log-linear piecewise structure on the ratio is a new attempt to improve parameter identifiability, particularly for the gain sensitivity, compared to previous strictly linear or additive models.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This piecewise log-linear ratio approach is another significant departure from previous attempts to address the identifiability challenge. If this structure also fails parameter recovery for either parameter, be prepared to think even further outside the box for the next iteration – consider entirely different inputs, different ways of combining inputs (e.g., multiplicative interactions, exponential terms), or conceptualizing fairness's role differently (e.g., as a non-linear modulator of sensitivity, or a threshold in a non-standard way). The key is finding a structure where the two parameters’ contributions to utility are as independent as possible across the dataset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7113196458160447,0.96376786036581,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
86,41.8789423164295,35.54953194902311,47.54903910389772,0.6666666666666666,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_value * split_perc_self * (1.0 / (1.0 + beta_unfairness * loss_below_fair))","A utility model for the responder's acceptance decision. The utility of accepting is calculated as a multiplicative combination of the proposed percentage share and a penalty factor based on perceived unfairness. The perceived unfairness is the degree to which the proposed share falls below a calculated fair share based on token contributions. The model uses two learnable parameters: 'beta_value' which scales the proposed share (gain term), and 'beta_unfairness' which scales the unfairness term within the denominator of the penalty factor. This multiplicative structure is specifically intended to improve robust parameter identifiability by giving the parameters mathematically separable influences on the utility function.",v0,"Okay, here are the improved instructions for the next run (Run 86 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,
      ""source"": ""calculated""
    },\\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,\\\
      ""source"": ""data""
    },\\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1,\
        ""exclusive_max"": true
      },\\\
      ""learnable"": false,
      ""source"": ""data""
    },\\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {\
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\\
      ""learnable"": false,
      ""source"": ""calculated""
    }\
  }\
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 86 of 100), based on the results of recent attempts.

The previous run (Run 85), which used an additive structure combining a power-law gain term and a linear loss term (`U_accept = split_perc_self^beta_value - beta_unfairness * loss_below_fair`), resulted in very poor parameter recovery (r=0.128 for `beta_value`, r=0.013 for `beta_unfairness`). This structure, despite achieving relatively high accuracy (0.741) and a decent BIC (360.19), failed critically on the primary objective: robust parameter identifiability.

Reviewing previous runs, Run 84, which used a multiplicative structure where scaled percentage gain was multiplied by a divisive fairness penalty factor (`U_accept = beta_value * split_perc_self * (1.0 / (1.0 + beta_unfairness * loss_below_fair))`), showed the *best parameter recovery results so far* (r=0.542 for `beta_value`, r=0.507 for `beta_unfairness`), although still below the 0.7 threshold. It also achieved the lowest BIC (42.99) among recent models. This suggests that this multiplicative/divisive structure is the most promising candidate discovered thus far for achieving parameter identifiability in this task, even if it hasn't fully succeeded yet.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation of individual differences and group effects, and overrides achieving a low BIC or high accuracy if recovery fails.

Given the relative success of the multiplicative structure from Run 84 compared to other attempts, we will revisit and implement this structure again for Run 86. The hypothesis is that the way `beta_value` scales the numerator (the gain term) and `beta_unfairness` scales a term in the denominator (the penalty factor) creates more distinct influence profiles on the overall utility function across trials than additive or power-law structures, making the parameters more independently identifiable.

Proposed Model Structure: The utility of accepting (`U_accept`) is calculated as a product of the participant's proposed percentage gain (`split_perc_self`, scaled by `beta_value`) and a penalty factor derived from the amount the offer is below the calculated fair share (`loss_below_fair`, scaled by `beta_unfairness`).

Here are the calculation steps:
1. Calculate `sum_tokens = token_self + token_opp`.
2. Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.
3. Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is 0 if the offer is fair or above fair, and positive if the offer is below fair, representing the degree of perceived loss or unfairness. Use `Max` for this calculation.
4. Calculate the utility using the multiplicative structure from Run 84: `U_accept = beta_value * split_perc_self * (1.0 / (1.0 + beta_unfairness * loss_below_fair))`. Use standard mathematical notation for multiplication and division.

In this structure, `beta_value` linearly scales the gain term derived from the proposed share. `beta_unfairness` scales the `loss_below_fair` term within the denominator of a penalty factor. This penalty factor decreases from 1 towards 0 as `loss_below_fair` increases. The utility is the scaled gain *multiplied* by this decreasing penalty factor. This differential influence – `beta_value` scaling the numerator (gain) and `beta_unfairness` affecting the denominator's decay rate (penalty) – is specifically intended to improve robust joint parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous models.

Design the model using this specific multiplicative/divisive structure: `U_accept = beta_value * split_perc_self * (1.0 / (1.0 + beta_unfairness * loss_below_fair))`. Remember to calculate intermediate variables (`sum_tokens`, `fair_perc_self_calc`, `loss_below_fair`) before the final `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula), using `IF` and `Max`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific multiplicative structure, where `beta_value` scales the base gain and `beta_unfairness` scales the penalty term within the denominator of a penalty factor, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous additive or simple divisive models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (sensitivity/scaling parameter for percentage gain) and `beta_unfairness` (sensitivity/aversion parameter affecting the penalty factor). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds.
    *   `beta_value`: Non-negative bounds, e.g., `[0.01, 10.0]`. Must be > 0 to allow gain influence.
    *   `beta_unfairness`: Non-negative bounds, e.g., `[0.0, 10.0]`. Must be >= 0 to represent aversion (or indifference).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is a scaled gain term multiplied by a penalty factor based on unfairness). Crucially, explain *how* this specific multiplicative structure, where `beta_value` scales the gain (numerator) and `beta_unfairness` affects the penalty factor's denominator, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous models.
*   **Think Outside the Box:** Parameter identifiability for *both* parameters simultaneously remains the persistent, difficult challenge. While Run 84's structure showed the most promise so far, it still didn't reach the 0.7 recovery threshold for both. This re-attempt aims to confirm and hopefully improve upon that. If this specific multiplicative structure *still* fails to achieve >= 0.7 recovery for *both* parameters in Run 86, the next iteration *must* consider even more fundamentally different structural approaches that move beyond standard gain/loss formulations. Think about models where parameters might influence decision noise, contribute additively in log-space, define aspiration levels, or influence the choice process in a more qualitative way than just scaling utility components. Focus intently on finding a functional form where varying `beta_value` has a clearly distinguishable effect on the utility curve across offers compared to varying `beta_unfairness`, allowing them to be estimated independently with high reliability. This is a prerequisite for better fit metrics and meaningful interpretation.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,0.33261600364486243,,,,,,0.45269064282991617,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84,42.98741527686191,35.581749496031975,49.62165753885539,0.6614774114774115,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_value * split_perc_self * (1.0 / (1.0 + beta_unfairness * loss_below_fair))","This model predicts responder behavior by calculating the utility of accepting a proposed offer. The utility is based on a multiplicative interaction between the participant's proposed percentage gain and a penalty factor related to perceived unfairness. The core utility is derived from the proposed percentage, scaled by 'beta_value'. This scaled gain is then multiplied by a penalty factor, which decreases as the amount the offer is below the calculated fair share ('loss_below_fair') increases. The 'loss_below_fair' is itself scaled by 'beta_unfairness'. This multiplicative structure, where the scaled gain is penalized by a factor based on scaled unfairness, is designed to improve robust parameter identifiability for both 'beta_value' and 'beta_unfairness' by creating mathematically distinct and separable influence profiles compared to previous additive or simple divisive models.",v0,"Okay, here are the improved instructions for the next run (Run 84 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,\\
      ""source"": ""data""
    },\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },\\
      ""learnable"": false,
      ""source"": ""data""
    },\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 84 of 100), based on the results of recent attempts.

The previous run (Run 83), which used an additive structure combining absolute percentage gain and relative fairness deviation (`U_accept = beta_value * split_perc_self + beta_unfairness * (relative_fairness - 1.0)`), resulted in extremely poor parameter recovery for *both* learnable parameters (`beta_value` r=-0.043, `beta_unfairness` r=0.048). This demonstrates that this specific structure failed completely to create separable influences for the parameters, leading to unreliable estimation. The BIC (595.90) and accuracy (0.615) were also poor, indicating a poor fit to the data.

This confirms the ongoing challenge: finding a model structure where the influences of *both* `beta_value` and `beta_unfairness` are mathematically distinct enough across trials to allow for robust parameter identifiability (parameter recovery >= 0.7 for *all* learnable parameters). Simply trying different linear or simple non-linear combinations of standard gain and penalty or absolute and relative terms has not achieved this crucial joint identifiability.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation of individual differences and group effects.

Given the repeated failures of previous structures, including the one in Run 83, we need to explore a fundamentally different way the parameters interact or influence the utility calculation. Instead of additive terms, let's consider a structure where the fairness penalty *modulates* the value derived from the proposed gain in a multiplicative or divisive manner, with each parameter scaling a different aspect of this interaction.

For Run 84, we will test a model structure where the utility derived from the proposed percentage gain (`split_perc_self`, scaled by `beta_value`) is penalized divisively by a term related to how far the offer is *below* the calculated fair share (`loss_below_fair`, scaled by `beta_unfairness`). This introduces a multiplicative interaction between the core gain term and the penalty term's influence, which might create more distinct parameter profiles than previous additive or simple divisive structures.

Proposed Model Structure: The utility of accepting (`U_accept`) is calculated as the product of the participant's proposed percentage (`split_perc_self`, scaled by `beta_value`) and a penalty factor based on the amount the offer is below the calculated fair share (`loss_below_fair`, scaled by `beta_unfairness`). The penalty factor will be `1.0 / (1.0 + beta_unfairness * loss_below_fair)`.

Here are the calculation steps:
1. Calculate `sum_tokens = token_self + token_opp`.
2. Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.
3. Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is 0 if the offer is fair or above fair, and positive if the offer is below fair, representing the degree of perceived loss or unfairness. Use `Max` for this calculation.
4. Calculate the utility: `U_accept = beta_value * split_perc_self * (1.0 / (1.0 + beta_unfairness * loss_below_fair))`.
In this structure, `beta_value` scales the initial potential utility from the offer amount. `beta_unfairness` scales the `loss_below_fair` term, and the term `(1.0 + beta_unfairness * loss_below_fair)` acts as a denominator for the penalty factor `(1.0 / ...)`, which then *multiplies* the scaled gain. This multiplicative relationship between the scaled gain (`beta_value * split_perc_self`) and the fairness penalty factor `(1.0 / (1.0 + beta_unfairness * loss_below_fair))` is hypothesized to generate mathematically distinct influence profiles for `beta_value` (overall sensitivity to scaled offer) and `beta_unfairness` (sensitivity to how unfairness penalizes that scaled offer), potentially improving robust joint parameter identifiability.

Design the model using this new multiplicative/divisive structure combining scaled absolute gain and a divisive fairness penalty factor: `U_accept = beta_value * split_perc_self * (1.0 / (1.0 + beta_unfairness * loss_below_fair))`. Remember to calculate intermediate variables (`sum_tokens`, `fair_perc_self_calc`, `loss_below_fair`) before the final `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula), using `IF` and `Max` as shown.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific multiplicative/divisive structure, where a gain term (scaled by `beta_value`) is multiplied by a penalty factor derived from `loss_below_fair` (scaled by `beta_unfairness`), is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (sensitivity to absolute percentage gain) and `beta_unfairness` (sensitivity to how loss below fair penalizes the scaled gain). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are appropriate. `beta_value` should be non-negative for increasing utility with percentage; `beta_unfairness` non-negative ensures the penalty factor decreases (or stays at 1) as `loss_below_fair` increases. Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is a scaled gain term multiplied by a penalty factor based on unfairness). Crucially, explain *how* this specific multiplicative/divisive structure, where the scaled gain is penalized by a factor based on `loss_below_fair` scaled by `beta_unfairness`, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous models.
*   **Think Outside the Box:** Parameter identifiability for *both* parameters simultaneously remains the persistent, difficult challenge. Neither the divisive structures nor the simple additive gain/penalty or absolute/relative variations tested have solved this. This new multiplicative/divisive structure is another attempt to find mathematically separable influences through a different type of parameter interaction. If this also fails to achieve >= 0.7 recovery for *both* parameters, the next iteration *must* consider even more fundamentally different structural approaches. Continue to focus intently on finding a functional form where varying `beta_value` has a clearly distinguishable effect on the utility curve across offers compared to varying `beta_unfairness`, allowing them to be estimated independently with high reliability, which is a prerequisite for better fit metrics. Consider models where parameters influence thresholding, decision-making rules, or alternative ways of conceptualizing and integrating fairness beyond standard additive or multiplicative structures. Think about how parameters could influence the exponent of a term, or scale terms that are then passed into a non-linear function together, or how parameters could determine which decision rule is applied.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,0.5074451268566921,,,,,,0.5417253093398626,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
76,43.01324391309081,35.58201795831797,49.67038383090815,0.6617826617826618,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (token_self / sum_tokens) * 100.0 if sum_tokens > 0 else 50.0
loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = (beta_value * split_perc_self) / (1.0 + beta_unfairness * loss_below_fair)","A divisive utility model predicting responder acceptance. The model calculates the utility of accepting an offer based on the participant's proposed percentage share (split_perc_self), scaled by a value sensitivity parameter (beta_value). This scaled value is then divided by a term that increases with the magnitude of unfairness (loss_below_fair, the amount the offer is below the calculated fair share), controlled by an unfairness sensitivity parameter (beta_unfairness). This divisive structure, where beta_unfairness influences the denominator's penalty, is designed to create distinct, non-linear interactions between the parameters, aiming to improve robust parameter identifiability compared to previous additive or simple multiplicative models.",v0,"Okay, here are the improved instructions for the next run (Run 76 of 100), based on the results of recent attempts.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. The model tested in the previous run (Run 75), the multiplicative structure `U_accept = split_perc_self * (beta_value - beta_unfairness * loss_below_fair)`, achieved a notably low BIC (57.07), suggesting it might capture some aspects of the data well. However, it catastrophically failed parameter recovery for *both* parameters (r = 0.100 and r = -0.043). This reinforces that achieving a good fit metric like BIC is insufficient if the underlying parameters cannot be reliably estimated. The previous multiplicative formulation, despite its promising BIC, resulted in highly confounded parameter influences.

Previous attempts across various additive and multiplicative structures have consistently struggled with parameter identifiability. This suggests that simply combining value and unfairness terms additively or with simple linear multiplication does not create sufficiently distinct influence profiles for the parameters across the range of possible offers in this task.

For Run 76, we will continue exploring non-additive structures, given the relatively good BIC performance of the previous multiplicative attempt. However, we need a structure where the parameters' influences are mathematically more separable to ensure robust identifiability. The failure of the previous multiplicative model suggests that multiplying the sensitivity factor directly by `split_perc_self` may have contributed to confounding, particularly for low offers.

Proposed Model Structure: A divisive utility model where the core value derived from the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, is *reduced* by a factor based on unfairness (`loss_below_fair`), controlled by `beta_unfairness`.

`U_accept = (beta_value * split_perc_self) / (1 + beta_unfairness * loss_below_fair)`

Here:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
The utility `U_accept` is calculated as the product of `beta_value` and `split_perc_self`, divided by `(1 + beta_unfairness * loss_below_fair)`.

In this structure, `beta_value` acts as a primary scaling factor for the 'value' component (`split_perc_self`), similar to previous models. `beta_unfairness`, however, specifically influences the *denominator* of the utility calculation. For offers that are fair or better (`loss_below_fair = 0`), the denominator is `(1 + 0) = 1`, and utility is simply `beta_value * split_perc_self`. For unfair offers (`loss_below_fair > 0`), the denominator increases proportionally to `loss_below_fair`, controlled by `beta_unfairness`, thus *reducing* the utility via division. This separation, where `beta_value` scales the numerator and `beta_unfairness` scales an additive term in the denominator, is hypothesized to create distinct and non-linearly interacting influence profiles for the parameters, potentially improving parameter identifiability compared to simple additive or the previous multiplicative forms.

Design the model using this divisive structure: `U_accept = (beta_value * split_perc_self) / (1 + beta_unfairness * loss_below_fair)`, where `loss_below_fair` and `fair_perc_self_calc` are calculated as specified above using `token_self`, `token_opp`, and `split_perc_self`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific divisive structure, by placing `beta_unfairness` in the denominator affecting the scaling of the entire `beta_value * split_perc_self` term, is intended to achieve robust parameter identifiability by creating mathematically distinct and interactive influence profiles, addressing the persistent identifiability failures seen in previous models. `beta_value` represents a base sensitivity to the offer percentage, and `beta_unfairness` represents how strongly increasing unfairness causes this sensitivity to be *divided down*.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (base value sensitivity) and `beta_unfairness` (unfairness divisor sensitivity). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and likely necessary for parameter recovery in this structure (e.g., `beta_unfairness` should be non-negative so the denominator increases with `loss_below_fair`, and `beta_value` should be non-negative for utility to increase with `split_perc_self` on fair offers). Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (divisive structure where utility from `split_perc_self` is reduced by unfairness). Crucially, explain *how* this specific divisive structure, where `beta_unfairness` influences the denominator, is specifically intended to improve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles for `beta_value` and `beta_unfairness`, addressing the limitations and failures of previous structures.
*   **Think Outside the Box:** Recognize that previous attempts with standard additive, squared additive, exponential additive, and linear multiplicative structures have consistently failed parameter recovery. The proposed divisive structure is another attempt at a non-additive interaction principle. Success in parameter recovery often requires exploring less standard structural principles. Continue to consider how parameters might interact via division, exponents, thresholds, or other non-linear functions that haven't been tried yet, focusing on creating distinctly shaped influences for each parameter across the range of possible offers.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.",,,,,,,,,,,,0.44510882559465353,,,,,,0.4361580135778061,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78,44.12257994306418,35.488063782655935,51.85766733676323,0.6663614163614163,"sum_tokens = token_self + token_opp
fair_perc_self_calc = If(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
utility_accept = (beta_value * split_perc_self) / (1.0 + beta_unfairness * (loss_below_fair)^2)","A divisive utility model predicting the likelihood of accepting an offer in the Ultimatum Game. Utility is calculated based on the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, and then divided by a penalty term. This penalty term increases quadratically with `loss_below_fair`, representing the difference between the fair share (based on token contributions) and the proposed share, scaled by `beta_unfairness`. The quadratic functional form in the denominator is specifically hypothesized to improve parameter identifiability, particularly for `beta_value`, by creating mathematically distinct and interacting influence profiles for the two learnable parameters across different offer types, facilitating more reliable parameter estimation compared to previous linear or exponential penalty structures.",v0,"Okay, here are the improved instructions for the next run (Run 78 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 78 of 100), based on the results of recent attempts.

The model tested in the previous run (Run 77), the exponential-divisive structure `U_accept = (beta_value * split_perc_self) / Exp(beta_unfairness * loss_below_fair)`, achieved reasonable fit metrics (BIC = 256.35, Accuracy = 0.612) and promising recovery for `beta_unfairness` (r = 0.710). This reinforces the idea that a divisive structure penalizing unfairness, calculated based on token contributions (`loss_below_fair`), is a valuable approach.

However, the **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. The previous model failed this critical benchmark for `beta_value` (r = -0.059). A model with poor parameter recovery is unusable, as the estimated parameters cannot be reliably interpreted.

This indicates that while the divisive principle and the fairness calculation are useful, the *specific exponential functional form* used in the denominator `Exp(beta_unfairness * loss_below_fair)` did not create sufficiently separable influence profiles, particularly failing to allow for robust identifiability of `beta_value`.

For Run 78, we will continue to build on the divisive structure but will explore an *alternative non-linear functional form* for the unfairness penalty in the denominator. Instead of a linear term or an exponential term, we will use a **quadratic term** based on `loss_below_fair`.

Proposed Model Structure: A divisive utility model where the core value derived from the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, is *reduced* by a factor based on the *squared* unfairness (`loss_below_fair` squared), controlled by `beta_unfairness`, in the denominator.

`U_accept = (beta_value * split_perc_self) / (1.0 + beta_unfairness * (loss_below_fair)^2)`

Here:
Calculate `sum_tokens = token_self + token_opp`.
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
The utility `U_accept` is calculated as the product of `beta_value` and `split_perc_self`, divided by the term `(1.0 + beta_unfairness * (loss_below_fair)^2)`.

In this structure, `beta_value` scales the numerator representing the base value from the offer percentage. `beta_unfairness` now influences the *rate* at which the squared unfairness penalizes utility via division. Adding `1.0` in the denominator ensures the denominator is never zero or negative (assuming `beta_unfairness` and `loss_below_fair` are non-negative) and that when `loss_below_fair` is zero (fair offer), the denominator is 1. For unfair offers (`loss_below_fair > 0`), the denominator `(1.0 + beta_unfairness * (loss_below_fair)^2)` increases quadratically with increasing `loss_below_fair`. This quadratic interaction, where `beta_value` scales linearly and `beta_unfairness` scales a quadratic penalty term, is hypothesized to create mathematically distinct and more separable influence profiles for the parameters across the range of possible offers compared to previous linear or exponential forms, thereby improving parameter identifiability, particularly for `beta_value`. `beta_value` represents the base sensitivity to the offer, and `beta_unfairness` represents how strongly increasing unfairness causes this sensitivity to be *quadratically divided down*.

Design the model using this new quadratic-divisive structure: `U_accept = (beta_value * split_perc_self) / (1.0 + beta_unfairness * (loss_below_fair)^2)`, where `loss_below_fair` and `fair_perc_self_calc` are calculated as specified above using `token_self`, `token_opp`, and `split_perc_self`. Remember to calculate intermediate variables before the final `U_accept`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific quadratic-divisive structure, by placing `beta_unfairness` as a multiplier for the *squared* `loss_below_fair` term in the denominator, is intended to achieve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles that are more separable than those generated by the previous linear or exponential denominator structures, specifically aiming to improve the recovery of `beta_value`.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Use standard operations like `*` for multiplication and `^` or `**` for power (or describe as ""squared"").
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (base value sensitivity) and `beta_unfairness` (unfairness quadratic penalty rate). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and likely necessary for parameter recovery in this structure (e.g., `beta_unfairness` should be non-negative so the denominator increases with `loss_below_fair`, and `beta_value` should be non-negative for utility to increase with `split_perc_self` on fair offers). Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (divisive structure where utility from `split_perc_self` is reduced by a *quadratic* penalty based on unfairness). Crucially, explain *how* this specific quadratic-divisive structure, where `beta_unfairness` influences the *rate* of quadratic increase in the denominator, is specifically intended to improve robust parameter identifiability, especially for `beta_value`, by creating mathematically distinct and *interacting* influence profiles for `beta_value` and `beta_unfairness`, addressing the recovery failures seen in previous models.
*   **Think Outside the Box:** Parameter identifiability is the most difficult challenge. Recognizing that previous attempts with linear and exponential penalties failed to recover `beta_value` suggests that the *form* of the penalty interaction is critical. The quadratic form is a hypothesis; if it doesn't work, continue considering how parameters might interact via other non-linear functions (logs, other powers, inverse functions) that haven't been tried yet, focusing intently on creating distinctly shaped influences for each parameter across the range of possible offers to improve their reliable estimation. The goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.",,,,,,,,,,,,0.5478856092564259,,,,,,0.4602601565270929,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
80,44.30736632217009,37.0972489040752,50.766429842546756,0.67002442002442,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
penalty_term = beta_unfairness * Log(1.0 + loss_below_fair)
U_accept = (beta_value * split_perc_self) / (1.0 + penalty_term)","A divisive utility model for responder behavior. The utility of accepting is based on the proposed percentage share for the participant, scaled by beta_value, which is then divided by a term reflecting the offer's unfairness plus one. The unfairness penalty in the denominator is calculated from the extent to which the offer is below a fair share (derived from token contributions), transformed by the natural logarithm of (1 + loss_below_fair), and scaled by beta_unfairness. This specific structure, utilizing a logarithmic penalty scaled by beta_unfairness in the denominator, aims to create mathematically distinct and interacting influence profiles for beta_value and beta_unfairness across different offers, thereby improving robust parameter identifiability compared to previous models.",v0,"Okay, here are the improved instructions for the next run (Run 80 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,\
      ""source"": ""data""
    },\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },\
      ""learnable"": false,
      ""source"": ""data""
    },\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 80 of 100), based on the results of recent attempts.

The model tested in the previous run (Run 79), the exponent-divisive structure `U_accept = (beta_value * split_perc_self) / (1.0 + (loss_below_fair)**beta_unfairness)`, unfortunately resulted in very poor parameter recovery for *both* `beta_value` (r = -0.046) and `beta_unfairness` (r = 0.268), and also performed worse on fit metrics (BIC = 64.26, Accuracy = 0.560) compared to the quadratic divisive model (Run 78, BIC = 44.12, Acc = 0.666).

This confirms that the exponent structure for the penalty term did not achieve the crucial objective of robustly identifying *both* parameters simultaneously, despite our hypothesis that it would create distinct influence profiles. A model with poor parameter recovery is unusable, regardless of its fit metrics.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. We must find a model structure that allows for reliable estimation of *both* `beta_value` and `beta_unfairness`.

For Run 80, we will continue to explore the divisive structure and the use of `loss_below_fair` based on token contributions, as these components seem relevant to the task and contributed to better fit in some previous attempts. However, we need a *different functional form* for the unfairness penalty in the denominator that can hopefully create more mathematically distinct and separable influences for *both* parameters.

Proposed Model Structure: A divisive utility model where the core value derived from the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, is *reduced* by a factor based on a *logarithmic* transformation of `loss_below_fair`, scaled by `beta_unfairness`, in the denominator.

`U_accept = (beta_value * split_perc_self) / (1.0 + penalty_term)`

Here:
Calculate `sum_tokens = token_self + token_opp`.
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
Calculate the penalty term `penalty_term = beta_unfairness * Log(1.0 + loss_below_fair)`. The natural logarithm `Log()` is used here. Note that `Log(1.0 + loss_below_fair)` is 0 when `loss_below_fair` is 0, so no specific conditional handling is needed for the penalty term itself being 0; it's implicitly handled by the log function's properties.
The utility `U_accept` is calculated as the product of `beta_value` and `split_perc_self`, divided by the term `(1.0 + penalty_term)`.

In this structure, `beta_value` scales the numerator representing the base value from the offer percentage. `beta_unfairness` scales a penalty term based on the natural logarithm of `1 + loss_below_fair`. Using a logarithmic function for the penalty means that increases in `loss_below_fair` lead to increases in the penalty, but with *diminishing returns*. This specific non-linear relationship is hypothesized to create a pattern of influence for `beta_unfairness` across different levels of unfairness that is mathematically distinct from the linear scaling effect of `beta_value` on the offer percentage. This difference in influence profiles might improve robust parameter identifiability for *both* `beta_value` and `beta_unfairness` simultaneously, addressing the recovery failures seen in previous models. `beta_value` represents the base sensitivity to the offer percentage, and `beta_unfairness` represents the sensitivity to the log-transformed unfairness penalty.

Design the model using this new log-penalty divisive structure: `U_accept = (beta_value * split_perc_self) / (1.0 + penalty_term)`, where `loss_below_fair` and `fair_perc_self_calc` are calculated as specified above using `token_self`, `token_opp`, and `split_perc_self`, and the `penalty_term` uses the natural logarithm of `1.0 + loss_below_fair` scaled by `beta_unfairness`. Remember to calculate intermediate variables before the final `U_accept`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific divisive structure, by placing `beta_unfairness` scaling a *logarithmic* penalty function of `loss_below_fair` in the denominator, is intended to achieve robust parameter identifiability for *both* parameters by creating mathematically distinct and *interacting* influence profiles that are more separable than those generated by previous linear, exponential, quadratic, or exponent-based forms, addressing the recovery failures seen in previous models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, the intermediate `penalty_term` (calculated as `beta_unfairness * Log(1.0 + loss_below_fair)`), and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Use `Log()` for the natural logarithm and `Max()` for the maximum function.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (base value sensitivity) and `beta_unfairness` (unfairness penalty sensitivity on log scale). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate for interpretability and likely necessary for parameter recovery in this structure (e.g., `beta_unfairness` should be non-negative so the denominator increases with `loss_below_fair` for `loss_below_fair > 0`, and `beta_value` should be non-negative for utility to increase with `split_perc_self` on fair offers). Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, `loss_below_fair`, and `penalty_term`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (divisive structure where utility from `split_perc_self` is reduced by a penalty based on the log of `1 + loss_below_fair`). Crucially, explain *how* this specific structure, where `beta_unfairness` scales a *logarithmic* transformation of `loss_below_below_fair` in the denominator, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and *interacting* influence profiles that are more separable than those generated by previous linear, exponential, quadratic, or exponent-based forms, addressing the recovery failures seen in previous models.
*   **Think Outside the Box:** Parameter identifiability for *both* parameters simultaneously is the persistent challenge. Recognize that various non-linear functions applied to `loss_below_fair` in the denominator, scaled linearly by `beta_unfairness` (like linear, quadratic, log) or used with `beta_unfairness` in the exponent, have not yet yielded satisfactory joint recovery. If this logarithmic structure also fails to achieve >= 0.7 recovery for *both* parameters, the next iteration must consider *fundamentally different* ways the two parameters could interact or represent psychological processes to create truly distinct and separable influences. Could `beta_value` and `beta_unfairness` influence *different aspects* of the offer evaluation (e.g., one affects baseline acceptance, the other penalty sensitivity)? Could the utility function structure be entirely different (e.g., comparing utility of accepting vs. rejecting, where parameters influence different parts of this comparison)? Continue to focus intently on how the functional form ensures that varying `beta_value` has a measurably different effect on the utility curve across offers than varying `beta_unfairness`, and that these effects are distinct enough to be estimated independently with high reliability.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.",,,,,,,,,,,,0.35172384291562253,,,,,,0.3541526696351523,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,44.33415635043557,41.78877042899865,46.61439790505614,0.7289377289377289,U_accept = (beta_zero + beta_amount * split_self) * (1 if split_perc_self >= fairness_threshold_perc else 0) + beta_utility_below_threshold * (1 if split_perc_self < fairness_threshold_perc else 0),"A piecewise utility model for responder acceptance decisions. The utility of accepting depends on whether the proposed percentage share (split_perc_self) is above or below a learned absolute percentage threshold (fairness_threshold_perc). If the offer percentage is at or above the threshold, utility is a linear function of the proposed amount (split_self), scaled by beta_amount, plus a base bias (beta_zero). If the offer percentage is below the threshold, utility is a fixed value defined by beta_utility_below_threshold. This structural break aims to improve parameter identifiability by separating the influences of amount sensitivity and rejection cost based on a learned fairness threshold.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions:
Previous attempts modeling responder behavior have consistently struggled with parameter recovery. The most recent model, which included a linear effect of the received amount and a non-linear power-law penalty for negative percentage deviations from fairness, achieved a good BIC but *still* failed significantly on parameter recovery (all recovery values < 0.7). This indicates that the chosen structure, even with a non-linear term, did not solve the fundamental identifiability issues in the data for this domain. Parameters related to the penalty term in particular had extremely low recovery.

For this run (28 of 100), the **absolute highest priority remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. A model is unusable if its parameters cannot be reliably estimated, regardless of its fit (BIC) or predictive accuracy on observed data. You must design a model structure where each learnable parameter has a distinct and separable influence on the predicted utility across the dataset's trials.

Given the repeated failure of structural approaches based on combining linear amount effects and various forms of fairness penalties (linear, quadratic, power-law) calculated from percentage deviation, it is evident that a fundamentally *different* modeling structure is required to improve parameter identifiability in this dataset. Sticking to variations of `split_self +/- f(fairness_deviation)` appears insufficient.

**Guidance for the Next Model (Focusing on Identifiability & Novelty):**

Design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer, prioritizing parameter identifiability through structural innovation. This requires moving significantly beyond the structures explored so far.

*   **Parameter Identifiability:** Focus intensely on designing the model such that the impact of changing one learnable parameter cannot be replicated simply by changing another. Consider potential correlations between variables (`split_self` and `split_perc_self` are highly correlated; `fair_perc_self` varies, affecting the range of deviations) and how your parameters interact with these variables.
*   **Explore Fundamentally New Structures:** The previous approaches seem stuck in similar structural forms. Think outside the box. Consider:
    *   **Ratio-based utility:** Model utility not based on percentage *difference* from fairness, but the *ratio* `split_perc_self / fair_perc_self`. How does utility change as this ratio deviates from 1? Maybe utility is a function of `log(split_perc_self / fair_perc_self)`.
    *   **Threshold or Piecewise Models:** Is there a specific percentage or amount threshold below which offers are perceived as extremely unfair, and utility drops sharply? Can you model utility with different functional forms above and below a learned threshold? (e.g., `if split_perc_self < threshold_perc: U = f1(...) else: U = f2(...)`).
    *   **Contextual Modulation of Sensitivity:** Instead of just adding terms, could a parameter *multiply* or *scale* the effect of another variable based on context? For example, could the sensitivity to the proposed amount (`split_self`) be different depending on `trial_type` (e.g., `beta_amount_equal * split_self` for trial_type=1, `beta_amount_unequal * split_self` for trial_type=2 or 3)? Or could fairness sensitivity depend on `combined_earning`? This adds complexity but might help disentangle parameters.
    *   **Reference Point Models:** Could utility depend on comparing the offer not just to the calculated `fair_perc_self`, but also to other reference points? (e.g., the minimum possible offer, the maximum possible offer, or even the perceived offer of the opponent).
    *   **Utility as a function of the *difference* between accepting and rejecting:** Although the output format is direct utility, think about the underlying decision. Accepting gives `split_self`, rejecting gives 0. The cost of rejecting is foregone money *plus* psychological costs (if any). How does this comparison drive utility?
*   **Calculated Variables:** You will likely need `fair_perc_self` calculated as `(token_self / (token_self + token_opp)) * 100` (handle the `token_self + token_opp == 0` case by setting `fair_perc_self` to 50). Include *all* calculated variables used in the model in your descriptions. Use percentage terms (`split_perc_self`) for fairness comparisons to potentially reduce collinearity with absolute amount (`split_self`) unless you have a specific, justified structure using both.
*   **Parameter Constraints:** Define clear, generous, finite numerical bounds for *all* learnable parameters. Consider carefully if parameters should be non-negative or unbounded based on their psychological interpretation and their role in making them identifiable.
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Use clear notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include bounds and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure, its key components, and how it relates to the decision process, with a focus on the *novelty* or structural changes aimed at identifiability.

Design a model that you believe has the best chance of achieving parameter recovery >= 0.7 for all its learnable parameters by adopting a structural form that explicitly addresses and avoids the apparent identifiability issues of previous linear/power-law approaches, while still aiming for good fit (lower BIC, higher accuracy). Think critically about how each parameter influences decisions and how this influence can be uniquely captured by the model structure given the characteristics of the dataset. Do not be afraid to propose a structure that is significantly different from previous attempts.

Please think through this step by step, focusing intently on designing a model structure that improves parameter identifiability through a novel approach, and ensure all output requirements are met. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
</previous_instructions>",0.12391758289597378,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.29224404708815194,,,,,,,0.1116847669672441,0.7475562777424851,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64,44.421522832322815,35.08479776014272,52.78567237615081,0.6904761904761905,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (token_self / max(1e-6, sum_tokens)) * 100.0
deviation = fair_perc_self_calc - split_perc_self
unfairness_penalty_term = max(0.0, deviation)
U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * unfairness_penalty_term^2)","A ratio model predicting responder utility based on the proposed percentage share and the perceived unfairness relative to token contributions. Utility is calculated as the proposed percentage scaled by 'beta_value', divided by a term that penalizes unfair offers. This penalty term is 1.0 plus the squared positive deviation from the calculated fair share, scaled by 'beta_unfairness'. Constraining 'beta_unfairness' to be non-negative ensures the penalty increases utility and maintains denominator stability. This structure attempts to improve parameter identifiability, particularly for 'beta_unfairness', by applying it as a scaling factor to a non-linear (squared) penalty term within the denominator, aiming for a more distinct influence compared to simpler additive or linear scaling models like those in Run 61 and Run 63.",v0,"Okay, here are the improved instructions for the next run, focusing intensely on designing a model structure that ensures parameter identifiability for *both* learnable parameters while building on the successes and addressing the failures of the previous attempts.

Instructions for Run 64 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 63) implemented a ratio model (`U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * unfairness_penalty_term)`, where `unfairness_penalty_term = max(0.0, fair_perc_self_calc - split_perc_self)`). This model achieved promising results for BIC (46.17) and Accuracy (0.691), and excellent recovery for `beta_value` (r = 0.925). However, it critically failed the parameter recovery test for `beta_unfairness` (r = 0.608).

Comparing this to the cubic additive penalty model (Run 61: BIC 44.74, Accuracy 0.661, `beta_value` r=0.917, `beta_unfairness` r=-0.172), we see a pattern: models combining a value term (`beta_value * split_perc_self`) with a fairness/penalty term struggle to recover the parameter associated with the penalty. While Run 63's ratio structure showed better recovery for the second parameter (0.608) than Run 61's additive structure (-0.172), it still wasn't sufficient.

The consistent challenge lies in designing the mathematical interaction such that the influence of `beta_value` and the influence of `beta_unfairness` are sufficiently independent across the data to allow for reliable simultaneous estimation of both. Simply scaling a linear or cubic penalty term (whether added, subtracted, or in a denominator) by the second parameter hasn't achieved this separation for `beta_unfairness`.

The challenge for this run is to refine the model structure, perhaps building on the promise of the ratio approach or exploring other non-linear combinations, specifically focusing on making the influence of `beta_unfairness` profoundly distinct and reliably estimable.

We need a mathematical structure for utility, still using exactly **2 learnable parameters**, where their influence on responder decisions (`trial_role == 1`) is profoundly separable and distinguishable. Given the failures so far, consider fundamentally different ways `beta_unfairness` interacts with the unfairness deviation (`max(0, fair_perc_self_calc - split_perc_self)`).

Think **""Out of the Box"" Again**: The problem might be that `beta_unfairness` is just linearly scaling a penalty term. What if `beta_unfairness` interacts with the *shape* of the penalty function itself? Or if the penalty term scaled by `beta_unfairness` is itself a non-linear function of the deviation?

Consider alternative strategies focusing on creating mathematically distinct parameter influences, building on previous attempts:

*   **Ratio with Non-linear Penalty Component:** Enhance the ratio structure from Run 63. Instead of `1.0 + beta_unfairness * unfairness_penalty_term`, make the penalty term in the denominator a non-linear function of `unfairness_penalty_term`, where `beta_unfairness` scales *that* non-linear function. For example, `U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * max(0.0, fair_perc_self_calc - split_perc_self)^2)`. (Using a quadratic penalty term scaled by beta_unfairness in the denominator). This structure makes the disutility grow quadratically with unfairness, controlled by `beta_unfairness`.
*   **Additive with Different Non-linear Penalty:** Revisit the additive penalty structure, but use a different non-linear function than the cubic one that failed in Run 61. For instance, `U = beta_value * split_perc_self - beta_unfairness * max(0, fair_perc_self_calc - split_perc_self)^2`. (Using a quadratic penalty term scaled by beta_unfairness). This might provide a less aggressive, more stable penalty shape than the cubic, potentially improving `beta_unfairness` recovery.
*   **Multiplicative with Non-linear Penalty Factor:** Explore `U = beta_value * split_perc_self * (1.0 - beta_unfairness * max(0, fair_perc_self_calc - split_perc_self)^2)`. This is similar to the ratio structure for large denominators, but the interaction is multiplicative. Be careful with the range of the penalty factor `(1.0 - ...)`.

The goal is to design a utility calculation `U` involving `split_perc_self`, `fair_perc_self_calc` (derived from `token_self`, `token_opp`), and exactly two learnable parameters (`beta_value`, `beta_unfairness`) such that the influence of `beta_value` on `U` is mathematically distinct from the influence of `beta_unfairness` across the range of possible offers. Focus on making `beta_unfairness` influence the utility in a way that is clearly non-linear and specifically tied to the *degree* of unfairness, distinguishing it from the general value sensitivity captured by `beta_value`.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and any other necessary intermediate variables like `deviation` and `unfairness_penalty_term = max(0.0, deviation)`. Ensure `fair_perc_self_calc` handles `sum_tokens == 0` correctly (defaulting to 0.5 or 50.0 depending on scale). Use a small epsilon (`1e-6` or similar) for numerical stability if needed (e.g., denominators, logarithms).
*   **Model Formula:** Define `U`. Include calculation steps for derived variables *before* the formula for `U`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names (e.g., `beta_value`, `beta_unfairness`). Define clear, generous, finite numerical bounds (e.g., [-10.0, 10.0], [-50.0, 50.0], [0.0, 100.0], etc. appropriate for the scale of their influence). For parameters scaling penalty terms (like `beta_unfairness`), consider if a positive constraint `[0.0, 10.0]` makes theoretical sense and aids recovery.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including U) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific non-linear, asymmetric, or interactive mathematical structure is fundamentally different from the previous failed models (simple additive/subtractive, simple multiplicative exponential, and the linear penalty in the ratio denominator) and how this difference is specifically intended to ensure robust parameter identifiability for *both* learnable parameters*, particularly improving the recovery of the fairness/penalty parameter. Explicitly explain why this new structure should allow for better recovery of the second parameter compared to previous attempts, referencing the previous results (especially Run 63's near-miss).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative and explore structures that make the second parameter's influence on utility more non-linear and distinct than simple linear scaling of a deviation term.

Please think through this step by step, then provide your model specification and variable descriptions.
</previous_instructions>",,,,,,,,,,,,0.4505898940348498,,,,,,0.829577447534558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68,44.57767105900792,35.48841074094412,52.720133427273396,0.6733821733821734,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (token_self / sum_tokens) * 100.0 if sum_tokens > 0 else 50.0
non_linear_penalty_term = max(0.0, fair_perc_self_calc - split_perc_self)^2
U_accept = (beta_value * split_perc_self) / (1.0 + beta_unfairness * non_linear_penalty_term)","A computational model predicting responder acceptance utility using a ratio structure. The utility is calculated as the participant's proposed percentage share (split_perc_self) scaled by a value sensitivity parameter (beta_value) in the numerator, divided by one plus a non-linear penalty term scaled by an unfairness sensitivity parameter (beta_unfairness) in the denominator. The non-linear penalty term is defined as the squared positive difference between the calculated fair percentage share (based on token contributions) and the proposed percentage share. This term is zero for offers equal to or exceeding the fair share, penalizing only offers below the fair share. The use of a squared penalty term within the denominator is designed to create a mathematically distinct, non-linear influence profile for beta_unfairness compared to the linear influence of beta_value, thereby aiming to improve robust parameter identifiability for both learnable parameters.",v0,"Okay, here are the improved instructions for the next run (Run 68), building on the analysis of previous attempts and maintaining the intense focus on achieving robust parameter identifiability for *both* learnable parameters.

Instructions for Run 68 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery.

Run 67 attempted to combine the promising *ratio structure* from Run 63 (`U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * BoundedRatioPenaltyTerm)`) with a *bounded ratio-based* penalty term (`max(0.0, 1.0 - (split_perc_self / fair_perc_self_calc))`). This approach resulted in extremely poor parameter recovery for *both* `beta_value` (r=0.034) and `beta_unfairness` (r=-0.029), indicating that this specific combination of structure and penalty term completely failed to create distinguishable parameter influences. Despite the theoretical reasoning, the mathematical relationship between the scaled value term and the scaled penalty term was too collinear or interdependent within this model form.

The historical performance suggests that ratio structures have shown *potential* for better `beta_value` recovery (Run 63: r=0.925), but consistently struggle with `beta_unfairness` recovery when using simple difference or ratio-based penalty terms. This strongly indicates that the *form* of the penalty term and *how it enters the utility function* relative to the value term is the key challenge for disentangling `beta_unfairness`.

For Run 68, let's continue to explore the ratio structure as it has shown the best single-parameter recovery, but we must drastically change the approach to the *penalty term* to improve `beta_unfairness` identifiability. The failure of simple difference (Run 63) and bounded ratio (Run 67) penalties suggests that these terms, when scaled by `beta_unfairness` in the denominator `1 + beta_unfairness * penalty`, still have an influence profile too similar to the `beta_value * split_perc_self` term in the numerator, making them hard to tell apart during fitting.

To achieve better identifiability for `beta_unfairness`, its impact needs to be mathematically distinct. A promising avenue is to make the penalty term itself a **non-linear function** of the difference or ratio that quantifies unfairness. This non-linearity should ideally create a shape of influence for `beta_unfairness` that is less correlated with the linear effect of `beta_value` across the range of `split_perc_self` values in the dataset.

Let's define a `NonLinearPenaltyTerm` that is zero for fair or generous offers, but increases non-linearly as the offer becomes less than the fair share. A concrete approach to explore is using the **squared positive difference** between the fair share and the offered share percentage: `max(0.0, fair_perc_self_calc - split_perc_self)^2`. This penalizes increasingly unfair offers at an accelerating rate.

Proposed Model Structure: Continue with the ratio model form, but use this new non-linear penalty term in the denominator.
`U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * NonLinearPenaltyTerm)`

Here, `NonLinearPenaltyTerm` is calculated as `max(0.0, fair_perc_self_calc - split_perc_self)^2`. This ensures the penalty is zero for fair or generous offers (`split_perc_self >= fair_perc_self_calc`) and applies a squared penalty otherwise. Special care must be taken in calculating `fair_perc_self_calc` when `sum_tokens` is zero (as before, default to 50.0).

The goal is to design a utility calculation `U` involving `split_perc_self`, `fair_perc_self_calc` (derived from `token_self`, `token_opp`), and exactly two learnable parameters (`beta_value`, `beta_unfairness`). Design the model using the ratio structure `U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * NonLinearPenaltyTerm)` where `NonLinearPenaltyTerm` is calculated as `max(0.0, fair_perc_self_calc - split_perc_self)^2`.

Focus on clearly defining this new, non-linear penalty term and articulating in the summary how its non-linear relationship with the degree of unfairness is intended to help disentangle the influences of `beta_value` (scaling the linear value) and `beta_unfairness` (scaling the non-linear penalty) and thus significantly improve robust parameter identifiability for *both* parameters within the ratio structure. Explain why this approach represents a departure from previous attempts that used linear or simple ratio-based penalty terms.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0` defaulting to 50.0), and a new `non_linear_penalty_term` based on `max(0.0, fair_perc_self_calc - split_perc_self)^2`. Include any other necessary intermediate variables.
*   **Model Formula:** Define `U` using the ratio structure: `U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * NonLinearPenaltyTerm)`. Include calculation steps for derived variables *before* the formula for `U`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` and `beta_unfairness`. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and may aid recovery.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including U, sum_tokens, fair_perc_self_calc, and your chosen non-linear penalty term variable) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.\
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (ratio structure, value numerator, non-linear penalty denominator). Crucially, explain *how the definition of the penalty term as the squared positive difference between fair share and offered share percentage* within the denominator of the ratio structure is specifically intended to improve robust parameter identifiability for *both* learnable parameters by creating mathematically distinct non-linear influences for `beta_unfairness` compared to the linear influence of `beta_value`. Explain how this builds on previous attempts by addressing the suspected collinearity issues arising from linear or simple ratio penalties.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative in how you define and utilize the non-linear penalty term or alternative interactions to maximize this distinction within a plausible utility framework. Find a model structure that truly shines in parameter recovery.",,,,,,,,,,,,0.44452568671389575,,,,,,0.4968035303795336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
61,44.74443461588408,40.0235977996952,48.97351759705329,0.6614774114774115,"fair_perc_self_calc = token_self / (token_self + token_opp) IF token_self + token_opp > 0 ELSE 0.5
deviation_from_fairness = fair_perc_self_calc - split_perc_self
unfairness_penalty_term = max(0, deviation_from_fairness)^3
U = beta_value * split_perc_self - beta_unfairness * unfairness_penalty_term","This model predicts a responder's decision to accept or reject an ultimatum game offer based on the utility of accepting. Utility is calculated as a combination of two terms: a linear component scaled by 'beta_value' representing the intrinsic utility of the proposed percentage share ('split_perc_self'), and a non-linear penalty component scaled by 'beta_unfairness'. The penalty component is based on the cubic deviation from the calculated fair share ('fair_perc_self_calc') and is only applied when the offer is less than fair for the participant. This cubic and asymmetric structure is designed to make the influence of the 'beta_unfairness' parameter mathematically distinct from 'beta_value', particularly for unfair offers, thereby improving the identifiability of both parameters.",v0,"Okay, here are the improved instructions for the next run, focusing intensely on designing a model structure that ensures parameter identifiability for *both* learnable parameters.

Instructions for Run 61 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous attempts have consistently struggled to achieve sufficient parameter identifiability for *both* of the two learnable parameters. The most recent model, which used a multiplicative interaction term (`split_perc_self * deviation_from_fairness`), resulted in recovery values of 0.336 and 0.628, failing the criterion for both parameters. This further reinforces that simple additive or linear/multiplicative interaction terms, even when combining value and fairness, do not create mathematically distinct enough contributions across the data space.

However, reviewing previous runs, one model (BIC ~286) achieved a recovery of 0.774 for its `beta_value` parameter by using a structure that combined a linear value term (`split_perc_self`) with a *piecewise linear* penalty term for unfairness (`max(0, fair_perc_self_calc - split_perc_self)`), scaled by a second parameter (`beta_unfairness_effect`, which had recovery 0.659). This suggests that separating the influence into a general value component and a specific fairness deviation component, particularly active for unfair offers, is a promising direction for `beta_value` identifiability.

The challenge for this run is to build upon this intuition and design a structure that maintains good recovery for the value parameter while significantly improving the recovery for the second parameter (the one associated with fairness/unfairness). This requires making the influence of the second parameter even *more* mathematically distinct from the first, especially for the range of unfair offers.

We need to find a mathematical structure for utility, still using exactly **2 learnable parameters**, where their influence on responder decisions (`trial_role == 1`) is fundamentally more separable and distinguishable than in previous models.

Based on the persistent difficulty with the second parameter's identifiability, focus your exploration for Run 61 on designing a utility function where:
1. One parameter primarily scales a term related to the perceived *value* of the offer (e.g., `split_perc_self`).
2. The second parameter scales a term that specifically captures sensitivity to *unfairness*, but crucially, this term should have a **stronger non-linear form** than previously attempted (linear or squared deviations), especially when the offer is *unfair* (`split_perc_self < fair_perc_self_calc`).

Consider strategies for the unfairness term (active when `split_perc_self < fair_perc_self_calc`):
*   **Strongly Non-linear Penalties:** Instead of piecewise linear or squared functions of `fair_perc_self_calc - split_perc_self`, explore exponential penalties (e.g., scaling `exp(k * (fair_perc_self_calc - split_perc_self))`), cubic penalties (scaling `(fair_perc_self_calc - split_perc_self)^3`), or other non-linear functions that increase rapidly as unfairness increases. You may need a fixed 'steepness' parameter (like `k`) within the function if you only have 2 learnable parameters.
*   **Ratio-based Non-linearities:** Explore non-linear functions (e.g., inverse, exponential of the inverse, polynomial) of the ratio of fair share to proposed share (`fair_perc_self_calc / split_perc_self`) when the offer is unfair.
*   **Threshold-dependent Sensitivity:** Design a model where the sensitivity to `split_perc_self` itself is modulated by fairness, but the modulation is highly non-linear or has sharp changes around the fairness point, controlled by the second parameter.

The goal is to create a structure where changes in the first parameter primarily affect the baseline utility across all offers, while changes in the second parameter have a disproportionately strong and non-linear effect *only* when offers are unfair, making their influences clearly distinguishable.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate necessary intermediate variables like `sum_tokens`, `fair_perc_self_calc`, and any other derived measures of value, fairness, or their interactions as needed by your model structure. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0. Use a small epsilon for numerical stability where necessary (e.g., in denominators, logarithms, exponential inputs if they could cause overflow/underflow).
*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags. Only the formula should be inside these tags.
*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code (e.g., `beta_value`, `beta_unfairness_penalty`, `beta_non_linear_fairness_sensitivity`, etc., reflecting their specific roles in your new non-linear/asymmetric structure). Define clear, generous, finite numerical bounds (e.g., [-10.0, 10.0] or wider/narrower if appropriate).
*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including U) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Provide detailed descriptions for *calculated* variables, explaining how they are derived and what they represent.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the participant's proposed percentage share percentage.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific non-linear, asymmetric, or threshold-dependent mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters*, making their roles mathematically distinct from previously attempted additive, linear, or simple piecewise structures.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable across the range of possible offers and fairness conditions, particularly by making the 'fairness penalty' term highly non-linear and distinct from the basic 'value' term. Be creative in designing this non-linear interaction.",,,,,,,,,,,,-0.17190833861918461,,,,,,0.9168363962483564,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63,46.16727227659617,34.29392174000835,56.80381546562277,0.6907814407814408,"sum_tokens = token_self + token_opp
fair_perc_self_calc = 50.0 if sum_tokens == 0 else (token_self / sum_tokens) * 100.0
deviation = fair_perc_self_calc - split_perc_self
unfairness_penalty_term = max(0.0, deviation)
U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * unfairness_penalty_term)","A ratio model calculates the utility of accepting a proposed offer for the responder. Utility is defined as the offered percentage scaled by 'beta_value', divided by a penalty term. The penalty term is 1 plus the positive deviation between the fair share (based on contribution) and the offered share, scaled by 'beta_unfairness'. 'beta_unfairness' is constrained to be non-negative, imposing a disutility for offers below the calculated fair share. This ratio structure and the non-linear penalty applied only to unfair offers create distinct mathematical influences for 'beta_value' (primarily scaling value across offers, linearly for fair/generous offers) and 'beta_unfairness' (specifically modulating utility non-linearly only in the unfair domain), which is intended to improve the identifiability of both parameters compared to previous additive or simple multiplicative models.",v0,"Okay, here are the improved instructions for the next run, focusing intensely on designing a model structure that ensures parameter identifiability for *both* learnable parameters while building on the successes and addressing the failures of the previous attempts.

Instructions for Run 63 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 62) implemented a model (`U = beta_value * split_perc_self * exp(-beta_unfairness * max_deviation_positive)`) which attempted to use a **multiplicative, non-linear modulation** of the value term based on unfairness. This model critically failed to achieve sufficient parameter recovery for *both* `beta_value` (r = 0.571) and `beta_unfairness` (r = -0.073). Furthermore, it resulted in significantly worse BIC (84.03) and accuracy (0.544) compared to previous attempts, including the cubic penalty model from Run 61. This indicates that the specific multiplicative exponential structure was not effective for capturing the decision process in this dataset *or* for ensuring parameter identifiability.

The cubic penalty model (Run 61: `U = beta_value * split_perc_self - beta_unfairness * max(0, fair_perc_self_calc - split_perc_self)^3`) achieved a much lower BIC (44.74) and good recovery for `beta_value` (r = 0.917), but still failed on `beta_unfairness` recovery (r = -0.172). This highlights that while a non-linear penalty term can improve fit and isolate one parameter in an *additive* structure, it may not provide sufficient identifiability for the penalty parameter itself.

The challenge for this run is to design a model where the influence of the two learnable parameters is mathematically distinct and sufficiently strong across the data space to allow for reliable estimation of *both*. We know that simple additive/subtractive penalty structures often struggle with the second parameter's recovery, and the specific multiplicative exponential structure attempted last failed completely.

We need to find a mathematical structure for utility, still using exactly **2 learnable parameters**, where their influence on responder decisions (`trial_role == 1`) is profoundly separable and distinguishable. Given the failures so far, consider fundamentally different ways the parameters can interact with `split_perc_self` and `fair_perc_self_calc`.

Think **""Out of the Box""**: Avoid simply scaling a fairness deviation and adding/subtracting it from a value term. The multiplicative modulation attempt in Run 62 also failed. The problem might lie in the *form* of the function linking the second parameter to fairness and value. Explore structures where the parameters' influence is not just additive or a simple multiplicative decay, but integrated in a novel way.

Consider alternative strategies focusing on creating mathematically distinct parameter influences:

*   **Different Non-linear Combinations:** Can utility be a combination of terms where *each* term involves a different parameter and uses a different non-linear function of the relevant variables (`split_perc_self`, `fair_perc_self_calc`, or derived deviations/ratios)? For example, `U = f(split_perc_self, beta_value) + g(fair_perc_self_calc, split_perc_self, beta_unfairness)`, where `f` and `g` are distinct non-linear functions (e.g., logarithmic, power, sigmoidal, exponential, but used differently than the failed Run 62 attempt).
*   **Ratio-based Utility (Revisited):** Structure utility as a ratio of components influenced by the parameters. E.g., `U = (ValueTerm(split_perc_self, beta_value)) / (PenaltyTerm(fair_perc_self_calc, split_perc_self, beta_unfairness))` or similar, being careful to handle potential divisions by zero (use epsilon).
*   **Interaction beyond Simple Modulation:** Explore structures where the parameters' influence is truly interactive, perhaps in a way that changes the *shape* of the utility function as a function of `split_perc_self` differently depending on the fairness and the second parameter's value. This might involve polynomials or other functions where coefficients or exponents are determined by the parameters and fairness.
*   ** piecewise Utility with Parameter-Controlled Breakpoints or Slopes:** While conceptually complex, a piecewise function where the transition point or the slope/curvature around the fairness point is controlled by the second parameter could create distinct influences.

The goal is to design a utility calculation `U` involving `split_perc_self`, `fair_perc_self_calc` (derived from `token_self`, `token_opp`), and exactly two learnable parameters (`beta_X`, `beta_Y`) such that the influence of `beta_X` on `U` is as mathematically independent from the influence of `beta_Y` as possible across the range of `split_perc_self` and `fair_perc_self_calc` values in the data, particularly focusing on making the second parameter's effect pronounced and non-linear specifically in the unfair regime, but in a way that *is* recoverable. Avoid structures where the parameters are simply scaling additive components.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and any other necessary intermediate variables. Ensure `fair_perc_self_calc` handles `sum_tokens == 0` correctly (defaulting to 0.5). Use a small epsilon (`1e-6` or similar) for numerical stability if needed (e.g., denominators, logarithms).
*   **Model Formula:** Define `U`. Include calculation steps for derived variables *before* the formula for `U`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds (e.g., [-10.0, 10.0], [-50.0, 50.0], [0.0, 100.0], etc. appropriate for the scale of their influence).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including U) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific non-linear, asymmetric, or interactive mathematical structure is fundamentally different from simple additive/subtractive penalties and the failed multiplicative exponential, and how this difference is intended to ensure robust parameter identifiability for *both* learnable parameters*. Explicitly explain why this new structure should allow for better recovery of the second parameter compared to previous attempts.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative and explore structures that move beyond simple additive penalties and simple multiplicative modulation.",,,,,,,,,,,,0.6077908351093765,,,,,,0.9248759523280351,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
31,48.112698816428775,40.93126771142997,54.54606418132354,0.71001221001221,"fair_perc_self = (token_self + token_opp > 0) * (token_self / (token_self + token_opp)) * 100 + (token_self + token_opp == 0) * 50
U = (split_perc_self < fair_perc_self) * beta_utility_below + (split_perc_self >= fair_perc_self) * (beta_utility_fair + beta_gain_slope * (split_perc_self - fair_perc_self))",A piecewise utility model predicting responder acceptance based on the proposed share relative to a calculated fair share derived from token contributions. The model defines distinct utility functions: a constant base utility for offers below the fair percentage (controlled by beta_utility_below) and a base utility at the fair point plus a linear gain function for offers at or above the fair percentage (controlled by beta_utility_fair and beta_gain_slope). This structure aims to improve parameter identifiability by creating a clear breakpoint at the fairness reference point.,v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = beta_param + epsilon_param * E + eta_param * N\n</MODEL>\n\n<VARIABLES>\n{\n  ""variables"": {\n    ""U_i"": {\n      ""description"": ""Utility of choosing action i"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\n      ""learnable"": false,\n      ""source"": ""calculated""\n    },\n    ""beta_param"": {\n      ""description"": ""Base tendency parameter (inherent preference)"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1\n      },\n      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""epsilon_param"": {\n      ""description"": ""Environmental sensitivity parameter"",\n      ""range"": {""min"": ""-inf"", ""max"": ""inf""},\n      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""E"": {\n      ""description"": ""Environmental cue value"",\n      ""range"": {""min"": ""-inf"", ""max"": ""inf""},\n      ""learnable"": false,\n      ""source"": ""data""\n    },\n    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},\n    ""N"": {\n      ""description"": ""Random noise drawn from normal distribution N(0,1)"",\n      ""distribution"": {\n        ""type"": ""normal"",\n        ""mean"": 0,\n        ""std"": 1\n      },\n      ""learnable"": false,\n      ""source"": ""calculated""\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions for Run 31 of 100:
Previous attempts at modeling responder behavior have consistently struggled with parameter recovery. While some parameters have shown good recovery (like `beta_base` in the last run), *all* learnable parameters must achieve a recovery correlation of >= 0.7 for the model to be considered usable. Complex non-linear forms (like power laws or explicit loss aversion parameters) used in the previous attempt did not lead to full parameter identifiability for the deviation-sensitive parameters.

For this run, the **absolute highest priority remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the primary objective. Improved BIC and accuracy are secondary goals that may follow from better parameter identifiability.

**Guidance for the Next Model (Piecewise Utility based on Fair Share):**

Design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer, prioritizing parameter identifiability by employing a piecewise structure based on the calculated fair percentage.

*   **Parameter Identifiability:** Focus intensely on designing the model structure such that each learnable parameter has a distinct and separable influence on the predicted utility. The failure of complex non-linear terms in the last run suggests simpler, structurally separated influences might be more identifiable.
*   **Reference Point & Deviations:** Continue to use the calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as a key reference point. Calculate `fair_perc_self` as `(token_self / (token_self + token_opp)) * 100`. **Crucially, handle the case where `token_self + token_opp == 0` by setting `fair_perc_self` to 50 in that instance.** Define the positive deviation from this reference point as `perc_gain_from_fair = max(0, split_perc_self - fair_perc_self)`.
*   **Piecewise Structure:** Instead of a single function for all offers, define the utility (`U`) based on whether the proposed percentage (`split_perc_self`) is below or at/above the calculated fair percentage (`fair_perc_self`). This creates a structural breakpoint at the fairness reference point.
    *   **For offers below the fair percentage (`split_perc_self < fair_perc_self`):** Define the utility in this region with a simple structure. Consider a fixed utility value (`beta_utility_below`) for all offers in this unfair region. This parameter should capture the base tendency to reject unfair offers.
    *   **For offers at or above the fair percentage (`split_perc_self >= fair_perc_self`):** Define the utility in this region with a different structure. Consider a linear function of the gain relative to the fair percentage, starting from a base utility level at the fair point (`beta_utility_fair`) and increasing with the gain (`beta_gain_slope * perc_gain_from_fair`).
*   **Calculated Variables:** Ensure `fair_perc_self` and `perc_gain_from_fair` (and any other variables you define based on data inputs) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs.
*   **Parameter Constraints:** Define clear, generous, finite numerical bounds for *all* learnable parameters. Ensure parameter names are suitable for Python code (e.g., `beta_utility_below`, `beta_utility_fair`, `beta_gain_slope`).
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags, clearly showing the piecewise structure based on the condition `split_perc_self >= fair_perc_self`. Use clear notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure, its key components (the use of fair percentage as a piecewise breakpoint, the distinct utility functions/parameters for offers below vs. at/above fair), and how it relates to the decision process, focusing on the structural choices aimed at identifiability.

Design a model with a piecewise structure using `fair_perc_self` as the breakpoint, separating utility below this point from utility at/above this point. Use distinct, simple functional forms (e.g., constant below, linear above fair) in each region, aiming for full parameter recovery by ensuring each learnable parameter's influence is clearly separable. Think critically about potential confounds and how your chosen structure mitigates them.

Please think through this step by step, focusing intently on designing a piecewise model structure that improves parameter identifiability based on the calculated fair percentage, and ensure all output requirements are met. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.924047411642714,0.40908457203704934,-0.07644683004970035,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
99,48.15808486171929,45.8425646188517,50.232405079288164,0.7854090354090355,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100)
deviation = split_perc_self - fair_perc_self_calc
U_accept = deviation_sensitivity * deviation + base_accept_utility","This model predicts the utility of accepting a proposed offer in the ultimatum game responder role. The utility of accepting (`U_accept`) is calculated as a linear combination of the deviation of the proposed self-share percentage from the calculated fair share percentage, scaled by a `deviation_sensitivity` parameter, plus a `base_accept_utility` parameter that represents an overall bias towards acceptance or rejection. The fair percentage is calculated based on the relative token contributions of the participant and opponent, defaulting to 50% if total tokens are zero. This structure uses exactly two learnable parameters: `deviation_sensitivity`, which quantifies sensitivity to fairness deviation, and `base_accept_utility`, which captures a general response bias. The structural difference between the inputs scaled by these parameters (the varying fairness deviation vs. a constant implicit input for the base utility) is designed to ensure robust parameter identifiability, allowing for reliable estimation of both parameters, unlike previous models that combined deviation with binary indicators correlated with the deviation.",v0,"Okay, here are the improved instructions for the next run (Run 99 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy. Specifically, *all* learnable parameters must have a parameter recovery value of >= 0.7.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {""min"": 0, ""max"": 1},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {""description"": ""Noise parameter"",
      ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,\n        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 99 of 100).

The previous model (`deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + self_more_contrib_intercept_shift * is_self_more_contrib`) showed mixed results. While it built on previous insights, the parameter recovery for both parameters (`deviation_sensitivity`: r = 0.655, `self_more_contrib_intercept_shift`: r = 0.387) fell short of the required >= 0.7 threshold. Its overall accuracy (0.697) and BIC (450.68) were also worse than the recent model with the best overall BIC (52.82) and the target based on the previously best-recovering deviation model (BIC < 116.37, accuracy > 0.749).

Robust parameter identifiability (recovery >= 0.7 for *all* learnable parameters) remains the **highest priority**. It MUST be achieved alongside improved accuracy and a better BIC.

We must learn from the repeated failure of models that use a linear combination of a continuous fairness deviation term (`split_perc_self - fair_perc_self_calc`) and a scaled **binary indicator** (like `is_below_fair` or `is_self_more_contrib`). The parameter scaling the binary term consistently fails recovery. This strongly suggests that in this structure, the binary indicator does not introduce a sufficiently distinct or non-confounding source of variance from the continuous deviation term for both parameters to be reliably estimated. Models scaling related continuous terms have also struggled with recovering both parameters.

You *must* include these standard calculation steps first before defining `U_accept`:\n1.  Calculate `sum_tokens = token_self + token_opp`.\n2.  Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.

Now, design a **novel** formula for `U_accept` incorporating exactly **two learnable parameters**. Focus intently on creating a structure where these two parameters scale inputs that have demonstrably *different structural roles* and *sources of variance* on the utility calculation to ensure robust identifiability for *both* parameters (target r >= 0.7 for both) and improve performance (accuracy > 0.697, BIC < 450.68, aiming towards the best BIC 52.82).

To achieve this, pair a parameter scaling the **deviation from fairness** (`split_perc_self - fair_perc_self_calc`) with a second parameter scaling an input that is **structurally distinct** and unlikely to be confounded with the deviation term in a linear combination. **Explicitly avoid using binary indicators** derived from offer comparison to fairness or contribution type (like `is_below_fair`, `is_unfair`, `is_self_more_contrib`) as the scaled input for the second parameter, as this strategy has repeatedly failed parameter recovery.

Consider these structural approaches for your two-parameter model, where one parameter scales `(split_perc_self - fair_perc_self_calc)` and the second parameter scales a distinct input:

1.  **Deviation + Constant (Intercept):** The second parameter scales a constant (e.g., 1). This adds a baseline utility shift independent of the offer details. Model form: `U_accept = deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + intercept_parameter`. This is a fundamental structure in linear models and should promote identifiability if the deviation input varies.
2.  **Deviation + Absolute Deviation:** The second parameter scales the absolute magnitude of the fairness deviation. Model form: `U_accept = deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + absolute_deviation_sensitivity * abs(split_perc_self - fair_perc_self_calc)`. This captures potential non-linear sensitivity (like loss aversion) around the fair point, and `(split_perc_self - fair_perc_self_calc)` and `abs(split_perc_self - fair_perc_self_calc)` are structurally different inputs.
3.  **Deviation + Total Resource:** The second parameter scales the total earnings available in the trial (`combined_earning`). Model form: `U_accept = deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + total_earning_sensitivity * combined_earning`. The total resource size is distinct from the fairness of the split percentage and could influence willingness to accept.

You *must* select one of these structural patterns (or propose a similarly novel structure with a clear justification) for your model. Choose descriptive, Python-safe names for your exactly two learnable parameters.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus `combined_earning` if using option 3, and `trial_type` if using it to derive other non-binary inputs. Calculate `sum_tokens`, `fair_perc_self_calc`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Include function definitions like `IF` if used.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Ensure the parameters' roles are structurally distinct as discussed above to promote identifiability (recovery >= 0.7 for BOTH).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, any derived variables, and any functions like `IF`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs with `source: ""data""` that appear in your <MODEL>.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea. **Crucially, explain *how* the specific structural difference between the inputs scaled by your two learnable parameters promotes robust parameter identifiability (recovery >= 0.7 for BOTH learnable parameters), explicitly addressing why this structure is expected to succeed where linear combinations of deviation and binary indicators failed.**

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) AND improved accuracy/BIC are the absolute priorities. Your goal is to find a novel structure with *exactly two* parameters where their influences are clearly distinguishable through structurally different mathematical roles, allowing for reliable estimation and better fits the human decision data. Be creative and think ""out of the box"" about how to use the available data variables and exactly two parameters to capture distinct aspects of the responder's decision process while maintaining identifiability and improving predictive power.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.9042363658719473,,,0.5449554230310514,
88,48.16518569817234,49.7439693035863,46.750858718322334,0.5879120879120879,U_accept = beta_gain * log(1.0 + gain_above_fair) - beta_loss * log(1.0 + loss_below_fair),"The model calculates the utility of accepting a proposed split percentage by evaluating its deviation from a calculated fair share based on token contributions. Utility is defined as an additive function where deviations above the fair share contribute positively to utility (scaled by beta_gain), and deviations below the fair share contribute negatively (scaled by beta_loss). Crucially, the magnitude of both gains and losses is transformed using the natural logarithm of (1 + deviation) before scaling. This non-linear transformation, combined with the mutually exclusive nature of positive gain_above_fair and loss_below_fair terms, is specifically designed to create distinct influence profiles for beta_gain (affecting utility primarily for fair or generous offers via log-transformed gains) and beta_loss (affecting utility primarily for unfair offers via log-transformed losses). This structural separation aims to enhance robust joint parameter identifiability compared to models using linear or simple multiplicative scaling of deviations.",v0,"Okay, here are the improved instructions for the next run, based on the ongoing efforts to achieve robust parameter identifiability and improved model performance.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,
      ""source"": ""calculated""
    },\\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,\\\
      ""source"": ""data""
    },\\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1,\
        ""exclusive_max"": true
      },\\\
      ""learnable"": false,
      ""source"": ""data""
    },\\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {\
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\\
      ""learnable"": false,
      ""source"": ""calculated""
    }\
  }\
}\
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 88 of 100).

Previous attempts utilizing simple linear or multiplicative scaling of gains, losses, or deviations from a calculated fair share have consistently struggled to achieve robust parameter identifiability (parameter recovery >= 0.7 for *all* learnable parameters simultaneously). This remains the **unwavering primary objective**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation and overrides achieving a low BIC or high accuracy if recovery fails. We need to find a structure where the influence of the two learnable parameters is sufficiently distinct across trials to be estimated independently with high reliability.

Given the persistent identifiability challenge with simple linear and multiplicative approaches, we must explore a **fundamentally different structural approach** for this run, focusing on alternative non-linear relationships or transformations.

Proposed Model Structure: Let's return to the concept of modeling utility based on the *deviation* of the proposed share from the calculated fair share, explicitly separating the influence of parameters on offers above versus below this fairness reference point. However, instead of linearly scaling these deviations, we will apply a non-linear transformation to their magnitude *before* scaling. Specifically, we will use the natural logarithm of (1 + deviation) to handle zero values gracefully and introduce a different utility curvature for gains and losses.

Here are the calculation steps:
1. Calculate `sum_tokens = token_self + token_opp`.
2. Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.
3. Calculate `gain_above_fair = Max(0.0, split_perc_self - fair_perc_self_calc)`. This term is positive if the offer is above the fair share, and 0 otherwise. Use `Max` for this calculation.
4. Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is positive if the offer is below the fair share, and 0 otherwise. Use `Max` for this calculation.
5. Calculate the utility using an additive structure based on the *logarithm* of these deviations: `U_accept = beta_gain * log(1.0 + gain_above_fair) - beta_loss * log(1.0 + loss_below_fair)`. Use `log` for the natural logarithm.

In this structure, `beta_gain` specifically scales the utility derived from the *logarithm* of the amount the offer is *above* the fair share (plus 1), while `beta_loss` specifically scales the penalty derived from the *logarithm* of the amount the offer is *below* the fair share (plus 1). Since a given offer can only be *either* above or below fair (or exactly fair), these two terms (`log(1.0 + gain_above_fair)` and `log(1.0 + loss_below_fair)`) are mutually exclusive in terms of contributing non-zero values to the sum (one will always be 0 or both will be 0). This design aims to create highly distinct influence profiles for `beta_gain` (affecting utility only when the offer is generous/fair or above, via log-transformed magnitude) and `beta_loss` (affecting utility only when the offer is unfair/below fair, via log-transformed magnitude), thereby improving robust joint parameter identifiability compared to linear scaling.

Design the model using this specific structure: `U_accept = beta_gain * log(1.0 + gain_above_fair) - beta_loss * log(1.0 + loss_below_fair)`. Remember to calculate intermediate variables (`sum_tokens`, `fair_perc_self_calc`, `gain_above_fair`, `loss_below_fair`) before the final `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula), using `IF`, `Max`, and `log`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific additive structure based on log-transformed deviations from fairness, where `beta_gain` scales log(1+gains *above* fairness) and `beta_loss` scales log(1+losses *below* fairness), is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles tied to different regions relative to the fairness point, mediated by a non-linear transformation.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `gain_above_fair`, `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use `log` for the natural logarithm.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_gain` (sensitivity/scaling parameter for log-transformed gains *above* fair share) and `beta_loss` (sensitivity/scaling parameter for log-transformed losses *below* fair share). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds.
    *   `beta_gain`: Non-negative bounds, e.g., `[0.0, 10.0]`. Must be >= 0.
    *   `beta_loss`: Non-negative bounds, e.g., `[0.0, 10.0]`. Must be >= 0 to represent aversion to loss scaled by log.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, `gain_above_fair`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""` (using `split_perc_self` as specified).
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is based on log-transformed deviations from a calculated fair share, scaled separately for gains above and losses below fairness). Crucially, explain *how* this specific deviation-based structure, where `beta_gain` applies only to log(1+gains >= fairness) and `beta_loss` applies only to log(1+losses < fairness), is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles tied to different ranges relative to the fairness point, mediated by the log transformation.
*   **Think Outside the Box:** Parameter identifiability for *both* parameters simultaneously remains the persistent, difficult challenge. Simple additive and multiplicative structures have failed. This iteration attempts a specific non-linear transformation (log) of the deviations. If this *still* fails to achieve >= 0.7 recovery for *both* parameters, the next iteration *must* consider even more fundamentally different structural approaches or non-linearities. Think about alternative non-linear transformations of the deviations, threshold/aspiration models, models where parameters influence decision noise directly, or other complex non-linear interactions that are more likely to yield distinguishable parameter influences across the range of offers in the dataset. Focus intently on finding a functional form where varying `beta_gain` has a clearly distinguishable effect on the utility curve across offers compared to varying `beta_loss`, allowing them to be estimated independently with high reliability. This is a prerequisite for better fit metrics and meaningful interpretation.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,0.3325352467895591,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6169848372822029,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32,49.95179791596241,43.721092228515445,55.53347176096698,0.6785714285714286,"total_tokens = token_self + token_opp
fair_perc_self = (token_self / total_tokens) * 100 * (total_tokens > 0) + 50 * (total_tokens == 0)
U = beta_utility_below + beta_gain_slope * max(0.0, split_perc_self - fair_perc_self)","A two-parameter piecewise utility model for responder decisions, predicting the utility of accepting an offer. The model uses a baseline utility parameter (beta_utility_below) for offers below a calculated fair percentage (fair_perc_self) and adds a linear gain component for offers at or above fair, scaled by a gain sensitivity parameter (beta_gain_slope). This structure, based on the fair percentage derived from token contributions, aims to improve parameter identifiability by simplifying the utility representation in the gain region.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = beta_param + epsilon_param * E + eta_param * N\n</MODEL>\n\n<VARIABLES>\n{\n  ""variables"": {\n    ""U_i"": {\n      ""description"": ""Utility of choosing action i"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\
      ""learnable"": false,\n      ""source"": ""calculated""\n    },\n    ""beta_param"": {\n      ""description"": ""Base tendency parameter (inherent preference)"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1\n      },\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""epsilon_param"": {\n      ""description"": ""Environmental sensitivity parameter"",\n      ""range"": {""min"": ""-inf"", ""max"": ""inf""},\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""E"": {\n      ""description"": ""Environmental cue value"",\n      ""range"": {""min"": ""-inf"", ""max"": ""inf""},\
      ""learnable"": false,\n      ""source"": ""data""\n    },\n    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},\
    ""N"": {\n      ""description"": ""Random noise drawn from normal distribution N(0,1)"",\n      ""distribution"": {\n        ""type"": ""normal"",\n        ""mean"": 0,\n        ""std"": 1\n      },\
      ""learnable"": false,\n      ""source"": ""calculated""\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions for Run 32 of 100:
The primary objective for this run, and all subsequent runs, remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals that may follow from better parameter identifiability.

Modeling responder utility, particularly how individuals value offers relative to a fairness reference point and how they respond to deviations above and below this point, presents significant challenges for parameter identifiability. Structuring the utility function to ensure learnable parameters have distinct and separable influences is crucial.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer, building upon the idea of a piecewise structure based on the calculated fair percentage. Specifically, design the model with a simpler structure for offers at or above the fair percentage that aims to improve the identifiability of parameters associated with sensitivity to gains.

*   **Parameter Identifiability:** Focus intensely on designing the model structure such that each learnable parameter has a distinct and separable influence on the predicted utility. Consider how changes in one parameter value should uniquely impact predictions, distinguishable from changes in other parameters.
*   **Reference Point & Deviations:** Continue to use the calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as a key reference point. Calculate `fair_perc_self` as `(token_self / (token_self + token_opp)) * 100`. **Crucially, handle the case where `token_self + token_opp == 0` by setting `fair_perc_self` to 50 in that instance.**
*   **Piecewise Structure Modification:** Define the utility (`U`) using a piecewise structure based on whether the proposed percentage (`split_perc_self`) is below or at/above the calculated fair percentage (`fair_perc_self`).
    *   **For offers below the fair percentage (`split_perc_self < fair_perc_self`):** Model the utility in this region as a constant baseline utility (`beta_utility_below`).
    *   **For offers at or above the fair percentage (`split_perc_self >= fair_perc_self`):** Model the utility as this *same baseline* (`beta_utility_below`) plus a linear term proportional to the percentage gain above fair. This gain term should be scaled by a single gain sensitivity parameter (`beta_gain_slope`). This structure can be concisely represented using the `max()` function for the gain term `max(0.0, split_perc_self - fair_perc_self)`.
    *   The resulting utility function structure should be: `U = beta_utility_below + beta_gain_slope * max(0.0, split_perc_self - fair_perc_self)`.
*   **Calculated Variables:** Ensure `fair_perc_self` and `U` are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`).
*   **Learnable Parameters:** Your model should have two learnable parameters: `beta_utility_below` and `beta_gain_slope`. Define clear, generous, finite numerical bounds for both, e.g., [-20, 20]. Ensure parameter names are suitable for Python code.
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags, including the calculation for `fair_perc_self` and the formula for `U` using the `max()` function as described above. Use clear notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the two-parameter piecewise model, its dependence on `fair_perc_self`, and how it aims to improve parameter identifiability by modeling a baseline utility for unfair offers and an added gain component for offers at/above fair relative to that baseline.

Think creatively about how different computational structures can capture responder behavior and fairness considerations while maintaining parameter identifiability. For this run, focus specifically on implementing and testing the proposed simplified piecewise structure to address the identifiability challenges observed in modeling the gain region.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.672966340141043,,0.7132280217248805,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
96,52.82326964990021,54.45181995889164,51.364359998095395,0.5445665445665446,"FUNCTION IF(condition, value_if_true, value_if_false) {
    IF condition THEN RETURN value_if_true ELSE RETURN value_if_false
}

sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
is_unfair = IF(split_perc_self < fair_perc_self_calc, 1, 0)

U_accept = sensitivity_to_offer * split_perc_self - aversion_strength * is_unfair","A utility model for responder acceptance decisions combining sensitivity to the proposed offer percentage and an aversion towards unfair offers based on relative token contributions. The model calculates the utility of accepting by linearly combining the proposed percentage to the participant, scaled by a 'sensitivity_to_offer' parameter, and a fixed penalty applied when the offer is below the calculated fair share, scaled by an 'aversion_strength' parameter. Parameter identifiability is structurally promoted by having 'sensitivity_to_offer' scale a continuous variable (the proposed percentage) and 'aversion_strength' scale a binary state variable (whether the offer is unfair), allowing the fitting process to distinguish between a general positive bias towards higher offers and a specific negative reaction to perceived unfairness.",v0,"Okay, here are the improved instructions for the next run (Run 96 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy. Specifically, *all* learnable parameters must have a parameter recovery value of >= 0.7.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {""min"": 0, ""max"": 1},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {""description"": ""Noise parameter"",
      ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 96 of 100).

The previous model, which attempted to scale the absolute offer percentage and the fairness deviation percentage independently, did not achieve robust parameter recovery (recovery < 0.7 for both parameters). This suggests that scaling these two particular inputs with two separate linear parameters resulted in influences on utility that were too mathematically similar or inputs too correlated across trials for the parameters to be reliably estimated.

In contrast, a simpler linear model combining the fairness deviation with a constant intercept *did* achieve robust parameter recovery (> 0.7 for both parameters: slope on deviation, and intercept). This indicates the success of a structural separation where one parameter scales a variable input (the ""slope"") and the other provides an independent offset (the ""intercept"").

Robust parameter identifiability remains the **unwavering primary objective** (recovery >= 0.7 for *all* learnable parameters). We must build on the structural principles that worked for identifiability, which seems to be having two parameters with fundamentally distinct types of influence on the utility calculation, rather than simply scaling different but potentially correlated continuous inputs.

You *must* include these standard calculation steps first before defining `U_accept`:\n
1.  Calculate `sum_tokens = token_self + token_opp`.\n
2.  Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.\n

Now, design a **novel** formula for `U_accept` incorporating exactly **two learnable parameters**. Focus intently on creating a structure where these two parameters have demonstrably *different types* of mathematical influence on the utility calculation to ensure robust identifiability.

Specifically, design the model such that:
*   One parameter acts like a ""slope"", scaling a continuous variable input that changes on each trial (e.g., `split_perc_self`, `split_perc_self - fair_perc_self_calc`).
*   The second parameter acts like an ""intercept"" or a ""bias"", providing a constant offset, or scaling a categorical/binary state variable (e.g., scaling 1 as an intercept, or scaling a calculated variable like `is_unfair = IF(split_perc_self < fair_perc_self_calc, 1, 0)`).

Avoid structures where both parameters scale continuous variables that are highly correlated linear transformations of each other (like scaling `split_perc_self` and `fair_perc_self_calc` or their difference simultaneously, as this failed recovery).

Think creatively about how the second parameter can capture a distinct psychological process (like a general bias, an aversion specific to unfair offers, or an effect tied to the contribution structure) by scaling a variable that behaves fundamentally differently across trials than the continuous variable scaled by the first parameter.

Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus potentially *other relevant data variables* from the list provided (`trial_type`, `combined_earning`, etc.) or calculated variables derived from them (like `is_unfair`).

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus potentially others from the provided list. Calculate `sum_tokens`, `fair_perc_self_calc`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Include function definitions like `IF` if used.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Ensure the parameters' roles are structurally distinct to promote identifiability (e.g., one scaling a continuous variable, the other a constant or binary state).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, any derived binary variables, and any functions like `IF`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs with `source: ""data""` that appear in your <MODEL>.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea. **Crucially, explain *how* the specific structure of your model ensures robust parameter identifiability for *both* learnable parameters (recovery >= 0.7). Clearly articulate the distinct mathematical/psychological roles of the two parameters and why their influences are expected to be separable by the fitting process, building on the lessons learned from the success of the deviation + intercept model and the failure of the previous model.**

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a novel structure with *exactly two* parameters where their influences are clearly distinguishable (e.g., one scaling a continuous variable, the other providing a constant or binary bias) and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) compared to the best previous run. Avoid structures that have failed recovery previously by ensuring the two parameters have fundamentally different mathematical roles that are likely to be identifiable. Be creative and think ""out of the box"" about how to use the available data variables and exactly two parameters to capture distinct aspects of the responder's decision process while maintaining identifiability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-0.023628961578457668,0.7196651548832308,,,,,
75,57.073634862828236,57.073634862828236,57.073634862828236,0.5106837606837606,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = split_perc_self * (beta_value - beta_unfairness * loss_below_fair)","A multiplicative utility model predicting responder acceptance. The utility of accepting an offer is calculated by multiplying the participant's proposed percentage share (split_perc_self) by an adjusted sensitivity scaling factor. This scaling factor starts at a base value (beta_value) and is linearly reduced by an unfairness sensitivity parameter (beta_unfairness) based on how much the proposed share falls below the calculated fair share (loss_below_fair). This structure aims to improve parameter identifiability by creating distinct influence profiles for beta_value (primarily affecting fair/better offers) and beta_unfairness (specifically reducing sensitivity for unfair offers), with these influences interacting multiplicatively with the offer percentage.",v0,"Okay, here are the improved instructions for the next run (Run 75 of 100), based on the results of recent attempts.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. The model tested in the previous run, the multiplicative model with exponential decay (`U_accept = beta_value * split_perc_self * exp(-beta_unfairness * loss_below_fair)`), achieved a very good BIC (65.61) compared to prior additive models, but critically failed parameter recovery for *both* parameters (r = -0.012 and r = 0.099). This indicates that despite potentially capturing some aspect of the data structure better (reflected in BIC), this specific multiplicative functional form still results in parameters whose influences are too confounded to be reliably estimated. The model is unusable without robust parameter recovery.

Previous attempts across various additive structures have also consistently failed parameter identifiability (recovery well below 0.7 for at least one parameter). This persistent failure pattern suggests that disentangling 'value' and 'unfairness' sensitivities is difficult in this task context, regardless of simple additive or the previous multiplicative exponential structure.

For Run 75, we will continue exploring multiplicative or interactive structures, as the previous multiplicative model's low BIC indicates this direction might be promising *if* identifiability can be solved. The problem isn't necessarily the multiplicative principle itself, but likely the specific way parameters and variables combine within the function. We need a structure where the influence of each parameter is distinct enough to be reliably estimated.

Proposed Model Structure: A multiplicative utility model where the participant's proposed percentage (`split_perc_self`) is scaled by a factor that represents their value sensitivity, *adjusted* downwards based on unfairness.

`U_accept = split_perc_self * (beta_value - beta_unfairness * loss_below_fair)`

Here:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp`.
Calculate `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
The term `(beta_value - beta_unfairness * loss_below_fair)` represents a variable scaling factor applied to `split_perc_self`. `beta_value` sets the base level of this scaling factor (when offers are fair or better). `beta_unfairness` dictates how much this scaling factor is linearly reduced as `loss_below_fair` increases.
Finally, `U_accept = split_perc_self * (beta_value - beta_unfairness * loss_below_fair)`. Note the multiplication of `split_perc_self` by this adjusted scaling factor.

In this structure, `beta_value` primarily influences the utility of fair or better offers (where `loss_below_fair = 0`) and contributes to the baseline scaling factor. `beta_unfairness` specifically influences how the scaling factor is reduced for unfair offers, thereby affecting the utility of offers below the fair share. This separation in how parameters contribute to the multiplicative scaling factor, and how that factor then scales the core offer percentage, is hypothesized to create more distinct parameter influences across the range of observed offers compared to previous structures, thus improving robust parameter identifiability.

Design the model using this multiplicative structure: `U_accept = split_perc_self * (beta_value - beta_unfairness * loss_below_fair)`, where `loss_below_fair` and `fair_perc_self_calc` are calculated as specified above using `token_self`, `token_opp`, and `split_perc_self`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific structure, by calculating a composite scaling factor `(beta_value - beta_unfairness * loss_below_fair)` and multiplying it by `split_perc_self`, is intended to achieve robust parameter identifiability by creating mathematically distinct and interactive influence profiles for `beta_value` and `beta_unfairness` across different offers, addressing the persistent identifiability failures seen in previous models. `beta_value` represents a base value sensitivity, and `beta_unfairness` represents how strongly increasing unfairness *reduces* this sensitivity before it scales the offer percentage.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include any other necessary intermediate variables like the scaling factor calculation if explicitly used in the <MODEL> tags.
*   **Model Formula:** Define `U_accept` using the structure: `sum_tokens = token_self + token_opp`, `fair_perc_self_calc = (token_self / sum_tokens) * 100.0` (handle `sum_tokens == 0`), `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`, `U_accept = split_perc_self * (beta_value - beta_unfairness * loss_below_fair)`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (base value sensitivity / scaling intercept) and `beta_unfairness` (sensitivity to unfairness reducing scaling factor). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and may aid recovery. Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (multiplicative structure where `split_perc_self` is scaled by an adjusted sensitivity factor). Crucially, explain *how* this specific structure, where the scaling factor `(beta_value - beta_unfairness * loss_below_fair)` combines the parameters before multiplying `split_perc_self`, is specifically intended to improve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles for `beta_value` and `beta_unfairness`, addressing the limitations and failures of previous structures.
*   **Think Outside the Box:** Recognize that previous attempts with standard additive structures and specific multiplicative/exponential forms have consistently failed parameter recovery. The proposed structure is another attempt at a non-additive interaction principle. Continue to consider non-obvious ways parameters might interact or influence decisions beyond simple additive or standard multiplicative combinations, as finding a structure that truly shines in parameter recovery often requires exploring less standard structural principles like this adjusted scaling factor approach or others (e.g., thresholding, gating, or more complex non-linear interactions).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.",,,,,,,,,,,,-0.043054580754213194,,,,,,0.1001741066067371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
65,57.073634862828236,57.073634862828236,57.073634862828236,0.49511599511599513,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF sum_tokens == 0 THEN 50.0 ELSE (token_self / sum_tokens) * 100.0 END IF
unfairness_penalty_term = max(0.0, fair_perc_self_calc - split_perc_self)
penalty_factor = max(0.0, 1.0 - unfairness_penalty_term / 100.0)
U = beta_value * split_perc_self * (penalty_factor)^beta_unfairness","This model predicts responder acceptance based on the utility of the proposed offer. Utility is calculated as a multiplicative function combining the inherent value of the offer (the proposed percentage for the participant, scaled by beta_value) and a penalty factor that depends on the offer's unfairness relative to the token-based fair split. The penalty factor ranges from 1 (for fair or better offers) down to 0 (for maximally unfair offers against the participant). The parameter beta_unfairness is in the exponent of this penalty factor, controlling the *rate* or *shape* of the utility decay as the offer becomes more unfair. This structure is designed to improve parameter identifiability by making the influence of beta_unfairness (controlling the non-linear penalty shape) mathematically distinct from the linear scaling of value by beta_value, particularly compared to previous models where beta_unfairness simply scaled a deviation term linearly or polynomially. The model has two learnable parameters: beta_value (value sensitivity) and beta_unfairness (unfairness decay rate).",v0,"Okay, here are the improved instructions for the next run, focusing intensely on designing a model structure that ensures parameter identifiability for *both* learnable parameters while building on the successes and addressing the failures of the previous attempts.

Instructions for Run 65 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

Run 64 continued the exploration of ratio models, using `U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * unfairness_penalty_term^2)`, which applied a quadratic penalty in the denominator. While it achieved a reasonable BIC (44.42) and Accuracy (0.690), and maintained excellent recovery for `beta_value` (r = 0.830), it again failed to recover `beta_unfairness` reliably (r = 0.451). This is an improvement over Run 61's cubic additive model (-0.172) and slightly worse than Run 63's linear ratio model (0.608) for the second parameter, but still below the critical 0.7 threshold. The consistent failure to recover the fairness/penalty parameter across various structures (additive, ratio, multiplicative exponential) suggests that simply scaling a deviation-based penalty term linearly or with a fixed power is insufficient to create a mathematically distinct influence from the `beta_value * split_perc_self` component. The problem is likely rooted in the correlation between the value term (`split_perc_self`) and the deviation term (`unfairness_penalty_term`), making it hard to disentangle the two beta influences.

For Run 65, we need a more sophisticated interaction that makes the role of `beta_unfairness` clearly distinguishable. Let's move away from `beta_unfairness` just scaling a deviation term and instead have it influence the *way* unfairness impacts utility. Consider a multiplicative structure where utility is the inherent value scaled by a penalty *factor* that depends on unfairness, and `beta_unfairness` controls the *rate* or *shape* of this penalty factor.

Think **""Out of the Box"" Again**: A promising approach is a power-law multiplicative decay: `U = beta_value * split_perc_self * (penalty_factor)^beta_unfairness`. Here, `penalty_factor` is a value between 0 and 1 that decreases as unfairness increases (e.g., calculated as `max(0.0, 1.0 - unfairness_penalty_term / MaximumPossibleUnfairness)`, where `MaximumPossibleUnfairness` is 100%). In this structure, `beta_unfairness` is in the exponent. A higher `beta_unfairness` means a steeper decline in utility as offers become more unfair (the penalty factor is raised to a larger positive power, making it smaller). This non-linear role for `beta_unfairness` should create a more distinct signature on the utility function, less correlated with the linear scaling of `split_perc_self` by `beta_value`, thereby improving parameter identifiability for *both* parameters. Design the model using this concept or another novel multiplicative interaction where `beta_unfairness` controls the *shape* of the penalty function.

The goal is to design a utility calculation `U` involving `split_perc_self`, `fair_perc_self_calc` (derived from `token_self`, `token_opp`), and exactly two learnable parameters (`beta_value`, `beta_unfairness`) such that the influence of `beta_value` on `U` is mathematically distinct from the influence of `beta_unfairness` across the range of possible offers. Focus on making `beta_unfairness` influence the utility in a way that is clearly non-linear and specifically tied to the *degree* of unfairness, distinguishing it from the general value sensitivity captured by `beta_value`.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and any other necessary intermediate variables like `deviation`, `unfairness_penalty_term = max(0.0, deviation)`, and a `penalty_factor` (e.g., `max(0.0, 1.0 - unfairness_penalty_term / 100.0)`). Ensure `fair_perc_self_calc` handles `sum_tokens == 0` correctly (defaulting to 50.0 when using percentage scale, e.g., `IF sum_tokens == 0 THEN 50.0 ELSE (token_self / sum_tokens) * 100.0 END IF`).
*   **Model Formula:** Define `U`. Include calculation steps for derived variables *before* the formula for `U`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names (e.g., `beta_value`, `beta_unfairness`). Define clear, generous, finite numerical bounds (e.g., [-10.0, 10.0], [-50.0, 50.0], [0.0, 100.0], etc. appropriate for the scale of their influence). For `beta_unfairness` in an exponent as suggested, a non-negative constraint like `[0.0, 10.0]` or `[0.0, 50.0]` is theoretically plausible (higher beta = stronger penalty) and may aid recovery.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including U, sum_tokens, fair_perc_self_calc, deviation, unfairness_penalty_term, penalty_factor) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific non-linear, asymmetric, or interactive mathematical structure (especially the role of `beta_unfairness`, e.g., in the exponent) is fundamentally different from the previous failed models (linear additive, cubic additive, linear ratio, quadratic ratio, simple multiplicative exponential) and how this difference is specifically intended to ensure robust parameter identifiability for *both* learnable parameters*, particularly improving the recovery of the fairness/penalty parameter (which failed in Run 64 at 0.451).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative and explore structures that make the second parameter's influence on utility more non-linear and distinct than simple linear scaling of a deviation term.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,-0.17857747168034707,,,,,,-0.08594341630919836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87,58.56304865898576,60.785330240133135,56.57225474254123,0.6196581196581197,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
gain_above_fair = Max(0.0, split_perc_self - fair_perc_self_calc)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_gain * gain_above_fair - beta_loss * loss_below_fair","This model calculates the utility of accepting an offer as a responder. The utility is based on the deviation of the proposed percentage share from a calculated fair percentage share, determined by the relative token contributions. Utility is an additive function of the amount the offer is above the fair share (scaled by beta_gain) and the amount the offer is below the fair share (scaled by -beta_loss). beta_gain scales positive utility from gains above fairness, while beta_loss scales negative utility from losses below fairness. This structure explicitly isolates the influence of each parameter to offers above or below the calculated fair share, aiming to improve the identifiability and reliable estimation of both parameters. The utility is converted to a probability of acceptance using a logistic function.",v0,"Okay, here are the improved instructions for the next run (Run 87 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,
      ""source"": ""calculated""
    },\\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,\\\
      ""source"": ""data""
    },\\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1,\
        ""exclusive_max"": true
      },\\\
      ""learnable"": false,
      ""source"": ""data""
    },\\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {\
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\\
      ""learnable"": false,
      ""source"": ""calculated""
    }\
  }\
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 87 of 100), based on the results of recent attempts.

The previous run (Run 86), which re-implemented the multiplicative structure (`U_accept = beta_value * split_perc_self * (1.0 / (1.0 + beta_unfairness * loss_below_fair))`), showed disappointing parameter recovery (r=0.453 for `beta_value`, r=0.333 for `beta_unfairness`), failing to meet the critical >= 0.7 threshold for *both* parameters. While it achieved a good BIC, this result reinforces that the multiplicative structure, despite its promise, does *not* lead to robust parameter identifiability for *both* parameters simultaneously in this task.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation and overrides achieving a low BIC or high accuracy if recovery fails. We need to find a structure where the influence of the two learnable parameters is sufficiently distinct across trials to be estimated independently with high reliability.

Given that previous attempts focusing on overall gain and overall loss scaling (additive, power-law, multiplicative) have struggled with joint parameter identifiability, we must explore a **fundamentally different structural approach** for Run 87.

Proposed Model Structure: Let's model the utility of accepting based on the *deviation* of the proposed share from the calculated fair share, explicitly separating the influence of parameters on gains (offers above fair) versus losses (offers below fair) relative to this fairness reference point.

Here are the calculation steps:
1. Calculate `sum_tokens = token_self + token_opp`.
2. Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.
3. Calculate `gain_above_fair = Max(0.0, split_perc_self - fair_perc_self_calc)`. This term is positive if the offer is above the fair share, and 0 otherwise. Use `Max` for this calculation.
4. Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is positive if the offer is below the fair share, and 0 otherwise. Use `Max` for this calculation.
5. Calculate the utility using an additive structure based on these deviations: `U_accept = beta_gain * gain_above_fair - beta_loss * loss_below_fair`.

In this structure, `beta_gain` specifically scales the utility derived from the amount the offer is *above* the fair share (`gain_above_fair`), while `beta_loss` specifically scales the penalty derived from the amount the offer is *below* the fair share (`loss_below_fair`). Since a given offer can only be *either* above or below fair (or exactly fair), these two terms (`gain_above_fair` and `loss_below_fair`) are mutually exclusive (one will always be 0 or both will be 0). This design aims to create highly distinct influence profiles for `beta_gain` (affecting utility only when the offer is generous/fair or above) and `beta_loss` (affecting utility only when the offer is unfair/below fair), thereby improving robust joint parameter identifiability.

Design the model using this specific deviation-based structure: `U_accept = beta_gain * gain_above_fair - beta_loss * loss_below_fair`. Remember to calculate intermediate variables (`sum_tokens`, `fair_perc_self_calc`, `gain_above_fair`, `loss_below_fair`) before the final `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula), using `IF` and `Max`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific additive structure based on deviations from fairness, where `beta_gain` scales only gains *above* fairness and `beta_loss` scales only losses *below* fairness, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles tied to different regions relative to the fairness point.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `gain_above_fair`, `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_gain` (sensitivity/scaling parameter for gains *above* fair share) and `beta_loss` (sensitivity/scaling parameter for losses *below* fair share). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds.
    *   `beta_gain`: Non-negative bounds, e.g., `[0.0, 10.0]`. Must be >= 0.
    *   `beta_loss`: Non-negative bounds, e.g., `[0.0, 10.0]`. Must be >= 0 to represent aversion to loss.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, `gain_above_fair`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""` (using `split_perc_self` as specified).
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is based on deviations from a calculated fair share, scaled separately for gains above and losses below fairness). Crucially, explain *how* this specific deviation-based structure, where `beta_gain` applies only to gains (>= fairness) and `beta_loss` applies only to losses (< fairness), is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles tied to different ranges relative to the fairness point.
*   **Think Outside the Box:** Parameter identifiability for *both* parameters simultaneously remains the persistent, difficult challenge. The multiplicative structure failed, and this new deviation-based structure is an attempt to achieve better separation. If this specific deviation-based structure *still* fails to achieve >= 0.7 recovery for *both* parameters in Run 87, the next iteration *must* consider even more fundamentally different structural approaches. Think about models where parameters might influence decision noise, contribute additively in log-space, define aspiration levels in a different way, or influence the choice process using non-linearities that are more complex than simple linear scaling of deviations. Focus intently on finding a functional form where varying `beta_gain` has a clearly distinguishable effect on the utility curve across offers compared to varying `beta_loss`, allowing them to be estimated independently with high reliability. This is a prerequisite for better fit metrics and meaningful interpretation.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,0.07569834806313874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.07253860123964127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79,64.26110975300276,54.565002683194116,72.94720566970634,0.5604395604395604,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (token_self / sum_tokens) * 100.0 if sum_tokens > 0 else 50.0
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
penalty_term = (loss_below_fair)**beta_unfairness if loss_below_fair > 0.0 else 0.0
U_accept = (beta_value * split_perc_self) / (1.0 + penalty_term)","A divisive utility model predicting acceptance of an offer based on the participant's proposed share relative to a calculated fair share. The utility for accepting is calculated as the product of a base value sensitivity parameter (beta_value) and the proposed percentage share, divided by one plus a penalty term. The penalty term is based on how much the proposed share is below the calculated fair share (loss_below_fair) raised to the power of an unfairness exponent parameter (beta_unfairness). This structure, placing beta_unfairness in the exponent, is hypothesized to create mathematically distinct and interacting non-linear profiles for the parameters' influences, enhancing parameter identifiability compared to previous additive or fixed-power structures in the denominator. The model includes two learnable parameters: beta_value and beta_unfairness, both constrained to [0.0, 100.0].",v0,"
Okay, here are the improved instructions for the next run (Run 79 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 79 of 100), based on the results of recent attempts.

The model tested in the previous run (Run 78), the quadratic-divisive structure `U_accept = (beta_value * split_perc_self) / (1.0 + beta_unfairness * (loss_below_fair)^2)`, achieved the best fit metrics (BIC = 44.12, Accuracy = 0.666) among recent attempts. This reinforces the idea that the divisive principle and the calculation of unfairness based on token contributions (`loss_below_fair`) are valuable.

However, the **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. The previous model failed this critical benchmark for *both* `beta_value` (r = 0.460) and `beta_unfairness` (r = 0.548). A model with poor parameter recovery is unusable, as the estimated parameters cannot be reliably interpreted, regardless of how well it fits the data on average.

This indicates that while the divisive principle and the fairness calculation are useful, the specific quadratic functional form used in the denominator `(1.0 + beta_unfairness * (loss_below_fair)^2)` still did not create sufficiently separable influence profiles to allow for robust identifiability of *both* parameters.

For Run 79, we will continue to build on the divisive structure and the calculation of `loss_below_fair` but will explore a *different alternative non-linear functional form* for the unfairness penalty in the denominator. Instead of multiplying `beta_unfairness` by a fixed power (like the square) or the exponential of `loss_below_fair`, we will now have `beta_unfairness` directly influence the *exponent* of the `loss_below_fair` term.

Proposed Model Structure: A divisive utility model where the core value derived from the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, is *reduced* by a factor based on `loss_below_fair` raised to the power of `beta_unfairness`, in the denominator.

`U_accept = (beta_value * split_perc_self) / (1.0 + (loss_below_fair)**beta_unfairness)`

Here:
Calculate `sum_tokens = token_self + token_opp`.
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
Calculate the penalty term `penalty_term`. If `loss_below_fair == 0.0`, `penalty_term` is 0.0. Otherwise, `penalty_term` is `(loss_below_fair)**beta_unfairness`. Handle this conditional calculation explicitly before the final `U_accept` formula.
The utility `U_accept` is calculated as the product of `beta_value` and `split_perc_self`, divided by the term `(1.0 + penalty_term)`.

In this structure, `beta_value` scales the numerator representing the base value from the offer percentage. `beta_unfairness` now directly influences the *shape* or *curvature* of the penalty curve by acting as the exponent. This structure is hypothesized to create mathematically distinct and more separable influence profiles for the parameters across the range of possible offers compared to previous forms, thereby improving parameter identifiability for *both* `beta_value` and `beta_unfairness`. `beta_value` represents the base sensitivity to the offer percentage, and `beta_unfairness` represents how strongly increasing unfairness causes this sensitivity to be *divided down* with a power-law relationship.

Design the model using this new exponent-scaled-by-parameter divisive structure: `U_accept = (beta_value * split_perc_self) / (1.0 + penalty_term)`, where `loss_below_fair` and `fair_perc_self_calc` are calculated as specified above using `token_self`, `token_opp`, and `split_perc_self`, and handling the `loss_below_fair == 0` case for the `penalty_term` calculation. Remember to calculate intermediate variables before the final `U_accept`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific divisive structure, by placing `beta_unfairness` as the *exponent* for the `loss_below_fair` term in the denominator penalty, is intended to achieve robust parameter identifiability for *both* parameters by creating mathematically distinct and *interacting* influence profiles that are more separable than those generated by previous linear, exponential, or quadratic denominator structures.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, the intermediate `penalty_term` (calculated as `(loss_below_fair)**beta_unfairness` if `loss_below_fair > 0.0`, and 0.0 otherwise), and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Use `**` for exponentiation.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (base value sensitivity) and `beta_unfairness` (unfairness penalty exponent rate). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and likely necessary for parameter recovery in this structure (e.g., `beta_unfairness` should be non-negative so the denominator increases with `loss_below_fair` for `loss_below_fair > 1`, and `beta_value` should be non-negative for utility to increase with `split_perc_self` on fair offers). Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, `loss_below_fair`, and `penalty_term`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (divisive structure where utility from `split_perc_self` is reduced by a penalty based on unfairness raised to a power). Crucially, explain *how* this specific structure, where `beta_unfairness` influences the *exponent* of the `loss_below_fair` term in the denominator, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and *interacting* influence profiles that are more separable than those generated by previous linear, exponential, or quadratic forms, addressing the recovery failures seen in previous models.
*   **Think Outside the Box:** Parameter identifiability is the most difficult challenge. Recognizing that previous attempts with fixed powers (linear, quadratic) and simple exponentials failed to recover both parameters suggests that the *form* of the penalty interaction is critical. Placing a parameter in the exponent is another hypothesis to create distinctly shaped influences. If this structure doesn't work, continue considering how parameters might interact via other novel non-linear functions (logs involving parameters, other inverse functions, different multiplicative or additive components *within* the denominator) that haven't been tried yet, focusing intently on creating distinctly shaped influences for *each* parameter across the range of possible offers to improve their reliable estimation. The goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.",,,,,,,,,,,,0.2684472209219512,,,,,,-0.045866707341935486,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
74,65.60659799594325,55.8950396638781,74.30653566841826,0.5665445665445665,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = MAX(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_value * split_perc_self * EXP(-beta_unfairness * loss_below_fair)","A multiplicative utility model for the responder's acceptance decision. The utility of accepting an offer is calculated as the product of a linear value component, proportional to the proposed percentage share and scaled by 'beta_value', and an exponential decay penalty. This penalty is based on the 'loss_below_fair', which is the percentage deviation of the offer below a fair share calculated from token contributions. The rate of this exponential decay is governed by 'beta_unfairness'. This multiplicative structure, combining a linear value term with an exponential multiplicative penalty based on deviation from fairness, is specifically designed to create mathematically distinct and interacting influence profiles for 'beta_value' and 'beta_unfairness' across different offers. This approach aims to improve robust parameter identifiability by moving beyond simple additive combinations, which have historically failed in this task context.",v0,"Okay, here are the improved instructions for the next run (Run 74 of 100), based on the results of recent attempts.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery.

The model tested in the previous run, an additive utility model with an exponential penalty term (`U_accept = beta_value * split_perc_self - beta_unfairness * exp(loss_below_fair)`), performed extremely poorly on all metrics (Average BIC: 950.97, Accuracy: 0.431). Crucially, parameter recovery was abysmal (beta_value: r=0.180, beta_unfairness: r=0.000). This demonstrates that this specific functional form, despite the hypothesis that the exponential term would create separability, failed entirely to capture the underlying behavior or yield identifiable parameters.

Previous attempts using simple additive structures (`U_accept = beta_value * split_perc_self - beta_unfairness * penalty`) with linear, squared, or ratio-based penalties have also consistently shown poor parameter identifiability (recovery well below 0.7 for at least one parameter, often both). Even the reference-dependent additive structure (`U_accept = beta_value * gain_above_fair - beta_unfairness * loss_below_fair`) in a recent run failed dramatically (recovery r=0.131 and r=0.015).

These repeated failures across various additive structures (simple linear, squared, ratio penalty, exponential penalty, reference-dependent gain/loss) strongly suggest that **a purely additive combination of value and unfairness terms might inherently lead to parameter confounding** in this task, regardless of the specific functional form of the penalty term. The influences of 'value' and 'unfairness' simply added together might be too correlated or indistinguishable across the observed data range.

For Run 74, we must fundamentally reconsider the *structural principle* by which value and unfairness combine to influence utility. Instead of adding or subtracting components, let's explore a **multiplicative or interactive structure**. The goal is to design a model where the influence of one parameter (e.g., unfairness sensitivity) *modulates* or *scales* the influence of the other (e.g., value sensitivity) in a non-additive way. This interaction is hypothesized to create distinct parameter dependencies on the input variables, thereby improving identifiability.

Proposed Model Structure: A multiplicative utility model where the value derived from the offer is discounted or scaled down based on its unfairness using an exponential decay.
`U_accept = beta_value * split_perc_self * exp(-beta_unfairness * loss_below_fair)`

Here:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0).
Calculate `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
Finally, `U_accept = beta_value * split_perc_self * exp(-beta_unfairness * loss_below_fair)`. Note the multiplication and the negative exponential.

In this structure, `beta_value` scales the potential utility from the offered percentage (`split_perc_self`) linearly. `beta_unfairness` scales the *rate of exponential decay* of this potential utility as the offer falls below the fair share (`loss_below_fair`). The term `exp(-beta_unfairness * loss_below_fair)` is a multiplicative factor between 0 and 1 (assuming `beta_unfairness >= 0` and `loss_below_fair >= 0`). A higher `beta_unfairness` or a larger `loss_below_fair` leads to a smaller multiplicative factor, thus reducing the overall utility. The key hypothesis for improved identifiability is that this **multiplicative interaction**, specifically the exponential decay applied to the linear value term, creates a fundamentally different and more separable influence landscape for `beta_value` and `beta_unfairness` compared to all tested additive structures.

Design the model using this multiplicative structure with exponential decay: `U_accept = beta_value * split_perc_self * exp(-beta_unfairness * loss_below_fair)`, where `loss_below_fair` is calculated as specified above using `fair_perc_self_calc` as the reference.

Focus on clearly defining the `loss_below_fair` term and articulating in the summary how this specific structure, by combining a linear value term with an *exponential multiplicative penalty* based on deviation from fair share, is intended to achieve robust parameter identifiability by creating mathematically distinct and interacting influence profiles for `beta_value` and `beta_unfairness` across different offers, addressing the persistent identifiability failures seen in purely additive structures.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include any other necessary intermediate variables.
*   **Model Formula:** Define `U_accept` using the multiplicative structure with exponential decay: `sum_tokens = token_self + token_opp`, `fair_perc_self_calc = (token_self / sum_tokens) * 100.0` (handle `sum_tokens == 0`), `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`, `U_accept = beta_value * split_perc_self * exp(-beta_unfairness * loss_below_fair)`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (value sensitivity) and `beta_unfairness` (unfairness sensitivity decay rate). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and may aid recovery. Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model\'s core idea (multiplicative structure, linear value term scaled by exponential unfairness penalty based on deviation from fair share). Crucially, explain *how* this structure, by using a *multiplicative* exponential decay function for the penalty term applied to the linear value term, is specifically intended to improve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles for `beta_value` and `beta_unfairness`, addressing the limitations and failures of previous purely additive structures.
*   **Think Outside the Box:** Recognize that previous attempts with standard additive structures and simple non-linear forms have consistently failed parameter recovery. The proposed multiplicative exponential decay structure is a deliberate departure, explicitly attempting to create functional separability through interaction. Continue to consider non-obvious ways parameters might interact or influence decisions beyond simple additive combinations, as finding a structure that truly shines in parameter recovery often requires exploring less standard structural principles like multiplication, gating, or thresholds.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters\' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.",,,,,,,,,,,,0.09890255875920674,,,,,,-0.012271453297639054,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
89,71.19183795080365,65.7865605496025,76.03406562271302,0.811965811965812,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_base_sensitivity * split_perc_self - beta_unfairness_penalty * loss_below_fair","The model predicts the utility of accepting a proposal based on a linear combination of the proposed percentage and a penalty term for offers below a calculated fair share. Utility is scaled by a base sensitivity parameter (beta_base_sensitivity) applied to the proposed percentage (split_perc_self). An additional disutility is incurred when the proposed percentage is below the fair share calculated from token contributions; this disutility is scaled by a separate unfairness penalty parameter (beta_unfairness_penalty) and applies only to the difference between the fair share and the proposed share (loss_below_fair). This additive structure, where beta_base_sensitivity influences utility across all offers and beta_unfairness_penalty specifically penalizes offers below the fairness threshold, is designed to create mathematically distinct influences tied to different regions relative to the fairness point, thereby improving robust joint parameter identifiability for both beta parameters.",v0,"Okay, here are the improved instructions for the next run (Run 89 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,
      ""source"": ""calculated""
    },\\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,\\\
      ""source"": ""data""
    },\\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1,\
        ""exclusive_max"": true
      },\\\
      ""learnable"": false,
      ""source"": ""data""
    },\\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {\
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\\
      ""learnable"": false,
      ""source"": ""calculated""
    }\
  }\
}\
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 89 of 100).

Previous attempts, including the log-transformed deviation model from the last run, have consistently failed to achieve robust parameter identifiability (parameter recovery >= 0.7 for *all* learnable parameters simultaneously). This remains the **unwavering primary objective**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation and overrides achieving a low BIC or high accuracy if recovery fails. We need to find a structure where the influence of the two learnable parameters is sufficiently distinct across trials to be estimated independently with high reliability.

Given the persistent identifiability challenge with previous linear, multiplicative, and simple non-linear deviation approaches, we must explore a **fundamentally different structural approach** for this run.

Proposed Model Structure: Let's design a model where one parameter captures a general sensitivity to the proposed percentage, and a second parameter specifically modifies sensitivity when the offer is perceived as unfair. This structure aims to create distinct parameter influences based on whether the offer is below or above the calculated fair share.

Here are the calculation steps for this specific structure:
1. Calculate `sum_tokens = token_self + token_opp`.
2. Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.
3. Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is positive if the offer is below the fair share, and 0 otherwise. Use `Max` for this calculation.
4. Calculate the utility using the proposed additive structure: `U_accept = beta_base_sensitivity * split_perc_self - beta_unfairness_penalty * loss_below_fair`.

In this structure, `beta_base_sensitivity` scales the utility derived from the raw `split_perc_self`, representing a general preference for higher offers. `beta_unfairness_penalty` scales an *additional* penalty term (`loss_below_fair`) that is only active when the offer is below the fair share. This creates a piecewise linear utility function relative to the `fair_perc_self_calc` point, with `beta_base_sensitivity` determining the slope above fair, and (`beta_base_sensitivity + beta_unfairness_penalty`) effectively determining the slope below fair. This specific piecewise structure, where one parameter affects the utility across all offers while the second adds a specific influence only below the fairness threshold, is intended to improve robust joint parameter identifiability by giving the parameters mathematically separable influence profiles tied to different ranges relative to the fairness point.

Design the model using this specific structure: `U_accept = beta_base_sensitivity * split_perc_self - beta_unfairness_penalty * loss_below_fair`. Remember to calculate intermediate variables (`sum_tokens`, `fair_perc_self_calc`, `loss_below_fair`) before the final `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula), using `IF`, `Max`, etc.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific additive structure, where utility is based on the proposed percentage itself (`split_perc_self`) and includes an additional penalty term (`loss_below_fair`) specifically for offers below fairness, is intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles tied to different regions relative to the fairness point.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_base_sensitivity` (scaling parameter for the proposed percentage) and `beta_unfairness_penalty` (additional penalty parameter for losses below the fair share). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds.
    *   `beta_base_sensitivity`: Non-negative bounds, e.g., `[0.0, 10.0]`. Should be >= 0, as higher percentage generally means higher utility.
    *   `beta_unfairness_penalty`: Non-negative bounds, e.g., `[0.0, 10.0]`. Should be >= 0 to represent an aversion/penalty scaled by `loss_below_fair`.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is based on a general sensitivity to the proposed percentage, with an *additional* penalty for offers below a calculated fair share, scaled by a separate parameter). Crucially, explain *how* this specific structure (`beta_base_sensitivity` scaling `split_perc_self` universally, `beta_unfairness_penalty` scaling `loss_below_fair` only below the fair threshold) is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles tied to different ranges relative to the fairness point.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7550817162105672,,,,,,,,,0.6135223050847003,,,,,,,,,,,,,
7,76.38915916657477,67.50695941962385,84.34612977321831,0.8211233211233212,"beta_gain * split_perc_self - gamma_unfairness * max(0, fair_perc_self - split_perc_self)","A utility model for responder behavior based on the received offer percentage and its deviation from a contribution-based fair share. Utility increases linearly with the received percentage scaled by beta_gain. A penalty, scaled by gamma_unfairness, is applied linearly for offers falling below the calculated fair percentage (fair_perc_self). The penalty is zero otherwise. This piecewise structure, with the fair share as a dynamic reference point, aims to improve parameter identifiability.",v0,"Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Achieve excellent parameter recovery (>= 0.7 correlation) for ALL learnable parameters.** This remains the *single most important* objective. Previous attempts have failed here, including the most recent one. A model is unusable if any learnable parameter cannot be reliably recovered, regardless of other metrics. Prioritize model structures that inherently enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables available during responder trials (trial_role == 1).
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

**Analysis of Previous Attempt (Run 6):** The most recent model (using `beta_gain`, `gamma_50`, `gamma_prop` with a linear gain and quadratic penalties below 50% and proportional norms) failed parameter recovery catastrophically (`beta_gain: r=0.142`, `gamma_50: r=0.113`, `gamma_prop: r=-0.052`). Despite using quadratic penalties, the parameters' influences were too correlated or easily substitutable given the data. The model's structure did not ensure distinct contributions from each parameter to the overall utility calculation. The accuracy (0.534) and BIC (783.00) were also poor, likely a consequence of the ill-defined parameters.

**Guidance for the Next Model (Radically Prioritizing Parameter Recovery):** The primary challenge is designing a utility function where each learnable parameter has a unique and non-overlapping impact on utility across the range of possible offers and fairness norms. Simple additive structures, even with non-linear features, have repeatedly failed to yield identifiable parameters in this task. We need a fundamentally different approach to structure the model equations.

*   **Design for Distinct Parameter Mechanisms:** Instead of merely scaling different terms or deviations, think about how parameters can control qualitatively different aspects of the utility function's shape or structure. For instance:
    *   Could one parameter define a *threshold* below which a penalty applies, while another controls the *steepness* of that penalty?
    *   Could parameters control *exponents* or other non-linear transformations applied to `split_perc_self` or fairness deviations, rather than just scaling them linearly or quadratically? For example, `Utility = split_perc_self^beta_exponent - gamma * deviation`.
    *   Could parameters modulate how different factors *interact multiplicatively*? E.g., `Utility = split_perc_self * (1 - gamma_fairness * deviation)`.
    *   Consider piecewise functions where parameters control the utility function *within specific ranges* of `split_perc_self` relative to the norms (e.g., below 50%, between 50% and proportional, above proportional).
*   **Focus on the Uniqueness of Each Parameter's ""Signature"":** Ensure that changing the value of one parameter produces a change in utility that cannot be mimicked by simultaneously adjusting the others. This often requires parameters to influence utility through different mathematical operations or in different regions of the input space.
*   **Leverage Percentages:** Continue to build the model using percentage-based values (`split_perc_self`, `fair_perc_self`).
*   **Simplicity Through Structure:** Aim for the minimum number of learnable parameters necessary *if and only if* their effects are clearly separable and identifiable. A model with fewer, identifiable parameters is vastly preferred over one with many, unidentifiable ones.

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded (-inf to inf).

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1) and use variables available in that context. The `fair_perc_self` variable (calculated as `(token_self / (token_self + token_opp)) * 100`) is useful for fairness calculations.

For run 7 of 100, please think through this step by step, focusing intently on how the *specific mathematical structure* of the model guarantees parameter identifiability and distinguishes the influence of each parameter, minimizing covariance based on the *specific* data characteristics. How can you define learnable parameters that control fundamentally different aspects of the utility function (e.g., slope, threshold, exponent, interaction weight) rather than just scaling additive components? Then provide your model specification, variable descriptions, target variable, and summary.",,,,,,,,,,,,,,0.5761885336274151,,,0.6376217310378662,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53,76.6903033771508,86.93544942141648,67.51236004582947,0.6361416361416361,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (sum_tokens > 0) ? (token_self / sum_tokens) * 100.0 : 50.0
perc_difference = split_perc_self - fair_perc_self_calc
abs_perc_difference = abs(perc_difference)
U = perc_difference >= 0 ? beta_gain_scale * split_perc_self : beta_loss_penalty * abs_perc_difference","A piecewise linear utility model for responder behavior. Utility (U) is calculated based on the proposed percentage share (split_perc_self) relative to the participant's fair share (fair_perc_self_calc), derived from token contributions. When the proposed share is fair or favorable (percentage difference >= 0), utility is linearly proportional to split_perc_self, scaled by the learnable parameter beta_gain_scale. When the proposed share is unfair (percentage difference < 0), utility is linearly penalized based on the absolute deviation from fairness, scaled by the learnable parameter beta_loss_penalty (expected to be negative). This piecewise structure, applying the two learnable parameters to scale different input terms (split_perc_self vs. absolute percentage difference) in distinct decision regimes (fair/favorable vs. unfair), is designed to enhance parameter identifiability. The utility is converted to an acceptance probability using a logistic function.",v0,"Okay, here are the improved instructions for the next run (Run 53), building on the results of the previous runs.

Instructions for Run 53 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 52) implemented a piecewise linear model based on fairness difference (`U = beta_value * split_perc_self + beta_unfairness_penalty * MIN(0.0, perc_difference)`). While achieving decent accuracy (0.763) and a fair BIC (311.86), it unfortunately failed the parameter recovery test for `beta_unfairness_penalty` (r = 0.561). `beta_value` recovered well (r = 0.768).

Analysis of Run 52's failure: The structure, while piecewise on the fairness difference, had `beta_value` scaling `split_perc_self` across all trials (implicitly or explicitly), while `beta_unfairness_penalty` scaled a term (`MIN(0.0, perc_difference)`) that is linearly related to `split_perc_self` and `fair_perc_self_calc` only in the ""unfair"" regime, and is zero otherwise. This likely led to confounding influences between `beta_value` and `beta_unfairness_penalty` in the ""unfair"" regime, preventing `beta_unfairness_penalty` from being uniquely identifiable from `beta_value`'s influence on `split_perc_self`. The parameters' roles were not sufficiently structurally independent across the data to allow for reliable separate estimation of `beta_unfairness_penalty`.

Recall the relative success of Run 1 (BIC 40.53, Acc 0.671, Recovery >= 0.727 for both parameters), which used a piecewise structure on the *ratio* (`R_val`) and applied *different functional forms* (log vs. linear difference) scaled by the two parameters (`beta_gain_log` and `beta_loss_linear_diff`) in distinct gain vs. loss regimes. This structure seems to have created more distinct influences for the two parameters.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** such that *both* parameters have truly distinct and identifiable influences on responder decisions (`trial_role == 1`). This means designing the model such that the parameters' effects are mathematically separable.

Based on this analysis, focus your exploration for Run 53 on model structures where the two learnable parameters scale terms that are mathematically distinct from each other, or whose influence is primarily expressed in distinct parts of the decision space. This can be effectively achieved via:

*   **Piecewise Structures with Orthogonal Parameter Roles per Regime:** Continue exploring piecewise functions based on the fairness point (e.g., `R_val >= 1`, `R_diff >= 0`, or `perc_difference >= 0`). The key is to assign one learnable parameter to scale a specific term or function *exclusively or primarily* in one regime (e.g., gains/favorable offers), and the *other* learnable parameter to scale a *different* term or function *exclusively or primarily* in the other regime (e.g., losses/unfair offers). This ensures the parameters' main influences are in different parts of the data space. Avoid structures like Run 52 where one parameter scales a common input across all data while the other scales a modified input in only one regime; instead, aim for parameters to scale terms that are functionally distinct and active in different, separated regions.
    *   *Structural Principle:* Design the model as `U = IF(Condition based on fairness, beta1 * f1(Inputs), beta2 * f2(Inputs))`, where `f1` and `f2` are different functions of the relevant inputs (like `split_perc_self`, `fair_perc_self_calc`, `R_val`, `R_diff`, etc.). This ensures `beta1` influences utility through `f1` only when the condition is met, and `beta2` influences utility through `f2` only when the condition is *not* met.
    *   *Example Ideas:*
        *   `U = IF(R_val >= 1.0, beta_gain_sensitivity * log(R_val + 1e-6), beta_loss_slope * (R_val - 1.0))` (Similar to Run 1, perhaps experiment with slightly different functions).
        *   `U = IF(split_perc_self >= fair_perc_self_calc, beta_pos_gain * split_perc_self, beta_neg_fairness * (fair_perc_self_calc - split_perc_self)^2)` (One parameter scales proposed share for gains, other scales squared fairness difference for losses - using squared difference helps make it non-linear).
        *   `U = IF(perc_difference >= 0, beta_good_offer * split_perc_self, beta_bad_offer * perc_difference)` (This is similar to Run 52, but perhaps re-evaluate if the linear terms were the issue - unlikely, but worth noting).
        *   `U = IF(perc_difference >= 0, beta_value_gain * sqrt(split_perc_self), beta_penalty_loss * abs(perc_difference))` (Using non-linear transformations like sqrt and absolute difference).
*   **Single Non-Linear Function with Orthogonal Parameter Roles:** If not using a piecewise structure, design a single non-linear function of the inputs (`split_perc_self`, `fair_perc_self_calc`, ratio, difference) such that the two parameters affect mathematically distinct properties of the function's shape or response. For example, one parameter could control the overall scale or sensitivity, while the other parameter controls an asymmetry, curvature, or a threshold effect in a non-linear way.

*   **Avoid:** Any model that simplifies to `U = c1 * Input1 + c2 * Input2` where `c1` and `c2` are simple combinations of `beta1` and `beta2`. Avoid structures where the terms scaled by `beta1` and `beta2` are highly correlated or linearly dependent across the dataset. This was the likely downfall of Run 52.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens = token_self + token_opp` and `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)`. Calculate intermediate variables like ratios (`R_val = split_perc_self / fair_perc_self_calc`), differences (`perc_difference = split_perc_self - fair_perc_self_calc`), or any other transformations (log, power, exp, sqrt, abs) as needed for your chosen structure.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-20.0, 20.0] or wider/narrower if appropriate for the variable scale).

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, any intermediate terms like ratios/differences/transformed values, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` includes the `sum_tokens > 0` condition.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the proposed percentage share for self.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, how its specific mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters (e.g., by having them scale distinct terms in different regimes, or influence separate, non-linearly related aspects of the utility calculation).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This requires making their contributions to utility as distinct and non-overlapping as possible, often by having them scale different, non-linearly related terms or influence different regimes of the decision space. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.30210031590250314,,,,,,,,,,,,,,,,,,,,0.05795372994803636,,,,,,,,,,,,,,,,,,,,,
22,80.82105696462087,77.18538864093398,84.0780098379237,0.7701465201465201,U = -slope_unfair_deviation * unfair_deviation + slope_generous_deviation * generous_deviation + acceptance_bias,"This model predicts the utility of accepting a proposed offer based on how the offer percentage deviates from the participant's fair share, calculated from token contributions. Utility decreases linearly with the magnitude of unfair deviations, increases linearly with the magnitude of generous deviations, and is adjusted by a general acceptance bias. The model has three learnable parameters: a non-negative slope for unfairness sensitivity, a non-negative slope for generosity sensitivity, and an acceptance bias parameter.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions:
In the previous run (Run 21), the model `U = sensitivity_base * split_perc_self - sensitivity_unfair * unfair_deviation + sensitivity_generous * generous_deviation` achieved good accuracy (0.832) but failed to achieve sufficient parameter recovery for all learnable parameters (`sensitivity_base`: r = 0.599, `sensitivity_unfair`: r = 0.613, `sensitivity_generous`: r = 0.053). The generosity sensitivity parameter showed particularly poor identifiability. This suggests that while the overall structure might capture some aspects of the data, the specific way the parameters are defined and interact in this additive linear form leads to issues in uniquely estimating them.

Comparing to earlier runs, Run 3 (`U = sensitivity_offer * split_perc_self - penalty_magnitude_unfairness * max(0, fair_perc_self - split_perc_self)`) had a better BIC (137.14) and slightly better recovery for `sensitivity_offer` (r = 0.620), although `penalty_magnitude_unfairness` (r = 0.541) was still below the target. Run 21's attempt to separate the generosity term did not improve recovery for the other terms and worsened BIC compared to Run 3. This indicates that linearly adding a sensitivity to the raw offer percentage alongside separate sensitivities for deviations might cause collinearity, especially between the base sensitivity and the generous deviation sensitivity.

For this run (Run 22), the primary goal remains to achieve parameter recovery >= 0.7 for *all* learnable parameters while maintaining good BIC and accuracy. To address the potential identifiability issues observed in Run 21, let's explore a model structure that removes the direct linear sensitivity to `split_perc_self` and instead focuses purely on how utility changes based on deviations from the calculated fair share, potentially including an overall bias term.

Consider a new model structure with three learnable parameters, focusing on deviations from fairness:
`U = -slope_unfair_deviation * max(0, fair_perc_self - split_perc_self) + slope_generous_deviation * max(0, split_perc_self - fair_perc_self) + acceptance_bias`

This model includes:
1.  `slope_unfair_deviation`: Represents the rate at which utility decreases for each percentage point the offer is below the fair share (`fair_perc_self`). A higher value means a stronger penalty for unfairness. This parameter should be non-negative. The term `max(0, fair_perc_self - split_perc_self)` captures the magnitude of the *unfair* deviation.
2.  `slope_generous_deviation`: Represents the rate at which utility increases for each percentage point the offer is above the fair share (`fair_perc_self`). A higher value means a stronger bonus for generosity. This parameter should be non-negative. The term `max(0, split_perc_self - fair_perc_self)` captures the magnitude of the *generous* deviation.
3.  `acceptance_bias`: Represents a general tendency to accept offers, independent of the specific offer value or its fairness. This parameter can be positive or negative (unbounded conceptually).

This structure posits that utility is primarily determined by how the offer *deviates* from fairness, with potentially different sensitivities (slopes) for unfairness and generosity, plus a baseline propensity to accept. By removing the `sensitivity_base * split_perc_self` term from Run 21, we aim to reduce potential collinearity and improve the identifiability of the deviation slopes and the bias term.

**Guidance for the Next Model (Run 22):**

Design a computational model implementing this specific three-parameter structure. The primary goal is to achieve parameter recovery >= 0.7 for *all* learnable parameters (`slope_unfair_deviation`, `slope_generous_deviation`, `acceptance_bias`) while maintaining good BIC and accuracy.

*   **Prioritize Parameter Recovery:** Implement the specified structure precisely to test if a purely deviation-based model (plus bias) improves the identifiability of the parameters compared to models that include a linear term for the raw offer percentage.
*   **Parameter Constraints:**
    *   `slope_unfair_deviation`: Define clear, generous, finite, non-negative bounds. Suggest [0, 20].
    *   `slope_generous_deviation`: Define clear, generous, finite, non-negative bounds. Suggest [0, 20].
    *   `acceptance_bias`: Define clear, generous, finite bounds. While conceptually unbounded, provide a generous finite range like [-10, 10].
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using the suggested structure: `U = -slope_unfair_deviation * max(0, fair_perc_self - split_perc_self) + slope_generous_deviation * max(0, split_perc_self - fair_perc_self) + acceptance_bias`. Ensure the formula includes the necessary variables and parameters.
*   **Calculated Variables:** Remember `fair_perc_self` needs to be calculated as `(token_self / (token_self + token_opp)) * 100`. Define `unfair_deviation = max(0, fair_perc_self - split_perc_self)` and `generous_deviation = max(0, split_perc_self - fair_perc_self)` as calculated variables to potentially simplify the model equation representation (though the previous explicit formula is also fine). Include `U`, `fair_perc_self`, `token_self`, `token_opp`, `split_perc_self`, `unfair_deviation`, and `generous_deviation` in your variable descriptions with `source: ""calculated""` or `source: ""data""` as appropriate.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for all variables used in the <MODEL> formula (`U`, `split_perc_self`, `fair_perc_self`, `token_self`, `token_opp`, `unfair_deviation`, `generous_deviation`, `slope_unfair_deviation`, `slope_generous_deviation`, `acceptance_bias`), including the specified bounds and learnable status for the parameters. Clearly describe the role of each variable.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure and its components.

Remember to use variables relevant for responder trials (`trial_role == 1`).

This shift towards a deviation-centric model is an attempt to build on the insights from previous runs while exploring a functional form that might improve parameter identifiability. If this structure still doesn't yield sufficient parameter recovery for all parameters, future runs will need to consider other fundamental shifts in how utility is modeled, such as non-linear effects of deviations, multiplicative interactions between offer and fairness, or entirely different value functions.

For run 22 of 100, please think through this step by step, focusing intently on implementing the specified deviation-based utility model structure. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.38266789114355027,-0.027307691897490896,0.6296570978244231,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
67,83.6258835006044,73.84646156006404,92.3866156556718,0.67002442002442,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (sum_tokens > 0) ? (token_self / sum_tokens * 100.0) : 50.0
raw_penalty = (fair_perc_self_calc > 0) ? (1.0 - (split_perc_self / fair_perc_self_calc)) : 0.0
bounded_ratio_penalty_term = max(0.0, raw_penalty)
U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * bounded_ratio_penalty_term)","A computational model predicting responder acceptance decisions using a ratio utility function. The utility (U) of accepting an offer is calculated as the proposed percentage share (scaled by beta_value) divided by a term reflecting the sensitivity to unfairness (1.0 plus a bounded ratio-based penalty scaled by beta_unfairness). The bounded ratio-based penalty term quantifies how much the offer falls short of the calculated fair share percentage, relative to the fair share, being zero for fair or generous offers and increasing towards one for increasingly unfair offers, explicitly handling cases where the fair share is zero. This ratio structure, combined with a penalty based on relative and bounded unfairness, is designed to create distinguishable influences for the beta_value (scaling the overall value component) and beta_unfairness (scaling the unfairness penalty component) across the range of possible offers, aiming to improve robust parameter identifiability.",v0,"Okay, here are the improved instructions for the next run (Run 67), building on the analysis of previous attempts and focusing intensely on achieving robust parameter identifiability for *both* learnable parameters.

Instructions for Run 67 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery.

Run 66 attempted an additive model structure `U = beta_value * split_perc_self - beta_unfairness * ratio_penalty_term` using a ratio penalty term defined as `max(0.0, (fair_perc_self_calc / safe_split_perc_self) - 1.0)`. This model performed poorly across the board: high BIC (527.89), moderate accuracy (0.656), and critically, terrible parameter recovery for *both* `beta_value` (r=0.293) and `beta_unfairness` (r=0.052). This suggests that the additive structure combined with this specific, potentially unbounded ratio penalty term did not effectively separate the influences of the two parameters, possibly due to numerical instability or high correlation between the terms being scaled.

In contrast, Run 63, which used a *ratio* model structure (`U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * unfairness_penalty_term)`) with a *difference-based* penalty term (`max(0.0, fair_perc_self_calc - split_perc_self)`), showed significantly better performance: lower BIC (46.17), higher accuracy (0.691), and better parameter recovery, particularly for `beta_value` (r=0.925). While `beta_unfairness` recovery (r=0.608) in Run 63 was still just shy of the 0.7 target, it was the best recovery for the unfairness parameter seen in models using a difference-based penalty term.

This suggests that a *ratio structure* might be more conducive to parameter identifiability for this task than additive structures. However, the simple difference penalty term used in Run 63 might still have too much correlation with `split_perc_self` to allow `beta_unfairness` to be fully disentangled from `beta_value`.

For Run 67, let's combine the promising *ratio structure* from Run 63 with a *different approach to the unfairness penalty term* – specifically, a ratio-based penalty, but one that is **bounded** and potentially less correlated with the value term than the simple difference.

Let's define the penalty based on the *relative* shortfall: `1 - (offered_share / fair_share)`, applicable only when the offer is less than fair. This term naturally ranges from 0 (when offered == fair) up to 1 (when offered == 0 and fair > 0).

Proposed Model Structure: A ratio model where the utility is the scaled value term divided by a term that increases with unfairness.
`U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * BoundedRatioPenaltyTerm)`

Here, `BoundedRatioPenaltyTerm` should be 0 for fair or generous offers and increase as the offer becomes more unfair *relative to the fair share*. A suitable definition that is bounded between 0 and 1 (when `split_perc_self >= 0`) is `max(0.0, 1.0 - (split_perc_self / fair_perc_self_calc))`. Special care must be taken when `fair_perc_self_calc` is 0 (which happens when `token_self` is 0 and `token_opp` > 0). If `fair_perc_self_calc` is 0, any non-negative offer is ""fair"" or ""generous"" relative to a 0 fair share, so the penalty should be 0.

The goal is to design a utility calculation `U` involving `split_perc_self`, `fair_perc_self_calc` (derived from `token_self`, `token_opp`), and exactly two learnable parameters (`beta_value`, `beta_unfairness`). Design the model using the ratio structure `U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * BoundedRatioPenaltyTerm)` where `BoundedRatioPenaltyTerm` is calculated as `max(0.0, 1.0 - (split_perc_self / fair_perc_self_calc))` but is explicitly set to 0 if `fair_perc_self_calc` is 0.

Focus on clearly defining this new, bounded ratio penalty term and articulating how its unique mathematical relationship with the input variables (compared to the simple difference penalty or the unbounded ratio penalty from Run 66) is intended to help disentangle the influences of `beta_value` and `beta_unfairness` and improve parameter identifiability within the ratio structure.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0` defaulting to 50.0), and a new `bounded_ratio_penalty_term` based on `max(0.0, 1.0 - (split_perc_self / fair_perc_self_calc))`, explicitly handling the case where `fair_perc_self_calc` is 0 (setting the penalty to 0 in that case). Include any other necessary intermediate variables.
*   **Model Formula:** Define `U` using the ratio structure: `U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * BoundedRatioPenaltyTerm)`. Include calculation steps for derived variables *before* the formula for `U`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` and `beta_unfairness`. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate (higher `beta_value` increases utility for better offers, higher `beta_unfairness` increases the denominator penalty for unfairness) and may aid recovery.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including U, sum_tokens, fair_perc_self_calc, and your chosen bounded ratio-based penalty term variable) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (ratio structure, value numerator, penalty denominator). Crucially, explain *how the definition of the penalty term based on the *bounded ratio* of offered share to fair share (instead of the simple difference or unbounded ratio) within the denominator of the ratio structure is intended to improve robust parameter identifiability for *both* learnable parameters*, particularly `beta_unfairness`, by creating distinct mathematical influences for the two parameters across the range of offers. Explain why this approach builds on the relative success of the Run 63 ratio model while attempting to address its parameter recovery limitations by using a better-behaved penalty term.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative in how you define and utilize the bounded ratio-based penalty term to maximize this distinction within the ratio structure. Find a model structure that truly shines in parameter recovery.",,,,,,,,,,,,-0.02936442370085266,,,,,,0.034255643253828606,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62,84.02767447545435,81.87546271193366,85.95569751360829,0.5439560439560439,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF sum_tokens == 0 THEN 50.0 ELSE (token_self / sum_tokens) * 100.0 END IF
deviation = fair_perc_self_calc - split_perc_self
max_deviation_positive = max(0, deviation)
U = beta_value * split_perc_self * exp(-beta_unfairness * max_deviation_positive)","This model calculates the utility of accepting a proposed offer for the participant acting as a responder. Utility is based on the participant's proposed percentage share (split_perc_self) and the perceived fairness of the offer, derived from token contributions (token_self, token_opp). The model uses two learnable parameters. beta_value linearly scales the perceived value of the proposed share. beta_unfairness controls an exponential decay factor that is applied multiplicatively to this value, but *only* when the offer is unfair (split_perc_self < fair_perc_self_calc). The rate of this exponential decay increases with beta_unfairness and the degree of unfairness. This multiplicative, conditional, and non-linear influence of the unfairness parameter is intended to be mathematically distinct from the linear scaling by beta_value, thus improving the identifiability of both parameters compared to simple additive penalty structures where the second parameter merely scales a deviation term subtracted from a value term.",v0,"Okay, here are the improved instructions for the next run, focusing intensely on designing a model structure that ensures parameter identifiability for *both* learnable parameters while building on the successes and addressing the failures of the previous attempts.

Instructions for Run 62 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 61) implemented a model (`U = beta_value * split_perc_self - beta_unfairness * max(0, fair_perc_self_calc - split_perc_self)^3`) based on the suggestion to use a linear value term and a **strongly non-linear (cubic), asymmetric unfairness penalty**. This model achieved a significantly lower BIC (44.74) than many previous attempts, indicating good fit to the data, and yielded excellent parameter recovery for `beta_value` (r = 0.917). However, it critically failed to achieve sufficient parameter recovery for `beta_unfairness` (r = -0.172). The negative recovery correlation is a strong indicator that this specific non-linear structure, while fitting the data well and isolating the value parameter, does *not* result in a recoverable estimate for the fairness parameter.

This demonstrates that simply adding a non-linear penalty term, even one as strong as cubic, is not sufficient to guarantee identifiability for the second parameter when combined additively/subtractively with a linear value term. We need a more sophisticated approach where the influence of the second parameter is mathematically distinct from the first across the data space, particularly for unfair offers, but in a way that allows for reliable estimation.

The challenge for this run is to leverage the insights from previous successes (e.g., the value term often has better recovery when structured linearly, and lower BIC suggests the combination of value and fairness is important) while fundamentally redesigning the second parameter's role to ensure its identifiability. The simple additive/subtractive structure `U = beta_value * ValueTerm ± beta_unfairness * PenaltyTerm` where PenaltyTerm is a function of deviation seems insufficient.

We need to find a mathematical structure for utility, still using exactly **2 learnable parameters**, where their influence on responder decisions (`trial_role == 1`) is profoundly separable and distinguishable. Given the failure of the cubic penalty term, consider strategies that move beyond simply scaling a pre-calculated non-linear deviation:

Think **""Out of the Box""**: You have a tendency to propose models that are variations of `Value + Penalty`. While this class of models is relevant, the specific implementations tried so far have failed on parameter recovery for the second parameter. The negative recovery for `beta_unfairness` is a clear sign that the cubic penalty term is not working as intended for identifiability. Explore structures where the two parameters interact with the variables in a fundamentally different mathematical way, or where the second parameter's influence is integrated into the model structure differently.

Consider alternative strategies for incorporating the second parameter, focusing on mathematical distinctiveness from the first parameter's influence (likely still tied to `split_perc_self`):

*   **Non-linear Weighting/Modulation:** Instead of scaling a separate penalty term, could the second parameter **non-linearly modulate the weight or sensitivity** of the `split_perc_self` term itself based on fairness? For example, `U = beta_value * split_perc_self * f(fair_perc_self_calc, split_perc_self, beta_unfairness)`, where `f` is a non-linear function that is 1 when the offer is fair/favorable and decreases rapidly as unfairness increases, controlled by `beta_unfairness`. This integrates the fairness sensitivity directly into the value term's contribution.
*   **Ratio-based Utility:** Can utility be structured based on ratios of `split_perc_self` and `fair_perc_self_calc` directly, perhaps scaled by the two parameters? E.g., `U = beta_value * log(split_perc_self + epsilon) + beta_unfairness * log((split_perc_self + epsilon) / (fair_perc_self_calc + epsilon))` or similar ratio-based forms that inherently capture relative value and fairness, each scaled by a different parameter.
*   **Hybrid Non-linearities:** Combine different non-linear functions in a novel way. For instance, one parameter scales a term involving an exponential or logarithmic function of the deviation/ratio, while the other scales a simpler function.
*   **Threshold-Influencing Parameter:** While predicting utility, could one parameter influence the ""baseline"" utility and the other parameter influence how far below fairness an offer must be before a strong, non-linear rejection pressure kicks in? This might involve piecewise functions with non-linear slopes or intercepts influenced by the second parameter.

The goal is to design a utility calculation `U` involving `split_perc_self`, `fair_perc_self_calc` (derived from `token_self`, `token_opp`), and exactly two learnable parameters (`beta_X`, `beta_Y`) such that the influence of `beta_X` on `U` is as mathematically independent from the influence of `beta_Y` as possible across the range of `split_perc_self` and `fair_perc_self_calc` values in the data, particularly focusing on making the second parameter's effect pronounced and non-linear specifically in the unfair regime, but in a way that *is* recoverable.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and any other necessary intermediate variables. Ensure `fair_perc_self_calc` handles `sum_tokens == 0` correctly (defaulting to 0.5). Use a small epsilon (`1e-6` or similar) for numerical stability if needed (e.g., denominators, logarithms).
*   **Model Formula:** Define `U`. Include calculation steps for derived variables *before* the formula for `U`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds (e.g., [-10.0, 10.0], [-50.0, 50.0], [0.0, 100.0], etc. appropriate for the scale of their influence).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including U) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific non-linear, asymmetric, or modulated mathematical structure is fundamentally different from simple additive/subtractive penalties and how this difference is intended to ensure robust parameter identifiability for *both* learnable parameters*. Explicitly contrast it with the failed cubic penalty approach and explain why the new structure should allow for better recovery of the second parameter.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. The excellent BIC and value recovery of the previous run are positive signs, but the second parameter's failure means that specific structure isn't the final answer. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative and explore structures that move beyond simple penalty scaling.",,,,,,,,,,,,-0.07266064044078871,,,,,,0.5713320801176489,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43,87.23791533778132,62.52366427067621,109.37776525206299,0.7796092796092796,"sum_tokens = token_self + token_opp
fair_perc_self = (token_self / sum_tokens) * 100.0 if sum_tokens > 0 else 50.0
epsilon_val = 1e-6
U = beta_split_sens * log(split_perc_self + epsilon_val) + beta_fair_sens * log(fair_perc_self + epsilon_val)","The model predicts the utility of accepting an offer based on a weighted sum of the natural logarithms of the proposed percentage share to the participant and the calculated fair percentage share. It utilizes two learnable sensitivity parameters: one scaling the influence of the logarithm of the proposed share (beta_split_sens) and one scaling the influence of the logarithm of the fair share (beta_fair_sens). A small constant (epsilon_val) is added to the percentages before taking the logarithm to handle zero values. This non-linear, separated logarithmic structure aims to improve the identifiability and recovery of the two learnable parameters by creating distinct contributions to the overall utility compared to simpler linear or ratio-based models. The utility is then transformed into a probability of acceptance using a logistic function.",v0,"Okay, here are the improved instructions for the next run (Run 43), building on the results of Run 42.

Instructions for Run 43 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery.

The previous run (Run 42) explored a model based on the ratio of the proposed share to the fair share (`U = beta_ratio_sens * R + beta_baseline`). This structure achieved the lowest BIC (31.19) and highest accuracy (0.798) so far, indicating a promising fit to the data. However, parameter recovery was mixed: `beta_baseline` recovered well (r = 0.916), but `beta_ratio_sens` failed significantly (r = 0.393). This suggests that while the ratio concept captures important variance, the *linear* relationship assumed by the model likely couples the influence of the sensitivity parameter (`beta_ratio_sens`) with the baseline utility (`beta_baseline`) in a way that prevents reliable, independent estimation from the available data.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`). Linear combinations, piecewise deviations, and simple linear relationships with derived ratios have shown issues with simultaneous parameter recovery for *both* parameters.

For this run, let's retain the use of the participant's proposed share (`split_perc_self`) and the calculated fair share (`fair_perc_self`), but explore a completely different mathematical form for utility that is *non-linear* with respect to these inputs and structured to potentially decouple the parameters' effects.

Design a computational model that predicts the utility of accepting an offer based on a **logarithmic relationship** with the proposed percentage share and the fair percentage share. This departs significantly from linear or ratio-linear models and might offer the necessary structural separation for improved parameter identifiability.

*   **Promising Structural Direction (Logarithmic):** Consider modeling utility (`U`) as a weighted sum of the logarithms of the proposed share and the fair share. This allows for diminishing sensitivity to percentage changes as values increase and treats the influence of the offer and the fair share somewhat independently.
    *   Calculate the necessary intermediate variables:
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Note: This calculation handles the case where `sum_tokens` is 0, setting `fair_perc_self` to 50.0. `fair_perc_self` will always be >= 0).
    *   Structure the utility (`U`) based on the logarithms of `split_perc_self` and `fair_perc_self` and your two learnable parameters. A promising form is:
        `U = beta_split_sens * log(split_perc_self + epsilon) + beta_fair_sens * log(fair_perc_self + epsilon)`
        *   You will need to define a small constant `epsilon` (e.g., 1e-6) to handle cases where `split_perc_self` or `fair_perc_self` might be zero, as `log(0)` is undefined. Although `fair_perc_self` is handled to be 50 when `sum_tokens=0`, `split_perc_self` can be 0.
    *   This structure models utility based on independent sensitivities to the *logarithm* of the offered share and the *logarithm* of the fair share, scaled by `beta_split_sens` and `beta_fair_sens` respectively. This attempts to identify the influence of the offer magnitude separate from the influence of what is perceived as fair, using a non-linear transformation that could aid recovery.

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens` and `fair_perc_self` as described above.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.
    *   Define `epsilon` as a constant within your model or variables description.

*   **Model Formula:** Define the utility (`U`) based on the logarithms of `split_perc_self` and `fair_perc_self`, your 2 learnable parameters, and the `epsilon` constant. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   `beta_split_sens`: Sensitivity to the logarithm of the proposed percentage share. Suggest generous bounds like [-10.0, 10.0].
    *   `beta_fair_sens`: Sensitivity to the logarithm of the fair percentage share. Suggest generous bounds like [-10.0, 10.0].
    Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds for both.

*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self`, `U`, and `epsilon` if treated as such) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`, etc.).

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables (like `sum_tokens`, `fair_perc_self`, `epsilon`) before the final formula for `U`. Use clear mathematical notation. Use `log()` for the natural logarithm.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by the *logarithms* of the proposed share and the fair share, and how this non-linear, separated structure aims to improve parameter identifiability compared to previous linear or linear-ratio based models.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This logarithmic approach is another significant departure from previous attempts and might offer the necessary structural separation. If this structure also fails parameter recovery, be prepared to think even further outside the box for the next iteration – perhaps fairness acts as a threshold, a reference point in a non-ratio/non-difference way, or maybe completely different inputs or variable transformations are needed.
</previous_instructions>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.39890879931917045,0.7706156219084567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25,101.98964460000767,100.98439191155931,102.89018346674266,0.8174603174603174,U = sensitivity_offer * split_perc_self - penalty_unfair_deviation_sq * unfair_deviation^2,A utility model predicting acceptance behavior in the responder role. Utility is calculated based on the proposed percentage for the participant and a quadratic penalty for unfair offers. The model has two learnable parameters: a sensitivity to the offered percentage (sensitivity_offer) and a sensitivity to the squared deviation from the fair share (penalty_unfair_deviation_sq). Unfair deviation is defined as the positive difference between the fair share (based on token contributions) and the proposed share for the participant.,v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions:
Previous attempts using simple linear combinations of `split_perc_self` and `unfair_deviation`, both with and without an `acceptance_bias` term, have consistently struggled to achieve acceptable parameter recovery (>= 0.7). For example, the previous run (Run 24) used the model `U = sensitivity_offer * split_perc_self - penalty_unfair_deviation * unfair_deviation` and resulted in poor recovery for both `sensitivity_offer` (r = 0.574) and `penalty_unfair_deviation` (r = 0.638), despite reasonable BIC and accuracy. This indicates that these simple linear structures may not capture the underlying decision process in a way that allows for reliable estimation of the parameter values.

For this run (Run 25), the absolute highest priority remains achieving parameter recovery >= 0.7 for *all* learnable parameters. While maintaining good BIC and accuracy is still important, interpretability through parameter recovery is essential. To address the persistent recovery issues with linear penalty terms and the `acceptance_bias`, we will explore a model that incorporates a non-linear effect of unfairness.

Consider the following model structure with two learnable parameters:
`U = sensitivity_offer * split_perc_self - penalty_unfair_deviation_sq * unfair_deviation^2`

This model retains the influence of the offered percentage (`split_perc_self`) but introduces a quadratic penalty for unfair offers (`unfair_deviation^2`). This assumes that the disutility of an unfair offer increases more rapidly as the deviation from fairness grows larger. `unfair_deviation` is calculated as `max(0, fair_perc_self - split_perc_self)`, where `fair_perc_self` is the fair share based on token contributions.

This structure has two learnable parameters:
1.  `sensitivity_offer`: Represents the rate at which utility increases linearly with the offered percentage (`split_perc_self`). This parameter should be non-negative.
2.  `penalty_unfair_deviation_sq`: Represents the rate at which utility decreases quadratically with the magnitude of the unfair deviation (`unfair_deviation`). This parameter should be non-negative.

**Guidance for the Next Model (Run 25):**

Design a computational model implementing this specific two-parameter structure: `U = sensitivity_offer * split_perc_self - penalty_unfair_deviation_sq * unfair_deviation^2`. The primary goal is to achieve parameter recovery >= 0.7 for *all* learnable parameters (`sensitivity_offer`, `penalty_unfair_deviation_sq`) while striving for good BIC and accuracy.

*   **Prioritize Parameter Recovery:** Implement the specified structure precisely. The focus is on determining if this non-linear penalty term improves the identifiability of the parameters compared to linear penalty terms.
*   **Parameter Constraints:**
    *   `sensitivity_offer`: Define clear, generous, finite, non-negative bounds. Suggest [0, 10].
    *   `penalty_unfair_deviation_sq`: Define clear, generous, finite, non-negative bounds. Since `unfair_deviation^2` can be large (up to 100^2 = 10000), the coefficient `penalty_unfair_deviation_sq` will likely be small. Suggest [0, 0.5].
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using the suggested structure. Ensure the formula includes the necessary variables and parameters.
*   **Calculated Variables:** Remember `fair_perc_self` needs to be calculated as `(token_self / (token_self + token_opp)) * 100`. Handle the case where `token_self + token_opp` is zero (e.g., by setting `fair_perc_self` to 50 in this case). Define `unfair_deviation = max(0, fair_perc_self - split_perc_self)` as a calculated variable. Include all variables used in the model (`U`, `split_perc_self`, `unfair_deviation`, `fair_perc_self`, `token_self`, `token_opp`, `sensitivity_offer`, `penalty_unfair_deviation_sq`) in your variable descriptions with `source: ""calculated""`, `source: ""data""`, or `source: ""learnable""` as appropriate.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for all variables used in the <MODEL> formula, including the specified bounds and learnable status for the parameters. Clearly describe the role of each variable.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure and its components, specifically mentioning the quadratic penalty.

Remember to use variables relevant for responder trials (`trial_role == 1`).

This model represents a test of whether incorporating a non-linear relationship (specifically, a quadratic penalty for unfairness) is necessary and sufficient to achieve parameter identifiability in this task, especially after linear models have failed. If this non-linear two-parameter model *still* struggles with parameter recovery (i.e., if either `sensitivity_offer` or `penalty_unfair_deviation_sq` falls below 0.7), it would further highlight the complexity of the decision process and necessitate even more ""out of the box"" thinking in future attempts, possibly exploring interaction terms between variables, different functional forms (e.g., exponential, sigmoidal), or entirely different types of variables.

For run 25 of 100, please think through this step by step, focusing intently on implementing the specified model structure with the quadratic penalty term and ensuring all output requirements are met. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.661175330190667,,,,,,,,,,0.6347316021079,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23,110.89995751615913,95.35568172888851,124.82503790892241,0.8467643467643468,U = sensitivity_offer * split_perc_self - penalty_unfair_deviation * unfair_deviation + acceptance_bias,"A linear utility model predicting the decision to accept or reject an offer in the ultimatum game responder role. The utility of accepting is determined by three components: a base sensitivity to the offered percentage (split_perc_self), a penalty applied when the offer is below a calculated fair share percentage (unfair_deviation), and a general acceptance bias. The fair share is derived from token contributions of the participant and opponent. The model includes three learnable parameters: sensitivity_offer (scaling the effect of the offered percentage, non-negative), penalty_unfair_deviation (scaling the penalty for unfair offers, non-negative), and acceptance_bias (a constant shift in utility).",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = beta_param + epsilon_param * E + eta_param * N\n</MODEL>\n\n<VARIABLES>\n{\n  ""variables"": {\n    ""U_i"": {\n      ""description"": ""Utility of choosing action i"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\n      ""learnable"": false,\n      ""source"": ""calculated""\n    },\n    ""beta_param"": {\n      ""description"": ""Base tendency parameter (inherent preference)"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1\n      },\n      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""epsilon_param"": {\n      ""description"": ""Environmental sensitivity parameter"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\
    ""E"": {\n      ""description"": ""Environmental cue value"",\n      ""range"": {""min"": ""-inf"", ""max"": ""inf""},\n      ""learnable"": false,\n      ""source"": ""data""\n    },\
    ""eta_param"": {\n      ""description"": ""Noise parameter"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1,\n        ""exclusive_max"": true\n      },\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\
    ""N"": {\n      ""description"": ""Random noise drawn from normal distribution N(0,1)"",\n      ""distribution"": {\n        ""type"": ""normal"",\n        ""mean"": 0,\n        ""std"": 1\n      },\
      ""learnable"": false,\n      ""source"": ""calculated""\n    }\n  }\n}\n</VARIABLES>\
<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions:
In the previous run (Run 22), the model `U = -slope_unfair_deviation * unfair_deviation + slope_generous_deviation * generous_deviation + acceptance_bias` achieved the best BIC (80.82) among recent attempts, suggesting it captures some aspects of the data structure efficiently. However, it suffered from significantly poor parameter recovery (`slope_unfair_deviation`: r = 0.383, `slope_generous_deviation`: r = -0.027, `acceptance_bias`: r = 0.630) and lower accuracy (0.770) compared to models that included a term for the raw offer percentage (e.g., Run 3: acc 0.813). The failure to recover any parameter above the 0.7 threshold makes the model unusable for interpretation.

Previous runs (like Run 3 and Run 21) that included a sensitivity to `split_perc_self` showed somewhat better, though still insufficient, recovery for the parameters related to the offer or unfairness. Across multiple models, the term related to `generous_deviation` has consistently shown very poor or even negative parameter recovery, suggesting it might be poorly identifiable or its effect is already captured by other terms.

For this run (Run 23), the primary goal remains to achieve parameter recovery >= 0.7 for *all* learnable parameters, alongside good BIC and accuracy. Let's combine the efficiency suggested by Run 22's BIC with elements from models that showed better partial recovery. Specifically, let's test a model structure that incorporates sensitivity to the raw offer percentage, a penalty for unfair deviations, and an acceptance bias, while *removing* the term for generous deviations which has proven problematic for identifiability.

Consider a new model structure with three learnable parameters:
`U = sensitivity_offer * split_perc_self - penalty_unfair_deviation * max(0, fair_perc_self - split_perc_self) + acceptance_bias`

This model includes:
1.  `sensitivity_offer`: Represents the rate at which utility increases with the offered percentage (`split_perc_self`). This parameter should be non-negative.
2.  `penalty_unfair_deviation`: Represents the rate at which utility decreases for each percentage point the offer is below the fair share (`fair_perc_self`). This parameter should be non-negative. The term `max(0, fair_perc_self - split_perc_self)` captures the magnitude of the *unfair* deviation.
3.  `acceptance_bias`: Represents a general tendency to accept offers, independent of the specific offer value or its fairness. This parameter can be positive or negative (unbounded conceptually).

This structure attempts to balance sensitivity to the absolute offer value with an explicit penalty for unfairness relative to contribution-based fairness, plus a baseline acceptance level. By excluding a separate generous deviation term, we aim to simplify the model and potentially improve the identifiability of the remaining parameters.

**Guidance for the Next Model (Run 23):**

Design a computational model implementing this specific three-parameter structure: `U = sensitivity_offer * split_perc_self - penalty_unfair_deviation * max(0, fair_perc_self - split_perc_self) + acceptance_bias`. The primary goal is to achieve parameter recovery >= 0.7 for *all* learnable parameters (`sensitivity_offer`, `penalty_unfair_deviation`, `acceptance_bias`) while maintaining good BIC and accuracy.

*   **Prioritize Parameter Recovery:** Implement the specified structure precisely. The focus is on determining if this combination of terms and removal of the generous deviation term improves the identifiability of the parameters.
*   **Parameter Constraints:**
    *   `sensitivity_offer`: Define clear, generous, finite, non-negative bounds. Suggest [0, 10].
    *   `penalty_unfair_deviation`: Define clear, generous, finite, non-negative bounds. Suggest [0, 20].
    *   `acceptance_bias`: Define clear, generous, finite bounds. Suggest [-10, 10].
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using the suggested structure. Ensure the formula includes the necessary variables and parameters.
*   **Calculated Variables:** Remember `fair_perc_self` needs to be calculated as `(token_self / (token_self + token_opp)) * 100`. Handle the case where `token_self + token_opp` is zero (e.g., by setting `fair_perc_self` to 50 in this case). Define `unfair_deviation = max(0, fair_perc_self - split_perc_self)` as a calculated variable. Include `U`, `fair_perc_self`, `unfair_deviation`, `token_self`, `token_opp`, and `split_perc_self` in your variable descriptions with `source: ""calculated""` or `source: ""data""` as appropriate.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for all variables used in the <MODEL> formula (`U`, `split_perc_self`, `fair_perc_self`, `token_self`, `token_opp`, `unfair_deviation`, `sensitivity_offer`, `penalty_unfair_deviation`, `acceptance_bias`), including the specified bounds and learnable status for the parameters. Clearly describe the role of each variable.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure and its components.

Remember to use variables relevant for responder trials (`trial_role == 1`).

This model represents an attempt to find a set of parameters that are both behaviorally relevant (contributing to good fit/BIC/accuracy) and statistically identifiable (high parameter recovery). If this specific structure still struggles with parameter recovery, particularly for `sensitivity_offer` or `penalty_unfair_deviation`, it may suggest deeper issues requiring more complex non-linear functional forms or potentially fewer parameters.

For run 23 of 100, please think through this step by step, focusing intently on implementing the specified combined deviation-based and offer-based utility model structure. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6999078499129645,,,,,,,,0.055229475508609634,0.6488612848720088,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97,116.37008313718479,134.26642845243535,100.33794045893951,0.7490842490842491,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100)
is_below_fair = IF(split_perc_self < fair_perc_self_calc, 1, 0)
U_accept = deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + below_fair_impact * is_below_fair","A utility model for responder decisions based on the fairness of the proposed split. The utility of accepting an offer is calculated as a linear combination of two terms, each scaled by a learnable parameter. The first term represents the continuous deviation of the proposed percentage share from the calculated fair share based on token contributions, scaled by 'deviation_sensitivity'. The second term is a binary indicator that is 1 if the proposed share is below the fair share, scaled by 'below_fair_impact'. This structure models sensitivity to the magnitude of unfairness (deviation_sensitivity) and a distinct impact specifically for offers falling below the fairness threshold (below_fair_impact). The combination of a linear term based on deviation and a step function based on the 'below fair' state is designed to provide structurally separable influences, enhancing the identifiability of the two learnable parameters.",v0,"Okay, here are the improved instructions for the next run (Run 97 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy. Specifically, *all* learnable parameters must have a parameter recovery value of >= 0.7.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {""min"": 0, ""max"": 1},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {""description"": ""Noise parameter"",
      ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,\n        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 97 of 100).

The previous model (`sensitivity_to_offer * split_perc_self - aversion_strength * is_unfair`) achieved a good BIC but suffered from very low accuracy (0.545) and, critically, failed parameter recovery for the `sensitivity_to_offer` parameter (-0.024). This indicates the model structure did not allow for robust estimation of both parameters and did not capture the behavioral data well.

Robust parameter identifiability (recovery >= 0.7 for *all* learnable parameters) remains a **primary objective**, but it MUST be achieved alongside significantly improved accuracy and a competitive BIC. A model with good recovery but poor predictive power is not useful.

We must learn from past results:
- Scaling the absolute offer percentage (`split_perc_self`) and a binary unfairness indicator (`is_unfair`) linearly failed parameter recovery for the offer sensitivity.
- Scaling the deviation from fairness (`split_perc_self - fair_perc_self_calc`) and a constant intercept *did* achieve robust recovery for both parameters (0.893, 0.786) and better accuracy (0.784).

This suggests that using the *deviation from fairness* as a core input for one parameter is effective for identifiability, especially when the second parameter introduces a distinct type of influence.

You *must* include these standard calculation steps first before defining `U_accept`:
1.  Calculate `sum_tokens = token_self + token_opp`.
2.  Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.

Now, design a **novel** formula for `U_accept` incorporating exactly **two learnable parameters**. Focus intently on creating a structure where these two parameters have demonstrably *different types* of mathematical influence on the utility calculation to ensure robust identifiability and improve accuracy.

Specifically, design the model such that:
*   One parameter scales the **deviation from fairness**: `(split_perc_self - fair_perc_self_calc)`. Using this as input for one parameter's term builds on the success of a previous high-recovery model. Choose a descriptive, Python-safe name for this parameter (e.g., `deviation_sensitivity`).
*   The second parameter must scale a variable that introduces a *structurally distinct* influence on utility compared to the continuous deviation. Think creatively about variables or states that are not simply linear transformations of the deviation or the absolute offer. Good candidates for the input scaled by the second parameter include:
    -   A categorical or binary variable derived from data (e.g., based on `trial_type`, or `is_unfair`, but consider how its influence is combined distinctly from the deviation term).
    -   A constant (like an intercept), though aim for novelty beyond the simple additive intercept model if possible.
    -   A variable representing the total potential gain or the contribution structure, scaled in a way that is not confounded with the deviation term.

Avoid structures that failed recovery previously:
- Do not have two parameters scaling inputs that are both continuous and highly correlated linear transformations of `split_perc_self` and `fair_perc_self_calc`.
- Do not use the structure `parameter_1 * split_perc_self + parameter_2 * is_unfair`, as this specifically failed recovery in the last run.

Your goal is to find a novel way to combine the influence of a fairness deviation parameter with a second parameter whose influence is clearly separable, leading to both robust recovery (>= 0.7 for BOTH) and substantially improved accuracy and BIC compared to the previous run (Accuracy: 0.545, BIC: 52.82).

Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus potentially *other relevant data variables* from the list provided (`trial_type`, `combined_earning`, etc.) or calculated variables derived from them (like `is_unfair`, binary indicators from `trial_type`).

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus potentially others from the provided list. Calculate `sum_tokens`, `fair_perc_self_calc`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Include function definitions like `IF` if used.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Ensure the parameters' roles are structurally distinct to promote identifiability (e.g., one scaling a continuous deviation, the other scaling a categorical/binary variable or constant).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, any derived binary variables, and any functions like `IF`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs with `source: ""data""` that appear in your <MODEL>.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea. **Crucially, explain *how* the specific structure of your model ensures robust parameter identifiability for *both* learnable parameters (recovery >= 0.7), building on the lessons learned from the success of the deviation + intercept model and the failure of the absolute offer + binary unfairness model.**

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) AND improved accuracy/BIC are the absolute priorities. Your goal is to find a novel structure with *exactly two* parameters where their influences are clearly distinguishable (e.g., one scaling the fairness deviation, the other scaling a non-confounding variable like a categorical indicator) and interact in a way that allows for reliable estimation and better fits the human decision data. Be creative and think ""out of the box"" about how to use the available data variables and exactly two parameters to capture distinct aspects of the responder's decision process while maintaining identifiability and improving predictive power.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8717032610978865,0.2635889877090706,,,
24,133.0594647989836,79.67675765639284,180.88147328088783,0.825091575091575,U = sensitivity_offer * split_perc_self - penalty_unfair_deviation * unfair_deviation,"A utility model predicting the decision to accept or reject a proposal in an ultimatum game. The utility of accepting is determined by a linear combination of the proposed percentage for the participant (split_perc_self) and a penalty term (unfair_deviation) representing the amount the offer falls below the calculated fair share based on token contributions. The model has two learnable parameters: sensitivity_offer, weighting the value of the proposed percentage, and penalty_unfair_deviation, weighting the cost of an unfair offer. Both learnable parameters are constrained to be non-negative. The utility is converted to a probability of acceptance using a logistic function.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions:
In the previous run (Run 23), the model `U = sensitivity_offer * split_perc_self - penalty_unfair_deviation * unfair_deviation + acceptance_bias` achieved excellent accuracy (0.847) and acceptable BIC (110.90). However, parameter recovery was poor for two out of three parameters (`penalty_unfair_deviation`: r = 0.649, `acceptance_bias`: r = 0.055), falling below the critical 0.7 threshold. This makes the model unusable for drawing meaningful conclusions about the fitted parameter values. The `acceptance_bias` term has shown consistently poor recovery across multiple previous model attempts.

For this run (Run 24), the absolute highest priority is to achieve parameter recovery >= 0.7 for *all* learnable parameters. While maintaining good BIC and accuracy is still important, interpretability through parameter recovery is essential. Based on the difficulties in recovering the `acceptance_bias` and the only marginal recovery of `penalty_unfair_deviation` in the last run, let's simplify the model structure to improve identifiability. The `generous_deviation` term has also proven problematic for recovery in past models and should continue to be excluded.

Consider a new, simpler model structure with two learnable parameters:
`U = sensitivity_offer * split_perc_self - penalty_unfair_deviation * unfair_deviation`

This model retains the influence of the raw offered percentage (`split_perc_self`) and a penalty for unfair offers (`unfair_deviation`) but removes the `acceptance_bias` term. `unfair_deviation` is calculated as `max(0, fair_perc_self - split_perc_self)`, where `fair_perc_self` is the fair share based on token contributions.

This structure has two learnable parameters:
1.  `sensitivity_offer`: Represents the rate at which utility increases with the offered percentage (`split_perc_self`). This parameter should be non-negative.
2.  `penalty_unfair_deviation`: Represents the rate at which utility decreases for each percentage point the offer is below the fair share (`fair_perc_self`). This parameter should be non-negative.

**Guidance for the Next Model (Run 24):**

Design a computational model implementing this specific two-parameter structure: `U = sensitivity_offer * split_perc_self - penalty_unfair_deviation * unfair_deviation`. The primary goal is to achieve parameter recovery >= 0.7 for *all* learnable parameters (`sensitivity_offer`, `penalty_unfair_deviation`) while striving for good BIC and accuracy.

*   **Prioritize Parameter Recovery:** Implement the specified structure precisely. The focus is on determining if this simplified two-parameter combination improves the identifiability of the remaining parameters.
*   **Parameter Constraints:**
    *   `sensitivity_offer`: Define clear, generous, finite, non-negative bounds. Suggest [0, 10].
    *   `penalty_unfair_deviation`: Define clear, generous, finite, non-negative bounds. Suggest [0, 20].
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using the suggested structure. Ensure the formula includes the necessary variables and parameters.
*   **Calculated Variables:** Remember `fair_perc_self` needs to be calculated as `(token_self / (token_self + token_opp)) * 100`. Handle the case where `token_self + token_opp` is zero (e.g., by setting `fair_perc_self` to 50 in this case). Define `unfair_deviation = max(0, fair_perc_self - split_perc_self)` as a calculated variable. Include `U`, `fair_perc_self`, `unfair_deviation`, `token_self`, `token_opp`, and `split_perc_self` in your variable descriptions with `source: ""calculated""` or `source: ""data""` as appropriate.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for all variables used in the <MODEL> formula (`U`, `split_perc_self`, `fair_perc_self`, `token_self`, `token_opp`, `unfair_deviation`, `sensitivity_offer`, `penalty_unfair_deviation`), including the specified bounds and learnable status for the parameters. Clearly describe the role of each variable.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure and its components.

Remember to use variables relevant for responder trials (`trial_role == 1`).

This model represents a test of whether removing the poorly recovered `acceptance_bias` term and focusing on the core influences of offer value and unfairness penalty can yield identifiable parameters. If this two-parameter model *still* struggles with parameter recovery (i.e., if either `sensitivity_offer` or `penalty_unfair_deviation` falls below 0.7), it would strongly suggest that the relationship between these variables and utility is not well captured by this simple linear form. In that case, more ""out of the box"" thinking, such as exploring non-linear functions (e.g., quadratic penalties, exponential sensitivity), interaction terms, or incorporating different variables, would be necessary in future attempts to find a structurally identifiable model.

For run 24 of 100, please think through this step by step, focusing intently on implementing the specified two-parameter model structure and ensuring all output requirements are met. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5743534473018027,,,,,,,,,0.6379077092037684,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18,137.14444281290872,106.35481217456902,164.7268202597547,0.8131868131868132,"utility = sensitivity_offer * split_perc_self - penalty_magnitude_unfairness * max(0, (token_self / (token_self + token_opp)) * 100 - split_perc_self)","A utility model for responder decisions combining linear sensitivity to the offered percentage with a penalty proportional to the magnitude of deviation from the participant's fair share when the offer is unfair. The model uses two non-negative learnable parameters: sensitivity_offer and penalty_magnitude_unfairness. Utility increases with the offered percentage and decreases when the offer falls below the fair share, with the penalty magnitude scaling with the degree of unfairness.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = beta_param + epsilon_param * E + eta_param * N\n</MODEL>\n\n<VARIABLES>\n{\n  ""variables"": {\n    ""U_i"": {\n      ""description"": ""Utility of choosing action i"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\n      ""learnable"": false,\n      ""source"": ""calculated""\n    },\n    ""beta_param"": {\n      ""description"": ""Base tendency parameter (inherent preference)"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1\n      },\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""epsilon_param"": {\n      ""description"": ""Environmental sensitivity parameter"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\
    ""E"": {\n      ""description"": ""Environmental cue value"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\n      ""learnable"": false,\n      ""source"": ""data""\n    },\n    ""eta_param"": {\n      ""description"": ""Noise parameter"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1,\n        ""exclusive_max"": true\n      },\n      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""N"": {\n      ""description"": ""Random noise drawn from normal distribution N(0,1)"",\n      ""distribution"": {\n        ""type"": ""normal"",\n        ""mean"": 0,\n        ""std"": 1\n      },\n      ""learnable"": false,\n      ""source"": ""calculated""\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Previous models designed to capture responder behavior have consistently struggled with parameter recovery, especially for parameters related to fairness perception. Run 16's attempt with a piecewise linear model based on deviation from fair share (`sensitivity_generous`, `penalty_slope_unfair`) resulted in extremely poor recovery (r=0.052 and r=0.049), making the model unusable. Simpler models, like one combining linear sensitivity with a fixed step penalty at the fair share threshold (Run 1), showed better recovery (r=0.620, r=0.513) and overall performance metrics, although still not meeting the 0.7 threshold for all parameters.

This suggests that the way fairness is incorporated is crucial for identifiability. Models that introduce sharp discontinuities or distinct slopes based *only* on the fair vs. unfair boundary might lead to parameter collinearity or lack of distinct influence across the range of offers.

For this run (Run 18), let's try a structural approach that combines a basic sensitivity to the offered amount with a penalty that is *proportional* to the *magnitude* of unfairness. This differs from previous attempts that used fixed penalties or piecewise slopes based on the fairness boundary alone.

Consider a model where the utility of accepting an offer (`U`) is calculated as:
`U = sensitivity_offer * split_perc_self - penalty_magnitude_unfairness * max(0, fair_perc_self - split_perc_self)`

This model has two learnable parameters:
1.  `sensitivity_offer`: Represents a base sensitivity to the percentage amount offered to the participant (`split_perc_self`). A higher value means greater utility for higher offers, regardless of fairness.
2.  `penalty_magnitude_unfairness`: Represents a multiplier for the penalty applied when the offer is strictly unfair (`split_perc_self < fair_perc_self`). The penalty term `max(0, fair_perc_self - split_perc_self)` captures the *degree* of unfairness (how much below fair the offer is). A positive value for this parameter imposes a disutility that increases linearly with the magnitude of the unfair deviation. If the offer is fair or generous, this penalty term is zero.

Both learnable parameters should be non-negative.

**Guidance for the Next Model (Run 18):**

Design a computational model implementing this specific structure. The goal is for `sensitivity_offer` and `penalty_magnitude_unfairness` to have sufficiently distinct effects on utility across different offers (especially unfair vs. fair/generous ones) to improve parameter recovery.

*   **Prioritize Parameter Recovery:** Design the model exactly following the specified structure to maximize the chances of `sensitivity_offer` and `penalty_magnitude_unfairness` being distinguishable and recoverable (>= 0.7).
*   **Parameter Constraints:** Define clear, generous, finite, non-negative bounds for `sensitivity_offer` and `penalty_magnitude_unfairness`. Suggest bounds [0, 20] for `sensitivity_offer` and [0, 10] for `penalty_magnitude_unfairness`. Both parameters must be non-negative.
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using the suggested structure: `U = sensitivity_offer * split_perc_self - penalty_magnitude_unfairness * max(0, fair_perc_self - split_perc_self)`. Ensure the formula includes the necessary variables (`split_perc_self`, `fair_perc_self`) and parameters. Remember `fair_perc_self` needs to be calculated as `(token_self / (token_self + token_opp)) * 100`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags, including the specified bounds and learnable status for `sensitivity_offer` and `penalty_magnitude_unfairness`. Include descriptions for all variables used in the <MODEL> formula (`U`, `split_perc_self`, `fair_perc_self`, `token_self`, `token_opp`, `sensitivity_offer`, `penalty_magnitude_unfairness`).
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags.

Think creatively about how this model captures preferences by valuing the offer itself (`split_perc_self`) while adding a specific, scaled penalty only for the degree of unfairness (`max(0, fair_perc_self - split_perc_self)`). This is different from just having different slopes above/below fair or fixed penalties. Aim for a structure where the parameters have distinct roles influencing the likelihood of acceptance across the full range of offers.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember `fair_perc_self` can be calculated as `(token_self / (token_self + token_opp)) * 100`. Use variables relevant for responder trials (`trial_role == 1`).

For run 18 of 100, please think through this step by step, focusing intently on designing a model structure that maximizes parameter identifiability given the data and the history of failed recovery attempts, specifically implementing the described model combining base linear sensitivity with a penalty proportional to unfairness magnitude. Then provide your model specification, variable descriptions, target variable, and summary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6204059928296952,0.5406614320214196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19,145.3888672356077,103.76987035819866,182.67255193828666,0.8315018315018315,"U = sensitivity_offer * split_perc_self - penalty_magnitude_unfairness * max(0, fair_perc_self - split_perc_self) + beta_bias","This model predicts the utility of accepting an offer for a participant acting as a responder. The utility is calculated based on the percentage share offered to the participant, a penalty term for receiving less than their fair share (calculated from token contributions), and a general acceptance bias. The model includes three learnable parameters: sensitivity to the offer percentage, a multiplier for the unfairness penalty magnitude, and an additive bias term. It is designed to be fitted on responder trials only.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = beta_param + epsilon_param * E + eta_param * N\n</MODEL>\n\n<VARIABLES>\n{\n  ""variables"": {\n    ""U_i"": {\n      ""description"": ""Utility of choosing action i"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\
      ""learnable"": false,\n      ""source"": ""calculated""\n    },\
    ""beta_param"": {\n      ""description"": ""Base tendency parameter (inherent preference)"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1\n      },\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\
    ""epsilon_param"": {\n      ""description"": ""Environmental sensitivity parameter"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\
    ""E"": {\n      ""description"": ""Environmental cue value"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\
      ""learnable"": false,\n      ""source"": ""data""\n    },\
    ""eta_param"": {\n      ""description"": ""Noise parameter"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1,\
        ""exclusive_max"": true\n      },\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\
    ""N"": {\n      ""description"": ""Random noise drawn from normal distribution N(0,1)"",\n      ""distribution"": {\
        ""type"": ""normal"",\n        ""mean"": 0,\n        ""std"": 1\n      },\
      ""learnable"": false,\n      ""source"": ""calculated""\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: The previous model (Run 18) achieved the best BIC (137.14) and accuracy (0.813) so far by combining a linear sensitivity to the offered percentage (`split_perc_self`) with a penalty proportional to the magnitude of deviation below the fair share (`max(0, fair_perc_self - split_perc_self)`). However, parameter recovery for both `sensitivity_offer` (r=0.620) and `penalty_magnitude_unfairness` (r=0.541) remained below the required 0.7 threshold, making the model unusable despite its good fit.

Poor parameter recovery suggests that the parameters might not have sufficiently distinct influences on utility across the data, or that some shared variance is being absorbed by multiple parameters.

For this run (Run 19), we will attempt to improve parameter recovery by adding a baseline bias term to the utility function. This new parameter, `beta_bias`, will capture the overall propensity to accept an offer, independent of the specific offer value or its fairness. By explicitly modeling this baseline, we hypothesize that `sensitivity_offer` and `penalty_magnitude_unfairness` may become more disentangled, leading to improved recovery.

Consider a model where the utility of accepting an offer (`U`) is calculated as:
`U = sensitivity_offer * split_perc_self - penalty_magnitude_unfairness * max(0, fair_perc_self - split_perc_self) + beta_bias`

This model has three learnable parameters:
1.  `sensitivity_offer`: Represents a base sensitivity to the percentage amount offered to the participant (`split_perc_self`). A higher value means greater utility for higher offers. This parameter should be non-negative.
2.  `penalty_magnitude_unfairness`: Represents a multiplier for the penalty applied when the offer is strictly unfair. The penalty term `max(0, fair_perc_self - split_perc_self)` captures the degree of unfairness. This parameter should be non-negative.
3.  `beta_bias`: Represents a general bias towards accepting offers, shifting the utility function up or down regardless of the offer details. This parameter can be positive or negative (i.e., unbounded).

**Guidance for the Next Model (Run 19):**

Design a computational model implementing this specific structure by adding the `beta_bias` term to the previously successful formula. The primary goal is to achieve parameter recovery >= 0.7 for *all* learnable parameters while maintaining good BIC and accuracy.

*   **Prioritize Parameter Recovery:** Implement the specified structure precisely to test if adding a bias term improves the identifiability of `sensitivity_offer` and `penalty_magnitude_unfairness`, and ensure `beta_bias` itself is recoverable.
*   **Parameter Constraints:**
    *   `sensitivity_offer`: Define clear, generous, finite, non-negative bounds. Suggest [0, 20].
    *   `penalty_magnitude_unfairness`: Define clear, generous, finite, non-negative bounds. Suggest [0, 10].
    *   `beta_bias`: Define clear, generous, finite bounds. Suggest [-10, 10]. This parameter can be positive or negative.
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using the suggested structure: `U = sensitivity_offer * split_perc_self - penalty_magnitude_unfairness * max(0, fair_perc_self - split_perc_self) + beta_bias`. Ensure the formula includes the necessary variables (`split_perc_self`, `fair_perc_self`) and parameters. Remember `fair_perc_self` needs to be calculated as `(token_self / (token_self + token_opp)) * 100`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for all variables used in the <MODEL> formula (`U`, `split_perc_self`, `fair_perc_self`, `token_self`, `token_opp`, `sensitivity_offer`, `penalty_magnitude_unfairness`, `beta_bias`), including the specified bounds and learnable status for the parameters.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure and its components, including the new bias term.

Remember that `fair_perc_self` can be calculated as `(token_self / (token_self + token_opp)) * 100`. Use variables relevant for responder trials (`trial_role == 1`).

While focusing on this specific structure for Run 19 to test the bias hypothesis, also keep in mind that creative, out-of-the-box approaches to modeling utility and fairness perception may be needed in future runs if recovery issues persist. Consider how deviations from fairness might be perceived non-linearly, or how the relative contributions (tokens) might influence utility in ways not solely captured by the fairness percentage itself.

For run 19 of 100, please think through this step by step, focusing intently on designing a model structure that addresses the parameter recovery issue observed in Run 18 by adding a baseline bias term. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
</previous_instructions>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5527896301656204,0.7411521976725685,0.002161281355630436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4,149.19642913764892,136.08077055144253,160.94587328779212,0.7756410256410255,"prop_share_perc = (token_self / (token_self + token_opp)) * 100
penalty_50 = max(0, 50 - split_perc_self)
penalty_prop = max(0, prop_share_perc - split_perc_self)
U = beta_self_interest * split_self - beta_50_aversion * penalty_50 - beta_proportional_aversion * penalty_prop","A utility model predicting responder acceptance decisions based on self-interest and aversion to two forms of unfairness. Utility is a weighted sum of the proposed absolute monetary gain, a penalty for offers below 50% of the total pot (in percentage points), and a penalty for offers below the proportional share based on token contributions (in percentage points). The penalties apply only when the offer falls below the respective reference points (50% or proportional share). The sensitivities to monetary gain, 50% unfairness, and proportional unfairness are captured by three non-negative learnable parameters (beta_self_interest, beta_50_aversion, beta_proportional_aversion). The utility is converted to a probability of acceptance using a logistic function.",v0,"<new_instructions>Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Pass parameter recovery tests.** This is the *most critical* objective. ALL learnable parameters must achieve a recovery correlation of **at least 0.7**. A model where parameters cannot be reliably recovered is unusable. Prioritize model structures that inherently enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables.
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

Achieving reliable parameter recovery for all learnable parameters (>= 0.7 correlation) is the primary challenge for models on this task based on previous attempts. Simply reducing the number of parameters has not consistently solved this issue. The focus needs to be on designing a model structure where the influence of each learnable parameter on the predicted utility is sufficiently distinct and minimally correlated with other parameters given the data structure.

Break away from standard linear or simple penalty-based utility structures if necessary. Explore alternative functional forms and novel ways to combine psychological influences (self-interest, aversion to 50% split unfairness, aversion to proportional split unfairness). Consider approaches inspired by reference-dependent utility theory (e.g., utility based on deviations from reference points rather than absolute values) or models that explicitly parameterize the sensitivities to different factors. Think creatively about how these components interact.

Given the variable 'combined_earning' across trials, carefully consider whether your model's core variables should operate on absolute amounts (£, using 'split_self') or percentages (%, using 'split_perc_self'). Using percentages might help normalize for the varying total pot size and improve parameter interpretability and identifiability.

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded.

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1).

First, reason step by step about:
*   The key psychological mechanisms involved in responder decisions (self-interest, fairness/inequality aversion relative to different reference points like 50% or proportional share).
*   Given the variable 'combined_earning', should the model work with absolute values (split_self) or percentages (split_perc_self) to best capture behavior and ensure parameter identifiability?
*   What alternative mathematical forms (e.g., reference-dependent utility, non-linear effects, direct parameterization of sensitivities to different factors) could capture these mechanisms in a way that allows for better parameter recovery than standard approaches?
*   How can the learnable parameters be defined and combined in the model structure to minimize covariance and maximize their unique, identifiable influence on the predicted utility?
*   What is the *minimum necessary* set of learnable parameters that can robustly capture the essential aspects of the behavior while prioritizing parameter identifiability?
*   How the model variables map to observable behavior (""accept"").

Please think through this step by step, then provide your model specification, variable descriptions, target variable, and summary.
</new_instructions>",,,,,,,0.41988664130819175,0.47027129454948846,0.32664659226936676,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,152.6563463383788,143.26920293027865,161.0656623081352,0.7841880341880342,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
U_accept = beta_sensitivity * (split_perc_self - fair_perc_self_calc) + beta_intercept","A linear utility model predicting responder acceptance based on the proposed share relative to a calculated fair share. Utility (U_accept) is modeled as a linear function of the deviation of the proposed share for the participant (split_perc_self) from the calculated fair share (fair_perc_self_calc), plus a constant intercept. The model uses two learnable parameters: beta_sensitivity, which scales the influence of the fairness deviation (acting as a slope), and beta_intercept, which provides a baseline utility offset. This structural separation between a slope parameter applied to a deviation term and an independent intercept parameter is designed to create mathematically distinct and separable influences on the utility function, thereby improving robust joint parameter identifiability compared to previous model attempts. Beta_sensitivity captures how strongly utility changes with fairness deviations, while beta_intercept captures a general propensity to accept regardless of the offer's relative fairness.",v0,"Okay, here are the improved instructions for the next run (Run 93 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 93 of 100).

The previous attempt using the additive structure with a log-transformed penalty (`U_accept = beta_value * split_perc_self - beta_penalty * log(1.0 + loss_below_fair)`) resulted in very poor parameter recovery (`beta_value`: 0.573, `beta_penalty`: 0.001). Neither parameter met the recovery threshold of 0.7. This indicates that the specific structural choice in the last run failed to create mathematically separable influences for both parameters, making them difficult to estimate reliably from the data. Robust parameter identifiability (recovery >= 0.7 for *all* learnable parameters simultaneously) remains the **unwavering primary objective**.

Given the persistent challenges in parameter recovery, we need to explore a fundamentally different structural approach that ensures the influence of the two learnable parameters is distinct and strongly identifiable. The failure of previous models that scaled `split_perc_self` directly with one parameter suggests this might be problematic for identifiability when another parameter captures fairness.

Let's explore a structure where the utility is directly tied to the *deviation* from the calculated fair share, rather than the absolute proposed share. We can then add an intercept term to capture a general bias or baseline utility. This structure uses two parameters with clearly separable mathematical roles: a slope parameter affecting the relationship between utility and fairness deviation, and an intercept parameter shifting the entire utility function.

Here are the standard calculation steps for necessary intermediate variables. You *must* include these first before defining `U_accept`:
1.  Calculate `sum_tokens = token_self + token_opp`.
2.  Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.

Now, design a novel formula for `U_accept` using `split_perc_self`, `fair_perc_self_calc`, and exactly two learnable parameters. The formula for `U_accept` must implement the principle that utility is a linear function of the *deviation* from the fair share, plus a constant offset.

A suggested specific structure to explore that follows this principle is:
`U_accept = beta_sensitivity * (split_perc_self - fair_perc_self_calc) + beta_intercept`

This structure represents utility as a linear sensitivity (`beta_sensitivity`) to the difference between the proposed share (`split_perc_self`) and the calculated fair share (`fair_perc_self_calc`), plus a baseline utility (`beta_intercept`). This creates two mathematically distinct influences: `beta_sensitivity` scales the deviation from fairness, affecting the *slope* of the utility function relative to this deviation, while `beta_intercept` acts as a constant *offset*, shifting the entire function up or down. This clear separation between a slope parameter and an intercept parameter based on deviation from fairness is intended to improve robust joint parameter identifiability compared to previous attempts by ensuring their effects on the utility are mathematically orthogonal (slope vs. intercept).

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names (e.g., `beta_sensitivity`, `beta_intercept`). Define clear, generous, finite numerical bounds.
    *   Parameter 1 (e.g., `beta_sensitivity`): Should represent the linear sensitivity to the *deviation from fair share* (`split_perc_self - fair_perc_self_calc`). This could be positive (more than fair is good) or negative, so bounds should allow both positive and negative values, e.g., `[-10.0, 10.0]`.
    *   Parameter 2 (e.g., `beta_intercept`): Should represent a baseline utility or general bias, independent of the specific offer/fairness deviation. Bounds should allow both positive and negative values and be generous, e.g., `[-100.0, 100.0]`.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and the function `IF`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is a linear function of the deviation from the fair share plus a baseline). **Crucially, explain *how* this specific structure with a slope parameter (`beta_sensitivity`) scaling the deviation (`split_perc_self - fair_perc_self_calc`) and an intercept parameter (`beta_intercept`) is designed to create mathematically distinct and separable influence profiles for the two learnable parameters.** Highlight that `beta_sensitivity` governs the rate at which utility changes as offers become more or less fair relative to the fairness point, while `beta_intercept` provides a constant offset to utility, and how this fundamental linear slope/intercept structure based on fairness deviation is intended to improve robust joint parameter identifiability compared to previous attempts.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately. Avoid structures that have failed recovery previously. Explore this specific additive structure centering around deviation from fairness as detailed above, focusing on how this structural separation creates distinct influence profiles for the two betas. Feel free to propose alternative mathematically distinct ways of combining influences if this exact formulation doesn't seem optimal, as long as the two parameters have demonstrably separable roles impacting utility (e.g., one scaling a deviation/ratio term, the other acting as an offset or scaling a different, non-collinear component).
</previous_instructions>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7862240491290533,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8928922914014249,,,,,,,,,,,
12,170.45934573191892,106.22005721899862,228.00704169141002,0.7646520146520146,"U = alpha * split_perc_self - beta_step * max(0, -sign(split_perc_self - (token_self / (token_self + token_opp)) * 100))","A utility model for responder decisions incorporating sensitivity to the offered percentage and a fixed penalty for offers below the proportional fair share. Utility is calculated as a linear function of the offered percentage scaled by 'alpha', minus a penalty 'beta_step' that is applied as a step function, being non-zero only when the offered percentage is strictly less than the fair share determined by token contributions. The model has two non-negative learnable parameters: alpha (sensitivity to offer) and beta_step (fixed unfairness penalty).",v0,"Based on the results of Run 11, where only the `alpha` parameter (base sensitivity to the offer) achieved sufficient recovery (r=0.798), while the parameters scaling deviations below (`beta`, r=0.399) and above (`gamma`, r=0.063) the fair share failed significantly, it's clear that disentangling the influence of these continuous deviation terms from the base sensitivity is proving very difficult for parameter recovery in this dataset. The piecewise linear structure attempted in Run 11, while conceptually sound, did not resolve the identifiability issues for the deviation parameters.

The most critical objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. A model is unusable otherwise. We must continue to prioritize model structures that make parameters influence the utility function in maximally separable ways.

For Run 12, let's pivot to a model where fairness acts as a more discrete, threshold-based influence rather than a continuous penalty or bonus based on the exact difference. This might help make the fairness parameter's effect more distinct and thus better identifiable.

Consider a model with a base sensitivity to the offered percentage, and a *fixed penalty* that is applied *only* when the offer falls below the proportional fair share (`fair_perc_self`). This treats ""being below fair"" as a distinct state that triggers a specific cost, rather than a continuous penalty proportional to the magnitude of unfairness. This structure aims to cleanly separate the effect of the offer's value from the effect of crossing the fairness threshold.

**Proposed Model Structure Idea:**

The utility of accepting an offer is determined by:
1.  A base utility proportional to the offered percentage (`split_perc_self`), scaled by a parameter `alpha`.
2.  An additional fixed penalty, scaled by a parameter `beta_step`, which is applied *only* if `split_perc_self` is less than `fair_perc_self`. If `split_perc_self` is equal to or greater than `fair_perc_self`, this penalty is zero.

This model has two learnable parameters: `alpha` representing the base value sensitivity, and `beta_step` representing the magnitude of the fixed penalty for unfair offers. Both parameters should be constrained to be non-negative, as a higher offer should generally increase utility (`alpha`), and offers below fair should incur a penalty (`beta_step`).

**Guidance for the Next Model (Run 12):**

Design a computational model implementing the ""step penalty"" concept described above. This model structure simplifies the influence of fairness on utility by making it a binary switch (on/off) based on the `fair_perc_self` threshold, rather than a continuous function of the deviation magnitude. This structural change is intended to improve the separability and identifiability of the `beta_step` parameter from `alpha`.

*   **Prioritize Parameter Recovery:** Design the model structure to ensure `alpha` and `beta_step` have maximally distinguishable effects on the utility function given the data.
*   **Parameter Constraints:** Define clear, generous, finite, non-negative bounds for both `alpha` and `beta_step`. Suggest `alpha` bounds [0, 10] and `beta_step` bounds [0, 100].
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using standard mathematical notation. Ensure the formula reflects the fixed penalty applied only when `split_perc_self < fair_perc_self`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags, including the specified bounds and learnable status for `alpha` and `beta_step`.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags.

Think creatively about how to implement the ""fixed penalty below fair"" logic concisely within the single expression required for the <MODEL> tag using available mathematical functions (`max`, `sign`, etc.) to represent the conditional nature of the penalty. The core logic is: Utility = (Sensitivity to Offer) * Offer - (Penalty Magnitude) * (Indicator: IsOfferBelowFair).

Focus intently on this distinct structural approach – separating base value sensitivity from a discrete fairness cost – as a strategy to finally achieve the necessary parameter identifiability.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember `fair_perc_self` can be calculated as `(token_self / (token_self + token_opp)) * 100`. Use variables available for responder trials (`trial_role == 1`).

For run 12 of 100, please think through this step by step, focusing intently on designing a model structure that maximizes parameter identifiability given the data and the history of failed recovery attempts, specifically implementing the described step penalty model. Then provide your model specification, variable descriptions, target variable, and summary.",,,,,,,,,,,,,0.6197808805319907,,,,,,,,,,,,,0.5126076628491772,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1,171.49502297168462,131.11426227952774,207.66945442507514,0.8705738705738706,"utility = beta_zero + beta_one * split_perc_self - beta_two * max(0, 50 - split_perc_self) - beta_three * max(0, proportional_perc - split_perc_self)","A utility model for responder decisions in an ultimatum game. The utility of accepting an offer is calculated based on a baseline utility (beta_zero), the offered percentage share to the participant (split_perc_self), and penalties for the offered percentage falling below two distinct fairness reference points. The penalties capture aversion to receiving less than a 50% equal split (weighted by beta_two) and aversion to receiving less than the participant's proportional share derived from token contributions (weighted by beta_three). Utility is converted to a probability of acceptance using a logistic function, which determines the predicted accept/reject decision.",v0,"Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.

First, reason step by step about:
* The key psychological mechanisms involved
* How these mechanisms interact
* What mathematical form could capture these interactions
* What parameters would be needed to calibrate the model
* How the model variables map to observable behavior",-0.04583956274621159,0.7646440831273859,0.4479001217511437,0.6427717829897684,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,178.4899353065996,137.56859782325114,215.14863346876592,0.8318070818070817,utility = sensitivity_base * split_perc_self - sensitivity_unfair * unfair_deviation + sensitivity_generous * generous_deviation,"A utility model for the responder's decision based on the percentage offer. Utility is a linear combination of the offered percentage itself, a penalty for unfair offers proportional to the magnitude of the unfair deviation, and a bonus for generous offers proportional to the magnitude of the generous deviation. It has three learnable parameters: a base sensitivity to the offer percentage (non-negative), a slope for the unfairness penalty (non-negative), and a slope for the generosity bonus (non-negative). Fairness is determined by relative token contributions.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: In the previous run (Run 20), the simplified model `U = sensitivity_offer * split_perc_self - penalty_unfair_flat * is_unfair` performed poorly across all metrics (BIC 475.35, Accuracy 0.654) and parameter recovery was very low for both parameters (`sensitivity_offer`: r = -0.135, `penalty_unfair_flat`: r = 0.159). This suggests that a fixed penalty for unfairness, irrespective of its magnitude, is not capturing participant behavior effectively, and the parameters are not identifiable in this structure.

In earlier runs, the model `U = sensitivity_offer * split_perc_self - penalty_magnitude_unfairness * max(0, fair_perc_self - split_perc_self)` (Run 3) showed promising performance (BIC 137.14, Accuracy 0.813) and near-target parameter recovery (`sensitivity_offer`: r = 0.620, `penalty_magnitude_unfairness`: r = 0.541). This indicates that sensitivity to the offer percentage combined with a penalty proportional to the *magnitude* of unfairness is a better approach. However, the recovery for both parameters was still below the 0.7 threshold, suggesting potential identifiability issues between these two terms.

For this run (Run 21), the primary goal remains to achieve parameter recovery >= 0.7 for *all* learnable parameters while maintaining good BIC and accuracy. To address the identifiability issues observed in Run 3, we will propose a model structure that distinguishes between the general sensitivity to the offered amount, the penalty for unfairness, and potentially a bonus for generosity, all based on the magnitude of deviation from fairness.

Consider a new model structure with three learnable parameters:
`U = sensitivity_base * split_perc_self - sensitivity_unfair_deviation_slope * max(0, fair_perc_self - split_perc_self) + sensitivity_generous_slope * max(0, split_perc_self - fair_perc_self)`

This model includes:
1.  `sensitivity_base`: Represents a baseline linear sensitivity to the offered percentage (`split_perc_self`). A higher value means greater utility for higher offers overall. This parameter should be non-negative.
2.  `sensitivity_unfair_deviation_slope`: Represents a penalty subtracted from the utility that scales linearly with the *magnitude* by which the offer is strictly unfair (`fair_perc_self - split_perc_self`). A higher value means a stronger penalty for each percentage point below fair. This parameter should be non-negative. The term `max(0, fair_perc_self - split_perc_self)` captures the magnitude of unfair deviation.
3.  `sensitivity_generous_slope`: Represents a bonus added to the utility that scales linearly with the *magnitude* by which the offer is strictly generous (`split_perc_self - fair_perc_self`). A higher value means a stronger bonus for each percentage point above fair. This parameter should be non-negative. The term `max(0, split_perc_self - fair_perc_self)` captures the magnitude of generous deviation.

This structure builds upon the successful aspects of Run 3 (linear sensitivity + magnitude penalty) by adding an explicit, magnitude-dependent bonus term for generous offers. By separating the effects of offers below fair and offers above fair, this might help to disentangle the influence of `sensitivity_base` from the fairness/generosity terms, potentially improving parameter identifiability for all three parameters.

**Guidance for the Next Model (Run 21):**

Design a computational model implementing this specific three-parameter structure. The primary goal is to achieve parameter recovery >= 0.7 for *all* learnable parameters (`sensitivity_base`, `sensitivity_unfair_deviation_slope`, `sensitivity_generous_slope`) while maintaining good BIC and accuracy.

*   **Prioritize Parameter Recovery:** Implement the specified structure precisely to test if adding the generosity slope term improves the identifiability of all three parameters.
*   **Parameter Constraints:**
    *   `sensitivity_base`: Define clear, generous, finite, non-negative bounds. Suggest [0, 20].
    *   `sensitivity_unfair_deviation_slope`: Define clear, generous, finite, non-negative bounds. Suggest [0, 20].
    *   `sensitivity_generous_slope`: Define clear, generous, finite, non-negative bounds. Suggest [0, 20].
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using the suggested structure: `U = sensitivity_base * split_perc_self - sensitivity_unfair_deviation_slope * max(0, fair_perc_self - split_perc_self) + sensitivity_generous_slope * max(0, split_perc_self - fair_perc_self)`. Ensure the formula includes the necessary variables and parameters.
*   **Calculated Variables:** Remember `fair_perc_self` needs to be calculated as `(token_self / (token_self + token_opp)) * 100`. Define `unfair_deviation = max(0, fair_perc_self - split_perc_self)` and `generous_deviation = max(0, split_perc_self - fair_perc_self)` as calculated variables. Include `U`, `fair_perc_self`, `token_self`, `token_opp`, `unfair_deviation`, and `generous_deviation` in your variable descriptions with `source: ""calculated""` or `source: ""data""` as appropriate.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for all variables used in the <MODEL> formula (`U`, `split_perc_self`, `fair_perc_self`, `token_self`, `token_opp`, `unfair_deviation`, `generous_deviation`, `sensitivity_base`, `sensitivity_unfair_deviation_slope`, `sensitivity_generous_slope`), including the specified bounds and learnable status for the parameters. Clearly describe the role of each variable.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure and its components.

Remember to use variables relevant for responder trials (`trial_role == 1`).

If this three-parameter piecewise slope structure still fails to yield sufficient parameter recovery for all parameters, future runs will need to explore more fundamentally different functional forms for utility, consider non-linear relationships, or incorporate how relative contributions (tokens) influence perceived fairness or utility in ways not solely captured by linear deviations from `fair_perc_self`. Think creatively about how fairness perceptions might interact with the value of the offer itself.

For run 21 of 100, please think through this step by step, focusing intently on designing a model structure that builds on previous successes while addressing parameter recovery issues by carefully separating the influence of offer value, unfair deviation magnitude, and generous deviation magnitude. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.053182962380620834,,,,,,0.5989034825829279,0.6133905718711627,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26,184.88096536504452,232.11254462696382,142.5693422762418,0.7286324786324786,Utility = beta_zero + beta_amount * split_self + beta_fair_gain * positive_deviation_from_fair + beta_fair_loss * negative_deviation_from_fair,"A utility model for responder decisions based on the absolute amount offered and the participant's percentage share relative to a fairness reference point determined by contributions. Utility is calculated as a linear combination of a baseline bias, the absolute monetary amount offered, and asymmetric sensitivities to receiving more or less than the calculated fair percentage. The model uses four learnable parameters: an intercept (beta_zero), sensitivity to amount (beta_amount), sensitivity to receiving more than fair (beta_fair_gain), and sensitivity to receiving less than fair (beta_fair_loss).",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions:
Previous attempts modeling responder behavior have explored linear combinations of the offered percentage (`split_perc_self`) and deviations from a calculated fair share (`unfair_deviation`), including models with linear and quadratic penalty terms for unfairness, and an acceptance bias term. Despite achieving reasonable BIC and accuracy in some cases, these models have consistently failed to achieve acceptable parameter recovery (>= 0.7 for all learnable parameters). This suggests that the current structural forms may not accurately capture the decision process or that the chosen variables/parameterizations are not sufficiently identifiable from the data.

For this run, the *absolute highest priority* remains achieving parameter recovery >= 0.7 for *all* learnable parameters. While maintaining good BIC and accuracy is still important, reliable parameter estimation is critical for interpretability.

Given that previous, relatively simple models based on `split_perc_self` and `unfair_deviation` have not yielded identifiable parameters, it is necessary to explore *alternative* model structures that might better represent the underlying psychology and improve parameter identifiability.

**Guidance for the Next Model:**

Design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer. Focus on developing a model structure that has the potential for better parameter recovery than previous attempts. This may require moving beyond simple linear or quadratic functions of `split_perc_self` and `unfair_deviation`.

*   **Prioritize Parameter Recovery:** This is the main goal. Consider how different components of the model contribute uniquely to utility across different scenarios, potentially making their scaling parameters more identifiable.
*   **Explore Alternative Structures:** Think ""out of the box"". Consider:
    *   **Interaction terms:** Does the effect of the offered percentage depend on how unfair the offer is? (e.g., a term like `sensitivity * split_perc_self * (1 - unfairness_measure)`).
    *   **Different fairness representation:** Instead of just `unfair_deviation`, could the *ratio* of offered share to fair share (`split_perc_self / fair_perc_self`) be relevant? Or perhaps both fair share and offered share influence utility separately?
    *   **Other variables:** Could `trial_type` (contribution structure) or `combined_earning` (total pot size) influence decision-making or moderate the effects of other variables?
    *   **Non-linearities:** Are there other non-linear effects besides a quadratic penalty? (e.g., threshold effects, exponential decay of utility with unfairness).
    *   **Multiple components:** Does utility arise from distinct psychological processes (e.g., valuing the offer, disliking inequality, a general willingness to accept)? How might these combine?
*   **Calculated Variables:** You will likely need `fair_perc_self` calculated as `(token_self / (token_self + token_opp)) * 100` (handle the `token_self + token_opp == 0` case by setting `fair_perc_self` to 50). `unfair_deviation = max(0, fair_perc_self - split_perc_self)` might still be useful, but consider if other measures of deviation or fairness are more appropriate for your chosen model structure. Include all calculated variables used in the model in your descriptions.
*   **Parameter Constraints:** Define clear, generous, finite numerical bounds for *all* learnable parameters. Non-negative constraints are often appropriate for parameters representing sensitivities or penalties, but consider if any parameters should be unbounded.
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags. Use clear notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include bounds and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure, its key components, and how it relates to the decision process.

Design a model that you believe has the best chance of achieving parameter recovery >= 0.7 for all its learnable parameters, while also fitting the data well (lower BIC, higher accuracy). Think creatively about how different aspects of the offer and context might interact to determine acceptance utility.

Please think through this step by step, focusing intently on designing a model structure that improves parameter identifiability, and ensure all output requirements are met. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",0.20534257268904005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.26693829671612,0.03184007951782476,0.2837478945669513,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
41,187.15171116358076,198.30326046267558,177.16178158314167,0.5961538461538461,"sum_tokens = token_self + token_opp
fair_perc_self = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)
gain_above_fair = MAX(0.0, split_perc_self - fair_perc_self)
loss_below_fair = MAX(0.0, fair_perc_self - split_perc_self)
U = beta_gain * gain_above_fair - beta_loss * loss_below_fair","This computational model predicts the utility of accepting a proposed offer for the participant when acting as a responder. The utility is based on the deviation of the proposed share percentage for the participant (`split_perc_self`) from a calculated fair share percentage (`fair_perc_self`), which is derived from the token contributions (`token_self`, `token_opp`). The model uses a piecewise linear structure: utility increases with gains above the fair share, scaled by a learnable parameter `beta_gain`, and decreases with losses below the fair share, scaled by a second learnable parameter `beta_loss`. This separation of sensitivity to positive and negative deviations from fairness aims to improve the identifiability and recovery of the two learnable parameters.",v0,"Instructions for Run 41 of 100:

The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals.

The previous run (Run 40) utilized the structure `U = beta_linear_relative * deviation_from_fair - beta_extra_loss * loss_below_fair`. This model successfully achieved good parameter recovery for `beta_linear_relative` (r = 0.815) but failed significantly for `beta_extra_loss` (r = 0.218). Overall model performance (BIC: 340.56, Accuracy: 0.630) was also not the best seen so far. The core issue remains achieving reliable recovery for *all* parameters simultaneously.

The poor recovery of `beta_extra_loss` in Run 40's structure suggests that its influence was not sufficiently distinct from `beta_linear_relative`, particularly in the region where offers were below the fair share. In that structure, utility below fair was `(beta_linear_relative + beta_extra_loss) * deviation_from_fair`, where the two parameters were linearly combined, making them difficult to estimate independently.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer. Your model should continue to utilize the proposed share percentage (`split_perc_self`) and a calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as key inputs.

*   **Parameter Identifiability is Paramount:** The central challenge remains designing a mathematical structure (formula for `U`) using these inputs and exactly **2 learnable parameters** such that these parameters have distinct and identifiable influences on utility, ensuring robust parameter recovery for *both*.
*   **Promising Structural Direction (Piecewise based on Deviation from Fair):** To address the identifiability issue observed in Run 40, specifically the coupling of parameters below fair, consider a piecewise linear model centered around the fair share. This approach explicitly separates sensitivity to deviations *above* the fair share from sensitivity to deviations *below* the fair share.
    Consider calculating the following intermediate variables:
    *   `sum_tokens = token_self + token_opp`
    *   `fair_perc_self = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)`
    *   `gain_above_fair = MAX(0.0, split_perc_self - fair_perc_self)` (The magnitude of the gain if the offer is above fair, otherwise 0)
    *   `loss_below_fair = MAX(0.0, fair_perc_self - fair_perc_self)` (The magnitude of the loss if the offer is below fair, otherwise 0)
    Then, structure the utility (`U`) based on these terms and your two learnable parameters, aiming for a structure like:
    `U = beta_gain * gain_above_fair - beta_loss * loss_below_fair`
    This structure models utility as increasing with gains above fair (scaled by `beta_gain`) and decreasing with losses below fair (scaled by `beta_loss`). Since `gain_above_fair` is zero when there's a loss, and `loss_below_fair` is zero when there's a gain, the influences of `beta_gain` and `beta_loss` are structurally separated based on whether the offer is above or below fair. This separation should significantly improve parameter identifiability compared to the linearly coupled structure of Run 40.
*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens`, `fair_perc_self`, `gain_above_fair`, and `loss_below_fair` as described above.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.
*   **Model Formula:** Define the utility (`U`) based on `gain_above_fair`, `loss_below_fair`, and your 2 learnable parameters. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.
*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code (e.g., `beta_gain`, `beta_loss`). Define clear, generous, finite numerical bounds. Given their interpretation as sensitivity to gains and losses, these parameters are typically expected to be positive. Suggest bounds like [0, 10] or [0, 20] for both.
*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self`, `gain_above_fair`, `loss_below_fair`, etc.) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`, etc.).
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by deviations from the fair share, and specifically how the piecewise structure separates the influence of gains above fair from losses below fair to improve parameter identifiability compared to previous structures.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This piecewise approach is a strong candidate for achieving that separation.",,,,,,,,,,,,,,-0.08255615810604339,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.011756888233128161,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33,188.73975537638816,194.59144471228524,183.49761701298038,0.7710622710622711,"fair_perc_self = (token_self + token_opp == 0) ? 50.0 : (token_self / (token_self + token_opp)) * 100.0
deviation_from_fair = split_perc_self - fair_perc_self
gain_deviation = max(0.0, deviation_from_fair)
loss_deviation = min(0.0, deviation_from_fair)
U = beta_utility_fair + beta_gain_slope * gain_deviation + beta_loss_slope * loss_deviation","A three-parameter piecewise linear utility model for responder acceptance decisions. It defines a fair percentage based on token contributions and calculates deviations (gains and losses) from this point. Utility is modeled as a baseline at the fair percentage plus separate linear terms for gains and losses, each scaled by distinct gain and loss sensitivity parameters. This structure aims to improve the identifiability of parameters representing baseline utility, gain sensitivity, and loss sensitivity.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions for Run 33 of 100:
The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals.

In the previous run (Run 32), the two-parameter piecewise model had a `beta_utility_below` recovery of 0.673 (missing the threshold) and `beta_gain_slope` recovery of 0.713 (barely exceeding it). Accuracy was 0.679 and BIC was 49.95. While this model attempted to simplify the gain region, the identifiability of the parameters, particularly the baseline utility, was still challenging.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer using a piecewise structure based on the calculated fair percentage, but this time defining the utility in terms of a baseline at the fair point and distinct linear slopes for gains and losses relative to that point. This structure aims to improve parameter identifiability by explicitly separating the value placed on the 'fair' outcome from the sensitivity to deviations above or below fair.

*   **Parameter Identifiability:** Design the model structure such that each learnable parameter has a distinct and separable influence on the predicted utility. Focus on ensuring that the parameter representing utility *at* the fair point is distinguishable from parameters representing sensitivity to gains *above* fair and sensitivity to losses *below* fair.
*   **Reference Point & Deviations:** Continue to use the calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as a key reference point. Calculate `fair_perc_self` as `(token_self / (token_self + token_opp)) * 100`. **Crucially, handle the case where `token_self + token_opp == 0` by setting `fair_perc_self` to 50 in that instance.** Calculate the deviation from fair: `deviation_from_fair = split_perc_self - fair_perc_self`. Then calculate positive and negative deviations: `gain_deviation = max(0.0, deviation_from_fair)` and `loss_deviation = min(0.0, deviation_from_fair)`.
*   **Piecewise Structure Modification:** Define the utility (`U`) as follows:
    *   Start with a baseline utility at the fair percentage: `beta_utility_fair`.
    *   For gains (offers at or above fair): Add a linear term scaled by a gain sensitivity parameter (`beta_gain_slope`) multiplied by the `gain_deviation`.
    *   For losses (offers below fair): Add a linear term scaled by a loss sensitivity parameter (`beta_loss_slope`) multiplied by the `loss_deviation`.
    *   The resulting utility function structure should be: `U = beta_utility_fair + beta_gain_slope * gain_deviation + beta_loss_slope * loss_deviation`.
*   **Calculated Variables:** Ensure `fair_perc_self`, `deviation_from_fair`, `gain_deviation`, `loss_deviation`, and `U` are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`).
*   **Learnable Parameters:** Your model should have three learnable parameters: `beta_utility_fair`, `beta_gain_slope`, and `beta_loss_slope`. Define clear, generous, finite numerical bounds for all, e.g., [-20, 20]. Ensure parameter names are suitable for Python code.
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags, including the calculations for `fair_perc_self`, `deviation_from_fair`, `gain_deviation`, `loss_deviation`, and the final formula for `U` using the `max()` and `min()` functions as described above. Use clear notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the three-parameter piecewise model. Explain it uses the fair percentage as a reference, models a baseline utility at the fair point, and includes distinct linear sensitivities for gains (offers above fair) and losses (offers below fair). Mention how this aims to improve parameter identifiability compared to previous structures.

Think creatively about how different computational structures can capture responder behavior and fairness considerations while maintaining parameter identifiability. For this run, focus specifically on implementing and testing the proposed piecewise structure with separate gain and loss slopes relative to the fair point utility to address the identifiability challenges observed previously.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.44074395728711663,0.45487433233661684,0.7839789212188216,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30,188.94850162015072,159.01277780011648,215.76592087559803,0.7750305250305249,U = beta_base + (perc_gain_from_fair)**alpha_param - lambda_param * (perc_loss_from_fair)**beta_param,"A computational model predicting the utility of accepting an offer based on deviations from a calculated fair percentage, inspired by Prospect Theory. The utility is defined by a base utility at the fair point (beta_base), an additive term for gains (perc_gain_from_fair raised to the power alpha_param), and a subtractive term for losses (lambda_param multiplied by perc_loss_from_fair raised to the power beta_param). Beta_base represents the acceptance utility at the fair percentage. Alpha_param and beta_param capture diminishing or increasing sensitivity to the magnitude of gains and losses, respectively. Lambda_param captures loss aversion. The model is applied only when the participant is the responder. This structure aims to improve parameter identifiability compared to simpler linear models by introducing non-linearity and asymmetric scaling relative to the fair reference point.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,\
      ""source"": ""calculated""\
    }\
  }\
}\
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions for Run 30 of 100:
Previous attempts at modeling responder behavior have consistently struggled with parameter recovery. Models based on linear, quadratic, power-law, simple threshold, and asymmetric linear effects from a fair percentage reference point have all failed to achieve recovery >= 0.7 for *all* learnable parameters. Despite sometimes achieving reasonable BIC or accuracy, models with unidentifiable parameters are unusable.

For this run, the **absolute highest priority remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the primary objective. Improved BIC and accuracy are secondary goals that may follow from better parameter identifiability.

While the previous attempt utilizing the calculated fair percentage as a reference point and modeling asymmetric deviations was a step towards a different structural approach, it still did not result in all parameters being reliably recoverable (specifically, parameters related to *gains* and the *base utility* remained problematic).

**Guidance for the Next Model (Refining Identifiability through Reference Point Deviations):**

Design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer, prioritizing parameter identifiability by refining the structural approach based on deviations from the fair percentage reference point.

*   **Parameter Identifiability:** Focus intensely on designing the model structure such that each learnable parameter has a distinct and separable influence on the predicted utility across the dataset's trials. Consider how parameters interact and how this interaction might lead to confounding effects on estimation.
*   **Reference Point & Deviations:** Continue to use the calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as a key reference point. Calculate `fair_perc_self` as `(token_self / (token_self + token_opp)) * 100`. **Crucially, handle the case where `token_self + token_opp == 0` by setting `fair_perc_self` to 50 in that instance.** Define deviations from this reference point as `perc_gain_from_fair = max(0, split_perc_self - fair_perc_self)` and `perc_loss_from_fair = max(0, fair_perc_self - split_perc_self)`.
*   **Refining Model Structure for Gains and Base Utility:** The linear additive structure `U = beta_base + beta_gain * perc_gain_from_fair - beta_loss * perc_loss_from_fair` from the previous attempt did not achieve full parameter recovery. While the `beta_loss` parameter (sensitivity to negative deviations) showed better recovery, the parameters related to *gains* (`beta_gain`) and the *base utility* (`beta_base`) were difficult to estimate reliably.
    *   Consider keeping the impact of losses as `- beta_loss * perc_loss_from_fair` or a similar linear form, as this component seemed more identifiable.
    *   Focus your ""out-of-the-box"" thinking and structural innovation on how positive deviations (`perc_gain_from_fair`) influence utility, and how the base utility (`beta_base`) is incorporated. The simple linear scaling `beta_gain * perc_gain_from_fair` appears insufficient for parameter identifiability in this dataset. Explore *alternative functional forms* for the gain component (e.g., non-linear transformations of `perc_gain_from_fair`, interaction terms between `beta_base` and gains, or piecewise functions specifically for gains) to create a more separable influence for the parameter(s) governing sensitivity to positive deviations. The goal is to design a structure where the influence of gains and the base utility can be distinctly estimated.
*   **Calculated Variables:** Ensure `fair_perc_self`, `perc_gain_from_fair`, and `perc_loss_from_fair` (and any other variables you define based on data inputs) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs.
*   **Parameter Constraints:** Define clear, generous, finite numerical bounds for *all* learnable parameters.
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Use clear notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure, its key components (especially the use of fair percentage as a reference point and the refined approach to modeling gains/base utility), and how it relates to the decision process, focusing on the structural choices aimed at identifiability.

Design a model that you believe has the best chance of achieving parameter recovery >= 0.7 for all its learnable parameters by carefully considering the functional form of utility relative to the calculated fair percentage, particularly focusing on how the impact of gains and the base utility can be made uniquely estimable given the dataset's characteristics. Think critically about potential confounds and how your chosen structure mitigates them.

Please think through this step by step, focusing intently on designing a model structure that improves parameter identifiability by refining the reference point and asymmetric deviations approach, especially for gains and the base utility, and ensure all output requirements are met. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",,,,,,,,,,0.7681292545216171,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.18295756664240545,0.1544800945484808,0.48530383936393556,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29,205.65902844579134,185.44890603117506,223.76392977555176,0.7835775335775336,utility = beta_base + beta_gain * perc_gain_from_fair - beta_loss * perc_loss_from_fair,"A utility model for the responder's acceptance decision based on deviations from a calculated fair percentage. The fair percentage is determined by the proportion of tokens contributed by the participant relative to the total. The model calculates positive and negative deviations of the proposed share from this fair percentage. The utility of accepting an offer is a linear combination of a base utility (beta_base) and these positive (perc_gain_from_fair) and negative (perc_loss_from_fair) deviations, each scaled by a potentially asymmetric sensitivity parameter (beta_gain and beta_loss). This structure aims to improve parameter identifiability by distinctly modeling the impact of offers better or worse than the fairness reference point.",v0,"<think>
The user wants updated instructions for the modeling LLM.
The goal is to improve model performance, especially parameter recovery (>= 0.7 for all).
The previous run's model (piecewise threshold) had poor recovery for most parameters, despite being a novel structure compared to earlier linear/power-law models. Only `beta_utility_below_threshold` recovered well. This suggests that while piecewise might be a good direction, the specific implementation (learned absolute percentage threshold, linear utility *above* threshold) failed to disentangle parameters like `beta_zero`, `beta_amount`, and `fairness_threshold_perc`.

The instructions need to:
1.  Reiterate the task, data, and standard output format.
2.  Emphasize the failure of *all* previous models (linear, quadratic, power-law, piecewise) on parameter recovery across *all* learnable parameters. State clearly that recovery < 0.7 for *any* parameter makes the model unusable, and this is the absolute highest priority.
3.  Point out that varying the functional form (linear, quadratic, power) or adding simple thresholds hasn't solved the fundamental identifiability problem.
4.  Suggest a *different* structural approach, building on the previous instruction's ideas but focusing on the deviation from the calculated fair percentage (`fair_perc_self`) as a central reference point.
5.  Specifically propose a model structure that incorporates asymmetric effects for offers *better than* vs. *worse than* the fair percentage, potentially like a piecewise linear function *around* the `fair_perc_self` reference point (e.g., `U = beta_base + beta_gain * max(0, offer - fair) + beta_loss * max(0, fair - offer)` using percentages). This is a common structure in behavioral economics/psychology (like Prospect Theory value functions centered at a reference point).
6.  Guide the LLM to define `fair_perc_self` as a calculated variable, handling the edge case.
7.  Guide the LLM to define the positive and negative deviation variables based on percentage differences from `fair_perc_self`.
8.  Remind the LLM about defining all variables (data, calculated, learnable), their bounds (finite, generous for learnable), and ensuring all variables in the <MODEL> are described in <VARIABLES>.
9.  Update the run number in the instructions.
10. Do not refer to the previous interaction or mention that I am improving previous instructions. The instructions should be self-contained as if they are the first time the LLM is receiving them, but incorporating lessons from the prior runs.

Drafting the core ""Instructions"" section:
- Start by clearly stating the objective: design a responder model (`trial_role == 1`) focusing on utility prediction for acceptance.
- Immediately state the main challenge observed in previous runs: consistent failure to achieve sufficient parameter recovery (all parameters >= 0.7). Mention that multiple structural variations have been attempted and failed recovery, despite sometimes achieving decent BIC or accuracy.
- Reiterate that parameter recovery is the *absolute highest priority* because models with unidentifiable parameters are unusable.
- Introduce the proposed structural theme: modeling utility based on the *relative* value of the offer compared to the *calculated fair percentage* derived from token contributions (`fair_perc_self`).
- Define `fair_perc_self` calculation clearly, including the edge case.
- Suggest creating calculated variables for positive and negative deviations from `fair_perc_self` using `split_perc_self`.
- Propose a specific structural form: utility as a baseline plus potentially different sensitivities (learnable parameters) for these positive and negative deviations. This moves beyond simple additive or multiplicative effects of amount/deviation and tests if sensitivity to *signed deviation from fair percentage* is a more identifiable component.
- Remind about bounds, variable descriptions, and the output format.
- Add a touch encouraging ""out-of-the-box"" thinking *within this suggested direction* to find the best functional form (e.g., maybe not strictly linear deviation, but powers, although linear is a good start given past power failures). But focus the novelty on the reference point and asymmetric deviation structure.

Refining the ""Guidance for the Next Model"":
- Keep the core requirements list.
- Merge the general instructions and the specific guidance.
- Structure: Intro -> Problem (Recovery Failure) -> Priority -> Lessons Learned (Past structures failed recovery) -> Proposed New Direction (Reference Point/Asymmetric Deviation) -> Specific Guidance (Calculate fair_perc_self, define deviations, suggest utility form) -> Output Requirements Reminder.

Okay, constructing the final instruction text. I will ensure it flows logically and clearly states the updated priority and the suggested structural approach, while retaining all necessary context and formatting requirements.Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions for Run 29 of 100:
Previous attempts at modeling responder behavior have consistently struggled with parameter recovery. Models based on linear, quadratic, or power-law combinations of proposed amount/percentage and unfair deviation have failed to achieve recovery >= 0.7 for all parameters. A recent attempt using a piecewise structure based on a learned percentage threshold also resulted in poor recovery for most parameters, rendering the model unusable despite exploring a different structural form.

For this run, the **absolute highest priority remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. A computational model is unusable if its parameters cannot be reliably estimated, regardless of its fit (BIC) or predictive accuracy on observed data. You must design a model structure where each learnable parameter has a distinct and separable influence on the predicted utility across the dataset's trials.

Given the repeated failure of previous structural approaches, it is clear that a fundamentally *different way of conceptualizing utility* in this task is needed to improve parameter identifiability. Sticking to variations of adding penalty terms to a base utility seems insufficient.

**Guidance for the Next Model (Focusing on Identifiability via Reference Point):**

Design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer, prioritizing parameter identifiability through a novel structural approach based on deviations from a reference point.

*   **Parameter Identifiability:** Focus intensely on designing the model such that the impact of changing one learnable parameter cannot be replicated simply by changing another. Consider the correlations between variables and how your parameters interact with them.
*   **Reference Point & Asymmetric Deviations:** A promising direction, different from previous models, is to model utility relative to a salient reference point. In this task, the calculated ""fair share"" based on token contributions is a strong candidate for a reference point.
    *   First, calculate the fair percentage for the participant (`fair_perc_self`) based on token contributions: `fair_perc_self = (token_self / (token_self + token_opp)) * 100`. **Crucially, handle the case where `token_self + token_opp == 0` by setting `fair_perc_self` to 50 in that instance.**
    *   Next, define variables representing the participant's proposed share (`split_perc_self`) relative to this calculated fair percentage. Consider the *deviations* from this fair percentage:
        *   `perc_gain_from_fair = max(0, split_perc_self - fair_perc_self)` (positive deviation)
        *   `perc_loss_from_fair = max(0, fair_perc_self - split_perc_self)` (negative deviation)
*   **Model Structure:** Design the utility function as a combination of a base utility (`beta_base`) and the influence of these deviations. Consider allowing for *asymmetric* sensitivities to positive vs. negative deviations relative to the fair percentage. A structure like `U = beta_base + beta_gain * perc_gain_from_fair + beta_loss * perc_loss_from_fair` is one example of this approach, allowing `beta_gain` and `beta_loss` to capture different impacts of receiving more or less than the fair share. Explore variations on how these deviations influence utility (e.g., perhaps non-linearly, but focus first on disentangling the *sources* of influence - base, gain deviation, loss deviation).
*   **Calculated Variables:** Ensure `fair_perc_self`, `perc_gain_from_fair`, and `perc_loss_from_fair` (and any other variables you define based on data) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs to these calculations (`token_self`, `token_opp`, `split_perc_self`).
*   **Parameter Constraints:** Define clear, generous, finite numerical bounds for *all* learnable parameters (`beta_base`, `beta_gain`, `beta_loss` etc.). Consider carefully if parameters should be constrained (e.g., non-negative/non-positive) based on the psychological interpretation and their role in making them identifiable, but generous finite bounds are required.
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Use clear notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include bounds and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure, its key components (especially the use of fair percentage as a reference point and asymmetric deviation sensitivities), and how it relates to the decision process, with a focus on the *novelty* or structural changes aimed at identifiability.

Design a model that you believe has the best chance of achieving parameter recovery >= 0.7 for all its learnable parameters by adopting a structural form that explicitly uses deviation from the calculated fair percentage as a reference point, allowing for potentially asymmetric influences of receiving more vs. less than fair. Think critically about how each parameter influences decisions and how this influence can be uniquely captured by the model structure given the characteristics of the dataset. Do not be afraid to propose a structure that is significantly different from previous attempts based on simple penalty terms or fixed thresholds.

Please think through this step by step, focusing intently on designing a model structure that improves parameter identifiability through a novel approach centered on the calculated fair percentage reference point and asymmetric deviations, and ensure all output requirements are met. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
</previous_instructions>",,,,,,,,,,0.6954164570594396,,,,0.46740941974721123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7479595049101375,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13,213.36733872147295,164.48174215454054,257.16068564601653,0.7286324786324786,"U = (alpha - beta_step * max(0, -sign(split_perc_self - fair_perc_self))) * split_perc_self","A piecewise linear utility model for the responder's decision. The utility of accepting an offer is proportional to the percentage offered to the participant (split_perc_self). The proportionality constant (slope) is determined by two parameters: a base sensitivity (alpha) applied when the offer is at or above the participant's fair share percentage (fair_perc_self), and a reduced sensitivity (alpha - beta_step) applied when the offer is strictly below the fair share percentage. Fair share is calculated based on token contributions. The model has two learnable non-negative parameters, alpha and beta_step, which influence the sensitivity to fair/generous and unfair offers, respectively.",v0,"Based on the history of model performance, particularly the challenge of achieving parameter recovery >= 0.7 for *all* learnable parameters, it is evident that the way fairness influences utility needs careful structural consideration. Models using continuous penalties or bonuses based on the magnitude of deviation from fair share, as well as a recent attempt using a fixed penalty step, have struggled to make their parameters sufficiently identifiable despite sometimes achieving good overall fit (BIC) and accuracy.

The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. A model is unusable otherwise. We must continue to prioritize model structures that allow parameters to influence the utility function in maximally separable and identifiable ways given the dataset characteristics.

For this run (Run 13), let's explore a model structure where fairness acts as a threshold that changes the *rate* at which utility accumulates with the offered percentage, rather than adding a fixed penalty or a penalty proportional to the deviation magnitude. This means the *slope* of the utility function with respect to `split_perc_self` will differ depending on whether the offer is below or above the calculated proportional fair share (`fair_perc_self`).

Consider a model where the utility of accepting an offer has:
1.  A base sensitivity (slope) to the offered percentage (`split_perc_self`), scaled by a parameter `alpha`. This slope applies when the offer is at or above the proportional fair share (`fair_perc_self`).
2.  A *reduced* sensitivity (slope) when the offer falls strictly below the proportional fair share. The reduction in slope is scaled by a parameter `beta_step`.

This structure posits that participants value offers less *per percentage point* when the offer is considered unfair based on token contributions. This model has two learnable parameters: `alpha` representing the primary slope (sensitivity to fair/generous offers) and `beta_step` representing the magnitude of the slope reduction for unfair offers. Both parameters should be constrained to be non-negative.

**Proposed Model Structure Idea:**

The utility `U` is calculated such that the relationship with `split_perc_self` is:
-   Linear with slope `alpha` when `split_perc_self >= fair_perc_self`.
-   Linear with slope `alpha - beta_step` when `split_perc_self < fair_perc_self`.

This creates a piecewise linear function of `split_perc_self` with a pivot point at `fair_perc_self`.

**Guidance for the Next Model (Run 13):**

Design a computational model implementing the ""piecewise slope"" concept described above. This model structure simplifies the influence of fairness by making it change the responsiveness to the offer based on the `fair_perc_self` threshold, aiming to improve the separability and identifiability of the parameters compared to previous continuous or fixed-step penalty approaches.

*   **Prioritize Parameter Recovery:** Design the model structure to ensure `alpha` and `beta_step` have maximally distinguishable effects on the utility function given the data.
*   **Parameter Constraints:** Define clear, generous, finite, non-negative bounds for both `alpha` and `beta_step`. Suggest `alpha` bounds [0, 10] and `beta_step` bounds [0, 10].
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using standard mathematical notation. Ensure the formula reflects the change in slope below `fair_perc_self`. You can implement the piecewise slope logic using an indicator function `I` that is 1 if `split_perc_self < fair_perc_self` and 0 otherwise. A mathematical way to write this indicator is `max(0, -sign(split_perc_self - fair_perc_self))`. The structure should be `U = (alpha - beta_step * I) * split_perc_self`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags, including the specified bounds and learnable status for `alpha` and `beta_step`. Remember that `fair_perc_self` is a calculated variable derived from `token_self` and `token_opp`.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags.

Think creatively about how to implement the ""slope changes below fair"" logic concisely within the single expression required for the <MODEL> tag using available mathematical functions (`max`, `sign`, etc.) to represent the conditional nature of the slope. The core logic is: Utility = (Effective Sensitivity) * Offer, where Effective Sensitivity depends on whether Offer < FairShare.

Focus intently on this distinct structural approach – separating base slope from a fairness-dependent slope reduction – as a strategy to finally achieve the necessary parameter identifiability.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember `fair_perc_self` can be calculated as `(token_self / (token_self + token_opp)) * 100`. Use variables relevant for responder trials (`trial_role == 1`).

For run 13 of 100, please think through this step by step, focusing intently on designing a model structure that maximizes parameter identifiability given the data and the history of failed recovery attempts, specifically implementing the described piecewise slope model. Then provide your model specification, variable descriptions, target variable, and summary.",,,,,,,,,,,,,0.366222677723078,,,,,,,,,,,,,0.5345283270611857,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
92,220.58382993823395,228.6468028058621,213.3607500776504,0.7515262515262515,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_value * split_perc_self - beta_penalty * log(1.0 + loss_below_fair)","This model predicts the utility of accepting a proposed offer based on the offered percentage and the degree of unfairness. Utility is calculated as an additive combination of a linear term scaled by `beta_value`, which represents sensitivity to the proposed share, and a penalty term scaled by `beta_penalty`, which captures aversion to unfairness. The penalty term uses a log-transformed value of the difference between the fair share and the proposed share (`loss_below_fair`), but only applies when the offer is unfair (`loss_below_fair > 0`). This additive structure, combined with the linear influence of `split_perc_self` and the non-linear, conditional influence of `loss_below_fair` via the log transformation, is designed to create distinct and separable influence profiles for `beta_value` and `beta_penalty`, aiming to improve robust parameter identifiability.",v0,"Okay, here are the improved instructions for the next run (Run 92 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,
      ""source"": ""calculated""
    },\\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,\\\
      ""source"": ""data""
    },\\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1,\
        ""exclusive_max"": true
      },\\\
      ""learnable"": false,
      ""source"": ""data""
    },\\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {\
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\\
      ""learnable"": false,
      ""source"": ""calculated""
    }\
  }\
}\
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 92 of 100).

The previous attempt using the multiplicative interaction structure (`U_accept = split_perc_self * (beta_gain - beta_unfairness * loss_below_fair)`) resulted in critically poor parameter recovery (`beta_gain`: -0.036, `beta_unfairness`: 0.281) and a high BIC (592.11), indicating this specific structure failed to create sufficiently distinct influence profiles for the parameters or introduced numerical instability. Robust parameter identifiability (recovery >= 0.7 for *all* learnable parameters simultaneously) remains the **unwavering primary objective**. We must find a model structure where the influence of the two learnable parameters is genuinely separable and reliably estimable.

Given the ongoing challenges, we need to continue exploring different structural principles. The failure of the previous multiplicative structure suggests that combining the 'gain' and 'penalty' influences within a single multiplicative factor tied to `split_perc_self` might be the core problem, leading to non-identifiability.

Let's return to the principle that utility is influenced by the offer itself (`split_perc_self`) and a penalty when the offer is unfair (`loss_below_fair > 0`), but structure these influences **additively** again, while introducing a **gentle non-linearity** to the penalty term to potentially improve identifiability compared to purely linear additive models.

Here are the standard calculation steps for necessary intermediate variables. You *must* include these first before defining `U_accept`:
1.  Calculate `sum_tokens = token_self + token_opp`.
2.  Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.
3.  Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is positive if the offer is below the fair share, and 0 otherwise. Use `Max` for this calculation.

Now, design a novel formula for `U_accept` using `split_perc_self`, `loss_below_fair`, and exactly two learnable parameters. The formula for `U_accept` must implement the principle of utility being an additive combination of:
1.  A term linear in `split_perc_self`, scaled by the first learnable parameter.
2.  A penalty term, scaled by the second learnable parameter, that is based on `loss_below_fair` but uses a gentle non-linear transformation (like `log(1.0 + loss_below_fair)`) and is only active when `loss_below_fair > 0`.

A suggested specific structure to explore that follows this principle is:
`U_accept = beta_value * split_perc_self - beta_penalty * log(1.0 + loss_below_fair)`
This structure represents utility as a base sensitivity to the proposed percentage (`beta_value`) combined additively with a penalty that is triggered by unfairness (`loss_below_fair > 0`) and scaled by `beta_penalty`. The use of the `log(1.0 + loss_below_fair)` transformation is a gentle non-linearity intended to capture diminishing sensitivity to increasing unfairness and differentiate the penalty's influence from the linear `split_perc_self` term, thereby improving identifiability.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names (e.g., `beta_value`, `beta_penalty`). Define clear, generous, finite numerical bounds.
    *   Parameter 1 (e.g., `beta_value`): Should represent the linear sensitivity to the proposed percentage (`split_perc_self`). Non-negative bounds, e.g., `[0.0, 10.0]`.
    *   Parameter 2 (e.g., `beta_penalty`): Should scale the non-linear penalty term based on `loss_below_fair`. Non-negative bounds, e.g., `[0.0, 10.0]`.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is an additive combination of a linear gain from the proposed percentage and a non-linear penalty for unfair offers). **Crucially, explain *how* this specific, additive structure with a linear term for `split_perc_self` and a log-transformed penalty term for `loss_below_fair` is designed to create mathematically distinct and separable influence profiles for the two learnable parameters.** Highlight that `beta_value` influences utility linearly based on `split_perc_self` across all trials, while `beta_penalty` *only* influences the utility calculation on unfair trials (where `loss_below_fair > 0`) through a different functional form (`log(1.0 + loss_below_fair)`), and how this separation and difference in functional form is intended to improve robust joint parameter identifiability compared to previous attempts (especially the failed multiplicative structure and the linear-additive structure).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately. Avoid structures that have failed recovery previously. Explore this specific additive structure combining linear gain and log-transformed penalty as detailed above, focusing on how this structural separation and difference in functional form creates distinct influence profiles for the two betas.",,,,,,,,,,,,,,,,,,0.5729555479926468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0007020712334596278,,,,,,,,,,,,
39,244.31621385681038,239.31592672509504,248.79563774563871,0.7936507936507936,"sum_tokens = token_self + token_opp
fair_perc_self = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)
loss_below_fair = MAX(0.0, fair_perc_self - split_perc_self)
U = beta_intercept + beta_split_perc * split_perc_self - beta_loss_penalty * loss_below_fair","This model predicts the utility of accepting an offer in the ultimatum game for the responder role. Utility is calculated as an additive combination of a baseline utility, a term linear in the proposed percentage share for the participant, and a penalty term applied only when the proposed share is below the participant's fair share based on token contributions. The model utilizes three learnable parameters: a baseline intercept (beta_intercept), a weight for the proposed percentage (beta_split_perc), and a weight for the loss below fairness (beta_loss_penalty). This additive structure is designed to enhance parameter identifiability compared to multiplicative or highly coupled model forms previously attempted, by ensuring each parameter has a distinct influence on the overall utility function.",v0,"Instructions for Run 39 of 100:

The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals.

Prior attempts have consistently struggled with parameter identifiability, leading to poor recovery values well below the 0.7 threshold across various model structures (simple linear combinations, piecewise linear centered on deviation, and most recently, Run 38's multiplicative structure `U = kappa * (split_perc_self - lambda_param * loss_below_fair - mu)`). The poor recovery in Run 38 (kappa: -0.014, lambda_param: 0.242, mu: 0.546) strongly suggests that scaling an entire expression containing other parameterized terms creates significant identifiability issues. The parameters' influences are too coupled.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer. Your model should utilize the proposed share percentage (`split_perc_self`) and a calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as key inputs.

*   **Parameter Identifiability is Paramount:** The central challenge is to design a mathematical structure (formula for `U`) using these inputs and **2 or 3 learnable parameters** such that these parameters have distinct and identifiable influences on utility across the range of possible offers. **Explicitly avoid structures where parameters multiply or scale expressions containing other learnable parameters or multiple variable terms** (like the `kappa * (...)` structure in Run 38). Instead, favor structures where parameters additively scale *individual* input terms or clearly defined derived variables. This additive separation tends to improve identifiability.
*   **Promising Structural Direction:** Based on the analysis of previous failures, a promising structural direction is a model where utility is an additive combination of a baseline offset, a term related to the proposed percentage, and a term related to losses below the fair percentage, each scaled by a distinct learnable parameter. Consider a structure like:
    `U = beta_intercept + beta_split_perc * split_perc_self - beta_loss_penalty * loss_below_fair`
    Where `loss_below_fair` is calculated as `max(0.0, fair_perc_self - split_perc_self)`.
    This structure uses 3 parameters (`beta_intercept`, `beta_split_perc`, `beta_loss_penalty`) that each contribute additively to the utility calculation, reducing the coupling seen in the previous multiplicative model. `beta_intercept` shifts the baseline, `beta_split_perc` scales the effect of the proposed offer, and `beta_loss_penalty` scales the specific penalty incurred only when the offer is below fair. This distinct influence is expected to aid parameter recovery.
*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self` directly as a data variable.
    *   Calculate `sum_tokens` as `token_self + token_opp`.
    *   Calculate `fair_perc_self` as `(token_self / sum_tokens) * 100.0` if `sum_tokens > 0`, and `50.0` if `sum_tokens == 0`.
    *   Calculate `loss_below_fair` as `max(0.0, fair_perc_self - split_perc_self)`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.
*   **Model Formula:** Define the utility (`U`) based on `split_perc_self`, `fair_perc_self` (or derived variables like `loss_below_fair`), and your 2 or 3 learnable parameters. Structure the formula to maximize the distinct, additive influence of each parameter.
*   **Learnable Parameters:** Your model must have exactly 2 or 3 learnable parameters. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-50, 50] or similar, depending on how parameters scale inputs). For the suggested structure, consider bounds like [-50, 50] for `beta_intercept`, [-10, 10] for `beta_split_perc`, and [0, 20] for `beta_loss_penalty`.
*   **Calculated Variables:** Ensure `sum_tokens`, `fair_perc_self`, `loss_below_fair` (and any other variables you calculate as intermediate steps) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`, etc.).
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by the proposed share and fairness/loss, and how your chosen additive mathematical structure and parameterization aim to improve parameter identifiability compared to previously attempted structures.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that out-of-the-box thinking *within* the constraints of parameter identifiability and the 2/3 parameter limit is encouraged.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-0.06712639225663083,0.6835300919685589,,,,,0.28265469443615715,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35,250.45103135924992,248.51659425032236,252.18396460266422,0.7252747252747253,"sum_tokens = token_self + token_opp
fair_perc_self = (token_self / sum_tokens) * 100 if sum_tokens > 0 else 50.0
deviation_from_fair = split_perc_self - fair_perc_self
U = beta_gain_slope * max(0.0, deviation_from_fair) + beta_loss_slope * min(0.0, deviation_from_fair)","A two-parameter piecewise linear model predicting the utility of accepting an offer in the responder role. The model calculates the participant's fair share percentage based on token contributions, treating zero total tokens as a 50/50 fair split. Utility at this fair point is implicitly fixed at zero. Utility then changes linearly with separate slopes (gain_slope and loss_slope) for deviations above and below the fair share. This structure aims to improve parameter identifiability by reducing parameters and fixing a reference point compared to previous models.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,\n      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions for Run 35 of 100:
The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals.

In the previous run (Run 34), the four-parameter discontinuous piecewise linear model (`beta_utility_at_fair_below`, `beta_slope_below`, `beta_utility_at_fair_above`, `beta_slope_above`) failed to achieve acceptable parameter recovery for the ""utility at fair"" parameters and the gain slope parameter (`r=0.064`, `r=0.055`, `r=0.644` respectively). The BIC was also high (268.71). This suggests the specific parameterization involving hypothetical utility levels *at* the fair point for two different extrapolated lines created identifiability problems.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer using a **simplified piecewise linear structure relative to the calculated fair percentage**. This structure aims to improve parameter identifiability by reducing the number of parameters and fixing the reference point utility.

*   **Parameter Identifiability:** Focus on creating a model structure where each learnable parameter has a clearly separable influence on the utility function across the range of possible offers. Reducing the number of parameters and defining utility relative to a fixed point (fair = 0) should help distinguish the impact of gain sensitivity from loss sensitivity.
*   **Reference Point & Deviations:** Continue to use the calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as a key reference point. Calculate `fair_perc_self` as `(token_self / (token_self + token_opp)) * 100`. **Handle the case where `token_self + token_opp == 0` by setting `fair_perc_self` to 50 in that instance.** Calculate the deviation from fair: `deviation_from_fair = split_perc_self - fair_perc_self`.
*   **Piecewise Structure:** Define the utility (`U`) using a piecewise linear structure where the utility *at the fair point* (`deviation_from_fair = 0`) is implicitly fixed at 0. The utility function will have separate linear slopes for gains and losses relative to this zero point.
    *   For gains (`deviation_from_fair > 0`), utility increases linearly with a slope `beta_gain_slope`.
    *   For losses (`deviation_from_fair < 0`), utility decreases linearly with a slope `beta_loss_slope` multiplied by the negative deviation.
    *   At the fair point (`deviation_from_fair = 0`), utility is 0.
*   **Model Formula:** Implement this structure with the following formula: `U = beta_gain_slope * max(0.0, deviation_from_fair) + beta_loss_slope * min(0.0, deviation_from_fair)`.
    *   In this formula, `max(0.0, deviation_from_fair)` captures the gain component (0 for losses, positive for gains).
    *   `min(0.0, deviation_from_fair)` captures the loss component (negative for losses, 0 for gains).
    *   `beta_gain_slope` should ideally be positive for utility to increase with gains.
    *   `beta_loss_slope` should ideally be positive for utility to decrease with losses (since `min` is negative).
*   **Calculated Variables:** Ensure `sum_tokens`, `fair_perc_self`, `deviation_from_fair`, and `U` are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`).
*   **Learnable Parameters:** Your model should have two learnable parameters: `beta_gain_slope` and `beta_loss_slope`. Define clear, generous, finite numerical bounds for both, e.g., [-20, 20]. Ensure parameter names are suitable for Python code.
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags, including the calculations for `sum_tokens`, `fair_perc_self`, `deviation_from_fair`, and the final formula for `U` using standard mathematical notation (e.g., `max`, `min`).
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the two-parameter piecewise linear model. Explain that it uses the fair percentage as a reference point, where utility is fixed at zero, and defines separate linear sensitivities (slopes) for offers above (gains) and below (losses) this point. Mention how this simplified structure aims to improve parameter identifiability compared to previous models.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.46842421127327427,0.7880913183072209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77,256.34872936302236,265.3386149011077,248.29529023515425,0.6117216117216117,"sum_tokens = token_self + token_opp
fair_perc_self_calc = If(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = (beta_value * split_perc_self) / Exp(beta_unfairness * loss_below_fair)","A divisive utility model for responder decisions. The utility of accepting an offer is based on the proposed percentage share for the participant (scaled by beta_value) and is reduced by an exponential penalty term in the denominator. The penalty is determined by the difference between the proposed share and a fairness calculation based on token contributions (loss_below_fair), scaled by beta_unfairness. This exponential divisive structure, where beta_unfairness influences the rate of exponential decay, is designed to create mathematically distinct and interacting influence profiles for the two parameters, aiming to improve parameter identifiability compared to previous linear models.",v0,"Okay, here are the improved instructions for the next run (Run 77 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,\
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,\
        ""max"": 1,\
        ""exclusive_max"": true
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,\
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 77 of 100), based on the results of recent attempts.

The model tested in the previous run (Run 76), the divisive structure `U_accept = (beta_value * split_perc_self) / (1.0 + beta_unfairness * loss_below_fair)`, achieved excellent fit metrics (BIC = 43.01, Accuracy = 0.662), significantly outperforming previous attempts in this regard. This suggests that the divisive principle and the calculation of `loss_below_fair` based on token contributions are capturing important aspects of the data.

However, the **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. The previous model, despite its good fit, failed this critical benchmark with recovery values of r = 0.436 for `beta_value` and r = 0.445 for `beta_unfairness`. A model with poor parameter recovery is unusable, as the estimated parameters cannot be reliably interpreted.

This indicates that while the divisive *principle* is promising for fit, the *specific functional form* used in the denominator (`1.0 + beta_unfairness * loss_below_fair`) did not create sufficiently separable influence profiles for `beta_value` and `beta_unfairness` to allow for robust parameter identifiability.

For Run 77, we will build on the success of the divisive structure's fit by exploring an alternative, potentially more strongly non-linear, functional form for the unfairness penalty in the denominator.

Proposed Model Structure: A divisive utility model where the core value derived from the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, is *reduced* by a factor based on unfairness (`loss_below_fair`), controlled by `beta_unfairness`, but using an exponential function in the denominator.

`U_accept = (beta_value * split_perc_self) / exp(beta_unfairness * loss_below_fair)`

Here:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
The utility `U_accept` is calculated as the product of `beta_value` and `split_perc_self`, divided by the exponential of (`beta_unfairness` times `loss_below_fair`).

In this structure, `beta_value` still scales the numerator representing the base value from the offer percentage. `beta_unfairness`, however, now influences the *rate* of an exponential decay in the denominator based on `loss_below_fair`. For offers that are fair or better (`loss_below_fair = 0`), the denominator is `exp(0) = 1`, and utility is `beta_value * split_perc_self`. For unfair offers (`loss_below_fair > 0`), the denominator `exp(beta_unfairness * loss_below_fair)` increases rapidly with increasing `loss_below_fair`, causing a strong, non-linear reduction in utility via division. This interaction, where `beta_value` scales linearly and `beta_unfairness` scales an exponential rate, is hypothesized to create mathematically distinct and more separable influence profiles for the parameters across the range of possible offers compared to the previous linear term in the denominator, thereby improving parameter identifiability. `beta_value` represents the base sensitivity to the offer, and `beta_unfairness` represents how strongly increasing unfairness causes this sensitivity to be *exponentially divided down*.

Design the model using this new exponential-divisive structure: `U_accept = (beta_value * split_perc_self) / exp(beta_unfairness * loss_below_fair)`, where `loss_below_fair` and `fair_perc_self_calc` are calculated as specified above using `token_self`, `token_opp`, and `split_perc_self`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific exponential-divisive structure, by placing `beta_unfairness` as a multiplier within the exponential function in the denominator, is intended to achieve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles that are more separable than those generated by the previous linear denominator structure.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Remember to use `exp()` for the exponential function.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (base value sensitivity) and `beta_unfairness` (unfairness exponential decay rate). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and likely necessary for parameter recovery in this structure (e.g., `beta_unfairness` should be non-negative so the denominator increases with `loss_below_fair`, and `beta_value` should be non-negative for utility to increase with `split_perc_self` on fair offers). Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (divisive structure where utility from `split_perc_self` is reduced by an *exponential* penalty based on unfairness). Crucially, explain *how* this specific exponential-divisive structure, where `beta_unfairness` influences the *rate* of exponential decay in the denominator, is specifically intended to improve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles for `beta_value` and `beta_unfairness`, addressing the limitations and recovery failures seen in previous models, including the previous linear-divisive attempt.
*   **Think Outside the Box:** Parameter identifiability is the most difficult challenge. Recognize that success often requires exploring less standard functional forms and interaction principles beyond simple sums or products. The previous divisive model got good fit but failed recovery; this suggests the *way* parameters interact is key. Continue to consider how parameters might interact via division, exponents, thresholds, logs, or other non-linear functions that haven't been tried yet, focusing intently on creating distinctly shaped influences for each parameter across the range of possible offers.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.
</thinking>```
Okay, here are the improved instructions for the next run (Run 77 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 77 of 100), based on the results of recent attempts.

The model tested in the previous run (Run 76), the divisive structure `U_accept = (beta_value * split_perc_self) / (1.0 + beta_unfairness * loss_below_fair)`, achieved excellent fit metrics (BIC = 43.01, Accuracy = 0.662), significantly outperforming previous attempts in this regard. This suggests that the divisive principle and the calculation of `loss_below_fair` based on token contributions are capturing important aspects of the data.

However, the **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. The previous model, despite its good fit, failed this critical benchmark with recovery values of r = 0.436 for `beta_value` and r = 0.445 for `beta_unfairness`. A model with poor parameter recovery is unusable, as the estimated parameters cannot be reliably interpreted.

This indicates that while the divisive *principle* is promising for fit, the *specific functional form* used in the denominator (`1.0 + beta_unfairness * loss_below_fair`) did not create sufficiently separable influence profiles for `beta_value` and `beta_unfairness` to allow for robust parameter identifiability.

For Run 77, we will build on the success of the divisive structure's fit by exploring an alternative, potentially more strongly non-linear, functional form for the unfairness penalty in the denominator.

Proposed Model Structure: A divisive utility model where the core value derived from the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, is *reduced* by a factor based on unfairness (`loss_below_fair`), controlled by `beta_unfairness`, but using an exponential function in the denominator.

`U_accept = (beta_value * split_perc_self) / exp(beta_unfairness * loss_below_fair)`

Here:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
The utility `U_accept` is calculated as the product of `beta_value` and `split_perc_self`, divided by the exponential of (`beta_unfairness` times `loss_below_fair`).

In this structure, `beta_value` still scales the numerator representing the base value from the offer percentage. `beta_unfairness`, however, now influences the *rate* of an exponential decay in the denominator based on `loss_below_fair`. For offers that are fair or better (`loss_below_fair = 0`), the denominator is `exp(0) = 1`, and utility is `beta_value * split_perc_self`. For unfair offers (`loss_below_fair > 0`), the denominator `exp(beta_unfairness * loss_below_fair)` increases rapidly with increasing `loss_below_fair`, causing a strong, non-linear reduction in utility via division. This interaction, where `beta_value` scales linearly and `beta_unfairness` scales an exponential rate, is hypothesized to create mathematically distinct and more separable influence profiles for the parameters across the range of possible offers compared to the previous linear term in the denominator, thereby improving parameter identifiability. `beta_value` represents the base sensitivity to the offer, and `beta_unfairness` represents how strongly increasing unfairness causes this sensitivity to be *exponentially divided down*.

Design the model using this new exponential-divisive structure: `U_accept = (beta_value * split_perc_self) / exp(beta_unfairness * loss_below_fair)`, where `loss_below_fair` and `fair_perc_self_calc` are calculated as specified above using `token_self`, `token_opp`, and `split_perc_self`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific exponential-divisive structure, by placing `beta_unfairness` as a multiplier within the exponential function in the denominator, is intended to achieve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles that are more separable than those generated by the previous linear denominator structure.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Remember to use `exp()` for the exponential function.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (base value sensitivity) and `beta_unfairness` (unfairness exponential decay rate). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and likely necessary for parameter recovery in this structure (e.g., `beta_unfairness` should be non-negative so the denominator increases with `loss_below_fair`, and `beta_value` should be non-negative for utility to increase with `split_perc_self` on fair offers). Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (divisive structure where utility from `split_perc_self` is reduced by an *exponential* penalty based on unfairness). Crucially, explain *how* this specific exponential-divisive structure, where `beta_unfairness` influences the *rate* of exponential decay in the denominator, is specifically intended to improve robust parameter identifiability by creating mathematically distinct and *interacting* influence profiles for `beta_value` and `beta_unfairness`, addressing the limitations and recovery failures seen in previous models, including the previous linear-divisive attempt.
*   **Think Outside the Box:** Parameter identifiability is the most difficult challenge. Recognize that success often requires exploring less standard functional forms and interaction principles beyond simple sums or products. The previous divisive model got good fit but failed recovery; this suggests the *way* parameters interact is key. Continue to consider how parameters might interact via division, exponents, thresholds, logs, or other non-linear functions that haven't been tried yet, focusing intently on creating distinctly shaped influences for each parameter across the range of possible offers.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.
```",,,,,,,,,,,,0.7103050063255903,,,,,,-0.058883795898159924,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34,268.71112814493205,199.13941584748312,331.03578707806344,0.7554945054945055,"sum_tokens = token_self + token_opp
fair_perc_self = (sum_tokens == 0) ? 50.0 : (token_self / sum_tokens) * 100.0
deviation_from_fair = split_perc_self - fair_perc_self
U = (deviation_from_fair < 0.0) ? (beta_utility_at_fair_below + beta_slope_below * deviation_from_fair) : (beta_utility_at_fair_above + beta_slope_above * deviation_from_fair)","A computational model for responder decisions using a discontinuous piecewise linear utility function. The model calculates a fair percentage based on token contributions and defines the utility of accepting based on the deviation of the proposed share from this fair reference point. For offers below the fair percentage, utility is a linear function with intercept 'beta_utility_at_fair_below' and slope 'beta_slope_below'. For offers at or above the fair percentage, utility is a different linear function with intercept 'beta_utility_at_fair_above' and slope 'beta_slope_above'. This structure uses four learnable parameters and aims to improve parameter identifiability by allowing distinct utility functions for the domains below and at/above the fair reference.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions for Run 34 of 100:
The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals.

In the previous run (Run 33), the three-parameter piecewise linear model based on utility *at* fair had significant parameter identifiability issues. `beta_utility_fair` (r=0.441) and `beta_gain_slope` (r=0.455) recovery was poor, while `beta_loss_slope` (r=0.784) was good. BIC was 188.74 and accuracy was 0.771. This suggests the specific way the parameters were defined around the 'fair' point made the baseline and gain sensitivity hard to distinguish.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer using a **discontinuous piecewise linear structure** based on the calculated fair percentage. This means the utility function will have two distinct linear components: one for offers below the fair percentage and one for offers at or above the fair percentage. This structure aims to improve parameter identifiability by allowing the utility function to have different ""intercepts"" and slopes on either side of the fair reference point.

*   **Parameter Identifiability:** Design the model structure such that each learnable parameter has a distinct and separable influence. Focus on ensuring the parameters defining the function *below* fair are distinguishable from each other and from the parameters defining the function *at or above* fair. This discontinuous structure might help separate the influence of the baseline utility level from the sensitivity to deviation within each domain (below vs. at/above fair).
*   **Reference Point & Deviations:** Continue to use the calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as a key reference point. Calculate `fair_perc_self` as `(token_self / (token_self + token_opp)) * 100`. **Crucially, handle the case where `token_self + token_opp == 0` by setting `fair_perc_self` to 50 in that instance.** Calculate the deviation from fair: `deviation_from_fair = split_perc_self - fair_perc_self`.
*   **Piecewise Structure:** Define the utility (`U`) using a conditional structure:
    *   If `deviation_from_fair < 0` (offer is below fair): `U = beta_utility_at_fair_below + beta_slope_below * deviation_from_fair`. This defines a linear function for the loss domain, where `beta_utility_at_fair_below` is the hypothetical utility at fair according to this loss function, and `beta_slope_below` is the sensitivity to losses. For utility to decrease as deviation decreases, `beta_slope_below` should ideally be positive.
    *   If `deviation_from_fair >= 0` (offer is at or above fair): `U = beta_utility_at_fair_above + beta_slope_above * deviation_from_fair`. This defines a linear function for the gain domain, where `beta_utility_at_fair_above` is the hypothetical utility at fair according to this gain function, and `beta_slope_above` is the sensitivity to gains. For utility to increase as deviation increases, `beta_slope_above` should ideally be positive.
*   **Calculated Variables:** Ensure `fair_perc_self`, `deviation_from_fair`, and `U` are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`).
*   **Learnable Parameters:** Your model should have four learnable parameters reflecting this piecewise structure: `beta_utility_at_fair_below`, `beta_slope_below`, `beta_utility_at_fair_above`, and `beta_slope_above`. Define clear, generous, finite numerical bounds for all, e.g., [-20, 20]. Ensure parameter names are suitable for Python code.
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags, including the calculations for `fair_perc_self`, `deviation_from_fair`, and the final conditional formula for `U` using notation suitable for mathematical expression (e.g., ternary operator `? :`).
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the four-parameter discontinuous piecewise linear model. Explain it uses the fair percentage as a reference, defines separate linear utility functions for offers below fair and at/above fair, each with its own intercept (at the fair point) and slope. Mention how this discontinuous structure aims to improve parameter identifiability compared to previous continuous or simpler piecewise structures.

Think creatively about how different functional forms and piecewise definitions can capture responder behavior and fairness considerations while maintaining parameter identifiability. For this run, focus specifically on implementing and testing the proposed discontinuous piecewise structure with separate parameters for the regions below and at/above the fair point.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions.
</previous_instructions>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.06387011940070132,0.7719141825889456,0.05510940443815364,0.6439497286642114,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58,286.16232437259384,282.03885325463744,289.8562672490964,0.7670940170940171,U = beta_value * split_perc_self + beta_unfairness_effect * unfair_deviation_magnitude,"This model predicts the utility of accepting an offer in the ultimatum game responder role using two learnable parameters. The utility is an additive combination of two components: the proposed percentage share for the participant (scaled by beta_value) and a penalty for unfair offers (scaled by beta_unfairness_effect). The unfairness penalty is calculated based on the magnitude of the negative deviation from the token-based fair share and is only active when the proposed share is below the fair share. This asymmetric and piecewise structure is designed to enhance parameter identifiability by allowing beta_value to influence utility across all offers based on their absolute value, while beta_unfairness_effect specifically penalizes the degree of unfairness only when the offer is disadvantageous, creating distinct functional roles for the parameters.",v0,"Okay, here are the improved instructions for the next run (Run 58), building on the results of the previous runs.

Instructions for Run 58 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 57) explored a multiplicative interaction model (`U = beta_zero + beta_interaction * split_perc_self * fairness_deviation`), based on the analysis that simple additive or piecewise scaling of value and fairness terms was failing parameter recovery. Unfortunately, Run 57 also failed the primary objective, with both `beta_zero` (r = 0.635) and `beta_interaction` (r = 0.584) falling below the 0.7 threshold. Its overall performance (BIC 465.08, Acc 0.697) was moderate.

Analysis of Run 57's failure and previous models: The core identifiability challenge persists. It seems that simply combining terms additively, piecewise, or even through a linear multiplicative interaction has not created sufficiently distinct influences for the two learnable parameters to be reliably estimated from this dataset. The influence of fairness/relative value is still somehow confounded with or overshadowed by the influence of the absolute proposed value, regardless of the exact mathematical form (linear, log, quadratic, interaction) or combination method (additive, piecewise) used previously.

We need to find a mathematical structure for utility, still using exactly **2 learnable parameters**, where their influence on responder decisions (`trial_role == 1`) is fundamentally more separable. This requires moving beyond models where parameters primarily serve as simple scaling factors for additive components.

Based on the persistent difficulty with previously explored structures (additive value + fairness, simple piecewise, linear + log, linear + interaction), focus your exploration for Run 58 on a model structure that combines features in a novel, potentially asymmetric, or piecewise manner designed specifically to isolate the influence of different components.

Consider the following strategy to enhance identifiability by making the parameters' influences active in distinct ways:

*   **Asymmetric Piecewise Additive Model:** Design a model where one parameter scales the *absolute proposed percentage share*, which influences utility on *all* trials. The second parameter, however, *only* influences utility on trials where the offer is *unfair* (below the calculated fair share), and it scales the *magnitude of that unfair deviation*.
    *   *Example Idea:* Let one parameter (`beta_value`) scale `split_perc_self`. Let the second parameter (`beta_unfairness_effect`) scale the magnitude of the *negative* deviation from fairness (`fair_perc_self_calc - split_perc_self`), but only when this deviation is positive (i.e., when `split_perc_self` is less than `fair_perc_self_calc`).
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensure this handles `sum_tokens=0`).
        *   `unfair_deviation_magnitude = max(0, fair_perc_self_calc - split_perc_self)` (This term is 0 if the offer is fair or better, and positive if unfair, representing the degree of unfairness).
        *   `U = beta_value * split_perc_self + beta_unfairness_effect * unfair_deviation_magnitude` (Combine linear value with a term representing the magnitude of unfairness, expecting `beta_unfairness_effect` to be negative to apply a penalty).
    *   *Structural Principle:* This structure attempts identifiability by having `beta_value` exert influence across the entire range of `split_perc_self`, while `beta_unfairness_effect` *only* exerts influence in the specific regime of unfair offers, scaling a term (`unfair_deviation_magnitude`) that specifically captures the *size* of the unfairness, not the proposed value itself. This asymmetry and specific targeting of unfairness magnitude by one parameter, while the other scales overall value, is intended to create more separable effects than previous models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and the specific intermediate term `unfair_deviation_magnitude` as defined above. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. For the suggested model, use names like `beta_value` and `beta_unfairness_effect`. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-10.0, 10.0] or wider/narrower if appropriate for the scale of your terms, which are percentages). Consider the expected sign (e.g., `beta_unfairness_effect` should likely be negative), but keep bounds wide enough.

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, `unfair_deviation_magnitude`, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` explicitly mentions the `sum_tokens > 0` condition and the default value. The description for `unfair_deviation_magnitude` should clarify what it represents and when it's non-zero.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the participant's proposed percentage share percentage.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific mathematical structure (e.g., the asymmetric piecewise influence, one parameter scaling overall value, the other scaling specific unfairness magnitude only in the loss domain) is designed to ensure parameter identifiability for *both* learnable parameters*, making their roles distinct from previously attempted additive or simple multiplicative models.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. Previous attempts combining linear value terms with linear or non-linear fairness/penalty terms (additively or piecewise) have struggled with the fairness parameter's recovery. Explore models with asymmetric piecewise structures like the one suggested, where parameters target different aspects (overall value vs. specific unfairness magnitude) and different regimes (all offers vs. only unfair offers), to create more separable influences. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation.",,,,,,,,,,,,,,,,,,0.7735260886369447,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6594151910310619,,,,,,,,,,,,,,,,
50,306.97131416331445,276.12155197443576,334.6075594575183,0.7811355311355311,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (token_self / sum_tokens) * 100
fairness_deviation = split_perc_self - fair_perc_self_calc
U = beta_value * split_perc_self + beta_fairness_deviation * fairness_deviation","A linear utility model for responder decisions. Utility to accept an offer is a weighted sum of the proposed percentage share for the participant and the deviation of this proposed share from the calculated fair share based on token contributions. The model uses two learnable parameters: beta_value, which scales the influence of the proposed percentage itself, and beta_fairness_deviation, which scales the influence of the fairness deviation. This structure aims to disentangle the value and fairness components of the offer to improve parameter identifiability.",v0,"Okay, here are the improved instructions for the next run (Run 50), building on the results of the previous runs.

Instructions for Run 50 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The last run (Run 49) attempted a simple additive linear model (`U = beta_proposed_share * split_perc_self + beta_fair_share * fair_perc_self_calc`) and unfortunately failed the primary objective of achieving parameter recovery >= 0.7 for *all* learnable parameters, specifically for `beta_proposed_share` (r = 0.378). It also resulted in a very high BIC (371.03), indicating poor model fit despite reasonable accuracy (0.730). This suggests the simple additive structure based on `split_perc_self` and `fair_perc_self_calc` did not yield sufficiently separable parameter influences or capture the data structure well.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`) while also improving model fit (lower BIC, higher accuracy). We seek a model that achieves full recovery and improves upon the current best recoverable model (BIC 40.45, accuracy 0.662, recovery >= 0.795).

Given the failure of the simple additive structure in Run 49 to achieve necessary identifiability, let's explore an alternative linear approach that might better disentangle the influences of the offer's value and its fairness. Design a model where utility is a linear combination of the *proposed percentage share itself* and the *deviation of the proposed share from the calculated fair percentage share*. This is a linear model, but with components structured differently than in Run 49.

*   **Structural Direction (Linear with Value and Deviation Components):** Model utility (`U`) as a weighted sum of the proposed share and the difference between the proposed share and the fair share.
    *   Calculate the necessary intermediate variables to get `fair_perc_self_calc`:
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensures `fair_perc_self_calc` is >= 0).
    *   Calculate the deviation from fairness:
        *   `fairness_deviation = split_perc_self - fair_perc_self_calc`
    *   Structure the utility (`U`) using a linear combination of the proposed share and this deviation:
        *   `U = beta_value * split_perc_self + beta_fairness_deviation * fairness_deviation`
    *   This structure uses two distinct learnable parameters (`beta_value` and `beta_fairness_deviation`) that separately weight the influence of the proposed percentage itself and the degree to which it deviates from fairness. The hope is that using `split_perc_self` and `fairness_deviation` (which is a transformation involving both `split_perc_self` and `fair_perc_self_calc`) as the components will provide more separable influences on utility compared to the simple additive model in Run 49.

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens`, `fair_perc_self_calc`, `fairness_deviation`, and `U`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.

*   **Model Formula:** Define the utility (`U`) using the linear combination. Include the calculation steps for necessary derived variables (`sum_tokens`, `fair_perc_self_calc`, `fairness_deviation`) before the final formula for `U`. Use clear mathematical notation.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   `beta_value`: Sensitivity parameter scaling the proposed percentage share (`split_perc_self`). Suggest generous bounds like [-10.0, 10.0].
    *   `beta_fairness_deviation`: Sensitivity parameter scaling the deviation from the fair percentage share (`fairness_deviation`). Suggest generous bounds like [-10.0, 10.0].
    Choose descriptive names suitable for Python code that reflect the variable they scale. Define clear, generous, finite numerical bounds for both.

*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self_calc`, `fairness_deviation`, `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`).

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea that utility is determined by a linear combination of the proposed percentage (`split_perc_self`) and its deviation from the calculated fair percentage (`fairness_deviation`). Explain how `beta_value` weights the influence of the proposed amount itself, and `beta_fairness_deviation` weights the influence of how far that offer is from the fairness point, noting that this structure attempts to improve parameter identifiability by separating these two influences on utility.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This linear approach with components representing value and fairness deviation is a new structural attempt after the previous additive model failed. If this structure also fails to meet the recovery criterion for both parameters, continue to be prepared to think outside the box for subsequent iterations – consider alternative ways fairness might modulate sensitivity, or other combinations/transformations of variables that might create more separable parameter influences within the 2-parameter constraint. The key is finding a structure where the two parameters’ contributions to utility are as independent as possible across the dataset.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,,,,,0.7175732851247132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6525415003825366,,,,,,,,,,,,,,,,,,,,,,,,
52,311.8590845366236,280.04247280349307,340.3614658808863,0.7634310134310135,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)
perc_difference = split_perc_self - fair_perc_self_calc
utility = beta_value * split_perc_self + beta_unfairness_penalty * MIN(0.0, perc_difference)","A responder model calculating utility based on the proposed share for self relative to a calculated fair share. Utility is a linear combination of the proposed percentage share itself, scaled by 'beta_value', and a penalty term for offers below the fair share, scaled by 'beta_unfairness_penalty'. The penalty term is zero for fair or above-fair offers and linearly decreases utility as the offer goes below fair. This piecewise structure on fairness deviation aims to provide distinct roles for the two learnable parameters, enhancing identifiability.",v0,"Okay, here are the improved instructions for the next run (Run 52), building on the results of the previous runs.

Instructions for Run 52 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The last run (Run 51) attempted a non-linear structure using a quadratic term for fairness deviation (`U = beta_value * split_perc_self - beta_fairness_penalty * (fair_perc_self_calc - split_perc_self)^2`). Unfortunately, this model performed poorly on parameter recovery (beta_value r=0.359, beta_fairness_penalty r=0.630) and had a high BIC (401.85), although accuracy was decent (0.758). The structure, despite being non-linear in the *inputs*, failed to create sufficiently distinct influences for the *two learnable parameters* across the data, leading to poor identifiability. The quadratic deviation term wasn't structurally separable enough from the linear proposed share term to allow independent estimation of their scaling parameters.

Crucially, the current best usable model in terms of parameter recovery (Run 1: BIC 40.53, Acc 0.671, Recovery >= 0.727 for both parameters) uses a *piecewise linear function* applied to the ratio of proposed share to fair share (`R_diff = R_val - 1`). This piecewise structure, separating the influence of gains vs. losses relative to fairness and scaling each side with a *different* parameter (`beta_gain_linear_diff` and `beta_loss_linear_diff`), proved successful in achieving parameter identifiability. This suggests that segmenting the utility function based on the fairness comparison point (`R_val == 1` or `R_diff == 0`) might be a promising strategy for creating distinct parameter influences.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`) while also aiming for improved model fit (lower BIC, higher accuracy). We seek a model that achieves full recovery (>= 0.7 for both parameters) and improves upon the current best recoverable model (Run 1 BIC 40.53, accuracy 0.671).

Based on the success of piecewise models and the failure of recent linear/simple non-linear combinations to ensure parameter identifiability, we should continue exploring structures that segment or non-linearly transform the relationship between proposed share and fairness in a way that gives the two parameters genuinely independent roles.

*   **Structural Direction (Variations on Piecewise/Non-Linear Ratios):** Focus on model structures that build upon the concept of comparing the proposed share to the calculated fair share (`fair_perc_self_calc`), potentially using ratios (`R_val`) or differences (`R_diff`), and applying non-linear or piecewise functions scaled by the two parameters. The goal is to avoid structures where the parameters' influences are simple linear combinations of the inputs or trivially confusable.
    *   **Avoid:** Any model that simplifies to `U = a * split_perc_self + b * fair_perc_self_calc`, regardless of how `fairness_deviation` is defined. Also avoid structures where the two parameters simply scale two terms that are highly correlated or linearly dependent in the data, as this will hinder recovery (as seen in Run 51).
    *   **Explore:**
        *   **Piecewise functions on Ratios/Differences:** Revisit the piecewise structure based on `R_val` or `R_diff` (where the split is at the fairness point, e.g., `R_val == 1` or `R_diff == 0`). Can we use different *non-linear* functions on each side, scaled by the two parameters? (e.g., `U = IF(R_diff >= 0, beta_gain * log(R_val), beta_loss * (R_diff)^2)` - ensure you have exactly 2 parameters).
        *   **Single Non-Linear Function with Two Parameters:** Can you devise a single non-linear function of `R_val` or `R_diff` (or `split_perc_self` and `fair_perc_self_calc`) where the two parameters influence different aspects of the function's shape (e.g., sensitivity and a curvature/asymmetry parameter)?
        *   **Transformations of Inputs:** Apply non-linear transformations (e.g., log, exponential, power) to `split_perc_self` and/or `fair_perc_self_calc` *before* combining them, but ensure the combination method (e.g., multiplicative interaction, non-linear sum) and parameter placement leads to distinct parameter influences.
        *   **Out of the Box:** Think creatively about how fairness perception might alter the *shape* of the value function for the proposed share, or how the proposed share influences sensitivity to fairness deviations. The structure `U = f(split_perc_self, fair_perc_self_calc, beta1, beta2)` must be such that `beta1` and `beta2` are identifiable. This often requires them to scale different non-linearly related terms or influence distinct regimes or components of the function.

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate intermediate variables as needed (e.g., `sum_tokens`, `fair_perc_self_calc`, ratios, differences, non-linear transformations like `R_val`, `R_diff`, logs, powers, etc.) and the final `U`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions. Remember `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` for robustness.

*   **Model Formula:** Define the utility (`U`) using your chosen non-linear structure. Include calculation steps for any necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   Choose descriptive names suitable for Python code.
    *   Ensure their placement in the formula is such that their influences on `U` are as distinct and independent as possible.
    *   Define clear, generous, finite numerical bounds for both (e.g., [-10.0, 10.0], [-5.0, 5.0], or adjusted based on the scale of the variables they interact with/scale).

*   **Calculated Variables:** Ensure all calculated variables used in the model (including `sum_tokens`, `fair_perc_self_calc`, any intermediate terms like ratios/differences/transformed values, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`). Ensure the description for `fair_perc_self_calc` includes the `sum_tokens > 0` condition.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the proposed percentage share.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea and how its structure aims to achieve parameter identifiability (e.g., by using distinct non-linear functions in different regimes, or parameters controlling different aspects of a non-linear function), explaining how the two parameters scale distinct components or influences within this non-linear structure.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This requires moving beyond simple linear combinations of the base inputs or their linear transformations, and finding clever ways to make the parameters' roles non-linearly separable across the range of data points. The success of the piecewise ratio model suggests segmenting based on fairness is a good direction. Be creative and explore different non-linear functions and interactions!",,,,,,,,,,,,,,,,,,0.7683976903431333,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5611715459895855,,,,,,,,,,,,,,,,,,,,,,
40,340.56112038486503,417.3137888798987,271.8035215247307,0.6297313797313797,"sum_tokens = token_self + token_opp
fair_perc_self = \begin{cases} 50.0 & \text{if } sum\_tokens = 0 \\ \frac{token\_self}{sum\_tokens} \times 100.0 & \text{if } sum\_tokens > 0 \end{cases}
deviation_from_fair = split_perc_self - fair_perc_self
loss_below_fair = \max(0.0, fair_perc_self - split_perc_self)
U = beta\_linear\_relative \times deviation\_from\_fair - beta\_extra\_loss \times loss\_below\_fair","The model predicts responder utility based on the deviation of the proposed percentage from a calculated fair percentage, which is determined by the participant's token contribution relative to the total tokens. Utility linearly increases with the proposed share relative to fair (parameterized by `beta_linear_relative`), and an additional penalty is applied when the offer is below fair, proportional to the magnitude of the loss below fair (parameterized by `beta_extra_loss`). This 2-parameter structure is designed to disentangle the effects of general fairness sensitivity and specific loss aversion, aiming for improved parameter identifiability.",v0,"Instructions for Run 40 of 100:

The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals.

The previous run (Run 39) utilized the additive structure `U = beta_intercept + beta_split_perc * split_perc_self - beta_loss_penalty * loss_below_fair`. This model achieved the best BIC (244.32) and accuracy (0.794) seen so far, demonstrating good predictive power. However, parameter recovery remained poor (`beta_intercept: r = -0.067`, `beta_split_perc: r = 0.684`, `beta_loss_penalty: r = 0.283`). Crucially, the recovery for `beta_intercept` and `beta_loss_penalty` was well below the 0.7 threshold, rendering the model unusable despite its predictive performance.

The poor recovery of the `beta_intercept` suggests it is highly collinear with other terms or that its influence is not well-identified separately from the effects captured by `split_perc_self` and `loss_below_fair`. The `beta_loss_penalty` also struggled, indicating its influence is not distinct enough in that specific additive structure.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer. Your model should utilize the proposed share percentage (`split_perc_self`) and a calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as key inputs.

*   **Parameter Identifiability is Paramount:** The central challenge remains designing a mathematical structure (formula for `U`) using these inputs and exactly **2 learnable parameters** such that these parameters have distinct and identifiable influences on utility. Building on previous attempts, particularly the piecewise approaches and the need to avoid the intercept term, consider models where utility is centered around the 'fair share' concept.
*   **Promising Structural Direction (Focus on Deviation from Fair):** Based on the failure of the intercept and the partial success of loss-related parameters in previous runs, a promising direction is a 2-parameter model that calculates utility based on the deviation from the fair share, explicitly allowing for potentially different sensitivities to deviations above and below fair, but using only *two* core parameters. Consider a structure like:
    `U = beta_linear_relative * deviation_from_fair - beta_extra_loss * max(0.0, -deviation_from_fair)`
    Where:
    *   `deviation_from_fair` is calculated as `split_perc_self - fair_perc_self`.
    *   `max(0.0, -deviation_from_fair)` is equivalent to `max(0.0, fair_perc_self - split_perc_self)`, representing the 'loss below fair'.
    This structure models utility as linearly dependent on the deviation from fair (`beta_linear_relative`), with an *additional* penalty (`beta_extra_loss`) applied specifically when the offer is below fair. Utility is implicitly zero when the offer is exactly fair (`split_perc_self == fair_perc_self`). This approach removes the problematic intercept and tries to make the influence of a general deviation (`beta_linear_relative`) and the specific loss penalty (`beta_extra_loss`) more separable, which should aid parameter recovery compared to previous structures.
*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self` directly as a data variable.
    *   Calculate `sum_tokens` as `token_self + token_opp`.
    *   Calculate `fair_perc_self` as `(token_self / sum_tokens) * 100.0` if `sum_tokens > 0`, and `50.0` if `sum_tokens == 0`.
    *   Calculate `deviation_from_fair` as `split_perc_self - fair_perc_self`.
    *   Calculate `loss_below_fair` as `max(0.0, fair_perc_self - split_perc_self)`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions.
*   **Model Formula:** Define the utility (`U`) based on `deviation_from_fair` and `loss_below_fair` (or equivalents), and your 2 learnable parameters. Structure the formula as described above or a similar deviation-based 2-parameter model that removes the intercept. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.
*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code (e.g., `beta_linear_relative`, `beta_extra_loss`). Define clear, generous, finite numerical bounds (e.g., [-10, 10] for a scaling parameter, [0, 20] for a penalty parameter that should typically be positive).
*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self`, `deviation_from_fair`, `loss_below_fair`, etc.) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`, etc.).
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by the deviation from the fair share and how your chosen 2-parameter mathematical structure aims to improve parameter identifiability compared to previous structures by removing the intercept and separating the roles of the parameters.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that out-of-the-box thinking *within* the constraints of **exactly 2 learnable parameters** and robust parameter identifiability is encouraged. Your goal is to find a mathematical structure that allows both parameters to be uniquely and reliably estimated from the data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.815021606501516,0.21814304553059827,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37,352.58789701105235,313.0333546369373,388.0221745545304,0.7765567765567766,"sum_tokens = token_self + token_opp
fair_perc_self = if sum_tokens > 0 then (token_self / sum_tokens) * 100 else 50.0
loss_relative_to_fair = max(0.0, fair_perc_self - split_perc_self)
U = beta_split * split_perc_self - beta_unfair * loss_relative_to_fair","This computational model predicts the utility of accepting a proposed offer for the responder. It uses two learnable parameters: beta_split, representing sensitivity to the proposed percentage share, and beta_unfair, representing sensitivity to the magnitude by which the offer falls below a calculated fair share based on token contributions. Utility is modeled as a linear combination of the proposed percentage and a penalty term for offers below the fair share. This structure is designed with two parameters to enhance parameter identifiability by distinguishing general offer evaluation from specific sensitivity to perceived unfairness.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,\n      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions for Run 37 of 100:
The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals.

Prior attempts have struggled with parameter identifiability, particularly for baseline/intercept terms and sensitivity to offers at or above a calculated fair percentage. Simple linear models and models with strict piecewise structures have shown limitations.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer using a **two-parameter structure focused on the offered percentage and a penalty for offers below the calculated fair percentage**. This approach aims to improve parameter identifiability by avoiding an explicit general intercept and disentangling sensitivity to the absolute offer value from sensitivity to ""losses"" relative to the fair share.

*   **Parameter Identifiability:** Focus on creating a model structure where the two parameters have clearly separable influences. Modeling utility as a function of the proposed share *plus* a penalty term specifically for the magnitude by which the offer falls below the fair share may offer better identifiability than previous structures.
*   **Key Inputs:** Continue to use the calculated fair percentage for the participant (`fair_perc_self`) based on token contributions alongside the proposed share percentage (`split_perc_self`).
*   **Calculated Variables:**
    *   Calculate `sum_tokens` as `token_self + token_opp`.
    *   Calculate `fair_perc_self` as `(token_self / sum_tokens) * 100` if `sum_tokens > 0`, and `50.0` if `sum_tokens == 0`.
    *   Calculate `loss_relative_to_fair` as the positive difference between `fair_perc_self` and `split_perc_self` when `split_perc_self` is less than `fair_perc_self`, and 0 otherwise. Use `max(0.0, fair_perc_self - split_perc_self)`.
*   **Model Formula:** Define the utility (`U`) using the following formula: `U = beta_split * split_perc_self - beta_unfair * loss_relative_to_fair`.
    *   `beta_split` represents sensitivity to the proposed percentage (`split_perc_self`).
    *   `beta_unfair` represents sensitivity to the magnitude of the loss relative to the fair percentage (`loss_relative_to_fair`).
*   **Learnable Parameters:** Your model should have two learnable parameters: `beta_split` and `beta_unfair`. Define clear, generous, finite numerical bounds for both, e.g., [-20, 20]. Ensure parameter names are suitable for Python code.
*   **Calculated Variables:** Ensure `sum_tokens`, `fair_perc_self`, `loss_relative_to_fair`, and `U` are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`).
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags, including the calculation steps for `sum_tokens`, `fair_perc_self`, `loss_relative_to_fair`, and the final formula for `U`. Use clear mathematical notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the two-parameter model. Explain that it models utility as a linear function of the proposed share minus a penalty proportional to the magnitude by which the offer falls below the calculated fair share. Mention how this structure with only two parameters and a specific penalty term aims to improve parameter identifiability compared to prior attempts.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions.
</previous_instructions>",,,,,,,,,,,0.6273797785485362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.51755076005946,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38,355.4969629645033,360.92632115172626,350.6331629217828,0.7576312576312576,"sum_tokens = token_self + token_opp
fair_perc_self = (token_self / sum_tokens) * 100.0 if sum_tokens > 0 else 50.0
loss_below_fair = max(0.0, fair_perc_self - split_perc_self)
U = kappa * (split_perc_self - lambda_param * loss_below_fair - mu)","A computational model predicting the utility of accepting an offer in the ultimatum game responder role. Utility is determined by the proposed percentage share for the participant (split_perc_self) and a calculated fair percentage based on token contributions (fair_perc_self). The model uses three learnable parameters: kappa scales the overall utility, lambda_param introduces a non-linear penalty for offers below the fair share, and mu provides a baseline offset. This structure aims to provide distinct parameter influences for improved identifiability compared to simple linear models.",v0,"Instructions for Run 38 of 100:

The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals.

Prior attempts have struggled significantly with parameter identifiability, leading to poor recovery values (e.g., the model in the previous run, Run 37, which used the formula `U = beta_split * split_perc_self - beta_unfair * loss_relative_to_fair`, failed to achieve recovery above 0.7 for either parameter). This suggests that simple linear combinations of the proposed percentage and deviations from fairness may not yield uniquely identifiable parameters given this dataset structure.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer. Your model should utilize the proposed share percentage (`split_perc_self`) and a calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as key inputs.

*   **Parameter Identifiability is Key:** The central challenge is to design a mathematical structure (formula for `U`) using these inputs and **2 or 3 learnable parameters** such that these parameters have distinct and identifiable influences on utility across the range of possible offers. Avoid structures where the effect of changing one parameter can be easily compensated by changing another parameter. Simple linear or piecewise linear combinations that start from an intercept, or linearly combine terms like `split_perc_self` and `loss_relative_to_fair` (as in Run 37), have shown poor identifiability in previous attempts. Encourage thinking about novel ways these factors might combine or interact to determine utility.
*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self` directly as a data variable.
    *   Calculate `sum_tokens` as `token_self + token_opp`.
    *   Calculate `fair_perc_self` as `(token_self / sum_tokens) * 100` if `sum_tokens > 0`, and `50.0` if `sum_tokens == 0`.
    *   Your model may use `split_perc_self` and `fair_perc_self` directly, or variables derived from them (e.g., difference, ratio, gain/loss deviation). Define any such derived variables within your <MODEL> tag calculations.
*   **Model Formula:** Define the utility (`U`) based on `split_perc_self`, `fair_perc_self` (or variables derived from them), and your 2 or 3 learnable parameters. Focus on finding a mathematical relationship that allows for better parameter recovery than previously attempted linear/piecewise structures.
*   **Learnable Parameters:** Your model must have exactly 2 or 3 learnable parameters. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-50, 50] or similar, depending on how parameters scale inputs).
*   **Calculated Variables:** Ensure `sum_tokens` and `fair_perc_self` (and any other variables you calculate as intermediate steps in your model) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`, etc.).
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for `sum_tokens`, `fair_perc_self`, and any other derived variables needed before the final formula for `U`. Use clear mathematical notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea of how utility is determined by the proposed share and fairness, and how your chosen mathematical structure and parameterization aim to improve parameter identifiability compared to previous attempts.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.2424187474063783,,,,,,,,,,,,,,-0.014009745779948558,0.5455835132794438,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,360.18727826000065,308.07844706106494,406.8681062090472,0.7411477411477412,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = split_perc_self^beta_value - beta_unfairness * loss_below_fair","This model predicts the utility of accepting an offer in the Ultimatum Game for the responder role. Utility is calculated as the difference between a power-law function of the proposed percentage share for the participant and a linear penalty based on how far the offer is below a calculated fair share. The parameter beta_value acts as an exponent on the percentage gain, shaping its utility. The parameter beta_unfairness linearly scales the penalty associated with offers below the fair share, representing sensitivity to unfairness. This structure, where beta_value affects a non-linearity on gain and beta_unfairness scales a linear loss, is designed to create mathematically distinct parameter influences to improve robust joint identifiability compared to previous model attempts.",v0,"Okay, here are the improved instructions for the next run (Run 85 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,\
      ""source"": ""data""
    },\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },\
      ""learnable"": false,
      ""source"": ""data""
    },\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 85 of 100), based on the results of recent attempts.

The previous run (Run 84), which used a multiplicative structure where scaled percentage gain was multiplied by a divisive fairness penalty factor (`U_accept = beta_value * split_perc_self * (1.0 / (1.0 + beta_unfairness * loss_below_fair))`), showed some improvement in parameter recovery compared to previous attempts (r=0.542 for `beta_value`, r=0.507 for `beta_unfairness`). The BIC (42.99) was also relatively low, suggesting a reasonable fit to the data. However, critically, **neither parameter achieved the required >= 0.7 parameter recovery threshold**. This indicates that, while better, this specific multiplicative/divisive structure still did not create sufficiently distinct influence profiles for `beta_value` and `beta_unfairness` to allow for reliable joint estimation across trials.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation of individual differences and group effects, and overrides achieving a low BIC or high accuracy if recovery fails.

Given the repeated failures of additive, simple divisive, and the multiplicative/divisive structure in Run 84 to achieve joint parameter identifiability, we need to explore a different way the parameters interact or apply influence. Instead of simple scaling or scaling within a divisive factor, let's consider a structure where one parameter influences the *shape* of the utility derived from gain, and the other influences the *magnitude* of the penalty derived from loss, applied in a direct additive manner.

For Run 85, we will test a model structure inspired by value functions in behavioral economics, where the utility from gain is non-linear (governed by an exponent scaled by `beta_value`) and the utility from loss is linear (scaled by `beta_unfairness`).

Proposed Model Structure: The utility of accepting (`U_accept`) is calculated as the difference between a non-linear function of the participant's proposed percentage gain (`split_perc_self`, raised to the power of `beta_value`) and a linear function of the amount the offer is below the calculated fair share (`loss_below_fair`, scaled by `beta_unfairness`).

Here are the calculation steps:
1. Calculate `sum_tokens = token_self + token_opp`.
2. Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.
3. Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is 0 if the offer is fair or above fair, and positive if the offer is below fair, representing the degree of perceived loss or unfairness. Use `Max` for this calculation.
4. Calculate the utility: `U_accept = split_perc_self^beta_value - beta_unfairness * loss_below_fair`. Use standard mathematical notation for the exponent (`^`).

In this structure, `beta_value` acts as an exponent on the proposed percentage gain, influencing the *shape* of the gain utility function (e.g., diminishing returns if < 1, increasing returns if > 1). `beta_unfairness` scales the linear penalty term `loss_below_fair`, influencing the *magnitude* of the loss aversion. This separation, where `beta_value` affects a non-linearity applied to the gain and `beta_unfairness` scales a linear loss, is hypothesized to generate mathematically distinct influence profiles across trials, potentially improving robust joint parameter identifiability for *both* parameters compared to previous models.

Design the model using this new additive structure combining a power-law gain term and a linear loss term: `U_accept = split_perc_self^beta_value - beta_unfairness * loss_below_fair`. Remember to calculate intermediate variables (`sum_tokens`, `fair_perc_self_calc`, `loss_below_fair`) before the final `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula), using `IF`, `Max`, and `^`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific structure, using an exponent on the gain term (scaled by `beta_value`) and linear scaling on the loss term (scaled by `beta_unfairness`), is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (sensitivity/shape parameter for percentage gain) and `beta_unfairness` (sensitivity/aversion parameter for loss below fair). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds.
    *   `beta_value` (exponent): Non-negative bounds appropriate for exponents, e.g., `[0.01, 10.0]`. Must be > 0 to avoid issues with `0^0`.
    *   `beta_unfairness` (loss aversion): Non-negative bounds, e.g., `[0.0, 10.0]`. Must be >= 0 to represent aversion (or indifference).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is a power-law gain term minus a linear loss term). Crucially, explain *how* this specific structure, where `beta_value` is an exponent on gain and `beta_unfairness` linearly scales loss, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous models.
*   **Think Outside the Box:** Parameter identifiability for *both* parameters simultaneously remains the persistent, difficult challenge. Neither the divisive structures, simple additive linear/non-linear combinations, nor the multiplicative/divisive structure have solved this. This new power-law gain / linear loss structure is another attempt to find mathematically separable influences through a different type of parameter application (exponent vs linear scale). If this also fails to achieve >= 0.7 recovery for *both* parameters, the next iteration *must* consider even more fundamentally different structural approaches. Continue to focus intently on finding a functional form where varying `beta_value` has a clearly distinguishable effect on the utility curve across offers compared to varying `beta_unfairness`, allowing them to be estimated independently with high reliability, which is a prerequisite for better fit metrics. Consider models where parameters influence thresholding, decision-making rules, or alternative ways of conceptualizing and integrating fairness beyond standard additive or multiplicative structures. Think about how parameters could influence the exponent of a term, or scale terms that are then passed into a non-linear function together, or how parameters could determine which decision rule is applied.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,0.013465435125651114,,,,,,0.127816530924259,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14,362.13362314571305,324.9556116079103,395.438925148328,0.6153846153846154,"utility = alpha_fair_param * split_perc_self * (1 - max(0, -sign(split_perc_self - fair_perc_self))) + alpha_unfair_param * split_perc_self * max(0, -sign(split_perc_self - fair_perc_self)))","A piecewise linear utility model for the responder's decision based on the offered percentage relative to their proportional fair share. The model has two learnable parameters: alpha_fair_param representing the sensitivity (slope) to offers perceived as fair or generous (at or above the fair share), and alpha_unfair_param representing sensitivity to offers perceived as unfair (below the fair share). Both slopes are non-negative and bounded between 0 and 10. Utility is calculated as a function of the offered percentage, with the slope determined by whether the offer is above/at or below the fair share. This utility determines the probability of acceptance via a logistic function.",v0,"Based on the results of the previous run (Run 13) and the history of models, we continue the critical task of developing a computational model for the responder decision in the ultimatum game that achieves parameter recovery of >= 0.7 for *all* learnable parameters. The previous attempts, including continuous deviation penalties, fixed step penalties, and the most recent piecewise slope *reduction* model (Run 13), have consistently failed to meet this parameter identifiability threshold, despite some achieving reasonable BIC and accuracy scores.

The piecewise slope reduction model (Run 13), defined as `U = (alpha - beta_step * max(0, -sign(split_perc_self - fair_perc_self))) * split_perc_self`, resulted in recovery values of r = 0.366 for `alpha` and r = 0.535 for `beta_step`. While conceptually promising, this specific formulation, where `beta_step` represents a *reduction* from a base slope `alpha`, did not sufficiently separate the influence of the two parameters for successful recovery. It appears that disentangling a base sensitivity (`alpha`) from a reduction (`beta_step`) applied in only one condition (`split_perc_self < fair_perc_self`) remains challenging.

For this run (Run 14), let's evolve the piecewise slope concept to potentially improve parameter identifiability. Instead of modeling a base slope and a reduction, let's model the two slopes *independently*. This means defining a distinct slope for offers at or above the fair share (`fair_perc_self`) and an entirely separate slope for offers strictly below the fair share.

Consider a model where the utility of accepting an offer (`U`) is a piecewise linear function of the offered percentage (`split_perc_self`) with a pivot point at the calculated proportional fair share (`fair_perc_self`). The model should have two learnable parameters representing these two independent slopes:
1.  A parameter `alpha_fair` representing the sensitivity (slope) to the offered percentage (`split_perc_self`) when the offer is at or above the proportional fair share (`split_perc_self >= fair_perc_self`).
2.  A parameter `alpha_unfair` representing the sensitivity (slope) to the offered percentage (`split_perc_self`) when the offer is strictly below the proportional fair share (`split_perc_self < fair_perc_self`).

The hypothesis is that by assigning separate parameters to control the slope in these two mutually exclusive conditions (offers >= fair vs. offers < fair), their individual contributions to utility will become more separable and thus more identifiable by the fitting process. Both `alpha_fair` and `alpha_unfair` should be constrained to be non-negative, reflecting the assumption that utility does not decrease as the offered percentage increases within either range, but their magnitude relative to each other captures the aversion to unfairness.

**Proposed Model Structure Idea (Run 14):**

The utility `U` is calculated such that the relationship with `split_perc_self` is:
-   Linear with slope `alpha_fair` when `split_perc_self >= fair_perc_self`.
-   Linear with slope `alpha_unfair` when `split_perc_self < fair_perc_self`.

This structure creates a piecewise linear function of `split_perc_self` with independent slopes.

**Guidance for the Next Model (Run 14):**

Design a computational model implementing the ""independent piecewise slopes"" concept described above. This model structure is a direct evolution of the previous attempt, aiming specifically to enhance parameter identifiability by having parameters influence the utility function in distinct regions defined by the `fair_perc_self` threshold.

*   **Prioritize Parameter Recovery:** Design the model structure to ensure `alpha_fair` and `alpha_unfair` have maximally distinguishable effects on the utility function given the data.
*   **Parameter Constraints:** Define clear, generous, finite, non-negative bounds for both `alpha_fair` and `alpha_unfair`. Suggest bounds [0, 10] for both.
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using standard mathematical notation. Ensure the formula reflects the change in slope above/at vs. below `fair_perc_self`. You can implement the piecewise slope logic using indicator functions. Let `I_unfair` be an indicator function that is 1 if `split_perc_self < fair_perc_self` and 0 otherwise. A mathematical way to write this indicator is `max(0, -sign(split_perc_self - fair_perc_self))`. Let `I_fair` be the opposite indicator, 1 if `split_perc_self >= fair_perc_self` and 0 otherwise, which can be written as `(1 - I_unfair)`. The structure should be `U = alpha_fair * split_perc_self * I_fair + alpha_unfair * split_perc_self * I_unfair`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags, including the specified bounds and learnable status for `alpha_fair` and `alpha_unfair`. Remember that `fair_perc_self` is a calculated variable derived from `token_self` and `token_opp`.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags.

Focus intently on implementing this distinct structural approach – modeling two independent slopes based on the fairness threshold – as the next strategy to achieve the necessary parameter identifiability. Think creatively about the precise mathematical formulation within the single expression for the <MODEL> tag.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember `fair_perc_self` can be calculated as `(token_self / (token_self + token_opp)) * 100`. Use variables relevant for responder trials (`trial_role == 1`).

For run 14 of 100, please think through this step by step, focusing intently on designing a model structure that maximizes parameter identifiability given the data and the history of failed recovery attempts, specifically implementing the described independent piecewise slope model. Then provide your model specification, variable descriptions, target variable, and summary.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.054650876570212,0.20391308024517574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15,369.24577395472943,296.9884295149973,433.97631168198944,0.6678876678876678,"U = alpha_sensitivity * split_perc_self - beta_unfair_penalty * max(0, -sign(split_perc_self - fair_perc_self)) + gamma_generous_bonus * max(0, sign(split_perc_self - fair_perc_self))","A utility model for responder decisions. Utility is based on a linear sensitivity to the offered percentage (split_perc_self) scaled by alpha_sensitivity. This base utility is additively adjusted: a fixed penalty beta_unfair_penalty is subtracted if the offer is strictly unfair (below the proportional fair share), and a fixed bonus gamma_generous_bonus is added if the offer is strictly generous (above the proportional fair share). Proportional fair share is determined by token contributions. Learnable parameters alpha_sensitivity, beta_unfair_penalty, and gamma_generous_bonus have non-negative finite bounds.",v0,"Based on the results of the previous run and the instructions used for that run, generate improved instructions for the *next* run. Aim to guide the main modeling LLM towards better performance (e.g., lower BIC, better parameter recovery, and higher accuracy). Please keep in mind that if any of the learnable parameters have a parameter recovery value of less than ~0.7, then the model is unusable. You will first be given the total context for the previous run, then reminded specifically of the instructions used for that run that you should update.


Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: The previous attempts to model responder behavior, including those using continuous deviation penalties, fixed step penalties, and different forms of piecewise linear sensitivity based on the proportional fair share, have consistently failed to achieve the required parameter recovery of >= 0.7 for *all* learnable parameters. Specifically, the most recent model, which attempted to use independent slopes for offers below versus at/above the fair share (`alpha_fair_param` and `alpha_unfair_param` scaling `split_perc_self` in their respective ranges), resulted in extremely poor parameter recovery (r = 0.055 and r = 0.204). This suggests that simply changing how `split_perc_self` is scaled based on the fairness threshold is insufficient for parameter identifiability with this dataset.

For this run (Run 15), let's try a structurally different approach to capture fairness sensitivity. Instead of modeling utility purely through sensitivities (slopes) that scale the offered percentage, let's consider a model that combines a base sensitivity to the offer percentage with *additive, fixed value adjustments* based on whether the offer is perceived as unfair or generous.

Consider a model where the utility of accepting an offer (`U`) has a base component that is linearly proportional to the offered percentage (`split_perc_self`), scaled by a learnable parameter representing general sensitivity. On top of this base utility, fixed positive or negative values are added depending on how the offer compares to the proportional fair share (`fair_perc_self`).

The model should have three learnable parameters:
1.  `alpha_sensitivity`: Represents the base linear sensitivity (slope) to the offered percentage (`split_perc_self`). This parameter influences utility regardless of the offer's fairness relative to the threshold.
2.  `beta_unfair_penalty`: Represents a fixed amount subtracted from utility *only* when the offer is strictly below the proportional fair share (`split_perc_self < fair_perc_self`). This captures the disutility of an unfair offer as a fixed penalty.
3.  `gamma_generous_bonus`: Represents a fixed amount added to utility *only* when the offer is strictly above the proportional fair share (`split_perc_self > fair_perc_self`). This captures the increased utility or positive feeling associated with a generous offer as a fixed bonus.

The hypothesis is that separating the general sensitivity to the offer value (`alpha_sensitivity`) from the distinct, fixed impacts of unfairness (`beta_unfair_penalty`) and generosity (`gamma_generous_bonus`) will create more separable parameter effects, thereby improving parameter identifiability compared to previous models that relied on scaling or slope adjustments relative to the fair threshold. Note that when `split_perc_self == fair_perc_self`, only the base sensitivity (`alpha_sensitivity`) should be active, with no penalty or bonus applied.

**Proposed Model Structure Idea (Run 15):**

The utility `U` is calculated as:
`U = (base sensitivity to split_perc_self) - (fixed penalty if unfair) + (fixed bonus if generous)`

Where:
- Base sensitivity: `alpha_sensitivity * split_perc_self`
- Fixed penalty if unfair: `beta_unfair_penalty` applied when `split_perc_self < fair_perc_self`
- Fixed bonus if generous: `gamma_generous_bonus` applied when `split_perc_self > fair_perc_self`

**Guidance for the Next Model (Run 15):**

Design a computational model implementing the ""base linear sensitivity + additive fixed adjustments"" concept described above. This structure aims to improve parameter identifiability by having parameters contribute to utility in structurally distinct ways (scaling vs. constant shifts) based on the fairness threshold.

*   **Prioritize Parameter Recovery:** Design the model structure to ensure `alpha_sensitivity`, `beta_unfair_penalty`, and `gamma_generous_bonus` have maximally distinguishable effects on the utility function given the data. This model is a direct attempt to address the poor recovery seen in models relying on modifying the slope based on fairness.
*   **Parameter Constraints:** Define clear, generous, finite, non-negative bounds for `alpha_sensitivity`, `beta_unfair_penalty`, and `gamma_generous_bonus`. Suggest bounds [0, 10] for `alpha_sensitivity`, [0, 20] for `beta_unfair_penalty`, and [0, 10] for `gamma_generous_bonus`. All parameters should be non-negative.
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using standard mathematical notation. Ensure the formula reflects the structure using indicator functions for strict inequality (`<` and `>`). You can implement the piecewise additive logic using indicator functions. Let `I_unfair` be an indicator function that is 1 if `split_perc_self < fair_perc_self` and 0 otherwise. A mathematical way to write this indicator is `max(0, -sign(split_perc_self - fair_perc_self))`. Let `I_generous` be an indicator function that is 1 if `split_perc_self > fair_perc_self` and 0 otherwise. A mathematical way to write this indicator is `max(0, sign(split_perc_self - fair_perc_self))`. The structure should be `U = alpha_sensitivity * split_perc_self - beta_unfair_penalty * I_unfair + gamma_generous_bonus * I_generous`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags, including the specified bounds and learnable status for `alpha_sensitivity`, `beta_unfair_penalty`, and `gamma_generous_bonus`. Remember that `fair_perc_self` is a calculated variable derived from `token_self` and `token_opp`.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags.

Focus intently on implementing this distinct structural approach – combining a base linear effect with threshold-based additive fixed effects – as the next strategy to achieve the necessary parameter identifiability. Think creatively about the precise mathematical formulation within the single expression for the <MODEL> tag. Try to break away from just modifying slopes based on fairness and explore how fixed penalties/bonuses can capture these effects.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember `fair_perc_self` can be calculated as `(token_self / (token_self + token_opp)) * 100`. Use variables relevant for responder trials (`trial_role == 1`).

For run 15 of 100, please think through this step by step, focusing intently on designing a model structure that maximizes parameter identifiability given the data and the history of failed recovery attempts, specifically implementing the described model combining base linear sensitivity with additive fixed penalties/bonuses based on the fairness threshold. Then provide your model specification, variable descriptions, target variable, and summary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.43172137118382303,0.16092248157652855,0.05524913476999196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49,371.0296921276916,360.0987560347489,380.8219890442861,0.7295482295482295,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)
U = beta_proposed_share * split_perc_self + beta_fair_share * fair_perc_self_calc","A linear utility model predicting the decision to accept or reject an offer. The utility of accepting (U) is calculated as an additive linear combination of the proposed percentage share to the participant (split_perc_self) and the calculated fair percentage share based on token contributions (fair_perc_self_calc). The model has two learnable parameters: beta_proposed_share, which weights the influence of the proposed percentage, and beta_fair_share, which weights the influence of the calculated fair percentage based on contributions. This additive structure is intended to create more distinct influences for the two parameters on utility compared to ratio or piecewise models.",v0,"Okay, here are the improved instructions for the next run (Run 49), building on the results of the previous runs.

Instructions for Run 49 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 48) attempted a piecewise hybrid model (log for gains, linear difference for losses) but unfortunately failed the primary objective, with `beta_gain_log` recovery coming in just below the 0.7 threshold (r = 0.699). This makes the model unusable, despite achieving slightly better accuracy (0.687) than the current best *recoverable* model (the piecewise log-log ratio model from Run 46 in the history, BIC 40.45, accuracy 0.662, recovery >= 0.795).

This failure in Run 48, especially the lower recovery for the 'gain' parameter, suggests that the specific hybrid piecewise structure, even when using elements from models that previously showed good recovery (like the log function in the log-log model and the linear difference form in the linear-linear_diff model), did not result in sufficient identifiability for both parameters simultaneously. The way the two parameters influence utility within that structure might still be too correlated across the data range.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`) while also improving model fit (lower BIC, higher accuracy).

Given the repeated struggles with achieving full parameter recovery using piecewise models based on the `R_val` ratio or `R_diff` difference, let's try a fundamentally different structural approach for this run. Instead of modeling utility as a transformation or scaling of a ratio or difference from fairness, let's explore a simpler additive linear model that gives separate weight to the *proposed percentage share* and the *calculated fair percentage share*.

Design a computational model that predicts the utility of accepting an offer based on a **linear combination of the proposed percentage share (`split_perc_self`) and the calculated fair percentage share (`fair_perc_self_calc`)**. This approach moves away from ratio-based or piecewise structures to see if a simpler additive relationship between these two key values can provide better parameter identifiability.

*   **Structural Direction (Additive Linear):** Model utility (`U`) as a weighted sum of `split_perc_self` and `fair_perc_self_calc`.
    *   Calculate the necessary intermediate variables to get `fair_perc_self_calc`:
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensures `fair_perc_self_calc` is >= 0).
    *   Structure the utility (`U`) using a simple linear combination:
        *   `U = beta_proposed_share * split_perc_self + beta_fair_share * fair_perc_self_calc`
    *   This structure uses two distinct learnable parameters (`beta_proposed_share` and `beta_fair_share`) that directly weight the contributions of the proposed percentage and the calculated fair percentage to the overall utility. The hope is that these distinct inputs (`split_perc_self` and `fair_perc_self_calc`) will allow for more separable estimation of their respective sensitivity parameters.

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate `sum_tokens`, `fair_perc_self_calc`, and `U`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions. Note that `epsilon_val`, `R_val`, and `R_diff` are *not* needed for this model structure.

*   **Model Formula:** Define the utility (`U`) using the linear combination. Include the calculation steps for necessary derived variables (`sum_tokens`, `fair_perc_self_calc`) before the final formula for `U`. Use clear mathematical notation.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   `beta_proposed_share`: Sensitivity parameter scaling the proposed percentage share (`split_perc_self`). Suggest generous bounds like [-10.0, 10.0].
    *   `beta_fair_share`: Sensitivity parameter scaling the calculated fair percentage share (`fair_perc_self_calc`). Suggest generous bounds like [-10.0, 10.0].
    Choose descriptive names suitable for Python code that reflect the variable they scale. Define clear, generous, finite numerical bounds for both.

*   **Calculated Variables:** Ensure all calculated variables used in the model (`sum_tokens`, `fair_perc_self_calc`, `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`).

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea that utility is determined by an additive linear combination of the proposed percentage (`split_perc_self`) and the calculated fair percentage (`fair_perc_self_calc`). Explain how `beta_proposed_share` weights the influence of the proposed amount and `beta_fair_share` weights the influence of the calculated fair amount, and note that this model is a departure from previous ratio/difference/piecewise structures, attempting to improve parameter identifiability through distinct additive components.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This additive linear approach is a significant shift from the previous ratio-based models. If this structure also meets recovery but doesn't significantly improve BIC/Accuracy, or if it fails recovery for either parameter, continue to be prepared to think outside the box for subsequent iterations – consider alternative ways fairness might modulate sensitivity, or other combinations of variables that might create more separable parameter influences within the 2-parameter constraint. The key is finding a structure where the two parameters’ contributions to utility are as independent as possible across the dataset.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.37841125192807196,0.7609272654758369,,,,,,,,,,,,,,,,,,,,,,,,,
94,382.308886064498,345.73065586416146,415.0768839522994,0.5357142857142857,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
U_accept = parameter_perc_fairness * (split_perc_self - fair_perc_self_calc) + parameter_context_intercept * (fair_perc_self_calc - 50.0)","This model predicts the utility of accepting a proposal for the responder based on two factors, each scaled by a distinct learnable parameter. The first factor is the difference between the proposed percentage share for the participant (split_perc_self) and the calculated fair share percentage based on token contributions (fair_perc_self_calc). This deviation is scaled by 'parameter_perc_fairness', capturing sensitivity to fairness relative to contribution. The second factor is the difference between the calculated fair share percentage and a 50% split. This represents a context-dependent baseline influenced by relative contributions and is scaled by 'parameter_context_intercept'. These two learnable parameters are designed for identifiability by scaling inputs that represent different components of the fairness calculation: 'parameter_perc_fairness' scales a *relative deviation from the calculated norm*, while 'parameter_context_intercept' scales the *calculated norm's position relative to a fixed 50% point*, thus having distinct mathematical roles in determining the overall utility.",v0,"Okay, here are the improved instructions for the next run (Run 94 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy. Specifically, *all* learnable parameters must have a parameter recovery value of >= 0.7.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {""min"": 0, ""max"": 1},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 94 of 100).

The previous attempt using a linear model based on deviation from fairness plus an intercept achieved robust parameter recovery (> 0.7 for both learnable parameters). This is a significant step towards finding a usable model structure. Now, the goal is to build on this success and explore variations that might capture responder behavior even better, aiming to improve BIC (lower is better) and accuracy (higher is better), *while strictly maintaining* the robust parameter identifiability achieved in the last run (recovery >= 0.7 for *all* learnable parameters). Robust parameter identifiability remains the **unwavering primary objective**.

Given the success of models where parameters have clearly separable roles, we need to explore alternative structural approaches that maintain this separation but potentially incorporate more nuance or additional relevant variables from the dataset.

You *must* include these standard calculation steps first before defining `U_accept`:
1.  Calculate `sum_tokens = token_self + token_opp`.
2.  Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.

Now, design a **novel** formula for `U_accept` using `split_perc_self`, `fair_perc_self_calc`, and potentially *other relevant data variables* from the list provided (like `token_self`, `token_opp`, `trial_type`, `combined_earning`), incorporating exactly **two learnable parameters**.

Focus on structures where the two learnable parameters have demonstrably distinct and separable influences on the utility calculation, similar to how a slope and an intercept affect a linear function, but potentially applied to different aspects of the trial. Consider how different components of the offer or context might independently contribute to utility, each scaled by one of the parameters.

Think creatively about how two parameters can capture distinct psychological processes related to fairness, self-interest, or context within the framework of the provided variables. Avoid simply adding a new term scaled by a third parameter. Instead, consider how your two parameters can scale different, potentially non-collinear, input variables or combinations of variables.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus potentially others from the provided list. Calculate `sum_tokens`, `fair_perc_self_calc`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Include function definitions like `IF` if used.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Ensure the parameters' roles are structurally distinct to promote identifiability. For example, one parameter could scale a term related to value/gain, while the other scales a term related to fairness or contextual factors, or one could scale an input linearly while the other scales a different input non-linearly, or one scales a deviation while the other scales the absolute value.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and any functions like `IF`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea. **Crucially, explain *how* the specific structure of your model, with exactly two learnable parameters, is designed to create mathematically distinct and separable influence profiles for those two parameters, in service of achieving robust joint parameter identifiability.** Highlight what aspect of the decision each parameter is intended to capture and how their mathematical roles differ.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure with *exactly two* parameters where their influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy). Avoid structures that have failed recovery previously. Explore alternative ways to combine inputs and parameters beyond the simple linear deviation+intercept structure, ensuring the two parameters have fundamentally different mathematical roles. Be creative and think ""out of the box"" about how to use the available data variables and exactly two parameters to capture distinct aspects of the responder's decision process while maintaining identifiability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.45831180930482296,0.45636983233730516,,,,,,,,,
8,393.85815584831795,397.4090924361134,390.67710848841784,0.7686202686202686,"U = beta_value * split_perc_self - gamma_fairness * max(0, (token_self / (token_self + token_opp)) * 100 - split_perc_self) - delta_inequity * max(0, 50 - split_perc_self)","A utility model for the responder's decision that combines the value of the offered percentage with distinct penalties based on deviations from fairness norms. Utility increases linearly with the offered percentage, scaled by 'beta_value'. An additional penalty is applied if the offer falls below the proportional share (calculated from token contributions), scaled by 'gamma_fairness'. A separate, additional penalty is applied if the offer falls below the 50% equal split, scaled by 'delta_inequity'. These parameters define piecewise linear segments of the utility function relative to the offered percentage, with breakpoints at the proportional share and 50%. The model uses three non-negative learnable parameters, constrained to [0, 100].",v0,"Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Achieve excellent parameter recovery (>= 0.7 correlation) for ALL learnable parameters.** This remains the *single most important* objective. Previous attempts have failed here, including the most recent one. A model is unusable if any learnable parameter cannot be reliably recovered, regardless of other metrics (like BIC or accuracy). Prioritize model structures that inherently enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables available during responder trials (trial_role == 1).
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

**Analysis of Previous Attempt (Run 7):** The most recent model (`beta_gain * split_perc_self - gamma_unfairness * max(0, fair_perc_self - split_perc_self)`) showed promising signs in terms of BIC (76.39) and accuracy (0.821), improving significantly over earlier models on these metrics. However, it **failed parameter recovery** (`beta_gain: r=0.576`, `gamma_unfairness: r=0.638`). Both parameters were below the critical 0.7 threshold, rendering the model unusable for reliable inference about participant strategies. This suggests that even this simple piecewise structure, with linear scaling of the offer value and a linear penalty below the proportional norm (`fair_perc_self`), still allows for the parameters' effects to be too correlated or substitutable given the data pattern. For example, the effect of increasing the offer percentage and the effect of reducing the unfairness penalty below the norm might overlap too much in how they influence the utility curve.

**Guidance for the Next Model (Radically Prioritizing Parameter Recovery - Continued):** The core challenge remains the design of a utility function where each learnable parameter has a truly unique ""signature"" on the utility curve, making its contribution statistically separable from the others during parameter fitting. Simple additive components scaled by parameters, even with basic thresholds (`max(0,...)`), have consistently failed to produce identifiable parameters in this dataset.

We need to explore model structures where parameters exert influence in more distinct, non-overlapping ways. Think beyond simple linear terms and single penalty thresholds. Consider how parameters can control the *shape* or *structure* of the utility function itself, especially around the crucial reference points (50% and the proportional share `fair_perc_self`).

*   **Model Multiple Norms Distinctly:** Explicitly incorporate both the 50% norm and the proportional norm (`fair_perc_self`). Can parameters control the sensitivity or type of penalty/bonus *relative to each norm separately*?
*   **Thresholds and Switching Points:** Design parameters that act as thresholds or switching points in the utility function based on the offer percentage (`split_perc_self`) relative to the norms. For instance, a parameter could define how far below 50% an offer must be to trigger a specific strong rejection tendency, while another parameter controls the sensitivity to offers *between* 50% and the proportional share, and another for offers *above* the proportional share.
*   **Parameterize the Shape/Exponent:** Could a parameter control the exponent of the fairness deviation (`max(0, fair_perc_self - split_perc_self)^parameter_exponent`) or the offer value (`split_perc_self^parameter_exponent`)? Parameters controlling exponents or non-linear transformations often have very different effects on the utility function shape than simple linear scales.
*   **Multiplicative Interactions:** Explore models where parameters scale *interactions* between different components, rather than just additive terms. For example, `Utility = split_perc_self * (1 - fairness_parameter * deviation)` or `Utility = split_perc_self - penalty_parameter * (deviation_below_50 + deviation_below_prop)`.
*   **Parameters Controlling Relative Weights of Norms:** Design a parameter that explicitly weighs the influence of the 50% norm versus the proportional norm in determining fairness sensitivity or the reference point, potentially in a non-linear or threshold-dependent manner (this was attempted in Run 1 but recovery was poor there too, suggesting the weighting mechanism needs refinement).

Focus on defining parameters whose effect on utility cannot be replicated by adjusting other parameters. This might involve parameters that:
- Change the slope of the utility function in specific ranges.
- Control the curvature (e.g., quadratic or exponential terms).
- Define thresholds where the functional form of utility changes.
- Modify how different factors (offer, 50% deviation, proportional deviation) combine, not just additively.

Continue to build the model using percentage-based values (`split_perc_self`, `fair_perc_self`). Aim for the minimum number of learnable parameters necessary *only if* their effects are clearly separable and identifiable. A model with fewer, identifiable parameters is vastly preferred over one with many, unidentifiable ones, regardless of seemingly good BIC or accuracy scores on validation data.

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded (-inf to inf).

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1) and use variables available in that context. The `fair_perc_self` variable (calculated as `(token_self / (token_self + token_opp)) * 100`) is useful for fairness calculations.

For run 8 of 100, please think through this step by step, focusing intently on how the *specific mathematical structure* of the model guarantees parameter identifiability and distinguishes the influence of each parameter, minimizing covariance based on the *specific* data characteristics. How can you define learnable parameters that control fundamentally different aspects of the utility function (e.g., slope, threshold, exponent, interaction weight, switching points based on norms) rather than just scaling additive components? Think creatively about non-linear and piecewise structures relative to *both* the 50% and proportional norms. Then provide your model specification, variable descriptions, target variable, and summary.

Previous Models:Not Provided

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,,,,,0.5915632741042313,0.29586841291640287,0.23703227947202077,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17,399.90265161852363,412.31474420101307,388.78348534671017,0.5521978021978022,"U = sensitivity_generous * (split_perc_self - fair_perc_self) - penalty_slope_unfair * max(0, fair_perc_self - split_perc_self)","A piecewise linear utility model for responder decisions. Utility is calculated based on the percentage deviation of the offered amount for the participant (split_perc_self) from the fair percentage (fair_perc_self, calculated from token contributions). The model uses two learnable parameters: sensitivity_generous represents the slope of the utility function for fair or generous offers, and penalty_slope_unfair adds to this slope when the offer is below fair, resulting in a steeper decrease in utility for unfair offers. Utility at the fair percentage is zero.",v0,"Based on the results of the previous run and the instructions used for that run, generate improved instructions for the *next* run. Aim to guide the main modeling LLM towards better performance (e.g., lower BIC, better parameter recovery, and higher accuracy). Please keep in mind that if any of the learnable parameters have a parameter recovery value of less than ~0.7, then the model is unusable. You will first be given the total context for the previous run, then reminded specifically of the instructions used for that run that you should update.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = beta_param + epsilon_param * E + eta_param * N\n</MODEL>\n\n<VARIABLES>\n{\n  ""variables"": {\n    ""U_i"": {\n      ""description"": ""Utility of choosing action i"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\n      ""learnable"": false,\n      ""source"": ""calculated""\n    },\n    ""beta_param"": {\n      ""description"": ""Base tendency parameter (inherent preference)"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1\n      },\n      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""epsilon_param"": {\n      ""description"": ""Environmental sensitivity parameter"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\
      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""E"": {\n      ""description"": ""Environmental cue value"",\n      ""range"": {\n        ""min"": ""-inf"",\n        ""max"": ""inf""\n      },\n      ""learnable"": false,\n      ""source"": ""data""\n    },\n    ""eta_param"": {\n      ""description"": ""Noise parameter"",\n      ""range"": {\n        ""min"": 0,\n        ""max"": 1,\n        ""exclusive_max"": true\n      },\n      ""learnable"": true,\n      ""source"": ""learnable""\n    },\n    ""N"": {\n      ""description"": ""Random noise drawn from normal distribution N(0,1)"",\n      ""distribution"": {\n        ""type"": ""normal"",\n        ""mean"": 0,\n        ""std"": 1\n      },\n      ""learnable"": false,\n      ""source"": ""calculated""\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: The previous models designed to capture responder behavior, whether using piecewise slopes on the offered amount or combining base sensitivity with fixed additive penalties/bonuses based on the proportional fair share, have consistently failed to achieve the required parameter recovery of >= 0.7 for *all* learnable parameters. The most recent attempt (Run 15), which used a base sensitivity to `split_perc_self` plus additive terms for unfair/generous offers (`alpha_sensitivity`, `beta_unfair_penalty`, `gamma_generous_bonus`), resulted in very poor recovery across all three parameters (0.432, 0.161, 0.055). This suggests that separating effects by simply adding constants above/below the fair threshold is not sufficient for identifiability.

For this run (Run 16), let's explore a different structural approach that focuses on the *deviation* from the proportional fair share (`fair_perc_self`) rather than the offered percentage (`split_perc_self`) itself as the primary input, and attempts to capture fairness sensitivity via a piecewise linear relationship *with respect to this deviation*. This approach uses only two learnable parameters, reducing model complexity compared to the last attempt, and anchors utility at 0 when the offer is exactly fair.

Consider a model where the utility of accepting an offer (`U`) is a piecewise linear function of the difference between the offered percentage and the fair percentage (`split_perc_self - fair_perc_self`).

The model should have two learnable parameters:
1.  `sensitivity_generous`: Represents the slope of the utility function when the offer is fair or generous (`split_perc_self >= fair_perc_self`). This parameter scales the positive deviation from the fair share. A higher value means greater positive utility for generous offers.
2.  `penalty_slope_unfair`: Represents the *additional* negative slope applied only when the offer is strictly unfair (`split_perc_self < fair_perc_self`). The effective slope for unfair offers will be `sensitivity_generous - penalty_slope_unfair`. A positive value for this parameter means that unfair offers result in a steeper decrease in utility compared to generous offers of the same magnitude of deviation, relative to the fair share.

The utility `U` should be 0 when `split_perc_self == fair_perc_self`. For offers above fair, utility increases linearly with deviation `(split_perc_self - fair_perc_self)` with slope `sensitivity_generous`. For offers below fair, utility decreases linearly with deviation `(split_perc_self - fair_perc_self)` with a steeper slope `(sensitivity_generous - penalty_slope_unfair)`.

**Proposed Model Structure Idea (Run 16):**

The utility `U` is calculated as a piecewise linear function of the deviation `deviation = split_perc_self - fair_perc_self`:
- If `deviation >= 0`: `U = sensitivity_generous * deviation`
- If `deviation < 0`: `U = (sensitivity_generous - penalty_slope_unfair) * deviation`

This structure can be expressed in a single formula using the positive unfair deviation term `max(0, fair_perc_self - split_perc_self)`, which is `max(0, -deviation)`.
`U = sensitivity_generous * (split_perc_self - fair_perc_self) - penalty_slope_unfair * max(0, fair_perc_self - split_perc_self)`

**Guidance for the Next Model (Run 16):**

Design a computational model implementing this piecewise linear relationship based on the *deviation from the fair share*. This structural approach is designed to potentially improve parameter identifiability by using fewer parameters and anchoring the utility at the fair point.

*   **Prioritize Parameter Recovery:** Focus on making `sensitivity_generous` and `penalty_slope_unfair` distinguishable. The structure should clearly isolate the added disutility slope for unfair offers.
*   **Parameter Constraints:** Define clear, generous, finite, non-negative bounds for `sensitivity_generous` and `penalty_slope_unfair`. Suggest bounds [0, 10] for `sensitivity_generous` and [0, 20] for `penalty_slope_unfair`. Both parameters must be non-negative.
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using the suggested structure: `U = sensitivity_generous * (split_perc_self - fair_perc_self) - penalty_slope_unfair * max(0, fair_perc_self - split_perc_self)`. Ensure the formula includes the necessary variables (`split_perc_self`, `fair_perc_self`) and parameters.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags, including the specified bounds and learnable status for `sensitivity_generous` and `penalty_slope_unfair`. Remember `fair_perc_self` is calculated as `(token_self / (token_self + token_opp)) * 100`. Include descriptions for all variables used in the <MODEL> formula.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags.

Think creatively about how this model captures responder preferences by making utility proportional to the distance from fairness, with a potentially different slope for unfair offers. This differs fundamentally from scaling the offered percentage itself. Aim for a structure that allows the two parameters to have distinct influences on the likelihood of acceptance across different offers, particularly those above vs. below the fair share.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember `fair_perc_self` can be calculated as `(token_self / (token_self + token_opp)) * 100`. Use variables relevant for responder trials (`trial_role == 1`).

For run 16 of 100, please think through this step by step, focusing intently on designing a model structure that maximizes parameter identifiability given the data and the history of failed recovery attempts, specifically implementing the described model combining base linear sensitivity with additive fixed penalties/bonuses based on the fairness threshold. Then provide your model specification, variable descriptions, target variable, and summary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.05196744957128427,0.04875230220380708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
51,401.85055591852904,305.655433642403,488.0253529575586,0.7582417582417582,"sum_tokens = token_opp + token_self
fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)
U = beta_value * split_perc_self - beta_fairness_penalty * (fair_perc_self_calc - split_perc_self)^2","A computational model predicting responder acceptance utility as a function of the proposed share and its deviation from a calculated fair share based on contributions. Utility is a linear combination of the proposed percentage scaled by 'beta_value' and a quadratic penalty term representing the squared deviation from the fair percentage, scaled by 'beta_fairness_penalty'. This non-linear structure is intended to provide distinct influences for the two learnable parameters, improving identifiability.",v0,"Okay, here are the improved instructions for the next run (Run 51), building on the results of the previous runs.

Instructions for Run 51 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The last run (Run 50) attempted a linear model structuring utility as a combination of the proposed share and the deviation from fairness (`U = beta_value * split_perc_self + beta_fairness_deviation * fairness_deviation`). While this model showed good accuracy (0.781) and a better BIC (306.97) compared to the previous simple additive linear model (Run 49, BIC 371.03), it unfortunately *failed the primary objective of achieving parameter recovery >= 0.7 for *all* learnable parameters*, specifically for `beta_fairness_deviation` (r = 0.653). `beta_value` just barely met the threshold (r = 0.718).

The reason the Run 50 model struggled with parameter identifiability is crucial to understand for future iterations. Although it used `fairness_deviation` (`split_perc_self - fair_perc_self_calc`), the utility function remained a simple *linear combination* of `split_perc_self` and `fair_perc_self_calc`. Specifically, `U = beta_value * split_perc_self + beta_fairness_deviation * (split_perc_self - fair_perc_self_calc)` simplifies to `U = (beta_value + beta_fairness_deviation) * split_perc_self - beta_fairness_deviation * fair_perc_self_calc`. This is mathematically equivalent to the simple additive linear model from Run 49 (`U = beta_proposed_share * split_perc_self + beta_fair_share * fair_perc_self_calc`), just with different parameter names. Using a linear combination of inputs that are themselves linearly dependent (like `split_perc_self` and `fairness_deviation`) does *not* create the necessary structural separability for parameter identifiability in a linear model. The influences of `beta_value` and `beta_fairness_deviation` on utility were confounded within this structure.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** that ensures both parameters have distinct, identifiable influences on responder decisions (`trial_role == 1`) while also aiming for improved model fit (lower BIC, higher accuracy). We seek a model that achieves full recovery (>= 0.7 for both parameters) and improves upon the current best recoverable model (BIC 40.45, accuracy 0.662, recovery >= 0.795).

Given that linear models based on additive combinations of `split_perc_self`, `fair_perc_self_calc`, or their linear transformations have consistently failed parameter recovery, we must explore fundamentally **different structural forms** for Run 51. Robust parameter recovery requires a model structure where the influence of each of the two learnable parameters is **structurally distinct** and **not linearly confounded** with the other parameter's influence across the dataset. This strongly suggests moving away from simple linear combinations of value and fairness components.

*   **Structural Direction (Non-Linear Interaction/Modulation):** Design a model where the interaction between the proposed share and fairness is captured in a non-linear way, or where fairness *modulates* the sensitivity to the proposed share (or vice-versa), rather than just adding an independent term.
    *   **Avoid:** Any model that simplifies to a simple linear combination of `split_perc_self` and `fair_perc_self_calc`. This includes the structure used in Run 50.
    *   **Explore:**
        *   **Multiplicative interactions:** Could utility be a product of value and fairness terms, perhaps after applying non-linear transformations (e.g., log, power)?
        *   **Fairness as a sensitivity modulator:** Could one parameter control sensitivity to the proposed share, but this sensitivity is itself a non-linear function involving fairness?
        *   **Revisit Non-Linear Functions of Ratios/Differences:** The piecewise logarithmic model (Run 1) achieved good recovery and BIC. Can we explore other non-linear functions (e.g., power, exponential) applied to the ratio (`R_val`) or difference (`fairness_deviation`) relative to fairness? Ensure that the two learnable parameters scale genuinely distinct aspects of this relationship (e.g., sensitivity to gains vs losses, but perhaps with different functional forms on either side, or scaling different parameters of a single non-linear function).
        *   **Consider:** How can you design a function `U = f(split_perc_self, fair_perc_self_calc, beta1, beta2)` where changing `beta1` has a distinct and independent effect on `U` compared to changing `beta2`, for a fixed set of inputs, in a way that cannot be replicated by simply scaling the inputs linearly?

*   **Key Inputs & Calculated Variables:**
    *   Use `split_perc_self`, `token_self`, `token_opp` as data variables.
    *   Calculate intermediate variables as needed (e.g., `sum_tokens`, `fair_perc_self_calc`, ratios, differences, non-linear transformations) and the final `U`.
    *   Ensure these calculated variables are included in your <MODEL> tag calculations and <VARIABLES> descriptions. Remember `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` for robustness.

*   **Model Formula:** Define the utility (`U`) using your chosen non-linear structure. Include calculation steps for any necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**.
    *   Choose descriptive names suitable for Python code.
    *   Ensure their placement in the formula is such that their influences on `U` are as distinct and independent as possible.
    *   Define clear, generous, finite numerical bounds for both (e.g., [-10.0, 10.0] or adjusted based on the scale of the variables they interact with/scale).

*   **Calculated Variables:** Ensure all calculated variables used in the model (including `sum_tokens`, `fair_perc_self_calc`, any intermediate terms like ratios/differences/transformed values, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`). Ensure the description for `fair_perc_self_calc` includes the `sum_tokens > 0` condition.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Include the calculation steps for necessary derived variables before the final formula for `U`. Use clear mathematical notation.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the proposed percentage share.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing your model. Explain the core idea and how its structure aims to achieve parameter identifiability (e.g., by using non-linear interactions, modulation, or non-linear functions of derived inputs), explaining how the two parameters scale distinct components or influences within this non-linear structure.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability *within* the constraints of **exactly 2 learnable parameters** is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This requires moving beyond simple linear combinations of the base inputs or their linear transformations. Be creative in exploring non-linear relationships or interactions between value and fairness that could lead to distinct parameter influences.",,,,,,,,,,,,,,,,,,0.35918979416690655,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6303102984239162,,,,,,,,,,,,,,,,,,,,,,,
59,432.0073060526961,379.9934966117133,478.60301034357656,0.7408424908424909,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)
deviation_from_fair = fair_perc_self_calc - split_perc_self
epsilon = 1e-6
unfair_deviation = IF(deviation_from_fair > epsilon, deviation_from_fair, 0)
squared_unfair_deviation = unfair_deviation * unfair_deviation
U = beta_value * split_perc_self + beta_unfairness_penalty_sensitivity * squared_unfair_deviation","This model predicts responder acceptance utility using two learnable parameters. The utility is a linear combination of the proposed percentage share for the participant, scaled by the parameter beta_value, and a squared deviation from the calculated fair share, scaled by the parameter beta_unfairness_penalty_sensitivity. The deviation term is only active (non-zero) when the proposed share is less than the fair share, representing an unfair offer. This asymmetric structure, where beta_value influences utility linearly across all offers and beta_unfairness_penalty_sensitivity influences utility non-linearly (via squaring) only for unfair offers, is designed to enhance parameter identifiability by creating distinct functional forms and domains of influence for the two parameters.",v0,"Okay, here are the improved instructions for the next run (Run 59), building on the results of the previous runs and focusing intensely on parameter identifiability.

Instructions for Run 59 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run implemented a promising asymmetric piecewise model structure (`U = beta_value * split_perc_self + beta_unfairness_effect * unfair_deviation_magnitude`). This model achieved the best BIC (286.16) and accuracy (0.767) seen so far, and successfully recovered one parameter (`beta_value`, r = 0.774). However, the second parameter, `beta_unfairness_effect`, failed to meet the primary objective (r = 0.659).

Analysis of the previous model's failure: Despite the asymmetric structure where `beta_unfairness_effect` only influenced utility for unfair offers, its influence within that regime was still linear with respect to the magnitude of the unfair deviation (`fair_perc_self_calc - split_perc_self`). In the unfair regime, the utility calculation effectively became a linear combination of `split_perc_self` and `fair_perc_self_calc` with coefficients related to both betas. This linear-linear relationship for unfair offers likely still causes confounding between the two parameters, preventing the unique estimation of `beta_unfairness_effect`.

We need to find a mathematical structure for utility, still using exactly **2 learnable parameters**, where their influence on responder decisions (`trial_role == 1`) is fundamentally more separable, especially for the parameter intended to capture sensitivity to unfairness. This requires modifying the mathematical form in the unfair regime to create a more distinct contribution for the fairness parameter.

Based on the persistent difficulty with the linear fairness term's recovery, focus your exploration for Run 59 on refining the asymmetric piecewise structure by introducing a *non-linear* transformation for the unfairness component.

Consider the following strategy to enhance identifiability by making the second parameter's influence non-linear and distinct:

*   **Refined Asymmetric Piecewise Model with Squared Unfairness:** Maintain the structure where one parameter scales the *absolute proposed percentage share* (active on all trials) and the second parameter scales a term related to unfairness (active *only* on unfair trials). However, make the unfairness term scale the *squared* magnitude of the unfair deviation, rather than the linear magnitude.
    *   *Example Idea:* Let one parameter (`beta_value`) scale `split_perc_self`. Let the second parameter (`beta_unfairness_penalty_sensitivity`) scale the *squared* magnitude of the negative deviation from fairness (`fair_perc_self_calc - split_perc_self`), but only when this deviation is positive (i.e., when `split_perc_self` is less than `fair_perc_self_calc`). Add a small epsilon for numerical stability before squaring.
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensure this handles `sum_tokens=0`).
        *   `deviation_from_fair = fair_perc_self_calc - split_perc_self` (Positive if unfair, 0 or negative if fair/favorable).
        *   `epsilon = 1e-6` // Small constant for numerical stability.
        *   `squared_unfair_deviation = IF(deviation_from_fair > 0, (deviation_from_fair + epsilon)^2, 0)` (This term is 0 if the offer is fair or better, and positive and squared if unfair).
        *   `U = beta_value * split_perc_self + beta_unfairness_penalty_sensitivity * squared_unfair_deviation` (Combine linear value with a term representing the squared magnitude of unfairness, expecting `beta_unfairness_penalty_sensitivity` to be negative to apply a penalty).
    *   *Structural Principle:* This structure attempts identifiability by having `beta_value` exert a linear influence across the entire range of `split_perc_self`, while `beta_unfairness_penalty_sensitivity` *only* exerts influence in the specific regime of unfair offers, scaling a term (`squared_unfair_deviation`) that has a *non-linear* relationship with the deviation magnitude. This combination of asymmetric influence and different functional forms (linear vs. squared) scaled by the two parameters is intended to create more separable effects than previous additive or linear piecewise models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, `deviation_from_fair`, `epsilon`, `squared_unfair_deviation`, and `U`. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0.
*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags. Include calculation steps for intermediate variables like `sum_tokens`, `fair_perc_self_calc`, `deviation_from_fair`, `epsilon`, `squared_unfair_deviation` *before* the final utility `U` calculation within the <MODEL> tags.
*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. For the suggested model, use names like `beta_value` and `beta_unfairness_penalty_sensitivity`. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-10.0, 10.0] or wider/narrower if appropriate for the scale of your terms). Consider the expected sign (e.g., `beta_unfairness_penalty_sensitivity` should likely be negative), but keep bounds wide enough.
*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, `deviation_from_fair`, `epsilon`, `squared_unfair_deviation`, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` explicitly mentions the `sum_tokens > 0` condition and the default value. The description for `squared_unfair_deviation` should clarify what it represents, its piecewise nature, and the squaring.
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the participant's proposed percentage share percentage.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific mathematical structure (e.g., the asymmetric piecewise influence combined with the non-linear, squared term for unfairness in the loss domain) is designed to ensure parameter identifiability for *both* learnable parameters*, making their roles distinct from previously attempted additive or linear piecewise models.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. Explore models with asymmetric piecewise structures like the one suggested, where parameters target different aspects (overall value vs. specific unfairness magnitude) and different regimes (all offers vs. only unfair offers), using *non-linear* transformations for at least one of the components, to create more separable influences. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation.",,,,,,,,,,,,,,,,,,0.3860750173519877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.609441545520595,,,,,,,,,,,,,,,
100,436.73203775029475,411.1830073981265,459.61971077411215,0.7075702075702075,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
deviation = split_perc_self - fair_perc_self_calc
U_accept = deviation_sensitivity * deviation + total_earning_sensitivity * combined_earning","The model predicts the utility of accepting an offer based on two components: the deviation of the proposed percentage split from the participant's calculated fair share and the total combined earning in the trial. The utility is a linear combination of these two inputs, scaled by two learnable parameters: deviation_sensitivity and total_earning_sensitivity. This structure is designed to improve parameter identifiability by scaling inputs with structurally distinct sources of variance: deviation captures fairness relative to contribution ratio, while total earning captures the absolute scale of the potential reward independent of the percentage split.",v0,"Okay, here are the improved instructions for the next run (Run 100 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy. Specifically, *all* learnable parameters must have a parameter recovery value of >= 0.7.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {""min"": 0, ""max"": 1},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {""description"": ""Noise parameter"",
      ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,\n        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 100 of 100).

The recent model (`deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + base_accept_utility`) achieved excellent BIC (48.16) and accuracy (0.785) but critically failed the parameter recovery test for the `base_accept_utility` parameter (r = 0.545). The recovery for `deviation_sensitivity` was strong (r = 0.904), but the requirement is >= 0.7 for *all* learnable parameters.

Robust parameter identifiability (recovery >= 0.7 for *all* learnable parameters) remains the **highest priority**. It MUST be achieved alongside improved accuracy and a better BIC than the previous best run with full recovery (BIC 52.82, Acc 0.544) and ideally better than the current best BIC/Accuracy (BIC 48.16, Acc 0.785).

We have repeatedly observed that models using a linear combination of a continuous fairness deviation term (`split_perc_self - fair_perc_self_calc`) and a scaled **binary indicator** (like `is_below_fair`, `is_unfair`, `is_self_more_contrib`) fail parameter recovery for the parameter scaling the binary term. Similarly, the attempt to scale a constant (intercept) alongside the deviation term in the last run also failed parameter recovery for the constant-scaling parameter. This indicates that in a simple linear sum, these terms do not introduce sufficiently distinct variance to uniquely identify both parameters when paired with fairness deviation.

You *must* include these standard calculation steps first before defining `U_accept`:
1.  Calculate `sum_tokens = token_self + token_opp`.
2.  Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.

Now, design a **novel** formula for `U_accept` incorporating exactly **two learnable parameters**. Focus intently on creating a structure where these two parameters scale inputs that have demonstrably *different structural roles* and *sources of variance* on the utility calculation to ensure robust identifiability for *both* parameters (target r >= 0.7 for both) and improve performance (accuracy > 0.785, BIC < 48.16).

Based on past failures, explicitly avoid pairing the fairness deviation term (`split_perc_self - fair_perc_self_calc`) with a binary indicator or a constant in a simple linear sum. Instead, pair a parameter scaling the **deviation from fairness** (`split_perc_self - fair_perc_self_calc`) with a second parameter scaling an input that is **structurally distinct** and unlikely to be confounded with the deviation term, or propose an alternative structure entirely using two parameters scaling distinct inputs.

Consider these structural approaches for your two-parameter model:

1.  **Deviation + Absolute Deviation:** One parameter scales the signed deviation from fairness, and the second parameter scales the absolute magnitude of the deviation. Model form: `U_accept = deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + absolute_deviation_sensitivity * abs(split_perc_self - fair_perc_self_calc)`. These two terms (`deviation` and `abs(deviation)`) vary differently across the range of offers, which should help disentangle the parameters.
2.  **Deviation + Total Resource:** One parameter scales the signed deviation from fairness, and the second parameter scales the total earnings available in the trial (`combined_earning`). Model form: `U_accept = deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + total_earning_sensitivity * combined_earning`. Total earnings is independent of the *percentage* split fairness.
3.  **Direct Offer Percentage + Fairness Benchmark:** One parameter scales the proposed percentage itself (`split_perc_self`), and the second parameter scales the calculated fairness benchmark percentage (`fair_perc_self_calc`). Model form: `U_accept = offer_perc_sensitivity * split_perc_self + fairness_benchmark_sensitivity * fair_perc_self_calc`. Both inputs are continuous percentages, but `split_perc_self` varies based on the offer, while `fair_perc_self_calc` varies based on pre-trial token contributions, providing distinct sources of variance.

You *must* select one of these structural patterns (or propose a similarly novel structure with a clear justification for how it ensures distinct parameter influences) for your model. Choose descriptive, Python-safe names for your exactly two learnable parameters.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus `combined_earning` if using option 2, or `fair_perc_self_calc` as a scaled input if using option 3. Calculate `sum_tokens`, `fair_perc_self_calc`, any other derived variables needed (like `deviation`, `abs(deviation)`), and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Include function definitions like `IF` if used.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Ensure the parameters' roles are structurally distinct as discussed above to promote identifiability (recovery >= 0.7 for BOTH).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, any derived variables, and any functions like `IF`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs with `source: ""data""` that appear in your <MODEL>.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea. **Crucially, explain *how* the specific structural difference between the inputs scaled by your two learnable parameters promotes robust parameter identifiability (recovery >= 0.7 for BOTH learnable parameters), explicitly addressing why this novel structure is expected to succeed where linear combinations of deviation with binary indicators or constants failed.**

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) AND improved accuracy/BIC are the absolute priorities. Your goal is to find a novel structure with *exactly two* parameters where their influences are clearly distinguishable through structurally different mathematical roles, allowing for reliable estimation and better fits the human decision data. Be creative and think ""out of the box"" about how to use the available data variables and exactly two parameters to capture distinct aspects of the responder's decision process while maintaining identifiability and improving predictive power.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5708040787912388,,,,0.5008060428266675
82,448.6584091759732,401.6020674484435,490.8130486402186,0.7148962148962149,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (token_self / sum_tokens) * 100.0 if sum_tokens > 0.0 else 50.0
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_value * Log(1.0 + split_perc_self) - beta_unfairness * loss_below_fair","An additive utility model for responder decisions. The utility of accepting is calculated as the sum of a positive gain term and a negative penalty term. The gain term is the natural logarithm of (1 + proposed share percentage for self), scaled by beta_value. The penalty term is a linear function of the loss below the calculated fair share percentage, scaled by beta_unfairness. The fair share is calculated based on token contributions, defaulting to 50% if total tokens are zero. This structure, combining a concave (logarithmic) gain function with a linear penalty function, is hypothesized to create sufficiently distinct influence profiles for the two learnable parameters (beta_value and beta_unfairness) to improve robust parameter identifiability compared to previous models.",v0,"Okay, here are the improved instructions for the next run (Run 82 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,\\
      ""source"": ""data""
    },\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },\\
      ""learnable"": false,
      ""source"": ""data""
    },\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 82 of 100), based on the results of recent attempts.

The additive model with an exponential penalty tested in the previous run (Run 81) yielded a very high BIC (722.89) and poor parameter recovery for both learnable parameters (`beta_value` r=0.305, `beta_unfairness` r=0.281). This indicates that this specific additive structure with an exponential penalty failed to capture participant behavior accurately and did not resolve the parameter identifiability issue. This adds to the evidence from previous divisive models that simply changing the non-linear function within similar structures hasn't achieved the required joint parameter recovery.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation of individual differences and group effects (like the impact of cocaine). We must continue to explore structures where the influences of *both* `beta_value` and `beta_unfairness` are mathematically separable enough for reliable joint estimation.

Let's continue exploring additive structures, but introduce different non-linearities. Instead of a linear gain term and an exponential penalty, we will try a structure with a *non-linear gain term* and a *linear penalty term*. This asymmetry in functional form, where one parameter scales a diminishing-returns-like gain and the other scales a constant-rate penalty, is hypothesized to create more distinct parameter influence profiles compared to previous attempts.

For Run 82, we will test an additive model where the utility is determined by a positive, *logarithmic* term related to the participant's proposed share (`split_perc_self`), and a negative, *linear* penalty term related to the offer's unfairness (`loss_below_fair`).

Proposed Model Structure: An additive utility model where the utility of accepting (`U_accept`) is calculated as a logarithmic function of the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, *minus* a linear penalty term based on `loss_below_fair`, scaled by `beta_unfairness`.

Here are the calculation steps:
Calculate `sum_tokens = token_self + token_opp`.
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
Calculate the utility: `U_accept = beta_value * Log(1.0 + split_perc_self) - beta_unfairness * loss_below_fair`.
The gain term uses `Log()` (natural logarithm). We add 1.0 to `split_perc_self` inside the log to handle the case where `split_perc_self` is 0, as `Log(0)` is undefined. This also implies diminishing marginal utility for higher percentage shares. The penalty term is simply linear in `loss_below_fair`, scaled by `beta_unfairness`.

In this structure, `beta_value` represents the sensitivity to the logarithmic value of the proposed percentage share (the potential gain), while `beta_unfairness` represents the strength of a linear penalty for offers below the fair share. The hypothesis is that combining a concave (logarithmic) gain function scaled by `beta_value` with a simple linear penalty function scaled by `beta_unfairness` will yield sufficiently distinct functional forms and influence profiles for each parameter across different offers to allow for robust joint identifiability, addressing the failures seen in previous, more correlated model structures. This also explores a different combination of functional forms (log + linear) compared to previous attempts (linear + exponential, various divisive forms).

Design the model using this new additive structure with a logarithmic gain and linear penalty: `U_accept = beta_value * Log(1.0 + split_perc_self) - beta_unfairness * loss_below_fair`. Remember to calculate intermediate variables (`sum_tokens`, `fair_perc_self_calc`, `loss_below_fair`) before the final `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Use `Log()` for the natural logarithm function and `Max()` for the maximum function.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific additive structure, by combining a logarithmic gain term scaled by `beta_value` with a linear penalty term based on `loss_below_fair` scaled by `beta_unfairness`, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles, exploring a different asymmetry in functional forms.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (logarithmic gain sensitivity) and `beta_unfairness` (linear penalty strength). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are appropriate. `beta_value` should be non-negative for increasing utility with gain; `beta_unfairness` should be non-negative for the penalty to reduce utility. Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model\'s core idea (additive structure where utility from `split_perc_self` is transformed logarithmically and reduced by a linear penalty based on `loss_below_fair`). Crucially, explain *how* this specific additive structure, combining a logarithmic gain term scaled by `beta_value` and a linear penalty term based on `loss_below_fair` scaled by `beta_unfairness`, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous models.
*   **Think Outside the Box:** Parameter identifiability for *both* parameters simultaneously remains the persistent, difficult challenge. Neither the divisive structures tested so far nor the additive-exponential structure have solved this. This new additive logarithmic-linear structure is another attempt to find mathematically separable influences. If this also fails to achieve >= 0.7 recovery for *both* parameters, the next iteration *must* consider even more fundamentally different approaches to how the parameters influence the decision process or how fairness is evaluated and penalized. Could one parameter affect the *comparison* process itself? Could parameters relate to thresholds or reference points? Could the model structure explicitly compare utility of accepting vs. rejecting in a non-standard way? Continue to focus intently on finding a functional form where varying `beta_value` has a clearly distinguishable effect on the utility curve across offers compared to varying `beta_unfairness`, allowing them to be estimated independently with high reliability, which is a prerequisite for better fit metrics.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,0.4313601064065788,,,,,,0.31524182303593007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
54,449.35608185179615,413.35313432443286,481.6087223450591,0.6968864468864469,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (sum_tokens > 0) ? (token_self / sum_tokens) * 100.0 : 50.0
perc_difference = split_perc_self - fair_perc_self_calc
epsilon = 1e-6 // Small constant to avoid sqrt(0) or squaring 0 when difference is exactly 0
U = (perc_difference >= 0) ? beta_gain_sensitivity * sqrt(perc_difference + epsilon) : beta_loss_sensitivity * (fair_perc_self_calc - split_perc_self + epsilon)^2","A piecewise utility model for responder decisions. Utility is calculated based on the difference between the proposed percentage share for the participant and their calculated fair share based on token contributions. When the proposed share is fair or favorable (difference >= 0), utility is proportional to a 'beta_gain_sensitivity' parameter scaling the square root of the gain percentage. When the proposed share is unfavorable (difference < 0), utility is proportional to a 'beta_loss_sensitivity' parameter scaling the squared percentage deviation from the fair share. This piecewise structure, combined with distinct non-linear functions (square root vs. squaring) scaled by the two learnable parameters in the gain and loss regimes, aims to create mathematically separable influences for each parameter, thereby enhancing parameter identifiability.",v0,"Okay, here are the improved instructions for the next run (Run 54), building on the results of the previous runs.

Instructions for Run 54 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 53) implemented a piecewise linear model based on fairness difference (`U = perc_difference >= 0 ? beta_gain_scale * split_perc_self : beta_loss_penalty * abs_perc_difference`). Despite having a piecewise structure, it unfortunately failed the parameter recovery test for *both* `beta_gain_scale` (r = 0.058) and `beta_loss_penalty` (r = 0.302). Its overall performance (BIC 76.69, Acc 0.636) was also poorer than several previous models.

Analysis of Run 53's failure: The structure assigned the parameters to scale `split_perc_self` in the gain regime and `abs_perc_difference` in the loss regime. While piecewise, these terms might still be too correlated or capture insufficiently distinct aspects of the offer value within their respective regimes for the parameters to be uniquely identified. Scaling linear terms, even in different parts of the input space, might not provide the necessary functional separation for robust estimation.

Recall the relative success of Run 1 (BIC 40.53, Acc 0.671, Recovery >= 0.727 for both parameters), which used a piecewise structure on the *ratio* (`R_val`) and applied *different functional forms* (log vs. linear difference) scaled by the two parameters (`beta_gain_log` and `beta_loss_linear_diff`) in distinct gain vs. loss regimes. This structure seems to have created more distinct influences for the two parameters.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** such that *both* parameters have truly distinct and identifiable influences on responder decisions (`trial_role == 1`). This requires making their contributions to utility as mathematically separable as possible across the data.

Based on this analysis, focus your exploration for Run 54 on model structures where the two learnable parameters scale terms that are mathematically **maximally distinct** from each other, or whose influence is primarily expressed through **different, non-linearly related aspects** of the utility calculation. Simply scaling different linear terms in different regimes (as in Run 53) appears insufficient.

Consider these strategies to enhance identifiability:

*   **Piecewise Structures with Orthogonal, Non-Linear Parameter Roles per Regime:** Continue exploring piecewise functions based on a fairness split (e.g., `perc_difference >= 0` or `R_val >= 1`). The key is to assign `beta1` to scale a term or function `f1(Inputs)` *exclusively or primarily* in one regime (favorable offers) and `beta2` to scale a *different* term or function `f2(Inputs)` *exclusively or primarily* in the other regime (unfavorable offers). Crucially, ensure that `f1` and `f2` are mathematically diverse and non-linearly related transformations of the inputs.
    *   *Structural Principle:* Design the model as `U = IF(Condition based on fairness, beta1 * f1(Inputs), beta2 * f2(Inputs))`, where `f1` and `f2` are distinct non-linear functions of the relevant inputs (like `split_perc_self`, `fair_perc_self_calc`, `R_val`, `perc_difference`). Avoid `f1` and `f2` being simple linear terms or linearly related transforms of the same base input.
    *   *Example Ideas (building on Run 1's success with ratio/log):*
        *   `U = IF(R_val >= 1.0, beta_gain_sensitivity * log(R_val + 1e-6), beta_loss_slope * (R_val - 1.0))` (Revisit Run 1 structure, perhaps refine bounds/inputs).
        *   `U = IF(perc_difference >= 0, beta_pos_gain * sqrt(split_perc_self), beta_neg_fairness * exp(perc_difference))` (Using non-linear functions like sqrt and exp on relevant terms).
        *   `U = IF(R_val >= 1.0, beta_gain_alpha * R_val^power_alpha, beta_loss_beta * (R_val - 1.0)^power_beta)` (Introducing exponents; note: this would require 4 parameters, maybe fix exponents or combine parameters differently). A better version: `U = IF(R_val >= 1.0, beta_gain_alpha * (R_val - 1.0)^2, beta_loss_beta * (1.0 - R_val)^2)` (Scaling squared deviation from 1 in each direction).

*   **Single Highly Non-Linear Function with Orthogonal Parameter Roles:** If not using a piecewise structure, design a single, complex non-linear function of the inputs where `beta1` and `beta2` clearly influence distinct mathematical properties of the function's output. For instance, one parameter could control a general sensitivity or intercept, while the other specifically modulates a non-linearity, asymmetry, or threshold effect around the fairness point *within the unified formula*. This requires a more sophisticated function design than simple additive linear models. Think about functions that capture diminishing sensitivity to increasing gains or increasing sensitivity to losses relative to a reference point.

*   **Avoid:** Any model structure where the terms scaled by `beta1` and `beta2` are linearly dependent or highly correlated across the dataset, or where one parameter's influence can be easily absorbed into the other. Linear scaling of simple input terms, even in piecewise models, appears insufficient for identifiability with this dataset, based on previous results.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens = token_self + token_opp` and `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)`. It is highly recommended to also calculate intermediate variables like ratios (`R_val = split_perc_self / fair_perc_self_calc`) and differences (`perc_difference = split_perc_self - fair_perc_self_calc`) and use non-linear transformations (log, power, exp, sqrt, abs) of these or the raw inputs as needed to create separable parameter influences.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-20.0, 20.0] or wider/narrower if appropriate for the variable scale). Consider whether parameters are expected to be positive or negative and set bounds accordingly, but keep them generous.

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, any intermediate terms like ratios/differences/transformed values, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` includes the `sum_tokens > 0` condition.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the proposed percentage share for self.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain how its specific mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters (e.g., by having them scale distinct *non-linear* terms in different regimes, or by influencing separate, non-linearly related aspects of the utility calculation).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. This requires making their contributions to utility as distinct and non-overlapping as possible, often by having them scale different, non-linearly related terms or influence different regimes of the decision space. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation. Do not simply repeat structures that have failed recovery; propose a genuinely different way for the two parameters to influence the decision process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.33243832498319686,0.5959625068998353,,,,,,,,,,,,,,,,,,,
36,450.0744209964658,402.22137896387284,492.9427711506636,0.7191697191697192,"sum_tokens = token_self + token_opp
fair_perc_self = { (token_self / sum_tokens) * 100, if sum_tokens > 0 ; 50, if sum_tokens == 0 }
U = beta_intercept + beta_split_perc * split_perc_self + beta_fair_perc * fair_perc_self","A three-parameter linear model predicting the utility of accepting an offer in the ultimatum game responder role. Utility is calculated as a linear combination of a baseline intercept, the proposed percentage share for the participant, and the participant's fair percentage share (based on token contributions), each weighted by a learnable parameter. This simple linear structure aims to improve parameter identifiability compared to piecewise or fixed-point models by using distinct inputs (proposed and fair percentages). The fair percentage is calculated based on token contributions, handling the zero-contribution case.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,\n      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {""description"": ""Noise parameter"", ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true}, ""learnable"": true, ""source"": ""learnable""},
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions for Run 36 of 100:
The primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is non-negotiable for a usable model. Improved BIC and accuracy are important secondary goals.

In the previous run (Run 35), the two-parameter piecewise linear model with utility fixed at 0 at the fair point failed to achieve acceptable parameter recovery for the gain slope (`r=0.468`) and had a high BIC (250.45). This suggests that models with strict piecewise structures or fixed utility points may be encountering identifiability issues, particularly for gain sensitivity and baseline utility components, despite simplifying the parameter space.

For this run, design a computational model for responder decisions (`trial_role == 1`) that predicts the utility of accepting an offer using a **simplified linear model based directly on the offered percentage and the calculated fair percentage**. This structure moves away from piecewise functions and fixed reference point utility values, aiming to improve parameter identifiability by modeling utility as a straightforward linear combination of key variables.

*   **Parameter Identifiability:** Focus on creating a model structure where parameters have a clearly separable influence. A simple linear combination of distinct inputs (`split_perc_self` and `fair_perc_self`) may offer better identifiability compared to piecewise slopes or utility intercepts at specific points, which have failed in previous runs.
*   **Key Inputs:** Continue to use the calculated fair percentage for the participant (`fair_perc_self`) based on token contributions as a key variable alongside the proposed share percentage (`split_perc_self`). Calculate `fair_perc_self` as `(token_self / (token_self + token_opp)) * 100`. **Handle the case where `token_self + token_opp == 0` by setting `fair_perc_self` to 50 in that instance.**
*   **Linear Structure:** Define the utility (`U`) using a simple linear combination of `split_perc_self` and `fair_perc_self` with a general intercept.
*   **Model Formula:** Implement this structure with the following formula: `U = beta_intercept + beta_split_perc * split_perc_self + beta_fair_perc * fair_perc_self`.
    *   `beta_intercept` represents a baseline utility.
    *   `beta_split_perc` represents sensitivity to the proposed percentage (expected positive).
    *   `beta_fair_perc` represents sensitivity to the participant's fair share percentage (expected negative, as a higher fair share makes a fixed offer less appealing).
*   **Calculated Variables:** Ensure `sum_tokens`, `fair_perc_self`, and `U` are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`).
*   **Learnable Parameters:** Your model should have three learnable parameters: `beta_intercept`, `beta_split_perc`, and `beta_fair_perc`. Define clear, generous, finite numerical bounds for all, e.g., [-20, 20]. Ensure parameter names are suitable for Python code.
*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags, including the calculations for `sum_tokens`, `fair_perc_self`, and the final formula for `U`. Use standard mathematical notation.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the three-parameter linear model. Explain that it models utility as a linear function of the proposed share and the calculated fair share, including a baseline intercept. Mention how this simple linear structure aims to improve parameter identifiability by moving away from piecewise functions and fixed reference points explored previously.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-0.22076600902843252,0.2722772596918361,0.5714813746681863,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98,450.6786536313548,427.1481191182759,471.75809079932134,0.6971916971916972,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
is_self_more_contrib = IF(trial_type == 3, 1.0, 0.0)
U_accept = deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + self_more_contrib_intercept_shift * is_self_more_contrib","This model predicts the utility of accepting a proposed split based on two main factors: the deviation of the proposed split percentage from the calculated fair percentage, and a specific bias applied when the participant's initial contribution was higher than the opponent's. The utility is a linear combination of the deviation from fairness, scaled by a 'deviation_sensitivity' parameter, and a binary indicator for the participant-more-contribution case, scaled by a 'self_more_contrib_intercept_shift' parameter. This structure ensures robust parameter identifiability because the 'deviation_sensitivity' captures the continuous influence of relative fairness, while the 'self_more_contrib_intercept_shift' applies a distinct, conditional offset only in a specific trial type, providing separable sources of variance for parameter estimation unlike previous models using binary 'below fair' indicators.",v0,"Okay, here are the improved instructions for the next run (Run 98 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy. Specifically, *all* learnable parameters must have a parameter recovery value of >= 0.7.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {""min"": 0, ""max"": 1},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {""description"": ""Noise parameter"",
      ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,\n        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 98 of 100).

The previous model (`deviation_sensitivity * (split_perc_self - fair_perc_self_calc) + below_fair_impact * is_below_fair`) achieved partial success. It showed robust recovery for the `deviation_sensitivity` parameter (r=0.872) and reasonable accuracy (0.749) and BIC (116.37). However, it critically failed parameter recovery for the `below_fair_impact` parameter (r=0.264). This indicates that while scaling the deviation from fairness works well, scaling the binary 'below fair' indicator in this linear combination structure did not provide sufficient distinctness for reliable estimation of the second parameter.

Robust parameter identifiability (recovery >= 0.7 for *all* learnable parameters) remains a **primary objective**, and it MUST be achieved alongside significantly improved accuracy (target > 0.749) and a better BIC (target < 116.37, aiming towards the BIC 52.82 of the best model so far). A model with good recovery but poor predictive power is not useful.

We must learn from past results:
- Scaling the deviation from fairness (`split_perc_self - fair_perc_self_calc`) with one parameter (`deviation_sensitivity`) has consistently shown good recovery when paired with inputs that have a sufficiently distinct structural influence (like a constant intercept), but *not* when paired with the `is_below_fair` binary indicator.
- Using the binary `is_unfair` (or `is_below_fair`) input scaled by a parameter did not recover well when combined with a parameter scaling either the absolute offer or the deviation from fairness.

This strongly suggests that the binary indicator `is_below_fair` (or `is_unfair`) is not a suitable input for the *second* parameter in a linear combination with `(split_perc_self - fair_perc_self_calc)` if we want robust identifiability for both. The influence of the binary term seems too correlated or mathematically confounded with the continuous deviation term in this structure.

You *must* include these standard calculation steps first before defining `U_accept`:
1.  Calculate `sum_tokens = token_self + token_opp`.
2.  Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.

Now, design a **novel** formula for `U_accept` incorporating exactly **two learnable parameters**. Focus intently on creating a structure where these two parameters have demonstrably *different types* of mathematical influence on the utility calculation to ensure robust identifiability and improve accuracy.

Specifically, design the model such that:
*   One parameter scales the **deviation from fairness**: `(split_perc_self - fair_perc_self_calc)`. Choose a descriptive, Python-safe name for this parameter (e.g., `deviation_sensitivity`).
*   The second parameter must scale a variable or constant that introduces a *structurally distinct* and *non-confounding* influence on utility compared to the continuous deviation. **Explicitly avoid using the binary indicator `is_below_fair` or `is_unfair` as the scaled input for this second parameter**, as this has repeatedly failed parameter recovery. Think creatively about variables or states that are not simply linear transformations of the deviation or the absolute offer. Good candidates for the input scaled by the second parameter include:
    -   A constant (like an intercept). This structure successfully recovered both parameters in a previous run, although the BIC was higher than the current best. Why did this work? How can we improve upon it?
    -   A variable reflecting the **total amount** (`combined_earning`) or potential gain, scaled in a way that is not confounded with the fairness deviation. How does the overall scale of the trial influence acceptance, independent of the split's fairness?
    -   A categorical variable based on the **contribution structure** (`trial_type`) or token counts (`token_self`, `token_opp`), perhaps scaling an indicator for specific contribution types (e.g., an additional bonus/penalty for equal splits, or unequal splits, that is *separate* from the fairness deviation).
    -   Consider simple non-linear terms if they offer distinct structural influence, although linear combinations of inputs are generally preferred for initial exploration.

Your goal is to find a novel way to combine the influence of a fairness deviation parameter with a second parameter whose influence is clearly separable and non-redundant with the deviation, leading to both robust recovery (>= 0.7 for BOTH) and substantially improved accuracy (target > 0.749) and BIC (target < 116.37) compared to the previous run.

Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus potentially *other relevant data variables* from the list provided (`trial_type`, `combined_earning`, etc.) or calculated variables derived from them (like binary indicators from `trial_type`).

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus potentially others from the provided list. Calculate `sum_tokens`, `fair_perc_self_calc`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Include function definitions like `IF` if used.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Ensure the parameters' roles are structurally distinct to promote identifiability (e.g., one scaling a continuous deviation, the other scaling a non-confounding variable like a categorical indicator or constant).
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, any derived binary variables, and any functions like `IF`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs with `source: ""data""` that appear in your <MODEL>.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea. **Crucially, explain *how* the specific structure of your model ensures robust parameter identifiability for *both* learnable parameters (recovery >= 0.7), explicitly addressing why the scaled inputs for the two parameters are structurally distinct and non-confounding, building on the success of models scaling deviation + constant and the failure of models scaling deviation + binary unfairness indicator.**

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) AND improved accuracy/BIC are the absolute priorities. Your goal is to find a novel structure with *exactly two* parameters where their influences are clearly distinguishable (e.g., one scaling the fairness deviation, the other scaling a non-confounding variable like a categorical indicator or constant) and interact in a way that allows for reliable estimation and better fits the human decision data. Be creative and think ""out of the box"" about how to use the available data variables and exactly two parameters to capture distinct aspects of the responder's decision process while maintaining identifiability and improving predictive power.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6548797605646544,,0.38711538812023316,,
57,465.07512450690257,455.850265425652,473.33906076718955,0.6971916971916972,"sum_tokens = token_self + token_opp
fair_perc_self_calc = If(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)
fairness_deviation = split_perc_self - fair_perc_self_calc
U = beta_zero + beta_interaction * split_perc_self * fairness_deviation","A utility model predicting responder acceptance based on the proposed percentage for the participant relative to a calculated fair share. Utility is calculated as a baseline (beta_zero) plus an interaction term. The interaction term scales the product of the proposed percentage (split_perc_self) and the deviation from fairness (fairness_deviation) by a sensitivity parameter (beta_interaction). This non-linear multiplicative structure between the offered value, fairness deviation, and the scaling parameter aims to create mathematically distinct influences for the two learnable parameters (beta_zero and beta_interaction), enhancing their identifiability compared to additive models. The fair share calculation is based on token contributions, defaulting to 50% if total tokens are zero.",v0,"Okay, here are the improved instructions for the next run (Run 57), building on the results of the previous runs.

Instructions for Run 57 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 55) implemented the suggested linear value plus non-linear relative fairness model (`U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)`). Unfortunately, while `beta_value` recovery was acceptable (r = 0.703), `beta_relative_fairness` recovery was very poor (r = 0.046). Its overall performance (BIC 503.13, Acc 0.688) was also not optimal.

Analysis of Run 55's failure and previous models: The persistent challenge lies in finding a model structure where the parameter related to fairness, relative value, or penalty has an influence that is mathematically distinct enough from the parameter related to absolute value or gain (`split_perc_self`) to be reliably estimated. We have now explored linear additive models, piecewise linear models, piecewise non-linear models, and a linear + log-relative model. In most cases, the parameter scaling the fairness/relative term has failed recovery (Runs 1, 2, 3, 4, 54, 55). This suggests that simply scaling different terms, even when one is a non-linear function of the ratio or deviation, does not provide sufficient identifiability for the second parameter in the context of this dataset. The influence of the fairness/relative term appears too correlated with or overshadowed by the influence of the absolute offer value term when combined additively or piecewise.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** such that *both* parameters have truly distinct and identifiable influences on responder decisions (`trial_role == 1`). We need to find a way for the second parameter (the fairness/relative/penalty related one) to exert its influence in a manner that is mathematically orthogonal or highly distinct from the first parameter's influence (the value/gain related one). We need to move beyond simple additive or piecewise combinations of value and fairness terms.

Based on this ongoing difficulty, focus your exploration for Run 57 on model structures where the two learnable parameters contribute to utility in fundamentally different mathematical ways, potentially through multiplicative interactions or by shaping the *form* of the utility function in relation to fairness, rather than just scaling simple additive components. Avoid structures that closely resemble the failed attempts (linear combinations of value/deviation, simple piecewise models, linear value + non-linear additive fairness/ratio).

Consider strategies to enhance identifiability by creating non-separable influences:

*   **Introduce Multiplicative Interactions:** Design a model where one parameter scales a term involving the interaction of the proposed percentage and the fairness deviation. This makes the fairness influence dependent on the offer size, potentially creating a more unique signature in the utility function.
    *   *Example Idea:* Let one parameter be a general intercept, and the other parameter scale a term that is the product of the offer percentage and the fairness deviation.
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensure this handles `sum_tokens=0`).
        *   `fairness_deviation = split_perc_self - fair_perc_self_calc` (Calculate the difference from fairness).
        *   `U = intercept + beta_interaction * split_perc_self * fairness_deviation` (Combine an intercept with an interaction term).
    *   *Structural Principle:* This structure (`U = beta1 + beta2 * X * Y`) is different from previous linear additive (`U = beta1 * X + beta2 * Y`) or simple piecewise structures. `beta1` provides a baseline utility, while `beta2` modulates utility based on a term (`split_perc_self * fairness_deviation`) that non-linearly combines offer size and fairness deviation. This non-linear interaction might make the parameters' effects more distinct.

*   **Explore Other Novel Structures:** Continue to think creatively about how two parameters can control mathematically separable aspects of the utility function. Could parameters control exponents in a power-law model of fairness deviation? Could one parameter define a specific threshold or anchor point on the percentage scale, with the second parameter defining sensitivity to deviations from that anchor point? Could parameters influence the shape (e.g., slope, curvature) of a continuous, non-linear function of fairness deviation? Focus on structures where the parameters' influence is not simply a scaled addition of terms already explored.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and necessary intermediate terms like `fairness_deviation`. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-50.0, 50.0] or wider/narrower if appropriate for the scale of your terms). Consider the expected sign and scale, but keep bounds wide enough.

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, intermediate terms like `fairness_deviation`, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` explicitly mentions the `sum_tokens > 0` condition and the default value.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the participant's proposed percentage share percentage.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters*, explicitly referencing the non-standard way the parameters influence utility (e.g., scaling an interaction term, defining curve shape, etc.) to make their roles distinct from previous failed attempts.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. Previous attempts combining linear value terms with linear or non-linear fairness/penalty terms (additively or piecewise) have struggled with the fairness parameter's recovery. Explore models with multiplicative interactions or parameters that define the shape of the utility function in fundamentally different ways to create more separable influences. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation.

Please think through this step by step, then provide your model specification and variable descriptions.
</think>
Instructions for Run 57 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 55) implemented the suggested linear value plus non-linear relative fairness model (`U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)`). Unfortunately, while `beta_value` recovery was acceptable (r = 0.703), `beta_relative_fairness` recovery was very poor (r = 0.046). Its overall performance (BIC 503.13, Acc 0.688) was also not optimal compared to some previous runs (e.g., BIC 76.69 from Run 4, though that model failed recovery badly).

Analysis of Run 55's failure and previous models: The persistent challenge lies in finding a model structure where the parameter related to fairness, relative value, or penalty has an influence that is mathematically distinct enough from the parameter related to absolute value or gain (`split_perc_self`) to be reliably estimated. We have now explored linear additive models, piecewise linear models, piecewise non-linear models, and a linear + log-relative model. In most cases, the parameter scaling the fairness/relative term has failed recovery (Runs 1, 2, 3, 4, 54, 55). This suggests that simply scaling different terms, even when one is a non-linear function of the ratio or deviation, does not provide sufficient identifiability for the second parameter in the context of this dataset. The influence of the fairness/relative term appears too correlated with or overshadowed by the influence of the absolute offer value term when combined additively or piecewise.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** such that *both* parameters have truly distinct and identifiable influences on responder decisions (`trial_role == 1`). We need to find a way for the second parameter (the fairness/relative/penalty related one) to exert its influence in a manner that is mathematically orthogonal or highly distinct from the first parameter's influence (the value/gain related one). We need to move beyond simple additive or piecewise combinations of value and fairness terms.

Based on this ongoing difficulty, focus your exploration for Run 57 on model structures where the two learnable parameters contribute to utility in fundamentally different mathematical ways, potentially through multiplicative interactions or by shaping the *form* of the utility function in relation to fairness, rather than just scaling simple additive components. Avoid structures that closely resemble the failed attempts (linear combinations of value/deviation, simple piecewise models, linear value + non-linear additive fairness/ratio).

Consider strategies to enhance identifiability by creating non-separable influences:

*   **Introduce Multiplicative Interactions:** Design a model where one parameter scales a term involving the interaction of the proposed percentage and the fairness deviation. This makes the fairness influence dependent on the offer size, potentially creating a more unique signature in the utility function.
    *   *Example Idea:* Let one parameter be a general intercept, and the other parameter scale a term that is the product of the offer percentage and the fairness deviation.
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensure this handles `sum_tokens=0`).
        *   `fairness_deviation = split_perc_self - fair_perc_self_calc` (Calculate the difference from fairness).
        *   `U = intercept + beta_interaction * split_perc_self * fairness_deviation` (Combine an intercept with an interaction term).
    *   *Structural Principle:* This structure (`U = beta1 + beta2 * X * Y`) is different from previous linear additive (`U = beta1 * X + beta2 * Y`) or simple piecewise structures. `intercept` provides a baseline utility, while `beta_interaction` modulates utility based on a term (`split_perc_self * fairness_deviation`) that non-linearly combines offer size and fairness deviation. This non-linear interaction might make the parameters' effects more distinct and recoverable.

*   **Explore Other Novel Structures:** Continue to think creatively about how two parameters can control mathematically separable aspects of the utility function. Could parameters control exponents in a power-law model of fairness deviation? Could one parameter define a specific threshold or anchor point on the percentage scale, with the second parameter defining sensitivity to deviations from that anchor point? Could parameters influence the shape (e.g., slope, curvature) of a continuous, non-linear function of fairness deviation? Focus on structures where the parameters' influence is not simply a scaled addition of terms already explored. Think about how the parameters can *jointly* determine the shape of the utility function across different offer percentages and fairness levels in a way that previous models have not.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and necessary intermediate terms like `fairness_deviation`. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-50.0, 50.0] or wider/narrower if appropriate for the scale of your terms). Consider the expected sign and scale, but keep bounds wide enough.

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, intermediate terms like `fairness_deviation`, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` explicitly mentions the `sum_tokens > 0` condition and the default value.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the participant's proposed percentage share percentage.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters*, explicitly referencing the non-standard way the parameters influence utility (e.g., scaling an interaction term, defining curve shape, etc.) to make their roles distinct from previous failed attempts.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. Previous attempts combining linear value terms with linear or non-linear fairness/penalty terms (additively or piecewise) have struggled with the fairness parameter's recovery. Explore models with multiplicative interactions or parameters that define the shape of the utility function in fundamentally different ways to create more separable influences. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation.",0.6352500311078967,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5842851912019807,,,,,,,,,,,,,,,,,
95,465.6902055957252,393.7871424721059,530.1033663106341,0.7231379731379731,U_accept = beta_offer_sensitivity * split_perc_self + beta_fairness_deviation_sensitivity * (split_perc_self - fair_perc_self_calc),"This model predicts the utility of accepting a proposed offer as a responder in an ultimatum game. The utility (U_accept) is a linear combination of two components: the absolute percentage share proposed to the participant (split_perc_self) and the deviation of this proposed share from a calculated fair share percentage based on token contributions (split_perc_self - fair_perc_self_calc). The model has two learnable parameters: beta_offer_sensitivity, which scales the influence of the absolute proposed percentage, and beta_fairness_deviation_sensitivity, which scales the influence of the deviation from fairness. This structure is designed for robust parameter identifiability because the two parameters scale inputs that represent fundamentally different aspects of the offer – its magnitude relative to the whole pie and its magnitude relative to a fairness benchmark. Mathematically, this forms a linear combination of split_perc_self and fair_perc_self_calc, whose coefficients are identifiable as long as these inputs are not perfectly correlated, allowing for the recovery of the two original parameters. The utility is converted to a probability of acceptance using a logistic function.",v0,"Okay, here are the improved instructions for the next run (Run 95 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy. Specifically, *all* learnable parameters must have a parameter recovery value of >= 0.7.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {""min"": 0, ""max"": 1},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {""min"": ""-inf"", ""max"": ""inf""},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {""min"": 0, ""max"": 1, ""exclusive_max"": true},
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 95 of 100).

The previous attempt to model utility as scaled components of fairness deviation and context deviation did not yield identifiable parameters (recovery < 0.7 for both). In contrast, a simpler linear model combining fairness deviation with a constant intercept *did* achieve robust parameter recovery (> 0.7 for both parameters) and better overall performance (BIC, accuracy).

Robust parameter identifiability remains the **unwavering primary objective** (recovery >= 0.7 for *all* learnable parameters). We must build on the structural principles that worked, namely having two parameters with clearly separable roles, rather than structures that failed identifiability tests.

You *must* include these standard calculation steps first before defining `U_accept`:
1.  Calculate `sum_tokens = token_self + token_opp`.
2.  Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.

Now, design a **novel** formula for `U_accept` using `split_perc_self`, `fair_perc_self_calc`, and potentially *other relevant data variables* from the list provided (like `token_self`, `token_opp`, `trial_type`, `combined_earning`), incorporating exactly **two learnable parameters**.

Focus intently on creating structures where the two learnable parameters have demonstrably distinct and separable influences on the utility calculation, similar to how a slope (scaling a variable input) and an intercept (providing a baseline offset) work in a linear model. Avoid structures where the two parameters scale inputs that are highly correlated or whose influence profiles on the utility function are mathematically too similar across trials, as this has led to poor recovery.

Think creatively about how two parameters can capture distinct psychological processes related to fairness, self-interest, or context within the framework of the provided variables. Do *not* simply scale two different calculated deviation terms if their influence cannot be separated by the fitting process. Instead, consider:
*   How one parameter scales the primary reward signal (`split_perc_self` or related to it) while the other scales a cost/bias/contextual term.
*   How one parameter scales a linear influence and the other scales a non-linear influence (e.g., a quadratic term, a log/exp transform, or a threshold/conditional effect like `IF(condition, input, 0)`).
*   How one parameter scales a continuous variable (like deviation from fairness) and the other scales a categorical or binary variable (like `trial_type` or a computed property like `is_unfair = IF(split_perc_self < fair_perc_self_calc, 1, 0)`).
*   How parameters might interact multiplicatively with different inputs, where one parameter's effect is gated or amplified differently than the other's.

Ensure the two parameters' contributions are structurally independent or interact in a way that makes them mathematically distinguishable. The goal is to maintain robust parameter recovery (like the linear deviation + intercept model) while attempting to improve model fit (BIC, accuracy) by better capturing the nuances of responder behavior using the available data variables.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables, plus potentially others from the provided list. Calculate `sum_tokens`, `fair_perc_self_calc`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`. Include function definitions like `IF` if used.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Ensure the parameters' roles are structurally distinct to promote identifiability.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and any functions like `IF`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs with `source: ""data""` that appear in your <MODEL>.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea. **Crucially, explain *how* the specific structure of your model, with exactly two learnable parameters, is designed to create mathematically distinct and separable influence profiles for those two parameters, specifically addressing *why* this structure is expected to yield robust joint parameter identifiability (recovery >= 0.7 for both), building on the lessons learned from previous successes and failures.** Highlight what aspect of the decision each parameter is intended to capture and how their mathematical roles fundamentally differ.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a novel structure with *exactly two* parameters where their influences are clearly distinguishable and interact in a way that allows for reliable estimation, ideally leading to improved fit metrics (BIC and accuracy). Avoid structures that have failed recovery previously. Explore alternative ways to combine inputs and parameters beyond simply scaling different deviation terms, ensuring the two parameters have fundamentally different mathematical roles that are likely to be identifiable. Be creative and think ""out of the box"" about how to use the available data variables and exactly two parameters to capture distinct aspects of the responder's decision process while maintaining identifiability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.538458265751437,0.3746867969694085,,,,,,,
20,475.34837462609323,298.68095123898667,633.6129414103762,0.6538461538461539,U = sensitivity_offer * split_perc_self - penalty_unfair_flat * is_unfair,"A responder utility model where utility is determined by a linear sensitivity to the percentage offered to the participant, and a fixed penalty subtracted if the offered percentage is strictly less than the contribution-based fair percentage. The model has two non-negative learnable parameters: sensitivity to the offer and the magnitude of the flat unfairness penalty.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0))
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

```json
{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
```
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta_param + epsilon_param * E + eta_param * N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta_param"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon_param"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta_param"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: In the previous run (Run 19), we attempted to improve parameter recovery of a successful model structure by adding a bias term (`beta_bias`). While accuracy improved (0.832) and recovery for `penalty_magnitude_unfairness` reached the target (0.741), recovery for `sensitivity_offer` decreased (0.553), and the new `beta_bias` parameter was essentially unrecoverable (0.002). This indicates the 3-parameter structure did not fully resolve the identifiability issues.

For this run (Run 20), the primary goal remains to achieve parameter recovery >= 0.7 for *all* learnable parameters. Given the failure of the `beta_bias` term to be recovered, we will simplify the model structure by removing it and also simplify the unfairness penalty term, aiming for better parameter disentanglement.

Consider a new model structure with two learnable parameters:
`U = sensitivity_offer * split_perc_self - penalty_unfair_flat * max(0, -sign(split_perc_self - fair_perc_self))`

This model includes:
1.  `sensitivity_offer`: Represents a linear sensitivity to the offered percentage (`split_perc_self`). A higher value means greater utility for higher offers. This parameter should be non-negative.
2.  `penalty_unfair_flat`: Represents a *fixed* penalty subtracted from the utility *only* when the offer is strictly unfair (`split_perc_self < fair_perc_self`). The term `max(0, -sign(split_perc_self - fair_perc_self))` evaluates to 1 for unfair offers and 0 otherwise. This parameter should be non-negative.

This structure removes the unrecoverable bias term and replaces the magnitude-dependent penalty (`penalty_magnitude_unfairness * magnitude_of_unfairness`) with a simpler, fixed penalty (`penalty_unfair_flat`) applied regardless of how *much* the offer is below fair, only whether it *is* below fair. This might help distinguish the overall sensitivity to the offered amount from the discrete effect of an offer being perceived as unfair.

**Guidance for the Next Model (Run 20):**

Design a computational model implementing this specific structure. The primary goal is to achieve parameter recovery >= 0.7 for *all* learnable parameters while maintaining good BIC and accuracy.

*   **Prioritize Parameter Recovery:** Implement the specified structure precisely to test if simplifying the penalty term improves the identifiability of `sensitivity_offer` and introduces a recoverable `penalty_unfair_flat`.
*   **Parameter Constraints:**
    *   `sensitivity_offer`: Define clear, generous, finite, non-negative bounds. Suggest [0, 20].
    *   `penalty_unfair_flat`: Define clear, generous, finite, non-negative bounds. Suggest [0, 10].
*   **Model Specification:** Provide the mathematical formula between <MODEL> tags using the suggested structure: `U = sensitivity_offer * split_perc_self - penalty_unfair_flat * max(0, -sign(split_perc_self - fair_perc_self))`. Ensure the formula includes the necessary variables and parameters.
*   **Calculated Variables:** Remember `fair_perc_self` needs to be calculated as `(token_self / (token_self + token_opp)) * 100`. The term `max(0, -sign(split_perc_self - fair_perc_self))` is a calculated binary variable indicating unfairness (1 if unfair, 0 otherwise). Include `U`, `fair_perc_self`, `token_self`, and `token_opp` in your variable descriptions with `source: ""calculated""` or `source: ""data""` as appropriate.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for all variables used in the <MODEL> formula (`U`, `split_perc_self`, `fair_perc_self`, `token_self`, `token_opp`, `sensitivity_offer`, `penalty_unfair_flat`), including the specified bounds and learnable status for the parameters. Clearly describe the role of `fair_perc_self` and how the `max(0, -sign(...))` term functions.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags, describing the model structure and its components.

Remember to use variables relevant for responder trials (`trial_role == 1`).

If this simplified structure still fails to yield sufficient parameter recovery for all parameters, future runs may need to explore fundamentally different functional forms for utility, consider non-linear relationships, or incorporate how relative contributions (tokens) influence perceived fairness or utility in ways not solely captured by the `fair_perc_self` calculation.

For run 20 of 100, please think through this step by step, focusing intently on designing a model structure that addresses the parameter recovery issue observed in Run 19 by simplifying the model and the unfairness penalty. Then provide your model specification, variable descriptions, target variable, and summary.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-0.13531894339904574,,,0.1591321267414017,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
60,475.8617745968535,430.32601024862345,516.6542301588096,0.7130647130647131,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
deviation_from_fairness = split_perc_self - fair_perc_self_calc
U = beta_value * split_perc_self + beta_fairness_interaction * split_perc_self * deviation_from_fairness","A utility model predicting responder acceptance based on two factors: the percentage offered to the participant and the interaction between this percentage and the deviation from a contribution-based fairness norm. Utility is calculated as a linear combination of the proposed percentage scaled by 'beta_value' and a multiplicative term combining the proposed percentage and the deviation from fairness, scaled by 'beta_fairness_interaction'. This multiplicative structure is designed to ensure distinct contributions from the two learnable parameters ('beta_value' captures general value sensitivity, 'beta_fairness_interaction' captures how fairness deviation modulates this value sensitivity), thereby enhancing parameter identifiability compared to models using only additive terms or simpler piecewise functions.",v0,"Okay, here are the improved instructions for the next run, focusing intensely on designing a model structure that ensures parameter identifiability for *both* learnable parameters.

Instructions for Run 60 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous attempts, including the one exploring an asymmetric piecewise structure with a squared unfairness term, have consistently struggled to achieve sufficient parameter identifiability for *both* of the two learnable parameters. This suggests that the simple additive or piecewise linear/squared structures explored so far, even with asymmetric domains, do not create mathematically distinct enough contributions from the two parameters to allow for their unique estimation from the data.

We need to find a mathematical structure for utility, still using exactly **2 learnable parameters**, where their influence on responder decisions (`trial_role == 1`) is fundamentally more separable and distinguishable than in previous models. This requires exploring mathematical forms beyond simple linear combinations or squared deviations.

Based on the persistent difficulty, focus your exploration for Run 60 on designing a utility function where the influence of the two parameters is inherently distinct through more complex or interactive non-linearities.

Consider the following strategies to enhance identifiability by making the two parameters' contributions qualitatively different:

*   **Explore Multiplicative Interactions:** Instead of purely additive terms, consider models where one parameter modulates the *relationship* between different factors. For example, a parameter could scale the *product* of a value term and a fairness term, or scale a non-linear function that combines value and fairness information.
*   **Non-linear Functions of Ratios or Deviations:** If using fairness measures like the ratio (`split_perc_self / fair_perc_self_calc`) or deviation (`split_perc_self - fair_perc_self_calc`), explore scaling *different* non-linear transformations of these measures with the two parameters. Avoid simple linear or squared terms if they seem to overlap in influence. Consider transformations like cubic terms, logarithms, exponentials, or sigmoids applied to the fairness measure, scaled by a parameter.
*   **Piecewise Functions with Distinct Forms:** If using a piecewise structure (e.g., one rule for fair/favorable offers, another for unfair), ensure the mathematical form of the utility function is fundamentally different across the pieces, and that the parameters scale terms whose influence is unique to that piece and distinct from terms in other pieces.
*   **Parameter Influencing Shape/Slope:** Design a model where one parameter primarily shifts or scales the utility function overall, while the second parameter primarily affects its *shape*, *slope*, or *curvature* with respect to `split_perc_self` or the fairness measure, making their effects distinguishable.

The goal is to create a structure where changes in the first parameter affect utility differently across the range of inputs compared to changes in the second parameter. **Avoid structures where, for any significant portion of the data, the utility is simply a linear combination of input variables scaled by the two parameters.**

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate necessary intermediate variables like `sum_tokens`, `fair_perc_self_calc`, and any other derived measures of value, fairness, or their interactions as needed by your model structure. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0. Use a small epsilon for numerical stability where necessary (e.g., in denominators, logarithms).
*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags. Only the formula should be inside these tags.
*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code (e.g., `beta_value`, `beta_fairness_sensitivity`, or names reflecting their specific roles in your new structure). Define clear, generous, finite numerical bounds (e.g., [-10.0, 10.0] or wider/narrower if appropriate).
*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including U) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Provide detailed descriptions for *calculated* variables, explaining how they are derived and what they represent.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the participant's proposed percentage share percentage.
*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain *how its specific mathematical structure (e.g., the use of a multiplicative term, a novel non-linear function, or a specific piecewise definition) is designed to ensure parameter identifiability for *both* learnable parameters*, making their roles mathematically distinct from previously attempted additive or simple piecewise linear/squared models.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable across the range of possible offers and fairness conditions. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation, moving beyond simple linear or squared additive components.",,,,,,,,,,,,,,,,,,0.3358642120439541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6284216441682062,,,,,,,,,,,,,,
70,479.63605495144463,411.0127295769417,541.1111172661035,0.6831501831501832,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (token_self / sum_tokens) * 100.0 if sum_tokens > 0 else 50.0
deviation = fair_perc_self_calc - split_perc_self
positive_deviation = max(0.0, deviation)
safe_split_perc_self = split_perc_self + 1e-6
ratio_penalty = positive_deviation / safe_split_perc_self
U_accept = beta_value * split_perc_self - beta_unfairness * ratio_penalty","An additive utility model for responder decisions. The utility of accepting an offer is calculated as the participant's proposed percentage scaled by a 'value' sensitivity parameter (beta_value), minus a penalty scaled by an 'unfairness' sensitivity parameter (beta_unfairness). The penalty is uniquely defined as the positive deviation from the calculated fair share percentage divided by the proposed share percentage (plus a small epsilon). This specific ratio-based penalty term, which grows non-linearly and steeply for low offers below the fair share, is hypothesized to create a mathematically distinct influence profile for the beta_unfairness parameter compared to the linear influence of beta_value. This distinction in influence across the range of offers is intended to improve the robust identifiability of both learnable parameters (beta_value and beta_unfairness) within the additive structure, addressing a key limitation of previous additive and ratio models. The model predicts acceptance based on this calculated utility, where higher utility corresponds to a higher probability of acceptance via a logistic function.",v0,"Okay, here are the improved instructions for the next run (Run 69), building on the analysis of previous attempts and maintaining the intense focus on achieving robust parameter identifiability for *both* learnable parameters.

Instructions for Run 69 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery.

Run 68 explored a ratio structure (`U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * NonLinearPenaltyTerm)`) using a squared positive difference as the non-linear penalty (`max(0.0, fair_perc_self_calc - split_perc_self)^2`). While this model showed a relatively good average BIC (44.58) and moderate accuracy (0.673), it failed the primary objective, yielding parameter recoveries of only r=0.497 for `beta_value` and r=0.445 for `beta_unfairness`. This result, combined with the history of other ratio models struggling with `beta_unfairness` recovery (e.g., Run 61, 63, 67), strongly suggests that putting the `beta_unfairness` term scaling a penalty in the denominator of a ratio structure, alongside `beta_value` scaling `split_perc_self` in the numerator, might inherently lead to identifiability issues due to correlated influences across offer ranges.

The challenge persists: how to mathematically disentangle the influence of sensitivity to the *value* of the offer (`beta_value` scaling `split_perc_self`) from the influence of sensitivity to *unfairness* (`beta_unfairness` scaling some measure of deviation from fairness)? Previous attempts using additive models (like Run 66) also failed parameter recovery when using relatively simple penalty forms.

For Run 69, let's return to an **additive utility structure**, but critically, use a **new and distinct functional form for the unfairness penalty term** that we hope will create a unique parameter landscape for `beta_unfairness`, allowing it to be disentangled from `beta_value`. Instead of a simple difference or the previous ratio penalties (`(Fair/Split) - 1`), let's define the penalty based on the **positive deviation from fairness *relative to the offered percentage***.

Proposed Model Structure: An additive model where utility is the value of the offer minus a penalty proportional to unfairness.
`U = beta_value * split_perc_self - beta_unfairness * RatioBasedPenalty`

Here, `RatioBasedPenalty` is calculated as the positive difference between the fair share and the proposed share, divided by the proposed share (plus a small epsilon to prevent division by zero). Specifically:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0).
Calculate `deviation = fair_perc_self_calc - split_perc_self`.
Define `positive_deviation = max(0.0, deviation)`. This ensures the penalty is only applied for offers below the fair share.
Define `safe_split_perc_self = split_perc_self + 1e-6` (using a small constant like 1e-6 to avoid division by zero, especially when `split_perc_self` is 0).
Define the `RatioBasedPenalty = positive_deviation / safe_split_perc_self`.
Finally, `U_accept = beta_value * split_perc_self - beta_unfairness * RatioBasedPenalty`.

The theoretical rationale for this specific penalty term (`(Fair - Split) / Split`) is that it grows non-linearly and increasingly steeply as the offer (`split_perc_self`) gets smaller, when it's below the fair share. This creates a potentially different influence profile for `beta_unfairness` (scaling this rapidly changing penalty) compared to `beta_value` (scaling the linearly changing `split_perc_self`) across the range of possible offers. This is an attempt to find a penalty form whose impact, when scaled by `beta_unfairness` in an additive model, is less correlated with the scaled value term than previous attempts.

Design the model using the additive structure `U_accept = beta_value * split_perc_self - beta_unfairness * RatioBasedPenalty`, where `RatioBasedPenalty` is calculated as specified above.

Focus on clearly defining the `RatioBasedPenalty` and articulating in the summary how its specific functional form (positive deviation divided by the offered percentage) within an additive utility structure is intended to help disentangle the influences of `beta_value` (scaling the linear value) and `beta_unfairness` (scaling this non-linear ratio penalty), thereby significantly improving robust parameter identifiability for *both* learnable parameters. Explain why this approach represents a departure from previous additive models and the recent ratio models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `deviation`, `positive_deviation`, `safe_split_perc_self`, `ratio_penalty`, and `U_accept`. Include any other necessary intermediate variables.
*   **Model Formula:** Define `U_accept` using the additive structure: `U_accept = beta_value * split_perc_self - beta_unfairness * ratio_penalty`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Ensure you handle potential division by zero explicitly with the small epsilon (e.g., 1e-6).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` and `beta_unfairness`. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and may aid recovery, but consider if allowing negative values for `beta_value` (risk-seeking for unfair offers) or `beta_unfairness` (liking unfairness) is theoretically plausible in this context. Let's stick with non-negative bounds for now unless the model strongly suggests otherwise.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, `deviation`, `positive_deviation`, `safe_split_perc_self`, and `ratio_penalty`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (additive structure, value term, ratio-based penalty term). Crucially, explain *how the definition of the penalty term as the positive deviation from fair share percentage divided by the offered percentage* within the additive structure is specifically intended to improve robust parameter identifiability for *both* learnable parameters by creating mathematically distinct non-linear influences for `beta_unfairness` compared to the linear influence of `beta_value`. Explain how this builds on previous attempts by exploring a new combination of additive structure and a distinctly shaped penalty term.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative in how you define and utilize the ratio-based penalty term or alternative interactions within an additive structure to maximize this distinction while remaining theoretically plausible. Find a model structure that truly shines in parameter recovery.",,,,,,,,,,,,0.09798684127619112,,,,,,0.22272139457293694,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11,481.0838989642641,425.22727965334553,531.1221204302954,0.6996336996336996,"utility = alpha * split_perc_self + beta * max(0, fair_perc_self - split_perc_self) + gamma * max(0, split_perc_self - fair_perc_self)","A piecewise linear utility model where the utility of an offer percentage is determined by a base sensitivity (alpha) plus additional sensitivities (beta, gamma) to the deviation from the calculated fair percentage. Beta penalizes amounts below fair, and gamma scales amounts above fair. This structure creates distinct linear segments for utility below and above the fair share, aiming to improve parameter identifiability.",v0,"Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Achieve excellent parameter recovery (>= 0.7 correlation) for ALL learnable parameters.** This remains the *single most important* objective. Previous attempts have consistently failed here. A model is unusable if any learnable parameter cannot be reliably recovered, regardless of other metrics (like BIC or accuracy). Prioritize model structures that inherently enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables available during responder trials (trial_role == 1).
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

**Analysis of Previous Attempt (Run 10):** The previous attempt implemented the multiplicative model `U = split_perc_self * (beta_value - gamma_proportional_aversion * max(0, fair_perc_self - split_perc_self))`. This model, despite having only two parameters and attempting a different structural form (multiplicative instead of additive penalties), **failed parameter recovery catastrophically** (`beta_value: r = 0.116`, `gamma_proportional_aversion: r = 0.175`). Its BIC (747.09) and accuracy (0.553) were also poor compared to the best performing models so far (e.g., Run 3 with BIC 76.39, accuracy 0.821, and recovery closer to 0.7). This strong failure suggests that this specific multiplicative structure did *not* achieve the desired parameter separability, possibly because the effects of `beta_value` and `gamma_proportional_aversion` on the utility curve were too highly correlated or substitutable given the data distribution. The failure to recover parameters renders the model unusable.

**Guidance for the Next Model (Radically Prioritizing Parameter Recovery - Continued):** We have now attempted several model structures (additive penalties, quadratic penalties, non-linear exponents, multiplicative interaction), and *all* have failed to meet the critical parameter recovery threshold of 0.7 for all learnable parameters. The most successful model in terms of recovery (Run 3, `beta_gain * split_perc_self - gamma_unfairness * max(0, fair_perc_self - split_perc_self)`) was a simple additive model with *only one* linear penalty term based *only* on the proportional fair share, using only two parameters.

The consistent failure of more complex or alternative structures highlights that parameter identifiability in this dataset is extremely challenging. We *must* simplify and focus parameters on capturing distinct influences.

For Run 11, let's rethink the model structure entirely, focusing on making parameters influence the utility function in maximally separable ways.

Consider these strategies, informed by the data characteristics and previous results:

*   **Revisit Parsimony (Run 3's success):** Run 3, despite not reaching the 0.7 threshold, had the best recovery and fit with only two parameters. Its structure involved a base sensitivity to the offer and a linear penalty *specifically tied to deviation below the proportional fair share*. Can we build on this simplicity or find a minimal variation that pushes recovery above 0.7? What if the penalty was only active when the offer is below *both* 50% AND the proportional share? Or only below the proportional share, but with a slightly different form?
*   **Piecewise Sensitivity:** Instead of additive penalties, define the utility function's *sensitivity* to `split_perc_self` differently depending on where `split_perc_self` falls relative to key thresholds (like `fair_perc_self`). For instance, one parameter could control the utility slope when the offer is equal to or above `fair_perc_self`, and another parameter could control the slope when the offer is below `fair_perc_self`. This means parameters define the function's rate of change in distinct regions, which might be more identifiable than scaling penalty terms.
    *   Example idea: `U = beta_fair * split_perc_self` when `split_perc_self >= fair_perc_self`, and `U = beta_unfair * split_perc_self + constant` (or a different form) when `split_perc_self < fair_perc_self`. The parameters `beta_fair` and `beta_unfair` control the slope in different offer ranges, making their effects potentially more separable.
*   **Novel Interactions:** Can you devise a novel way for parameters to interact with `split_perc_self` and `fair_perc_self` that hasn't been tried? Avoid simple additive penalties like in Run 6, 4, 9 and the failed multiplicative structure from Run 10. Think about how parameters could control thresholds, scaling factors *within* different regions, or the shape of the utility curve in a fundamentally different way.

For Run 11, propose a model structure that is distinctly different from simple additive penalties and the failed multiplicative structure. Focus on creating parameters whose effects are clearly distinguishable based on the data distribution of offers relative to fairness norms. **Prioritize structural forms that you believe inherently lead to lower parameter covariance and higher identifiability.**

Define clear, generous finite bounds for all learnable parameters. They should likely be non-negative if they represent sensitivities or aversion strengths.

Continue to build the model using percentage-based values (`split_perc_self`, `fair_perc_self`).

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1) and use variables available in that context. The `fair_perc_self` variable (calculated as `(token_self / (token_self + token_opp)) * 100`) is useful for fairness calculations.

For run 11 of 100, please think through this step by step, focusing intently on designing a model structure that maximizes parameter identifiability given the data and the history of failed recovery attempts. Explore structural forms beyond simple additive penalties or the specific multiplicative form from Run 10. How can parameters capture the influence of the offer and fairness norms in distinct, separable ways? Then provide your model specification, variable descriptions, target variable, and summary.
</think>",,,,,,,,,,,,,0.7976545124296989,,,,,,,,0.3992461437638667,,,,0.06348435930022495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
56,503.1268151971338,463.4001202394993,538.715312763348,0.6877289377289377,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)
epsilon = 1e-6
fair_perc_self_calc_safe = fair_perc_self_calc + epsilon
R_val = split_perc_self / fair_perc_self_calc_safe
U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)","The model calculates the utility of accepting an offer as a linear combination of two terms: the proposed percentage for the participant (`split_perc_self`) and the logarithm of the ratio between the proposed percentage and the participant's calculated fair percentage (`R_val`). The term scaling `split_perc_self` captures sensitivity to absolute value (scaled by `beta_value`), while the term scaling the logarithm of the ratio captures sensitivity to relative fairness/unfairness (scaled by `beta_relative_fairness`). This structure aims to ensure identifiability of the two learnable parameters by having them modulate distinct mathematical forms representing absolute and relative value aspects of the offer. An epsilon is added to the ratio's denominator and log input to handle edge cases like zero fair percentage or zero proposed percentage.",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = same as above but in £

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able
to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,\n        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like \'lambda\' and use alternatives like \'learning_rate\' or \'lambda_param\'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.


Instructions: Okay, here are the improved instructions for the next run (Run 55), building on the results of the previous runs.

Instructions for Run 55 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 54) implemented a piecewise non-linear model based on fairness difference (`U = (perc_difference >= 0) ? beta_gain_sensitivity * sqrt(perc_difference + epsilon) : beta_loss_sensitivity * (fair_perc_self_calc - split_perc_self + epsilon)^2`). It unfortunately failed the parameter recovery test for *both* `beta_gain_sensitivity` (r = 0.332) and `beta_loss_sensitivity` (r = 0.596). Its overall performance (BIC 449.36, Acc 0.697) was also relatively poor.

Analysis of Run 54's failure and previous models: The piecewise structure in Run 54 assigned the two learnable parameters to scale distinct non-linear functions (`sqrt` vs. `square`) of the fairness deviation in gain vs. loss regimes. However, this approach, similar to previous attempts that scaled linear terms (Run 4) or combined linear terms (Run 1, 3), did not create sufficiently distinct influences for the parameters to be reliably estimated. We've observed a pattern where parameters scaling the absolute value of the offer or gain component (`beta_value` in Runs 1 & 3) sometimes achieve decent recovery, but parameters specifically scaling the *fairness deviation* or *loss penalty* (`beta_fairness_deviation`, `beta_unfairness_penalty`, `beta_loss_sensitivity`) consistently fail to reach the 0.7 threshold. This suggests the influence of the fairness deviation/penalty term is either too correlated with the absolute value term, or its impact is not captured in a mathematically separable way by simply scaling it.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** such that *both* parameters have truly distinct and identifiable influences on responder decisions (`trial_role == 1`). We need to find a way for the second parameter (the fairness/penalty related one) to exert its influence in a manner that is mathematically orthogonal or highly distinct from the first parameter's influence (the value/gain related one).

Based on this analysis, focus your exploration for Run 55 on model structures where the two learnable parameters contribute to utility in fundamentally different mathematical ways. Simply scaling different terms (linear or non-linear) within or across regimes hasn't consistently worked. Consider structures where one parameter influences the sensitivity to the *absolute* value of the offer (`split_perc_self`), while the other parameter influences sensitivity to the *relative* fairness (e.g., the ratio `R_val = split_perc_self / fair_perc_self_calc`) via a non-linear transformation.

Consider this strategy to enhance identifiability:

*   **Combine Linear Value with Non-Linear Relative Fairness:** Design a model where one parameter scales the proposed percentage itself (`split_perc_self`), capturing a general sensitivity to the absolute value being offered, and the second parameter scales a non-linear function of the *ratio* of the proposed percentage to the calculated fair percentage (`R_val`). The non-linear function of the ratio should capture the specific impact of relative fairness/unfairness.
    *   *Structural Principle:* A potentially promising structure is `U = beta_value * split_perc_self + beta_relative_fairness * f(R_val)`, where `f` is a non-linear function like `log`, `sqrt`, or a power function. This allows `beta_value` to influence the baseline utility based on the offer size, while `beta_relative_fairness` modulates this based on how fair the offer is, via a distinct mathematical operation on the ratio.
    *   *Example Idea (building on the ratio concept):* Consider using the logarithm of the ratio. `log(R_val)` is 0 when the offer is fair (R_val=1), positive for favorable offers (R_val>1), and negative for unfavorable offers (R_val<1), with decreasing marginal impact as R_val increases.
        *   `U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)`
        *   Where `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensure this handles `sum_tokens=0` correctly).
        *   `R_val = IF(fair_perc_self_calc > 0, split_perc_self / fair_perc_self_calc, IF(split_perc_self > 0, 100.0, epsilon))` (Handle `fair_perc_self_calc = 0`. If fair is 0 and split is positive, treat as a large ratio (e.g., 100.0). If both are 0, treat R_val as close to 0 to get a large negative log).
        *   `epsilon = 1e-6` (Small constant for log input).

*   **Explore Other Structures:** If the above doesn't work, continue to think creatively about how two parameters can control mathematically separable aspects of the utility function. Could one parameter control a threshold or an exponent? Could they define the shape of a smooth function (e.g., sigmoid, power law) that takes fairness deviation as input? Avoid structures that simply scale linearly dependent terms.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and necessary intermediate terms like `R_val` and `epsilon`. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-50.0, 50.0] or wider/narrower if appropriate). Consider the expected sign and scale, but keep bounds wide enough.

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, intermediate terms like `R_val` and `epsilon`, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` explicitly mentions the `sum_tokens > 0` condition and the default value.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the proposed percentage share for self.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain how its specific mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters (e.g., by having them scale distinct mathematical forms like linear value and non-linear relative fairness).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. Previous attempts scaling different terms (linear or non-linear) within or across regimes have struggled, particularly with the fairness/penalty parameter. Explore combining a term based on absolute value with a term based on a non-linear transformation of relative fairness (like the log of the ratio), or other structures that create highly distinct roles for the two parameters. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation.Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = same as above but in £

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able
to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\nLet me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like \'lambda\' and use alternatives like \'learning_rate\' or \'lambda_param\'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

<think>
The user wants improved instructions for Run 55.
Primary goal: Parameter Recovery >= 0.7 for *all* 2 learnable parameters.
Secondary goals: Lower BIC, higher Accuracy.
Stateless LLM requires full context (task, variables, previous run analysis, objective, output format).

Previous run (Run 54) details:
- Model: Piecewise non-linear (`sqrt` for gain, `square` for loss), scaling the transformed fairness difference.
- Results: BIC 449.36, Acc 0.697, Rec beta_gain_sensitivity=0.332, beta_loss_sensitivity=0.596.
- Failure: Poor recovery for *both* parameters.

Analysis of previous runs provided:
- Run 1: Linear combination of `split_perc_self` and `fairness_deviation`. Rec: beta_value=0.718, beta_fairness_deviation=0.653. (Failed recovery on fairness param)
- Run 2: Linear combo of `split_perc_self` and squared fairness penalty. Rec: beta_value=0.359, beta_fairness_penalty=0.630. (Failed recovery on both)
- Run 3: Piecewise linear penalty below fair. Rec: beta_value=0.768, beta_unfairness_penalty=0.561. (Failed recovery on penalty param)
- Run 4: Piecewise linear, scaling `split_perc_self` vs `abs_perc_difference`. Rec: beta_gain_scale=0.058, beta_loss_penalty=0.302. (Failed recovery on both, very low)
- Run 5 (Run 54): Piecewise non-linear (`sqrt` vs `square`). Rec: beta_gain_sensitivity=0.332, beta_loss_sensitivity=0.596. (Failed recovery on both)

Pattern: Models often struggle to recover the parameter associated with fairness deviation or penalty terms, even when the parameter scaling the absolute value/gain is recovered well (Runs 1, 3). Simply scaling different terms (linear or non-linear) in piecewise models (Runs 4, 5) or combining them linearly (Runs 1, 2) hasn't consistently yielded distinct parameter influences for *both*.

New Strategy Idea: Combine a linear term for the absolute value (`split_perc_self`) with a *non-linear* term for *relative* fairness (ratio `R_val`). This attempts to create two dimensions of influence: absolute value sensitivity and non-linear relative fairness sensitivity.
Proposed structure: `U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)`
Need to define `R_val` and handle the `fair_perc_self_calc = 0` case.
`R_val = split_perc_self / fair_perc_self_calc` if `fair_perc_self_calc > 0`.
If `fair_perc_self_calc == 0` (participant contributed 0 tokens), the fair share is 0. Any offer > 0 is infinitely fair in ratio terms, but absolute value still matters. Setting `R_val` to a large constant (e.g., 100.0) if `split_perc_self > 0` and fair is 0 seems like a reasonable heuristic for a very high relative gain. If both are 0, `R_val` should be handled to produce a large negative log utility (representing getting nothing when expecting nothing, maybe very low utility). Let's define `R_val = IF(fair_perc_self_calc > 0, split_perc_self / (fair_perc_self_calc + epsilon), IF(split_perc_self > 0, 100.0, epsilon))` where epsilon is small. This avoids division by zero and log of zero. Using `(fair_perc_self_calc + epsilon)` in the denominator is slightly safer mathematically than checking `>0` then dividing. Let's refine:
`fair_perc_self_calc_safe = fair_perc_self_calc + epsilon` (add epsilon always to denominator)
`R_val = split_perc_self / fair_perc_self_calc_safe`
`U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)`
This looks cleaner. Need to define `fair_perc_self_calc_safe` and `epsilon` as calculated variables.

The instructions should:
- State run number (55).
- Restate primary objective (rec >= 0.7 for *both*).
- Summarize Run 54 results and failure (poor rec on both, using piecewise non-linear).
- Analyze pattern of failure across previous runs (fairness param recovery consistently failing).
- Propose the new strategy: combine linear `split_perc_self` with non-linear `log(R_val)`. Explain *why* this might help identifiability (absolute value vs. log-relative value).
- Provide the specific model structure proposed.
- Remind about 2 params, bounds, variable descriptions (all variables used!), target variable, summary.
- Encourage out-of-the-box thinking beyond the specific example, but stress the need for mathematically separable parameter influences.
- Include full task/variable context.

Make sure to update the Run number in the instructions title and body.
Ensure variable descriptions cover all variables in the proposed model structure, including calculated intermediates and constants like epsilon.
Use clear Python-compatible names for parameters.

Model structure to propose:
```
sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)
epsilon = 1e-6
fair_perc_self_calc_safe = fair_perc_self_calc + epsilon
R_val = split_perc_self / fair_perc_self_calc_safe
U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)
```
Variable descriptions needed for: `token_self`, `token_opp`, `split_perc_self` (data); `sum_tokens`, `fair_perc_self_calc`, `epsilon`, `fair_perc_self_calc_safe`, `R_val`, `U` (calculated); `beta_value`, `beta_relative_fairness` (learnable).

Check variable sources and learnable flags. Range for epsilon should be fixed min/max. Range for U should be -inf/inf. Bounds for learnable parameters should be finite and generous.

This new structure combines absolute value and relative fairness in a non-additive, non-piecewise way (within the second term). This might provide the necessary mathematical separation.
Okay, here are the improved instructions for the next run (Run 55), building on the results of the previous runs.

Instructions for Run 55 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 54) implemented a piecewise non-linear model based on fairness difference (`U = (perc_difference >= 0) ? beta_gain_sensitivity * sqrt(perc_difference + epsilon) : beta_loss_sensitivity * (fair_perc_self_calc - split_perc_self + epsilon)^2`). It unfortunately failed the parameter recovery test for *both* `beta_gain_sensitivity` (r = 0.332) and `beta_loss_sensitivity` (r = 0.596). Its overall performance (BIC 449.36, Acc 0.697) was also relatively poor compared to the better-fitting models.

Analysis of Run 54's failure and previous models: The piecewise structure in Run 54 assigned the two learnable parameters to scale distinct non-linear functions (`sqrt` vs. `square`) of the fairness deviation in gain vs. loss regimes. However, this approach, similar to previous attempts that scaled linear terms (Run 4) or combined linear terms (Run 1, 3), did not create sufficiently distinct influences for the parameters to be reliably estimated. We've observed a pattern where parameters scaling the absolute value of the offer or gain component (`beta_value` in Runs 1 & 3) sometimes achieve decent recovery, but parameters specifically scaling the *fairness deviation* or *loss penalty* (`beta_fairness_deviation`, `beta_unfairness_penalty`, `beta_loss_sensitivity`) consistently fail to reach the 0.7 threshold. This strongly suggests the influence of the fairness deviation/penalty term is either too correlated with the absolute value term across trials, or its impact is not captured in a mathematically separable way by simply scaling a function of the deviation.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** such that *both* parameters have truly distinct and identifiable influences on responder decisions (`trial_role == 1`). We need to find a way for the second parameter (the fairness/penalty related one) to exert its influence in a manner that is mathematically orthogonal or highly distinct from the first parameter's influence (the value/gain related one).

Based on this analysis, focus your exploration for Run 55 on model structures where the two learnable parameters contribute to utility in fundamentally different mathematical ways, beyond simply scaling different terms additively or piecewise. Consider structures where one parameter influences the sensitivity to the *absolute* value of the offer (`split_perc_self`), while the other parameter influences sensitivity to the *relative* fairness (e.g., the ratio `R_val = split_perc_self / fair_perc_self_calc`) via a non-linear transformation.

Consider this strategy to enhance identifiability:

*   **Combine Linear Value with Non-Linear Relative Fairness:** Design a model where one parameter scales the proposed percentage itself (`split_perc_self`), capturing a general sensitivity to the absolute value being offered, and the second parameter scales a non-linear function of the *ratio* of the proposed percentage to the calculated fair percentage (`R_val`). The non-linear function of the ratio should capture the specific impact of relative fairness/unfairness. This approach aims for parameter identifiability by having parameters scale terms that represent different dimensions (absolute vs. relative) via different mathematical operations (linear vs. non-linear transformation).
    *   *Structural Principle:* A potentially promising structure is `U = beta_value * split_perc_self + beta_relative_fairness * f(R_val)`, where `f` is a non-linear function like `log`, `sqrt`, or a power function. This allows `beta_value` to influence the baseline utility based on the offer size, while `beta_relative_fairness` modulates this based on how fair the offer is, via a distinct mathematical operation on the ratio.
    *   *Example Idea (using the logarithm of the ratio, addressing edge cases):* The logarithm of the ratio `log(R_val)` is 0 when the offer is fair (R_val=1), positive for favorable offers (R_val>1), and negative for unfavorable offers (R_val<1), offering a non-linear representation of relative value.
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensure this handles `sum_tokens=0`).
        *   `epsilon = 1e-6` (Small constant to handle division by zero and log of zero/negative).
        *   `fair_perc_self_calc_safe = fair_perc_self_calc + epsilon` (Add epsilon to denominator to prevent division by zero when fair_perc_self_calc is 0).
        *   `R_val = split_perc_self / fair_perc_self_calc_safe` (Calculate the ratio using the safe fair value).
        *   `U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)` (Combine linear split percentage with log-transformed ratio).

*   **Explore Other Structures:** If the above specific example doesn't seem optimal, continue to think creatively about how two parameters can control mathematically separable aspects of the utility function. Could one parameter control a threshold or an exponent in a non-trivial way? Could they define the shape of a smooth function (e.g., sigmoid, power law) that takes fairness deviation as input, where the parameters affect different parameters of the function (e.g., slope and inflection point)? Avoid structures that simply scale linearly dependent terms or rely solely on simple piecewise linear or piecewise scaled non-linear functions, as these have failed recovery for one or both parameters previously.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and necessary intermediate terms like `R_val`, `epsilon`, `fair_perc_self_calc_safe`. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-50.0, 50.0] or wider/narrower if appropriate). Consider the expected sign and scale, but keep bounds wide enough.

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, intermediate terms like `epsilon`, `fair_perc_self_calc_safe`, `R_val`, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` explicitly mentions the `sum_tokens > 0` condition and the default value.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the participant's proposed percentage share percentage.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain how its specific mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters (e.g., by having them scale distinct mathematical forms like linear value and non-linear relative fairness).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. Previous attempts scaling different terms (linear or non-linear) within or across regimes have struggled, particularly with the fairness/penalty parameter. Explore combining a term based on absolute value with a term based on a non-linear transformation of relative fairness (like the log of the ratio), or other structures that create highly distinct roles for the two parameters. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation.",,,,,,,,,,,,,,,,,,0.7026160562410235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.046304499478996855,,,,,,,,,,,,,,,,,,
66,527.8896279440379,378.5042321805769,661.7140449821384,0.6562881562881563,"sum_tokens = token_self + token_opp
fair_perc_self_calc = 50.0 if sum_tokens == 0 else (token_self / sum_tokens) * 100.0
safe_split_perc_self = max(1e-6, split_perc_self)
ratio_penalty_term = max(0.0, (fair_perc_self_calc / safe_split_perc_self) - 1.0)
U = beta_value * split_perc_self - beta_unfairness * ratio_penalty_term","An additive utility model for responder decisions based on the offered percentage share and a penalty for relative unfairness. Utility is calculated as a linear combination of the proposed percentage share (`split_perc_self`), scaled by `beta_value`, and a ratio-based unfairness penalty term, scaled by `beta_unfairness`. The unfairness penalty is derived from the ratio of the calculated fair share (based on token contributions) to the proposed share, applying only when the offer is less than fair. This ratio-based penalty term introduces a non-linear (inverse) relationship with the proposed share, distinct from the linear relationship of the value term. This difference in mathematical influence is specifically designed to enhance the identifiability of the two learnable parameters, `beta_value` and `beta_unfairness`, allowing for more robust parameter recovery compared to models using a simple difference-based penalty term. Both learnable parameters are constrained to be non-negative within generous finite bounds [0.0, 100.0].",v0,"Okay, here are the improved instructions for the next run, focusing intensely on designing a model structure that ensures parameter identifiability for *both* learnable parameters while building on the successes and addressing the failures of the previous attempts.

Instructions for Run 66 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

Run 65 explored a power-law multiplicative decay structure: `U = beta_value * split_perc_self * (penalty_factor)^beta_unfairness`, where `penalty_factor` was based on the simple unfairness difference. This attempt was unfortunately unsuccessful, yielding poor parameter recovery for *both* `beta_value` (r = -0.086) and `beta_unfairness` (r = -0.179), alongside worse BIC (57.07) and accuracy (0.495) compared to previous models. This indicates that placing `beta_unfairness` directly in the exponent of a penalty factor derived from the simple difference did not create the necessary distinct mathematical influence and likely introduced new identifiability or modeling issues, potentially related to how the penalty factor reaches its bounds or how the exponent interacts with the value term.

Previous attempts have shown that linear additive models struggle with the unfairness parameter (Run 61: cubic additive, `beta_unfairness` r = -0.172) and ratio models applying a penalty based on the *difference* also haven't consistently reached the 0.7 threshold for the unfairness parameter (Run 63: linear ratio, `beta_unfairness` r = 0.608; Run 64: quadratic ratio, `beta_unfairness` r = 0.451). While Run 63's linear ratio model achieved the best `beta_unfairness` recovery so far (0.608), it was still just shy of the target, suggesting the simple difference-based penalty term (`max(0.0, fair_perc_self_calc - split_perc_self)`) might be too correlated with the value term (`split_perc_self`) to allow perfect separation of the two parameters' influences in these structures.

For Run 66, let's try a different approach to defining the *unfairness penalty term* itself. Instead of using the *absolute difference* between the fair share and the offered share, let's define the penalty based on the *ratio* of the fair share to the offered share. This captures *relative* unfairness – how much smaller the offer is *compared to* the fair share.

Consider an additive model structure similar to Run 61, but using this new ratio-based penalty term.
Proposed Structure Concept: `U = beta_value * split_perc_self - beta_unfairness * RatioPenaltyTerm`.
Here, `RatioPenaltyTerm` should be zero for fair or generous offers and increase as the offer becomes more unfair relative to the fair share. A possible calculation for `RatioPenaltyTerm` is `max(0.0, (fair_perc_self_calc / split_perc_self) - 1.0)`. This term represents the proportional difference between the fair share and the offered share, but only when the offer is less than fair. *Care must be taken to handle the case where `split_perc_self` is zero or very small, which would represent a maximally unfair offer*. Using `max(1e-6, split_perc_self)` in the denominator can prevent division errors and produce a very large penalty term for zero/very small offers, which is conceptually appropriate.

The goal is to design a utility calculation `U` involving `split_perc_self`, `fair_perc_self_calc` (derived from `token_self`, `token_opp`), and exactly two learnable parameters (`beta_value`, `beta_unfairness`). Design the model using an additive structure where `beta_value` scales the `split_perc_self` and `beta_unfairness` scales a *ratio-based* penalty term calculated as described above. Focus on clearly defining this new penalty term and articulating how its unique mathematical relationship with the input variables (compared to the simple difference penalty) is intended to help disentangle the influences of `beta_value` and `beta_unfairness` and improve parameter identifiability.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0` defaulting to 50.0), and a new `ratio_penalty_term` based on `max(0.0, (fair_perc_self_calc / max(1e-6, split_perc_self)) - 1.0)` or a similar ratio-based definition of relative unfairness. Include any other necessary intermediate variables.
*   **Model Formula:** Define `U` using an additive structure: `U = beta_value * split_perc_self - beta_unfairness * RatioPenaltyTerm`. Include calculation steps for derived variables *before* the formula for `U`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` and `beta_unfairness`. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Given the additive structure and penalty term definition, non-negative bounds for both parameters (e.g., `[0.0, 50.0]`, `[0.0, 100.0]`) are theoretically appropriate (higher `beta_value` increases utility for better offers, higher `beta_unfairness` increases penalty for unfairness) and may aid recovery.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including U, sum_tokens, fair_perc_self_calc, and your chosen ratio-based penalty term variable) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (additive structure, value and penalty components). Crucially, explain *how the definition of the penalty term based on the *ratio* of fair share to offered share (instead of the simple difference) creates a different mathematical relationship between the terms scaled by `beta_value` and `beta_unfairness`, and how this difference is specifically intended to improve robust parameter identifiability for *both* learnable parameters*, particularly for `beta_unfairness`, which has struggled in previous models. Explain why this ratio-based approach might be better at separating the influences than previous additive or ratio models using the simple difference penalty.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative in how you define and utilize the ratio-based penalty term to maximize this distinction.",,,,,,,,,,,,0.052138928191076295,,,,,,0.2930086751666915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
71,586.6696638186905,615.6067352671404,560.7468706461208,0.6501831501831502,"sum_tokens = token_self + token_opp
fair_perc_self_calc = 50.0 if sum_tokens == 0 else (token_self / sum_tokens) * 100.0
deviation = fair_perc_self_calc - split_perc_self
positive_deviation = max(0.0, deviation)
squared_positive_deviation = positive_deviation * positive_deviation
U_accept = beta_value * split_perc_self - beta_unfairness * squared_positive_deviation","An additive utility model for responder decisions. The utility of accepting an offer is calculated as a linear function of the participant's proposed percentage share, scaled by 'beta_value', minus a penalty term. This penalty is proportional to the squared positive deviation between the fair share percentage (based on token contributions) and the proposed share percentage, scaled by 'beta_unfairness'. The squared form of the penalty term creates a non-linear (quadratic) relationship with increasing deviation below the fair share, aiming to mathematically distinguish the influence of 'beta_unfairness' from the linear influence of 'beta_value' across different offers. This structure is hypothesized to improve robust parameter identifiability for both learnable parameters compared to previous additive or ratio models.",v0,"Okay, here are the improved instructions for the next run (Run 71), building on the analysis of previous attempts and maintaining the intense focus on achieving robust parameter identifiability for *both* learnable parameters.

Instructions for Run 71 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery.

The previous run (Run 69) implemented an additive model structure with a ratio-based unfairness penalty term (`U_accept = beta_value * split_perc_self - beta_unfairness * (positive_deviation / safe_split_perc_self)`). Despite the theoretical rationale that the non-linear shape of the ratio penalty would help disentangle parameter influences, the results showed very poor parameter recovery (beta_value: r=0.223, beta_unfairness: r=0.098). This confirms that merely introducing non-linearity in the penalty term within an additive structure is not sufficient for identifiability if the shape of the penalty's influence remains too correlated with the influence of the value term across the data range.

Crucially, previous explorations (like the ratio model tested in Run 68, `U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * NonLinearPenaltyTerm)`) showed that using a **squared positive difference** penalty (`max(0.0, fair_perc_self_calc - split_perc_self)^2`) resulted in a significantly better BIC (44.58) compared to Run 69's BIC (479.64). This suggests that the squared deviation penalty term might better capture the underlying structure of participant behavior related to unfairness, even though its identifiability was poor when placed in the denominator of a ratio model.

For Run 71, let's leverage the potential descriptive power of the squared deviation penalty term, but combine it with the simpler **additive utility structure**. The hypothesis is that using an additive structure (`U = ValueTerm - PenaltyTerm`) with a **linearly scaled value term** (`beta_value * split_perc_self`) and a **non-linearly scaled penalty term** based on the squared deviation from fairness (`beta_unfairness * (squared positive deviation)`) will create sufficiently distinct mathematical influences for `beta_value` and `beta_unfairness` across the range of possible offers, leading to robust parameter identifiability.

Proposed Model Structure: An additive model where utility is the value of the offer minus a penalty proportional to the *squared* deviation from fairness.
`U_accept = beta_value * split_perc_self - beta_unfairness * SquaredPositiveDeviation`

Here, `SquaredPositiveDeviation` is calculated as the square of the positive difference between the fair share percentage and the proposed share percentage. Specifically:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0).
Calculate `deviation = fair_perc_self_calc - split_perc_self`.
Define `positive_deviation = max(0.0, deviation)`. This ensures the penalty is only applied for offers below the fair share.
Define the `SquaredPositiveDeviation = positive_deviation^2`.
Finally, `U_accept = beta_value * split_perc_self - beta_unfairness * SquaredPositiveDeviation`.

The theoretical rationale for this specific combination is to create a clear mathematical contrast: `beta_value` scales a term that changes linearly with the offer (`split_perc_self`), while `beta_unfairness` scales a term that changes quadratically with the *deviation* from fairness (for offers below fair). This difference in how the two terms scale with the input variables (`split_perc_self` and `fair_perc_self_calc`) is intended to make their individual contributions to the overall utility function more distinguishable than in previous attempts, thereby improving the reliability of their estimated values during parameter recovery.

Design the model using the additive structure `U_accept = beta_value * split_perc_self - beta_unfairness * squared_positive_deviation`, where `squared_positive_deviation` is calculated as specified above.

Focus on clearly defining the `squared_positive_deviation` term and articulating in the summary how its specific functional form (the square of the positive deviation from fair share) combined with the additive structure and the linear value term is intended to help disentangle the influences of `beta_value` (scaling the linear value) and `beta_unfairness` (scaling this non-linear, squared penalty), thereby significantly improving robust parameter identifiability for *both* learnable parameters. Explain why this specific combination is being tested based on the learnings from Run 69's additive structure failure and Run 68's better BIC with the squared penalty.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `deviation`, `positive_deviation`, `squared_positive_deviation`, and `U_accept`. Include any other necessary intermediate variables.
*   **Model Formula:** Define `U_accept` using the additive structure: `U_accept = beta_value * split_perc_self - beta_unfairness * squared_positive_deviation`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` and `beta_unfairness`. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and may aid recovery. Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, `deviation`, `positive_deviation`, and `squared_positive_deviation`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (additive structure, linear value term, squared positive deviation penalty term). Crucially, explain *how the squared form of the penalty term* within the additive structure is specifically intended to improve robust parameter identifiability for *both* learnable parameters by creating a mathematically distinct, non-linear influence profile for `beta_unfairness` compared to the linear influence of `beta_value`. Explain how this new combination is a targeted attempt to overcome the identifiability failures of previous additive and ratio models, building on prior observations about the BIC performance of the squared penalty.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative in how you define and utilize the different functional forms and structures to maximize this distinction while remaining theoretically plausible. Find a model structure that truly shines in parameter recovery.",,,,,,,,,,,,0.25603326705006835,,,,,,0.16717542675032274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,592.1108055006791,686.3883600011114,507.65382959404195,0.6358363858363859,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = split_perc_self * (beta_gain - beta_unfairness * loss_below_fair)","A utility model for responder acceptance decisions. Utility for accepting is based on the proposed percentage share for the participant, scaled by a base gain parameter (beta_gain). This sensitivity is reduced on unfair trials (where the proposed share is below the fair share based on token contributions) by a penalty term. This penalty term is calculated as the degree of unfairness (loss_below_fair) scaled by a parameter (beta_unfairness), and crucially, this scaled penalty is multiplied by the proposed percentage share itself (split_perc_self). This multiplicative interaction (split_perc_self * beta_unfairness * loss_below_fair) is designed to create distinct influence profiles for beta_gain and beta_unfairness across trials, where beta_gain influences utility proportionally to split_perc_self on all trials, while beta_unfairness's influence is limited to unfair trials and weighted by both the unfairness magnitude and the proposed percentage, thus promoting robust joint parameter identifiability.",v0,"Okay, here are the improved instructions for the next run (Run 91 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,
      ""source"": ""calculated""
    },\\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,\\\
      ""source"": ""data""
    },\\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1,\
        ""exclusive_max"": true
      },\\\
      ""learnable"": false,
      ""source"": ""data""
    },\\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {\
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\\
      ""learnable"": false,
      ""source"": ""calculated""
    }\
  }\
}\
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 91 of 100).

Previous attempts, including the non-linear additive structure from the last run (`U_accept = beta_gain * split_perc_self - beta_unfairness * (Exp(loss_below_fair) - 1.0)`), have continued to fail robust parameter identifiability (parameter recovery >= 0.7 for *all* learnable parameters simultaneously). The last run's recovery was particularly poor (`beta_gain`: 0.543, `beta_unfairness`: 0.451) and resulted in a very high BIC (poor fit), suggesting the exponential penalty term (`Exp(loss_below_fair) - 1.0`) likely created numerical instability or an overly steep and problematic utility landscape. Structures involving extreme non-linearities like untransformed exponentials on potentially large inputs (`loss_below_fair` can be up to 100) should be avoided moving forward.

Robust parameter identifiability (recovery >= 0.7 for *all* learnable parameters) remains the **unwavering primary objective**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation and overrides achieving a low BIC or high accuracy if recovery fails. We need to find a structure where the influence of the two learnable parameters is sufficiently distinct across trials to be estimated independently with high reliability.

Given the persistent identifiability challenges with previous linear, additive, multiplicative, power-law, log-transformed, and steep non-linear approaches, we must explore a **fundamentally different structural principle** for this run. The goal is to create a model where the value of the offer (`split_perc_self`) contributes to utility, but this contribution is *systematically modified* or *penalized* when the offer is unfair, and this modification is scaled by the unfairness parameter (`beta_unfairness`). Crucially, this modification should **interact multiplicatively** with the `split_perc_self` term itself.

Here are the standard calculation steps for necessary intermediate variables. You *must* include these first before defining `U_accept`:
1. Calculate `sum_tokens = token_self + token_opp`.
2. Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.
3. Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is positive if the offer is below the fair share, and 0 otherwise. Use `Max` for this calculation.

Now, design a novel formula for `U_accept` using `split_perc_self`, `loss_below_fair`, and exactly two learnable parameters. The formula for `U_accept` must implement the principle of a *gain term* from `split_perc_self` whose sensitivity is *penalized* by an amount scaled by `beta_unfairness` only when `loss_below_fair > 0`, and this penalty term should *interact multiplicatively* with `split_perc_self`. Avoid adding a separate, independent penalty term like in previous failed additive models.

A suggested specific structure to explore for `U_accept` that follows this principle is:
`U_accept = beta_gain * split_perc_self - beta_unfairness * loss_below_fair * split_perc_self`
This can also be written as:
`U_accept = split_perc_self * (beta_gain - beta_unfairness * loss_below_fair)`
This structure represents the utility as a base sensitivity to the proposed percentage (`beta_gain`) that is reduced (`- beta_unfairness * loss_below_fair`) proportionally to the degree of unfairness *and* the proposed percentage itself. This creates a multiplicative interaction between the proposed value and the scaled unfairness penalty, which is structurally distinct from previous attempts.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds.
    *   Parameter 1 (e.g., `beta_gain`): Should represent the base sensitivity to the proposed percentage (`split_perc_self`). Non-negative bounds, e.g., `[0.0, 10.0]`.
    *   Parameter 2 (e.g., `beta_unfairness`): Should scale the interaction term that reduces sensitivity to the proposed percentage based on `loss_below_fair`. Non-negative bounds, e.g., `[0.0, 10.0]`.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is based on a sensitivity to the proposed percentage, but this sensitivity is penalized by an interaction term involving unfairness). **Crucially, explain *how* your specific, novel mathematical structure (the multiplicative interaction `loss_below_fair * split_perc_self` scaled by `beta_unfairness`) creates mathematically distinct and separable influence profiles for the two learnable parameters tied to the offer value and unfairness, and how this is intended to improve robust joint parameter identifiability compared to previous linear or simple additive/multiplicative structures.** Highlight that `beta_gain` influences the utility from `split_perc_self` across all trials, while `beta_unfairness` *only* influences the utility calculation on unfair trials (where `loss_below_fair > 0`) through this specific interaction term, and this influence is weighted by the magnitude of `split_perc_self`.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately. Avoid structures that have failed recovery previously, like simple linear combinations or independent non-linear additive penalties (especially steep ones like exponentials). Explore this specific multiplicative interaction structure as detailed above, focusing on how it creates distinct influence profiles for the two betas.",,,,,,,,,,,,0.2806396705102636,,-0.03576683831350315,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,595.9002386368456,486.38536548718014,694.0073125000877,0.6147741147741148,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
relative_fairness = split_perc_self / fair_perc_self_calc
U_accept = beta_value * split_perc_self + beta_unfairness * (relative_fairness - 1.0)","An additive utility model for responder behavior. Utility for accepting an offer is calculated as a linear combination of two components: the absolute percentage of the proposed share for the participant, scaled by beta_value, and the deviation of the proposed share from the calculated relative fairness (relative fairness ratio minus 1), scaled by beta_unfairness. This structure aims to improve parameter identifiability by allowing beta_value to capture sensitivity to the magnitude of gain and beta_unfairness to capture sensitivity to the offer's fairness relative to contribution, hypothesizing these influences are sufficiently distinct across trials for reliable joint estimation compared to simpler gain/penalty structures.",v0,"Okay, here are the improved instructions for the next run (Run 83 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,\\
      ""source"": ""data""
    },\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },\\
      ""learnable"": false,
      ""source"": ""data""
    },\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 83 of 100), based on the results of recent attempts.

The additive model with a logarithmic gain and linear penalty tested in the previous run (Run 82) yielded a high BIC (448.66) and poor parameter recovery for both learnable parameters (`beta_value` r=0.315, `beta_unfairness` r=0.431). This indicates that this specific additive structure, despite achieving decent accuracy, failed to adequately fit the data and did not resolve the persistent parameter identifiability issue for *both* parameters simultaneously. This reinforces the observation from previous attempts (both divisive and additive structures) that simply changing the non-linear function applied to 'gain' or 'penalty' within these standard structures has not achieved the required joint parameter recovery (>= 0.7 for all learnable parameters).

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation of individual differences and group effects (like the impact of cocaine). We must continue to explore structures where the influences of *both* `beta_value` and `beta_unfairness` are mathematically separable enough for reliable joint estimation.

Given the failure of previous attempts, including Run 82, we need to explore a model structure that introduces a *different kind of relationship* between the gain, the fairness perception, and the parameters than previously tested additive or simple divisive forms. Instead of one parameter scaling a gain term and the other scaling a penalty term (be it additive or divisive in the denominator), let's consider a structure where utility is based on the absolute offer value *and* the offer value relative to fairness, with parameters scaling these distinct components.

For Run 83, we will test an additive model where the utility is a combination of a term based on the *absolute proposed percentage* for the participant (`split_perc_self`) and a term based on the *relative fairness* of the offer (`split_perc_self` relative to `fair_perc_self_calc`).

Proposed Model Structure: An additive utility model where the utility of accepting (`U_accept`) is calculated as a linear function of the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, *plus* a linear function of the deviation from *relative fairness* (`relative_fairness - 1.0`), scaled by `beta_unfairness`.

Here are the calculation steps:
Calculate `sum_tokens = token_self + token_opp`.
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `relative_fairness = split_perc_self / fair_perc_self_calc`. This represents the proposed share as a ratio of the calculated fair share. This term will be > 1 for offers above fair, = 1 for fair offers, and < 1 for offers below fair. Note: Since `fair_perc_self_calc` defaults to 50.0 when `sum_tokens` is 0, and `token_self`, `token_opp` are non-negative, `fair_perc_self_calc` should always be >= 0, and only 0 if `token_self == 0` and `sum_tokens > 0` and `token_opp == 0`, which implies `sum_tokens == 0`, handled by the default. So `fair_perc_self_calc` will always be >= 0, and > 0 unless both token_self and token_opp are 0. However, the calculation explicitly handles `sum_tokens == 0` resulting in 50.0, so division by zero should not occur in practice assuming valid token counts. If `split_perc_self` is 0, `relative_fairness` will be 0.
Calculate the utility: `U_accept = beta_value * split_perc_self + beta_unfairness * (relative_fairness - 1.0)`.
In this structure, `beta_value` represents the sensitivity to the absolute percentage value of the proposed share, while `beta_unfairness` represents the sensitivity to how the proposed share compares *relatively* to the fair share (deviation from a ratio of 1). The term `(relative_fairness - 1.0)` is negative for unfair offers, zero for fair offers, and positive for offers above fair.

The hypothesis is that this specific additive combination, scaling a linear term based on the absolute offer percentage (`split_perc_self`) with `beta_value` and scaling a term based on the *relative* deviation from fairness (`relative_fairness - 1.0`) with `beta_unfairness`, will create mathematically distinct and separable influence profiles for the two parameters across different offers. This structural difference, moving away from simple gain/penalty formulations, is intended to improve robust parameter identifiability for *both* parameters, addressing the failures seen in previous, more correlated model structures, and hopefully leading to better fit metrics as a result.

Design the model using this new additive structure combining absolute linear gain and relative fairness deviation: `U_accept = beta_value * split_perc_self + beta_unfairness * (relative_fairness - 1.0)`. Remember to calculate intermediate variables (`sum_tokens`, `fair_perc_self_calc`, `relative_fairness`) before the final `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Use `IF` for the conditional calculation of `fair_perc_self_calc`.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific additive structure, by combining a term scaled by `beta_value` based on the absolute percentage with a term scaled by `beta_unfairness` based on the relative deviation from fairness, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `relative_fairness`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (sensitivity to absolute percentage gain) and `beta_unfairness` (sensitivity to relative fairness deviation). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are appropriate. `beta_value` should be non-negative for increasing utility with percentage; `beta_unfairness` non-negative ensures higher relative fairness leads to higher utility. Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `relative_fairness`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (additive structure combining absolute percentage gain and relative fairness deviation). Crucially, explain *how* this specific additive structure, combining a term scaled by `beta_value` based on `split_perc_self` and a term scaled by `beta_unfairness` based on `relative_fairness - 1.0`, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous models.
*   **Think Outside the Box:** Parameter identifiability for *both* parameters simultaneously remains the persistent, difficult challenge. Neither the divisive structures nor the simple additive gain/penalty variations tested have solved this. This new additive structure combining absolute percentage and relative fairness deviation is another attempt to find mathematically separable influences. If this also fails to achieve >= 0.7 recovery for *both* parameters, the next iteration *must* consider even more fundamentally different structural approaches. This could involve models where parameters influence thresholding, decision-making rules (e.g., comparing utility difference to zero), or alternative ways of conceptualizing fairness beyond a single 'loss' or 'ratio' term being linearly or non-linearly scaled. Continue to focus intently on finding a functional form where varying `beta_value` has a clearly distinguishable effect on the utility curve across offers compared to varying `beta_unfairness`, allowing them to be estimated independently with high reliability, which is a prerequisite for better fit metrics.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,0.04752196043530044,,,,,,-0.042833666318579273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
72,600.8138221871161,761.8700043823703,456.53432563720077,0.5326617826617827,"sum_tokens = token_self + token_opp
fair_perc_self_calc = case when sum_tokens > 0 then (token_self / sum_tokens) * 100.0 else 50.0 end
gain_above_fair = max(0.0, split_perc_self - fair_perc_self_calc)
loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_value * gain_above_fair - beta_unfairness * loss_below_fair","A reference-dependent additive utility model for the responder's decision. The model calculates a 'fair share percentage' based on the relative token contributions of the participant and opponent, using this as a reference point. The utility of accepting an offer is then calculated as a linear combination of the deviation above the fair share (gain) and the deviation below the fair share (loss). The model includes two learnable parameters: 'beta_value' scaling the utility derived from gains (offers above fair) and 'beta_unfairness' scaling the disutility derived from losses (offers below fair). This structure is designed to improve parameter identifiability by assigning distinct sensitivities to outcomes in different domains (gains vs. losses) relative to the fairness reference, thereby creating more separable influences of the parameters on the overall utility profile across different types of offers compared to simpler additive models.",v0,"Okay, here are the improved instructions for the next run (Run 72), based on the results of previous attempts, with the absolute focus remaining on achieving robust parameter identifiability.

Instructions for Run 72 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery.

The previous run (Run 71) implemented an additive model structure with a squared positive deviation penalty (`U_accept = beta_value * split_perc_self - beta_unfairness * squared_positive_deviation`). Despite the theoretical rationale that the squared penalty would create a distinct non-linear influence compared to the linear value term, the results showed *very poor* parameter recovery for both parameters (beta_value: r=0.167, beta_unfairness: r=0.256). This indicates that this specific additive structure, even with a non-linear penalty form, did not sufficiently disentangle the influences of the parameters. Run 71 also showed poor BIC (586.67) and accuracy (0.650) compared to some earlier attempts, further confirming this structure was not effective.

Previous attempts using simple additive structures or ratio structures with various penalty terms (including the squared deviation penalty which performed better on BIC in a ratio model in Run 68, though still with poor recovery) have consistently failed to achieve robust parameter identifiability. This suggests that these common model structures may inherently lead to confounding between a 'value' parameter and an 'unfairness' parameter when both influence the overall utility calculation in a straightforward linear-additive or simple multiplicative/ratio manner.

For Run 72, we need a fundamentally different conceptual approach to model structure that is more likely to create mathematically distinct contributions for `beta_value` and `beta_unfairness`. Instead of viewing utility as `Value - Penalty`, let's explore a **reference-dependent framework**. The fair share percentage (`fair_perc_self_calc`) can naturally serve as a reference point. Utility could then be defined based on deviations *above* this reference (gains) and deviations *below* this reference (losses/unfairness), with potentially different sensitivities for these gains and losses.

This approach, inspired by concepts like Prospect Theory in behavioral economics, posits that the impact of an outcome depends on its relation to a reference point, and that losses relative to the reference point loom larger than equivalent gains. Applying this here, we can hypothesize that participants evaluate offers based on how much they deviate from their perceived fair share, with separate sensitivities for favorable deviations (above fair) and unfavorable deviations (below fair).

Proposed Model Structure: A reference-dependent additive model where utility is calculated based on deviations from the fair share percentage, with separate parameters for gains and losses relative to this reference.
`U_accept = beta_value * GainAboveFair - beta_unfairness * LossBelowFair`

Here:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0).
Calculate `GainAboveFair = max(0.0, split_perc_self - fair_perc_self_calc)`. This term is non-zero only for offers above or equal to the fair share.
Calculate `LossBelowFair = max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
Finally, `U_accept = beta_value * GainAboveFair - beta_unfairness * LossBelowFair`. Note the subtraction, representing the negative utility (disutility) from a loss.

In this structure, `beta_value` scales the utility derived from exceeding the fair share (Gain sensitivity), and `beta_unfairness` scales the *disutility* derived from falling short of the fair share (Loss/Unfairness sensitivity). The key hypothesis for improved identifiability is that `beta_value` primarily influences utility for offers *at or above* the fair share, while `beta_unfairness` primarily influences utility for offers *at or below* the fair share. Because these two ""domains"" (gains vs. losses relative to fair) are largely disjoint in terms of where the respective terms (`GainAboveFair` and `LossBelowFair`) are active, the influence of `beta_value` on the overall utility profile should be mathematically distinct from the influence of `beta_unfairness`, leading to better identifiability.

Design the model using this reference-dependent additive structure: `U_accept = beta_value * gain_above_fair - beta_unfairness * loss_below_fair`, where `gain_above_fair` and `loss_below_fair` are calculated as specified above using `fair_perc_self_calc` as the reference.

Focus on clearly defining the `gain_above_fair` and `loss_below_fair` terms and articulating in the summary how this specific structure, by assigning `beta_value` to scale 'gains' relative to fairness and `beta_unfairness` to scale 'losses' relative to fairness, is intended to achieve robust parameter identifiability. Explain why this reference-dependent approach is being tested as a way to overcome the identifiability failures seen in previous additive and ratio structures that used single penalty terms. This is a more ""out-of-the-box"" structure compared to previous attempts, aimed directly at the identifiability challenge.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `gain_above_fair`, `loss_below_fair`, and `U_accept`. Include any other necessary intermediate variables.
*   **Model Formula:** Define `U_accept` using the reference-dependent additive structure: `U_accept = beta_value * gain_above_fair - beta_unfairness * loss_below_fair`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (gain sensitivity) and `beta_unfairness` (loss/unfairness sensitivity). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and may aid recovery. Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, `gain_above_fair`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (reference-dependent additive structure, fair share as reference, separate gain/loss sensitivities). Crucially, explain *how* this structure, by having `beta_value` scale gains (above fair) and `beta_unfairness` scale losses (below fair), is specifically intended to improve robust parameter identifiability by creating mathematically distinct influences across different offers, addressing the limitations of previous simple additive/ratio structures.
*   **Think Outside the Box:** Recognize that previous attempts with standard additive/ratio forms and various penalty definitions have failed parameter recovery. This new model structure is a significant departure intended to directly tackle the identifiability problem by assigning parameters roles in distinct parts of the outcome space relative to a reference. Continue to consider non-obvious ways parameters might interact or influence decisions beyond simple linear combinations.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative in how you define and utilize the different functional forms and structures to maximize this distinction while remaining theoretically plausible. Find a model structure that truly shines in parameter recovery.",,,,,,,,,,,,0.01499033737564506,,,,,,0.13067568840131769,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90,602.0628289403517,630.7282456322769,576.3833931538354,0.6382783882783883,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_gain * split_perc_self - beta_unfairness * (Exp(loss_below_fair) - 1.0)","This model predicts the utility of accepting a proposed offer based on two components: a linear sensitivity to the percentage offered to the participant and a non-linear penalty applied when the offer is below a calculated fair share. The utility is a weighted sum where the gain from the proposed percentage is scaled by 'beta_gain', and an exponential function of the loss below fairness is scaled by 'beta_unfairness'. For offers at or above the fair share, only the gain component contributes to utility. This structure aims to improve parameter identifiability by giving 'beta_gain' influence across all offers proportional to the percentage received, while 'beta_unfairness' specifically and non-linearly affects only unfair offers based on the magnitude of the discrepancy from fairness, creating mathematically distinct response profiles for the two parameters.",v0,"Okay, here are the improved instructions for the next run (Run 90 of 100).

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,
      ""source"": ""calculated""
    },\\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": true,
      ""source"": ""learnable""
    },\\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {\
        ""min"": ""-inf"",
        ""max"": ""inf""
      },\
      ""learnable"": false,\\\
      ""source"": ""data""
    },\\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {\
        ""min"": 0,
        ""max"": 1,\
        ""exclusive_max"": true
      },\\\
      ""learnable"": false,
      ""source"": ""data""
    },\\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {\
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\\
      ""learnable"": false,
      ""source"": ""calculated""
    }\
  }\
}\
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The variable names you choose in your <VARIABLES> section must be used exactly as specified in the <MODEL> formula. Please ensure these names can be used directly in Python code (e.g., avoid Python reserved words). Also, ensure consistency between the variable names in your JSON and the variable names in the provided data description where applicable (e.g., use `split_perc_self` exactly as provided).

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 90 of 100).

Previous attempts, including the specific additive structure from the last run (`U_accept = beta_base_sensitivity * split_perc_self - beta_unfairness_penalty * loss_below_fair`), have consistently failed to achieve robust parameter identifiability (parameter recovery >= 0.7 for *all* learnable parameters simultaneously). In the last run, `beta_base_sensitivity` had recovery of only 0.614, rendering the model unusable despite having the highest accuracy so far.

Robust parameter identifiability (recovery >= 0.7 for *all* learnable parameters) remains the **unwavering primary objective**. A model structure that allows for reliable estimation of *all* its parameters is essential for meaningful interpretation and overrides achieving a low BIC or high accuracy if recovery fails. We need to find a structure where the influence of the two learnable parameters is sufficiently distinct across trials to be estimated independently with high reliability.

Given the persistent identifiability challenge with previous linear, multiplicative, power-law, log-transformed, and specific piecewise additive approaches, we must explore a **fundamentally different structural approach** for this run. The specific structure `U_accept = beta_base_sensitivity * split_perc_self - beta_unfairness_penalty * loss_below_fair` did *not* achieve identifiability for `beta_base_sensitivity`. This suggests that simply combining a linear term for the proposed percentage and an additive penalty for losses below fairness is not sufficient for distinguishing the parameters' influences.

Proposed Model Structure Principle: Design a model where the two learnable parameters influence the utility function in mathematically distinct and separable ways. One parameter should primarily relate to the intrinsic value or gain from the proposed offer (`split_perc_self`), while the other should *specifically* capture the cost or disutility associated with the offer being *below* the calculated fair share, but in a way that is structurally different from previous linear or simple additive penalties. Think about how these two influences can combine or interact non-linearly to create unique signatures for each parameter's impact on the utility across different offers relative to the fairness point.

Here are the standard calculation steps for necessary intermediate variables. You *must* include these first before defining `U_accept`, but the formula for `U_accept` itself must be structurally different from the linear combination used in the previous run:
1. Calculate `sum_tokens = token_self + token_opp`.
2. Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data. Use `IF` for the conditional calculation.
3. Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is positive if the offer is below the fair share, and 0 otherwise. Use `Max` for this calculation.

Now, design a novel formula for `U_accept` using `split_perc_self`, `loss_below_fair` (and potentially `fair_perc_self_calc` directly or the deviation `split_perc_self - fair_perc_self_calc`), and exactly two learnable parameters. The formula for `U_accept` must be mathematically distinct from `beta_base_sensitivity * split_perc_self - beta_unfairness_penalty * loss_below_fair`. Think outside the box. How can you use non-linear functions, interactions, or thresholds in a way that makes the influence of the ""value"" parameter and the ""unfairness"" parameter truly distinguishable?

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds.
    *   Parameter 1 (e.g., `beta_value`, `beta_gain_sensitivity`): Should scale or otherwise influence the utility related to the magnitude of the proposed percentage (`split_perc_self`). Non-negative bounds, e.g., `[0.0, 10.0]`.
    *   Parameter 2 (e.g., `beta_unfairness`, `beta_loss_aversion`): Should scale or otherwise influence a penalty or disutility term related to how much the offer is below the fair share (`loss_below_fair` or `fair_perc_self_calc - split_perc_self`). Non-negative bounds, e.g., `[0.0, 10.0]`.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model formula and data descriptions.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (utility is based on a sensitivity to the proposed percentage, combined with a penalty for offers below a calculated fair share). **Crucially, explain *how* your specific, novel mathematical structure creates mathematically distinct and separable influence profiles for the two learnable parameters tied to the offer value and unfairness, and how this is intended to improve robust joint parameter identifiability compared to previous linear or simple additive/multiplicative structures.**

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately. Avoid structures that have failed recovery previously, like simple linear combinations of `split_perc_self` and `loss_below_fair`. Explore novel mathematical relationships.",,,,,,,,,,,,0.4510497136673999,,0.5431350733968806,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
81,722.8909016488223,803.9685770301822,650.2588174530207,0.5659340659340659,"sum_tokens = token_self + token_opp
fair_perc_self_calc = If(sum_tokens == 0, 50.0, (token_self / sum_tokens) * 100.0)
loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_value * split_perc_self - beta_unfairness * (Exp(loss_below_fair) - 1.0)","An additive utility model for responder decisions. The utility of accepting is calculated as the sum of a linear term representing the value of the proposed share, scaled by beta_value, and a penalty term representing the aversion to unfairness. The penalty is an exponential function of the difference between the fair share based on contributions and the proposed share, scaled by beta_unfairness. This structure, combining a linear gain term with an exponential penalty term, is designed to enhance parameter identifiability by creating distinct functional dependencies on the two learnable parameters, beta_value and beta_unfairness, compared to previous divisive model structures.",v0,"Okay, here are the improved instructions for the next run (Run 81 of 100), based on the results of recent attempts.

Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = when participant is the proposer, what is the amount proposed to the opponent (in £)

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\\
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },\\
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\\
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },\\
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,\\
      ""source"": ""data""
    },\\
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },\\
      ""learnable"": false,
      ""source"": ""data""
    },\\
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },\\
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (β) constrained to [0,1] and an unconstrained environmental sensitivity parameter (ε). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

Instructions: Okay, here are the improved instructions for the next run (Run 81 of 100), based on the results of recent attempts.

The logarithmic divisive model tested in the previous run (Run 80) unfortunately failed to achieve robust parameter recovery for *both* learnable parameters (`beta_value` r=0.354, `beta_unfairness` r=0.352), despite providing reasonable fit metrics. This confirms that, similar to previous divisive structures (linear, exponential, quadratic, exponent-based), this specific structure and the way the parameters interacted did not create sufficiently distinct influence profiles for reliable joint parameter estimation.

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. A model with poor parameter recovery is unusable for interpreting individual differences or group comparisons, regardless of its aggregate fit. We must find a model structure where the influences of *both* `beta_value` and `beta_unfairness` are mathematically separable enough to be reliably estimated simultaneously.

We need to explore model structures that are *fundamentally different* from the divisive models explored so far. Instead of reducing utility multiplicatively through a denominator, let's explore an *additive* structure where different components of the offer's value contribute positively or negatively to the overall utility.

For Run 81, we will test an additive model where the utility is determined by a positive term related to the participant's proposed share (`split_perc_self`), and a negative penalty term related to the offer's unfairness (`loss_below_fair`). To address the persistent identifiability issue, we will make the penalty term non-linear, specifically using an exponential function, hoping this contrast with the linear value term will create more distinct parameter influences.

Proposed Model Structure: An additive utility model where the utility of accepting (`U_accept`) is calculated as a linear function of the participant's proposed percentage (`split_perc_self`), scaled by `beta_value`, *minus* an exponential penalty term based on `loss_below_fair`, scaled by `beta_unfairness`.

Here are the calculation steps:
Calculate `sum_tokens = token_self + token_opp`.
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0), using `token_self` and `token_opp` from the data.
Calculate `loss_below_fair = Max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
Calculate the utility: `U_accept = beta_value * split_perc_self - beta_unfairness * (Exp(loss_below_fair) - 1.0)`.
The penalty term uses `Exp()` (natural exponential function). We subtract 1.0 from `Exp(loss_below_fair)` so that when `loss_below_fair` is 0 (fair or better offers), the penalty term `(Exp(0) - 1.0)` is `(1.0 - 1.0) = 0`, and no penalty is applied. For `loss_below_fair > 0`, `Exp(loss_below_fair) - 1.0` is positive, resulting in a positive penalty scaled by `beta_unfairness`.

In this structure, `beta_value` represents the linear sensitivity to the proposed percentage share (the potential gain), while `beta_unfairness` represents the strength of an exponential penalty for offers below the fair share. The hypothesis is that the *linear* contribution from `beta_value` combined additively with the *exponential* contribution from `beta_unfairness` will yield sufficiently distinct functional forms and influence profiles for each parameter across different offers to allow for robust joint identifiability, addressing the failures seen in previous, more correlated model structures.

Design the model using this new additive structure with an exponential penalty: `U_accept = beta_value * split_perc_self - beta_unfairness * (Exp(loss_below_fair) - 1.0)`. Remember to calculate intermediate variables (`sum_tokens`, `fair_perc_self_calc`, `loss_below_fair`) before the final `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Use `Exp()` for the natural exponential function and `Max()` for the maximum function.

Focus on clearly defining all intermediate terms and articulating in the summary how this specific additive structure, by combining a linear gain term scaled by `beta_value` with an exponential penalty term based on `loss_below_fair` scaled by `beta_unfairness`, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles, moving away from the complex, potentially correlated interactions in the divisive models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include calculation steps for derived variables *before* the formula for `U_accept`.
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (linear gain sensitivity) and `beta_unfairness` (exponential penalty strength). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate for interpretability and likely necessary for parameter recovery in this structure (e.g., non-negative `beta_value` ensures utility increases with percentage share; non-negative `beta_unfairness` ensures the penalty term reduces utility). Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (additive structure where utility from `split_perc_self` is reduced by an exponential penalty based on `loss_below_fair`). Crucially, explain *how* this specific additive structure, combining a linear term scaled by `beta_value` and an exponential penalty term based on `loss_below_fair` scaled by `beta_unfairness`, is specifically intended to improve robust parameter identifiability for *both* parameters by creating mathematically distinct and separable influence profiles compared to previous divisive models.
*   **Think Outside the Box:** Parameter identifiability for *both* parameters simultaneously remains the persistent, difficult challenge. The exploration of various functions within divisive structures (linear, exponential, quadratic, exponent, log) has not solved this. Now we are trying an additive structure with a non-linear penalty. If this additive-exponential structure also fails to achieve >= 0.7 recovery for *both* parameters, the next iteration must consider *even more fundamentally different* ways the two parameters could interact or represent psychological processes to create truly distinct and separable influences. Could one parameter affect choice consistency (like a temperature parameter) while the other scales value? Could parameters influence the weighing of different attributes? Could the model structure involve comparison of utility of accepting vs. rejecting? Continue to focus intently on how the functional form ensures that varying `beta_value` has a measurably different effect on the utility curve across offers than varying `beta_unfairness`, and that these effects are distinct enough to be estimated independently with high reliability.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable and interact in a way that allows for reliable estimation for *both* parameters, ideally leading to improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.

Please think through this step by step, then provide your model specification and variable descriptions.",,,,,,,,,,,,0.2814050736635516,,,,,,0.3047659490534733,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10,747.0915026230294,824.317003433474,677.9103248136728,0.5534188034188035,"U = split_perc_self * (beta_value - gamma_proportional_aversion * max(0, fair_perc_self - split_perc_self))","A multiplicative utility model for ultimatum game responders. The utility of accepting an offer is determined by the offered percentage (split_perc_self). This base value sensitivity is scaled by beta_value. Additionally, if the offer is below the participant's proportional fair share (fair_perc_self, calculated from token contributions), the sensitivity is reduced by a term proportional to the degree of unfairness, scaled by gamma_proportional_aversion. This structure aims to separate the influence of pure value sensitivity from the cost associated with proportional unfairness. The model has two learnable parameters: beta_value (base sensitivity) and gamma_proportional_aversion (proportional aversion strength), both constrained to be non-negative.",v0,"Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Achieve excellent parameter recovery (>= 0.7 correlation) for ALL learnable parameters.** This remains the *single most important* objective. Previous attempts have consistently failed here. A model is unusable if any learnable parameter cannot be reliably recovered, regardless of other metrics (like BIC or accuracy). Prioritize model structures that inherently enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables available during responder trials (trial_role == 1).
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

**Analysis of Previous Attempt (Run 9):** The most recent model (`beta * split_perc_self - gamma_prop * max(0, fair_perc_self - split_perc_self)**delta_prop - gamma_50 * max(0, 50 - split_perc_self)**delta_50`) attempted to improve parameter identifiability by introducing exponents on the penalty terms. However, this model **failed parameter recovery catastrophically** (all parameters had r < 0.4, some near zero or negative), and also resulted in **significantly worse BIC (825.09)** and **lower accuracy (0.513)** compared to most previous models. This indicates that adding exponents and more parameters in this additive penalty structure did not help and likely made the parameters less identifiable while also providing a poor fit to the data's underlying structure. The failure to recover parameters renders the model unusable, regardless of the poor BIC and accuracy.

**Guidance for the Next Model (Radically Prioritizing Parameter Recovery - Continued):** We *must* abandon model structures that have repeatedly failed parameter recovery. Simple additive components scaled by learnable parameters, even with piecewise activation or non-linear exponents, have proven to be non-identifiable in this task context, likely due to high correlation or substitutability among parameter effects.

The focus for Run 10 *must* be on defining learnable parameters that influence the utility function in ways that are *fundamentally and qualitatively distinct* from each other, making their effects separable from the data. This requires moving beyond simple additive terms.

Consider:
*   **Parsimony:** Can parameter recovery be achieved with *fewer* learnable parameters? Models with fewer parameters often have better identifiability. Run 3, with only two parameters, achieved the highest recovery seen so far (though still below 0.7).
*   **Multiplicative Interactions:** Could parameters control the *rate* at which the value of an offer is discounted based on its unfairness? For example, `Utility = split_perc_self * (beta - gamma * max(0, fair_perc_self - split_perc_self))`. Here, `beta` sets the base sensitivity to the split (especially when fair), while `gamma` controls how much that sensitivity is *reduced* when the offer is below the proportional fair share. Their effects might be more separable than simple additive penalties.
*   **Piecewise Functions with Distinct Slopes:** Design a model where parameters define the *slope* or sensitivity to `split_perc_self` in different regions relative to the norms (50%, proportional). For instance, one parameter defines the slope when the offer is above the proportional share, and another defines the slope when it's below. This is different from scaling additive penalties; parameters control the function's rate of change directly.

For Run 10, let's specifically explore a model structure based on a **multiplicative interaction** between the offer value and proportional unfairness, building on the insights from Run 3's relative success with the proportional norm and aiming for a more identifiable structure.

Propose a model where utility is proportional to the offer percentage (`split_perc_self`), but the proportionality constant (sensitivity) is reduced by a term dependent on the degree of proportional unfairness.
`U = split_perc_self * (beta_value - gamma_proportional_aversion * max(0, fair_perc_self - split_perc_self))`

In this structure:
*   `beta_value` represents the base sensitivity to the proposed share. It's the sole driver of utility when the offer is equal to or above the proportional fair share (`max(...)` term is zero).
*   `gamma_proportional_aversion` controls how much the sensitivity to `split_perc_self` is decreased when the offer is *below* the proportional fair share. This parameter influences the utility function's slope *only* in the unfair region, and its effect is weighted by `split_perc_self`.

Define clear, generous finite bounds for `beta_value` and `gamma_proportional_aversion`. Both should likely be constrained to be non-negative, as increasing the offer should generally increase utility, and increasing proportional unfairness should decrease it.

Continue to build the model using percentage-based values (`split_perc_self`, `fair_perc_self`).

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded (-inf to inf).

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1) and use variables available in that context. The `fair_perc_self` variable (calculated as `(token_self / (token_self + token_opp)) * 100`) is useful for fairness calculations.

For run 10 of 100, please think through this step by step, focusing intently on how the *specific multiplicative structure* proposed here guarantees parameter identifiability and distinguishes the influence of each parameter, minimizing covariance based on the *specific* data characteristics. How do `beta_value` and `gamma_proportional_aversion` affect the utility function shape in unique ways that make them separable? Then provide your model specification, variable descriptions, target variable, and summary.
</think>",,,,,,,,,,,,,,,,,,0.11641401766866438,,,,,,0.17535371553841375,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6,783.0021726156795,865.3844657464147,709.2013683527291,0.5341880341880342,"U = beta_gain * split_perc_self - gamma_50 * max(0, 50 - split_perc_self)^2 - gamma_prop * max(0, fair_perc_self - split_perc_self)^2","A utility model for the responder's decision. Utility of acceptance is calculated as a linear gain from the participant's proposed percentage share, minus quadratic penalties that activate specifically when the proposed share falls below two distinct fairness norms: 50% of the total pot and the participant's proportional share based on token contributions. Learnable parameters scale the sensitivity to the linear gain (beta_gain), the quadratic penalty for offers below 50% (gamma_50), and the quadratic penalty for offers below the proportional share (gamma_prop). Utility is converted to acceptance probability via a logistic function.",v0,"<new_instructions>Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Achieve excellent parameter recovery (>= 0.7 correlation) for ALL learnable parameters.** This remains the *single most important* objective. Previous attempts have failed here. A model is unusable if any learnable parameter cannot be reliably recovered, regardless of other metrics. Prioritize model structures that inherently enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables available during responder trials (trial_role == 1).
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

**Analysis of Previous Attempt (Run 5):** The last model achieved a very good BIC and decent accuracy. However, it critically failed the parameter recovery test for most of its learnable parameters (`beta_split`, `beta_unfairness`, `alpha` all < 0.5). This indicates that despite using percentage scaling and a non-linear (quadratic) penalty term based on a dynamic reference, the influences of the parameters were still too correlated or not sufficiently distinct given the data. The parameters controlling sensitivity to the split percentage, sensitivity to deviation below the reference, and the weighting of the reference point likely had overlapping or substitutable effects on the overall utility.

**Guidance for the Next Model (Radically Prioritizing Parameter Recovery):** We need a fundamentally different approach to ensure parameters have separable and identifiable effects. Simple additive structures, even with non-linear terms on features, have not worked.

*   **Push Beyond Simple Penalties:** Instead of just adding or subtracting penalties from a base utility, consider models where parameters scale different *qualities* of the offer or fairness in distinct ways.
*   **Explore Different Non-linearities and Interactions:** If quadratic didn't work for the deviation, consider other functional forms (e.g., exponential decay of utility for low offers, piecewise functions with sharp transitions around norms like 50% or proportional share, multiplicative interactions between sensitivity parameters and fairness deviations). Think about how parameters could define the *shape* or *rate* of change of utility, not just the scale of an added/subtracted term.
*   **Distinct Parameter Roles:** Design parameters so their influence on utility is as orthogonal as possible. Could one parameter control the steepness of the utility curve *only* for offers below a norm, while another controls it *only* for offers above? Could a parameter modulate the *impact* of the proportional share norm relative to the 50% norm, but in a way that is less directly tied to a simple weighted average reference point that the penalty then scales?
*   **Focus on Percentages (Still Important):** Continue to use `split_perc_self` and `fair_perc_self` (derived from `token_self`, `token_opp`) for calculating utility components related to the offer and fairness to maintain consistent scaling. Mixing percentages and absolute values has proven problematic.
*   **Simplicity is Key (But Not at the Cost of Identifiability):** Aim for the minimum number of learnable parameters required to capture the essential trade-offs (self-interest vs. fairness) *while guaranteeing parameter identifiability*. A complex model with poor recovery is useless. A simpler model with excellent recovery is valuable.

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded (-inf to inf).

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1).

First, reason step by step about:
*   The key psychological mechanisms involved in responder decisions (self-interest, aversion to 50% split unfairness, aversion to proportional split unfairness).
*   **Crucial Design Choice:** How to formulate the utility function using percentage-based values (`split_perc_self`, `fair_perc_self`) in a way that inherently makes the learnable parameters' influences *distinct* and *identifiable*, avoiding the collinearity seen in previous models.
*   **Mandatory Non-linear Structure for Identifiability:** How to incorporate non-linear functional forms or parameter interactions that ensure parameters have separable effects. Think outside the box – can parameters define thresholds, exponents, or interaction terms that are clearly distinct from each other's roles?
*   How the learnable parameters are defined to have maximally distinct and identifiable effects on utility, minimizing covariance based on the *specific* data characteristics (e.g., correlation between split_perc_self and fair_perc_self).
*   What is the *minimum necessary* set of learnable parameters that can robustly capture the essential aspects of the behavior while prioritizing parameter identifiability (>= 0.7 recovery)?
*   How the model variables map to observable behavior (""accept"") via the logistic function.

Please think through this step by step, focusing intently on how the *specific mathematical structure* of the model guarantees parameter identifiability and distinguishes the influence of each parameter, then provide your model specification, variable descriptions, target variable, and summary.
</new_instructions>",,,,,,,,,,,,,,0.14154460119612688,0.1134330708491668,-0.05192887942240071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3,820.9953104963729,937.8976554491799,716.2702931428166,0.5054945054945055,"fairness_norm_prop_perc = (token_self / (token_self + token_opp)) * 100
fairness_norm_effective_perc = fifty_fifty_weight * 50 + (1 - fifty_fifty_weight) * fairness_norm_prop_perc
penalty_amount = max(0, fairness_norm_effective_perc - split_perc_self)
Utility = split_self - penalty_weight * penalty_amount","A utility model predicting responder acceptance based on a trade-off between the monetary amount received (split_self) and a penalty for unfairness. Unfairness is measured by the degree to which the proposed percentage share (split_perc_self) falls below a calculated effective fairness norm. This effective norm is a weighted average of a 50% split and the proportional share based on token contributions. The model includes two learnable parameters: a penalty_weight scaling the impact of the unfairness penalty, and a fifty_fifty_weight determining the contribution of the 50% norm versus the proportional norm to the effective fairness norm. The utility of acceptance is compared to the utility of rejection (zero), and a logistic function translates this utility difference into a probability of acceptance.",v0,"<think>The user wants updated instructions for the computational modeling LLM.
The goal is to improve BIC, accuracy, and parameter recovery (specifically aiming for > 0.7).
The LLM is stateless and cannot access previous interactions.
I need to provide instructions that guide the LLM based on the observation that the previous model had poor parameter recovery for most parameters, likely due to identifiability issues or parameter covariance.
The instructions should encourage exploring alternative, potentially simpler, or differently structured models to improve recovery while maintaining performance.
I must *only* output the new instructions text, enclosed in `<new_instructions>` tags, and do any thinking within `<think>` tags.

Previous Run Analysis:
- Model complexity: 4 learnable parameters.
- Outcome: Good accuracy, acceptable BIC, very poor parameter recovery for beta_zero, beta_two, and beta_three.
- Problem: Likely parameter identifiability or high covariance. The model might be trying to capture too many nuanced effects with correlated parameters, given the data.

Instruction Goals:
1. Explicitly state the importance of parameter recovery (> 0.7 threshold).
2. Emphasize parameter identifiability and avoiding high covariance as a primary design goal.
3. Suggest strategies:
    - Consider simpler models (fewer parameters).
    - Explore different functional forms beyond simple linear combinations or max functions.
    - Maybe focus on fewer psychological mechanisms if modeling all simultaneously hinders recovery.
4. Reiterate the task requirements (responder only, dataset variables, utility model).
5. Update the initial step-by-step reasoning prompt to incorporate identifiability and recovery considerations.
6. Keep the output format requirements clear (<MODEL>, <VARIABLES>, etc.).
7. Avoid mentioning specific run numbers or outcomes directly, frame it as general challenges for modeling this task.

Drafting new instructions:
- Start by setting the priorities: BIC, Accuracy, and *crucially* Parameter Recovery (all learnable parameters must have recovery > 0.7). Emphasize that poor recovery makes a model unusable.
- State that achieving good parameter recovery has been a key challenge for models on this task.
- Instruct the LLM to prioritize designing a model where parameters are highly identifiable from the data. Mention that high covariance between parameters is a common pitfall leading to poor recovery.
- Suggest that this might require simplifying the model (fewer learnable parameters) or rethinking how different psychological influences (like self-interest, 50% fairness, proportional fairness) are mathematically combined or weighted.
- Encourage exploration of alternative model structures or functional forms that might simplify the parameter space or reduce dependencies between parameters. Think about threshold models, different penalty structures, or models focusing on fewer distinct fairness reference points if needed for identifiability.
- Reiterate the task context: predicting responder acceptance based on offer percentage, potentially influenced by fairness considerations (50% split, proportional split based on tokens). List key available variables again.
- Reiterate the output format requirements.
- Modify the step-by-step reasoning prompt to explicitly include considerations for identifiability, covariance, and model simplicity/complexity trade-offs.

Refining wording:
- ""Design your computational model with parameter identifiability as a *primary* design goal. Models for this task have previously struggled with recovering parameters reliably (aiming for > 0.7 correlation), often due to high covariance or over-parameterization.""
- ""Carefully consider how different psychological factors (like self-interest and aversion to different types of unfairness) are combined mathematically. Highly correlated parameters can make the model unusable, even if it fits the data well.""
- ""Explore models with fewer learnable parameters if necessary. A simpler model with identifiable parameters is preferred over a complex one with unidentifiable parameters.""
- ""Do not be constrained by standard linear utility model structures. Think creatively about alternative mathematical forms (e.g., threshold models, non-linear penalties) or ways to combine influences that reduce parameter interdependencies and improve identifiability.""
- Update reasoning points: ""How does the model structure ensure that each learnable parameter can be uniquely estimated from the data? What are potential sources of covariance between parameters, and how can the structure minimize this? Is the number of learnable parameters appropriate for the complexity of the psychological process and the data available?""

Okay, planning complete. I will generate the instructions based on these points.<new_instructions>Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Pass parameter recovery tests.** This is a *primary* objective. All learnable parameters must ideally achieve a recovery correlation of **at least 0.7**. A model where parameters cannot be reliably recovered is unusable. Prioritize model structures that enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables.
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

Design your model with parameter identifiability and low parameter covariance as *critical* considerations. Computational models for this task have previously faced significant challenges in reliably recovering learnable parameters, even when fitting the data well. This often stems from parameters being too highly correlated or the model being over-parameterized relative to the available data.

Carefully consider the number of learnable parameters. While complex psychological processes are at play, a simpler model with well-identifiable parameters is substantially more valuable than a complex one with parameters that cannot be reliably estimated. Bias towards fewer learnable parameters if it improves identifiability and recovery.

Do not feel restricted to standard linear utility model structures. Explore alternative functional forms, different ways to combine influences (e.g., non-linear relationships, threshold effects, multiplicative interactions), or potentially focus on a subset of psychological mechanisms if attempting to model all simultaneously leads to parameter identifiability issues. Think creatively about how self-interest, aversion to a 50% split, and aversion to a proportional split might interact or be weighted in a way that allows their unique contributions to be estimated.

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded.

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1).

First, reason step by step about:
*   The key psychological mechanisms involved in responder decisions (self-interest, fairness/inequality aversion relative to different reference points like 50% or proportional share).
*   How these mechanisms might interact mathematically.
*   What mathematical form could capture these interactions while ensuring parameter identifiability and minimizing covariance.
*   What is the minimum necessary set of parameters needed to capture the essential aspects of the behavior *robustly*, allowing for reliable parameter recovery?
*   How the model variables map to observable behavior (""accept"").
*   How the chosen model structure and parameters will impact parameter recovery.

Please think through this step by step, then provide your model specification, variable descriptions, target variable, and summary.
</new_instructions>",,,,,-0.11713712224883525,0.46941125998743766,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9,825.0875338810046,910.0354469978112,748.9883617138653,0.5131257631257631,"U = beta * split_perc_self - gamma_prop * max(0, fair_perc_self - split_perc_self)**delta_prop - gamma_50 * max(0, 50 - split_perc_self)**delta_50","A utility model for responder decisions. Utility of accepting is based on the proposed share (split_perc_self), scaled by beta. Penalties are applied for offers below the proportional fair share (fair_perc_self) and below the 50% split. Each penalty is scaled by a gamma parameter (gamma_prop, gamma_50) and shaped by an exponent parameter (delta_prop, delta_50), allowing for non-linear penalty functions. The model uses five learnable parameters: beta, gamma_prop, delta_prop, gamma_50, and delta_50.",v0,"Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Achieve excellent parameter recovery (>= 0.7 correlation) for ALL learnable parameters.** This remains the *single most important* objective. Previous attempts, including the most recent one, have consistently failed here. A model is unusable if any learnable parameter cannot be reliably recovered, regardless of other metrics (like BIC or accuracy). Prioritize model structures that inherently enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables available during responder trials (trial_role == 1).
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

**Analysis of Previous Attempt (Run 8):** The most recent model (`beta_value * split_perc_self - gamma_fairness * max(0, proportional_share - split_perc_self) - delta_inequity * max(0, 50 - split_perc_self)`) showed some improvements in BIC (393.86) and accuracy (0.769) compared to some earlier models, but it **failed parameter recovery spectacularly** (`beta_value: r=0.592`, `gamma_fairness: r=0.296`, `delta_inequity: r=0.237`). All three parameters were significantly below the critical 0.7 threshold. This confirms the diagnosis from earlier runs: simple additive components scaled by learnable parameters, even in a piecewise linear form distinguishing between the 50% and proportional norms, still results in highly correlated or substitutable parameters given the data structure. The effects of increasing the general value of the offer (`beta_value`) and decreasing the penalties (`gamma_fairness`, `delta_inequity`) are too similar, and the effects of the two different penalties also overlap too much, especially when offers are low.

**Guidance for the Next Model (Radically Prioritizing Parameter Recovery - Continued):** We *must* move beyond simple linear, additive structures if we are to achieve parameter identifiability. The challenge is to design parameters that influence the utility function in ways that are *qualitatively* different from each other, particularly around the critical reference points (50% and the proportional share, `fair_perc_self`).

Consider how parameters could control:
*   **Non-linear penalties/bonuses:** Instead of linear penalties scaled by parameters, could parameters control the *exponent* of a deviation term (e.g., `deviation^parameter_exponent`)? Or could penalties be quadratic or exponential functions controlled by parameters? For example, `gamma_50 * max(0, 50 - split_perc_self)^exponent_50` and `gamma_prop * max(0, fair_perc_self - split_perc_self)^exponent_prop`. Parameters controlling exponents (`exponent_50`, `exponent_prop`) or scaling non-linear terms (`gamma_50`, `gamma_prop`) might have more distinct effects.
*   **Switching slopes/curvatures:** Design a model where the sensitivity to `split_perc_self` is different depending on whether the offer is, say, below the proportional share, between the proportional share and 50%, or above 50%. Parameters could define the *slopes* or *curvatures* in each of these ranges. This creates piecewise functions, but the parameters control the *shape of the segments*, not just scale an additive penalty.
*   **Relative influence via non-additive means:** Can a parameter mediate the *interaction* between the offer value and deviation from norms? For example, `Utility = split_perc_self * (1 - beta_aversion * total_deviation)`. Or could a parameter affect *how* the 50% norm and the proportional norm combine their influence, perhaps in a multiplicative or non-linear way?
*   **Thresholds:** Could a parameter define a *threshold* below which acceptance probability drops sharply, independent of the specific penalty calculation? (e.g., if `split_perc_self < beta_rejection_threshold`, Utility is very low). This might be challenging to make identifiable alongside other parameters, but it's a structural departure.

Focus on defining parameters whose effect on utility cannot be replicated by simply scaling other terms or shifting the function linearly. Parameters should ideally affect the utility function's slope, curvature, or point of inflection in a way that is unique to that parameter.

Continue to build the model using percentage-based values (`split_perc_self`, `fair_perc_self`). Aim for the minimum number of learnable parameters necessary *only if* their effects are clearly separable and identifiable. A model with fewer, identifiable parameters is vastly preferred over one with many, unidentifiable ones, regardless of seemingly good BIC or accuracy scores on validation data.

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded (-inf to inf).

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1) and use variables available in that context. The `fair_perc_self` variable (calculated as `(token_self / (token_self + token_opp)) * 100`) is useful for fairness calculations.

For run 9 of 100, please think through this step by step, focusing intently on how the *specific mathematical structure* of the model guarantees parameter identifiability and distinguishes the influence of each parameter, minimizing covariance based on the *specific* data characteristics. How can you define learnable parameters that control fundamentally different aspects of the utility function (e.g., changing slopes in ranges, exponents, multiplicative interactions, switching points based on norms) rather than just scaling additive components? Think creatively about non-linear and piecewise structures relative to *both* the 50% and proportional norms. Then provide your model specification, variable descriptions, target variable, and summary.",,,,,,,,,,,,,,,0.14613632300654314,-0.0967275523648008,,,,,0.3898815771679789,0.07643275582853844,0.036527203630835774,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
73,950.9738946724698,1063.143271219475,850.4888281824443,0.4307081807081807,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (token_self / sum_tokens) * 100.0
loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)
U_accept = beta_value * split_perc_self - beta_unfairness * exp(loss_below_fair)","An additive utility model predicting the participant's decision to accept or reject an offer as a responder. The utility of accepting is calculated as a combination of a linear gain term based on the offered percentage (split_perc_self), scaled by the learnable parameter beta_value, and an exponential penalty term for offers falling below a calculated fair share based on token contributions (loss_below_fair), scaled by the learnable parameter beta_unfairness. The model calculates a fair percentage (fair_perc_self_calc) based on token ratio (handling the zero sum case), and then determines the loss below this fair share. The exponential form of the unfairness penalty is specifically hypothesized to create a non-linear influence profile distinct from the linear value term, aiming to improve the identifiability and separability of the beta_value and beta_unfairness parameters during model fitting and recovery tests compared to models with linear or squared penalty terms. The calculated utility U_accept is then transformed into a probability of acceptance using a logistic function.",v0,"Okay, here are the improved instructions for the next run (Run 73), based on the results of previous attempts, with the absolute focus remaining on achieving robust parameter identifiability.

Instructions for Run 73 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery.

The previous run (Run 72) implemented a reference-dependent additive model (`U_accept = beta_value * gain_above_fair - beta_unfairness * loss_below_fair`). Despite the theoretical rationale that separating parameters into gain and loss domains relative to a fair share reference would create distinct influences, the results showed *extremely poor* parameter recovery for both parameters (beta_value: r=0.131, beta_unfairness: r=0.015). Furthermore, this model exhibited very poor overall fit (BIC: 600.81, Accuracy: 0.533), significantly worse than many earlier attempts. This indicates that this specific reference-dependent structure, while conceptually distinct, did not capture the underlying behavior well, leading to unidentifiable parameters. Poor model fit often directly results in poor parameter recovery.

Previous attempts using simple additive structures or ratio structures with linear, squared, or ratio-based penalty terms have also consistently failed to achieve robust parameter identifiability, suggesting these common forms may inherently lead to confounding when a 'value' and 'unfairness' parameter both influence utility.

For Run 73, we need to continue exploring structures that can both provide a reasonable fit to the data *and* create mathematically distinct influences for the 'value' and 'unfairness' parameters. The failure of the Run 72 model suggests that just changing the conceptual framework (additive vs. reference-dependent) isn't enough; the specific *functional forms* by which the parameters scale variables must ensure separability.

Let's revisit an additive structure but incorporate a strongly non-linear penalty term in a novel way. The goal is to ensure that the influence of the 'unfairness' parameter grows much more rapidly or in a fundamentally different shape than the linear influence of the 'value' parameter across the range of possible offers.

Proposed Model Structure: An additive utility model with a linear value term and an **exponential penalty** term based on deviation below the fair share.
`U_accept = beta_value * split_perc_self - beta_unfairness * exp(loss_below_fair)`

Here:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0).
Calculate `loss_below_fair = max(0.0, fair_perc_self_calc - split_perc_self)`. This term is non-zero only for offers below or equal to the fair share.
Finally, `U_accept = beta_value * split_perc_self - beta_unfairness * exp(loss_below_fair)`. Note the subtraction for the penalty.

In this structure, `beta_value` linearly scales the value derived from the offered percentage (`split_perc_self`). `beta_unfairness` scales a penalty that increases exponentially with the magnitude of the 'loss' or deviation below the fair share (`loss_below_fair`). The key hypothesis for improved identifiability is that this **exponential form** of the penalty creates a functional relationship between `loss_below_fair` and disutility that is dramatically non-linear and distinct from the linear relationship between `split_perc_self` and utility driven by `beta_value`. This aims to make the influence of `beta_value` and `beta_unfairness` on the overall utility profile easily separable during fitting and recovery tests, overcoming the limitations seen with linear or squared penalty terms.

Design the model using this additive structure with an exponential penalty: `U_accept = beta_value * split_perc_self - beta_unfairness * exp(loss_below_fair)`, where `loss_below_fair` is calculated as specified above using `fair_perc_self_calc` as the reference.

Focus on clearly defining the `loss_below_fair` term and articulating in the summary how this specific structure, by combining a linear value term with an *exponential* unfairness penalty term, is intended to achieve robust parameter identifiability by creating mathematically distinct influence profiles for `beta_value` and `beta_unfairness` across different offers. Explain why this specific non-linear form is being tested as a potential way to overcome the identifiability failures seen in previous structures.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `loss_below_fair`, and `U_accept`. Include any other necessary intermediate variables.
*   **Model Formula:** Define `U_accept` using the additive structure with exponential penalty: `U_accept = beta_value * split_perc_self - beta_unfairness * exp(loss_below_fair)`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` (value sensitivity) and `beta_unfairness` (unfairness sensitivity). Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and may aid recovery. Stick with `[0.0, 100.0]` for both.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, and `loss_below_fair`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (additive structure, linear value term, exponential unfairness penalty based on deviation from fair share). Crucially, explain *how* this structure, by using the exponential function for the penalty term, is specifically intended to improve robust parameter identifiability by creating a mathematically distinct influence profile for `beta_unfairness` compared to the linear influence of `beta_value`, addressing the limitations of previous simple additive/ratio structures and the failed reference-dependent model.
*   **Think Outside the Box:** Recognize that previous attempts with standard functional forms (linear, squared, ratio) have failed parameter recovery. The exponential function is a deliberate choice to introduce a different kind of non-linearity, explicitly attempting to create functional separability between parameters. Continue to consider non-obvious ways parameters might interact or influence decisions beyond simple linear combinations, as finding a structure that truly shines in parameter recovery often requires exploring less standard forms.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates and hopefully improved fit metrics (BIC and accuracy) as a result of capturing the underlying process more accurately.",,,,,,,,,,,,0.0003521612964923729,,,,,,0.17952795104094188,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2,,,,,"proportional_percentage = token_self / (token_self + token_opp)
fairness_reference_point = beta_fairness_weight * 0.5 + (1 - beta_fairness_weight) * proportional_percentage
utility = beta_intercept + beta_own_share * split_perc_self - beta_fair_deviation * abs(split_perc_self - fairness_reference_point)","A utility model predicting responder acceptance based on a combination of self-interest and aversion to unfairness. Utility increases with the proposed percentage share to the self (parameterized by beta_own_share). A penalty is applied for deviations from an internal fairness reference point. This reference point is dynamically calculated as a weighted average of a 50% split and the proportional split based on token contributions, with the weighting determined by a learnable parameter (beta_fairness_weight). The sensitivity to deviations from this combined reference is controlled by another learnable parameter (beta_fair_deviation). A baseline utility is included as an intercept (beta_intercept). The model uses four learnable parameters and maps utility to acceptance probability via a logistic function.",v0,"<think>The user wants updated instructions for the computational modeling LLM.
The goal is to improve BIC, accuracy, and parameter recovery (specifically aiming for > 0.7).
The LLM is stateless and cannot access previous interactions.
I need to provide instructions that guide the LLM based on the observation that the previous model had poor parameter recovery for most parameters, likely due to identifiability issues or parameter covariance.
The instructions should encourage exploring alternative, potentially simpler, or differently structured models to improve recovery while maintaining performance.
I must *only* output the new instructions text, enclosed in `<new_instructions>` tags, and do any thinking within `<think>` tags.

Previous Run Analysis:
- Model complexity: 4 learnable parameters.
- Outcome: Good accuracy, acceptable BIC, very poor parameter recovery for beta_zero, beta_two, and beta_three.
- Problem: Likely parameter identifiability or high covariance. The model might be trying to capture too many nuanced effects with correlated parameters, given the data.

Instruction Goals:
1. Explicitly state the importance of parameter recovery (> 0.7 threshold).
2. Emphasize parameter identifiability and avoiding high covariance as a primary design goal.
3. Suggest strategies:
    - Consider simpler models (fewer parameters).
    - Explore different functional forms beyond simple linear combinations or max functions.
    - Maybe focus on fewer psychological mechanisms if modeling all simultaneously hinders recovery.
4. Reiterate the task requirements (responder only, dataset variables, utility model).
5. Update the initial step-by-step reasoning prompt to incorporate identifiability and recovery considerations.
6. Keep the output format requirements clear (<MODEL>, <VARIABLES>, etc.).
7. Avoid mentioning specific run numbers or outcomes directly, frame it as general challenges for modeling this task.

Drafting new instructions:
- Start by setting the priorities: BIC, Accuracy, and *crucially* Parameter Recovery (all learnable parameters must have recovery > 0.7). Emphasize that poor recovery makes a model unusable.
- State that achieving good parameter recovery has been a key challenge for models on this task.
- Instruct the LLM to prioritize designing a model where parameters are highly identifiable from the data. Mention that high covariance between parameters is a common pitfall leading to poor recovery.
- Suggest that this might require simplifying the model (fewer learnable parameters) or rethinking how different psychological influences (like self-interest, 50% fairness, proportional fairness) are mathematically combined or weighted.
- Encourage exploration of alternative model structures or functional forms that might simplify the parameter space or reduce dependencies between parameters. Think about threshold models, different penalty structures, or models focusing on fewer distinct fairness reference points if needed for identifiability.
- Reiterate the task context: predicting responder acceptance based on offer percentage, potentially influenced by fairness considerations (50% split, proportional split based on tokens). List key available variables again.
- Reiterate the output format requirements.
- Modify the step-by-step reasoning prompt to explicitly include considerations for identifiability, covariance, and model simplicity/complexity trade-offs.

Refining wording:
- ""Design your computational model with parameter identifiability as a *primary* design goal. Models for this task have previously struggled with recovering parameters reliably (aiming for > 0.7 correlation), often due to high covariance or over-parameterization.""
- ""Carefully consider how different psychological factors (like self-interest and aversion to different types of unfairness) are combined mathematically. Highly correlated parameters can make the model unusable, even if it fits the data well.""
- ""Explore models with fewer learnable parameters if necessary. A simpler model with identifiable parameters is preferred over a complex one with unidentifiable parameters.""
- ""Do not be constrained by standard linear utility model structures. Think creatively about alternative mathematical forms (e.g., threshold models, non-linear penalties) or ways to combine influences that reduce parameter interdependencies and improve identifiability.""
- Update reasoning points: ""How does the model structure ensure that each learnable parameter can be uniquely estimated from the data? What are potential sources of covariance between parameters, and how can the structure minimize this? Is the number of learnable parameters appropriate for the complexity of the psychological process and the data available?""

Okay, planning complete. I will generate the instructions based on these points.<new_instructions>Design a computational model for the described ultimatum game responder task. Your model must meet the following criteria:
1.  **Predict only responder behavior** (target variable: ""accept"").
2.  **Pass parameter recovery tests.** This is a *primary* objective. All learnable parameters must ideally achieve a recovery correlation of **at least 0.7**. A model where parameters cannot be reliably recovered is unusable. Prioritize model structures that enhance parameter identifiability from the data.
3.  **Be applicable to the dataset:** Use only the provided data structure and variables.
4.  **Predict utility:** Utility values will be converted to acceptance probabilities using a logistic function (with temperature 1). A utility > 0 implies > 50% probability of acceptance.

Design your model with parameter identifiability and low parameter covariance as *critical* considerations. Computational models for this task have previously faced significant challenges in reliably recovering learnable parameters, even when fitting the data well. This often stems from parameters being too highly correlated or the model being over-parameterized relative to the available data.

Carefully consider the number of learnable parameters. While complex psychological processes are at play, a simpler model with well-identifiable parameters is substantially more valuable than a complex one with parameters that cannot be reliably estimated. Bias towards fewer learnable parameters if it improves identifiability and recovery.

Do not feel restricted to standard linear utility model structures. Explore alternative functional forms, different ways to combine influences (e.g., non-linear relationships, threshold effects, multiplicative interactions), or potentially focus on a subset of psychological mechanisms if attempting to model all simultaneously leads to parameter identifiability issues. Think creatively about how self-interest, aversion to a 50% split, and aversion to a proportional split might interact or be weighted in a way that allows their unique contributions to be estimated.

For any learnable parameters, you must specify generous, finite numerical bounds. The utility variable may be unbounded.

Provide your formal mathematical model between <MODEL> tags. Ensure *only* the mathematical formula is within these tags.
Provide variable descriptions in JSON format between <VARIABLES> tags. Ensure learnable parameter names are clear and suitable for coding. Spell out Greek variables.
Specify the target variable using <target_variable> tags.
Provide a concise, descriptive summary of the model between <SUMMARY> tags.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
Remember to only model responder trials (where trial_role == 1).

First, reason step by step about:
*   The key psychological mechanisms involved in responder decisions (self-interest, fairness/inequality aversion relative to different reference points like 50% or proportional share).
*   How these mechanisms might interact mathematically.
*   What mathematical form could capture these interactions while ensuring parameter identifiability and minimizing covariance.
*   What is the minimum necessary set of parameters needed to capture the essential aspects of the behavior *robustly*, allowing for reliable parameter recovery?
*   How the model variables map to observable behavior (""accept"").
*   How the chosen model structure and parameters will impact parameter recovery.

Please think through this step by step, then provide your model specification, variable descriptions, target variable, and summary.
</new_instructions>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2,,,,,Error: unsupported format string passed to NoneType.__format__,,v0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16,,,,,No metadata,,v0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
55,,,,,"sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)
epsilon_param = 1e-6
fair_perc_self_calc_safe = fair_perc_self_calc + epsilon_param
r_val = split_perc_self / fair_perc_self_calc_safe
utility = beta_value * split_perc_self + beta_relative_fairness * log(r_val + epsilon_param)","A utility model for responder decisions combining absolute value and relative fairness. Utility is a linear combination of the proposed percentage share for the participant, scaled by 'beta_value', and the logarithm of the ratio of the proposed share to the calculated fair share, scaled by 'beta_relative_fairness'. This structure aims to improve parameter identifiability by having the two learnable parameters scale terms representing different dimensions (absolute value vs. relative fairness) via different mathematical operations (linear vs. logarithmic transformation).",v0,"Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = same as above but in £

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able
to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):
Let me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,\n        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like \'lambda\' and use alternatives like \'learning_rate\' or \'lambda_param\'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.


Instructions: Okay, here are the improved instructions for the next run (Run 55), building on the results of the previous runs.

Instructions for Run 55 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 54) implemented a piecewise non-linear model based on fairness difference (`U = (perc_difference >= 0) ? beta_gain_sensitivity * sqrt(perc_difference + epsilon) : beta_loss_sensitivity * (fair_perc_self_calc - split_perc_self + epsilon)^2`). It unfortunately failed the parameter recovery test for *both* `beta_gain_sensitivity` (r = 0.332) and `beta_loss_sensitivity` (r = 0.596). Its overall performance (BIC 449.36, Acc 0.697) was also relatively poor.

Analysis of Run 54's failure and previous models: The piecewise structure in Run 54 assigned the two learnable parameters to scale distinct non-linear functions (`sqrt` vs. `square`) of the fairness deviation in gain vs. loss regimes. However, this approach, similar to previous attempts that scaled linear terms (Run 4) or combined linear terms (Run 1, 3), did not create sufficiently distinct influences for the parameters to be reliably estimated. We've observed a pattern where parameters scaling the absolute value of the offer or gain component (`beta_value` in Runs 1 & 3) sometimes achieve decent recovery, but parameters specifically scaling the *fairness deviation* or *loss penalty* (`beta_fairness_deviation`, `beta_unfairness_penalty`, `beta_loss_sensitivity`) consistently fail to reach the 0.7 threshold. This suggests the influence of the fairness deviation/penalty term is either too correlated with the absolute value term, or its impact is not captured in a mathematically separable way by simply scaling it.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** such that *both* parameters have truly distinct and identifiable influences on responder decisions (`trial_role == 1`). We need to find a way for the second parameter (the fairness/penalty related one) to exert its influence in a manner that is mathematically orthogonal or highly distinct from the first parameter's influence (the value/gain related one).

Based on this analysis, focus your exploration for Run 55 on model structures where the two learnable parameters contribute to utility in fundamentally different mathematical ways. Simply scaling different terms (linear or non-linear) within or across regimes hasn't consistently worked. Consider structures where one parameter influences the sensitivity to the *absolute* value of the offer (`split_perc_self`), while the other parameter influences sensitivity to the *relative* fairness (e.g., the ratio `R_val = split_perc_self / fair_perc_self_calc`) via a non-linear transformation.

Consider this strategy to enhance identifiability:

*   **Combine Linear Value with Non-Linear Relative Fairness:** Design a model where one parameter scales the proposed percentage itself (`split_perc_self`), capturing a general sensitivity to the absolute value being offered, and the second parameter scales a non-linear function of the *ratio* of the proposed percentage to the calculated fair percentage (`R_val`). The non-linear function of the ratio should capture the specific impact of relative fairness/unfairness.
    *   *Structural Principle:* A potentially promising structure is `U = beta_value * split_perc_self + beta_relative_fairness * f(R_val)`, where `f` is a non-linear function like `log`, `sqrt`, or a power function. This allows `beta_value` to influence the baseline utility based on the offer size, while `beta_relative_fairness` modulates this based on how fair the offer is, via a distinct mathematical operation on the ratio.
    *   *Example Idea (building on the ratio concept):* Consider using the logarithm of the ratio. `log(R_val)` is 0 when the offer is fair (R_val=1), positive for favorable offers (R_val>1), and negative for unfavorable offers (R_val<1), with decreasing marginal impact as R_val increases.
        *   `U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)`
        *   Where `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensure this handles `sum_tokens=0` correctly).
        *   `R_val = IF(fair_perc_self_calc > 0, split_perc_self / fair_perc_self_calc, IF(split_perc_self > 0, 100.0, epsilon))` (Handle `fair_perc_self_calc = 0`. If fair is 0 and split is positive, treat as a large ratio (e.g., 100.0). If both are 0, treat R_val as close to 0 to get a large negative log).
        *   `epsilon = 1e-6` (Small constant for log input).

*   **Explore Other Structures:** If the above doesn't work, continue to think creatively about how two parameters can control mathematically separable aspects of the utility function. Could one parameter control a threshold or an exponent? Could they define the shape of a smooth function (e.g., sigmoid, power law) that takes fairness deviation as input? Avoid structures that simply scale linearly dependent terms.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and necessary intermediate terms like `R_val` and `epsilon`. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-50.0, 50.0] or wider/narrower if appropriate). Consider the expected sign and scale, but keep bounds wide enough.

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, intermediate terms like `R_val` and `epsilon`, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` explicitly mentions the `sum_tokens > 0` condition and the default value.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the proposed percentage share for self.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain how its specific mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters (e.g., by having them scale distinct mathematical forms like linear value and non-linear relative fairness).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. Previous attempts scaling different terms (linear or non-linear) within or across regimes have struggled, particularly with the fairness/penalty parameter. Explore combining a term based on absolute value with a term based on a non-linear transformation of relative fairness (like the log of the ratio), or other structures that create highly distinct roles for the two parameters. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation.Task Description: In this ultimatum game experiment, the participant acts as both a proposer and a responsder. They are the proposer 14 times and the responsder 36 times. Each is designated by a trial number in the data set. This gives 50 instances for the learnable parameters to be trained (although really it may rather be 36 or 14 depending on how the computational model should be created). There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable ""token_self"". The other variable, ""token_opp"", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent. The rounds are a mixed, with the participant doing a handful of turns as a responder, then a handful as a proposer, and then a handful as a responder, and so on.

Again, the participant gets to act as both a proposer and responder - this is reflected in the data. If you are guessing the proposed amount, you should only be making this guess when trial_role == 2, since this is when the data within the proposed_perc and proposed_amount columns are relevent.

Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same ""fairness"".

Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

trial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]
trial_role = the role of the participant [1 = responder, 2 = proposer]
token_opp = number of tokens found by the opponent
token_self = number of tokens found by the participant
combined_earning = amount of money earned in total (note that each token found is £3)
split_opp = the proposed share of the opponent (in £)
split_self = the proposed share for the participant (in £)
splitperc_opp = same but in %
split_perc_self = same but in %
accept = whether the participant responded accept (1) or reject (0)
accepted_amount = if accepted, what is the amount gain (in £)
accepted_perc = same as above but in %
proposed_perc = when participant is the proposer, what is the % proposed to the opponent
proposed_amount = same as above but in £

Desired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified).

The model should:
1. Predict only responder behavior (should target ""accept = whether the participant responded accept (1) or reject (0)"")
2. You will have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.
3. Be applicable to the dataset given within the task description (i.e. the model should be able
to be run using only the provided dataset structure).
4. Predict utility, where less than 0 is less than 50% likely.

For any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).

Then, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.
For example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal.

Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied.

{
  ""variables"": {
    ""variable_name"": {
      ""description"": ""Clear description"",
      ""range"": {
        ""min"": value,
        ""max"": value,
        ""inclusive_min"": boolean,
        ""inclusive_max"": boolean
      },
      ""distribution"": {
        ""type"": ""distribution type"",
        ""parameters"": {}
      },
      ""learnable"": boolean,
      ""source"": ""learnable/data/calculated""
    }
  }
}
</VARIABLES>

Example #1:
<EXAMPLE>
(Model of a Random Choice Task):\nLet me think through this step by step...
[your reasoning here]

<MODEL>
U_i = beta + epsilon*E + eta*N
</MODEL>

<VARIABLES>
{
  ""variables"": {
    ""U_i"": {
      ""description"": ""Utility of choosing action i"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""calculated""
    },
    ""beta"": {
      ""description"": ""Base tendency parameter (inherent preference)"",
      ""range"": {
        ""min"": 0,
        ""max"": 1
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""epsilon"": {
      ""description"": ""Environmental sensitivity parameter"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": true,
      ""source"": ""learnable""
    },
    ""E"": {
      ""description"": ""Environmental cue value"",
      ""range"": {
        ""min"": ""-inf"",
        ""max"": ""inf""
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""eta"": {
      ""description"": ""Noise parameter"",
      ""range"": {
        ""min"": 0,
        ""max"": 1,
        ""exclusive_max"": true
      },
      ""learnable"": false,
      ""source"": ""data""
    },
    ""N"": {
      ""description"": ""Random noise drawn from normal distribution N(0,1)"",
      ""distribution"": {
        ""type"": ""normal"",
        ""mean"": 0,
        ""std"": 1
      },
      ""learnable"": false,
      ""source"": ""calculated""
    }
  }
}
</VARIABLES>
<SUMMARY>
A linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (beta) constrained to [0,1] and an unconstrained environmental sensitivity parameter (epsilon). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.
</SUMMARY>
</EXAMPLE>

Please be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.

The parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like \'lambda\' and use alternatives like \'learning_rate\' or \'lambda_param\'.

Model Type:
Your model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.

<think>
The user wants improved instructions for Run 55.
Primary goal: Parameter Recovery >= 0.7 for *all* 2 learnable parameters.
Secondary goals: Lower BIC, higher Accuracy.
Stateless LLM requires full context (task, variables, previous run analysis, objective, output format).

Previous run (Run 54) details:
- Model: Piecewise non-linear (`sqrt` for gain, `square` for loss), scaling the transformed fairness difference.
- Results: BIC 449.36, Acc 0.697, Rec beta_gain_sensitivity=0.332, beta_loss_sensitivity=0.596.
- Failure: Poor recovery for *both* parameters.

Analysis of previous runs provided:
- Run 1: Linear combination of `split_perc_self` and `fairness_deviation`. Rec: beta_value=0.718, beta_fairness_deviation=0.653. (Failed recovery on fairness param)
- Run 2: Linear combo of `split_perc_self` and squared fairness penalty. Rec: beta_value=0.359, beta_fairness_penalty=0.630. (Failed recovery on both)
- Run 3: Piecewise linear penalty below fair. Rec: beta_value=0.768, beta_unfairness_penalty=0.561. (Failed recovery on penalty param)
- Run 4: Piecewise linear, scaling `split_perc_self` vs `abs_perc_difference`. Rec: beta_gain_scale=0.058, beta_loss_penalty=0.302. (Failed recovery on both, very low)
- Run 5 (Run 54): Piecewise non-linear (`sqrt` vs `square`). Rec: beta_gain_sensitivity=0.332, beta_loss_sensitivity=0.596. (Failed recovery on both)

Pattern: Models often struggle to recover the parameter associated with fairness deviation or penalty terms, even when the parameter scaling the absolute value/gain is recovered well (Runs 1, 3). Simply scaling different terms (linear or non-linear) in piecewise models (Runs 4, 5) or combining them linearly (Runs 1, 2) hasn't consistently yielded distinct parameter influences for *both*.

New Strategy Idea: Combine a linear term for the absolute value (`split_perc_self`) with a *non-linear* term for *relative* fairness (ratio `R_val`). This attempts to create two dimensions of influence: absolute value sensitivity and non-linear relative fairness sensitivity.
Proposed structure: `U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)`
Need to define `R_val` and handle the `fair_perc_self_calc = 0` case.
`R_val = split_perc_self / fair_perc_self_calc` if `fair_perc_self_calc > 0`.
If `fair_perc_self_calc == 0` (participant contributed 0 tokens), the fair share is 0. Any offer > 0 is infinitely fair in ratio terms, but absolute value still matters. Setting `R_val` to a large constant (e.g., 100.0) if `split_perc_self > 0` and fair is 0 seems like a reasonable heuristic for a very high relative gain. If both are 0, `R_val` should be handled to produce a large negative log utility (representing getting nothing when expecting nothing, maybe very low utility). Let's define `R_val = IF(fair_perc_self_calc > 0, split_perc_self / (fair_perc_self_calc + epsilon), IF(split_perc_self > 0, 100.0, epsilon))` where epsilon is small. This avoids division by zero and log of zero. Using `(fair_perc_self_calc + epsilon)` in the denominator is slightly safer mathematically than checking `>0` then dividing. Let's refine:
`fair_perc_self_calc_safe = fair_perc_self_calc + epsilon` (add epsilon always to denominator)
`R_val = split_perc_self / fair_perc_self_calc_safe`
`U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)`
This looks cleaner. Need to define `fair_perc_self_calc_safe` and `epsilon` as calculated variables.

The instructions should:
- State run number (55).
- Restate primary objective (rec >= 0.7 for *both*).
- Summarize Run 54 results and failure (poor rec on both, using piecewise non-linear).
- Analyze pattern of failure across previous runs (fairness param recovery consistently failing).
- Propose the new strategy: combine linear `split_perc_self` with non-linear `log(R_val)`. Explain *why* this might help identifiability (absolute value vs. log-relative value).
- Provide the specific model structure proposed.
- Remind about 2 params, bounds, variable descriptions (all variables used!), target variable, summary.
- Encourage out-of-the-box thinking beyond the specific example, but stress the need for mathematically separable parameter influences.
- Include full task/variable context.

Make sure to update the Run number in the instructions title and body.
Ensure variable descriptions cover all variables in the proposed model structure, including calculated intermediates and constants like epsilon.
Use clear Python-compatible names for parameters.

Model structure to propose:
```
sum_tokens = token_self + token_opp
fair_perc_self_calc = IF(sum_tokens > 0, (token_self / sum_tokens) * 100.0, 50.0)
epsilon = 1e-6
fair_perc_self_calc_safe = fair_perc_self_calc + epsilon
R_val = split_perc_self / fair_perc_self_calc_safe
U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)
```
Variable descriptions needed for: `token_self`, `token_opp`, `split_perc_self` (data); `sum_tokens`, `fair_perc_self_calc`, `epsilon`, `fair_perc_self_calc_safe`, `R_val`, `U` (calculated); `beta_value`, `beta_relative_fairness` (learnable).

Check variable sources and learnable flags. Range for epsilon should be fixed min/max. Range for U should be -inf/inf. Bounds for learnable parameters should be finite and generous.

This new structure combines absolute value and relative fairness in a non-additive, non-piecewise way (within the second term). This might provide the necessary mathematical separation.
Okay, here are the improved instructions for the next run (Run 55), building on the results of the previous runs.

Instructions for Run 55 of 100:

The unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters. This is the critical benchmark for model usability. While improved BIC and accuracy are valuable secondary goals, they must not come at the expense of robust parameter recovery. A model with excellent fit but poor parameter identifiability is not suitable for drawing meaningful conclusions about the underlying cognitive processes.

The previous run (Run 54) implemented a piecewise non-linear model based on fairness difference (`U = (perc_difference >= 0) ? beta_gain_sensitivity * sqrt(perc_difference + epsilon) : beta_loss_sensitivity * (fair_perc_self_calc - split_perc_self + epsilon)^2`). It unfortunately failed the parameter recovery test for *both* `beta_gain_sensitivity` (r = 0.332) and `beta_loss_sensitivity` (r = 0.596). Its overall performance (BIC 449.36, Acc 0.697) was also relatively poor compared to the better-fitting models.

Analysis of Run 54's failure and previous models: The piecewise structure in Run 54 assigned the two learnable parameters to scale distinct non-linear functions (`sqrt` vs. `square`) of the fairness deviation in gain vs. loss regimes. However, this approach, similar to previous attempts that scaled linear terms (Run 4) or combined linear terms (Run 1, 3), did not create sufficiently distinct influences for the parameters to be reliably estimated. We've observed a pattern where parameters scaling the absolute value of the offer or gain component (`beta_value` in Runs 1 & 3) sometimes achieve decent recovery, but parameters specifically scaling the *fairness deviation* or *loss penalty* (`beta_fairness_deviation`, `beta_unfairness_penalty`, `beta_loss_sensitivity`) consistently fail to reach the 0.7 threshold. This strongly suggests the influence of the fairness deviation/penalty term is either too correlated with the absolute value term across trials, or its impact is not captured in a mathematically separable way by simply scaling a function of the deviation.

The core challenge persists: designing a mathematical structure for utility using exactly **2 learnable parameters** such that *both* parameters have truly distinct and identifiable influences on responder decisions (`trial_role == 1`). We need to find a way for the second parameter (the fairness/penalty related one) to exert its influence in a manner that is mathematically orthogonal or highly distinct from the first parameter's influence (the value/gain related one).

Based on this analysis, focus your exploration for Run 55 on model structures where the two learnable parameters contribute to utility in fundamentally different mathematical ways, beyond simply scaling different terms additively or piecewise. Consider structures where one parameter influences the sensitivity to the *absolute* value of the offer (`split_perc_self`), while the other parameter influences sensitivity to the *relative* fairness (e.g., the ratio `R_val = split_perc_self / fair_perc_self_calc`) via a non-linear transformation.

Consider this strategy to enhance identifiability:

*   **Combine Linear Value with Non-Linear Relative Fairness:** Design a model where one parameter scales the proposed percentage itself (`split_perc_self`), capturing a general sensitivity to the absolute value being offered, and the second parameter scales a non-linear function of the *ratio* of the proposed percentage to the calculated fair percentage (`R_val`). The non-linear function of the ratio should capture the specific impact of relative fairness/unfairness. This approach aims for parameter identifiability by having parameters scale terms that represent different dimensions (absolute vs. relative) via different mathematical operations (linear vs. non-linear transformation).
    *   *Structural Principle:* A potentially promising structure is `U = beta_value * split_perc_self + beta_relative_fairness * f(R_val)`, where `f` is a non-linear function like `log`, `sqrt`, or a power function. This allows `beta_value` to influence the baseline utility based on the offer size, while `beta_relative_fairness` modulates this based on how fair the offer is, via a distinct mathematical operation on the ratio.
    *   *Example Idea (using the logarithm of the ratio, addressing edge cases):* The logarithm of the ratio `log(R_val)` is 0 when the offer is fair (R_val=1), positive for favorable offers (R_val>1), and negative for unfavorable offers (R_val<1), offering a non-linear representation of relative value.
        *   `sum_tokens = token_self + token_opp`
        *   `fair_perc_self_calc = IF(sum_tokens > 0.0, (token_self / sum_tokens) * 100.0, 50.0)` (Ensure this handles `sum_tokens=0`).
        *   `epsilon = 1e-6` (Small constant to handle division by zero and log of zero/negative).
        *   `fair_perc_self_calc_safe = fair_perc_self_calc + epsilon` (Add epsilon to denominator to prevent division by zero when fair_perc_self_calc is 0).
        *   `R_val = split_perc_self / fair_perc_self_calc_safe` (Calculate the ratio using the safe fair value).
        *   `U = beta_value * split_perc_self + beta_relative_fairness * log(R_val + epsilon)` (Combine linear split percentage with log-transformed ratio).

*   **Explore Other Structures:** If the above specific example doesn't seem optimal, continue to think creatively about how two parameters can control mathematically separable aspects of the utility function. Could one parameter control a threshold or an exponent in a non-trivial way? Could they define the shape of a smooth function (e.g., sigmoid, power law) that takes fairness deviation as input, where the parameters affect different parameters of the function (e.g., slope and inflection point)? Avoid structures that simply scale linearly dependent terms or rely solely on simple piecewise linear or piecewise scaled non-linear functions, as these have failed recovery for one or both parameters previously.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc`, and necessary intermediate terms like `R_val`, `epsilon`, `fair_perc_self_calc_safe`. Ensure `fair_perc_self_calc` is handled correctly when `sum_tokens` is 0.

*   **Model Formula:** Define the utility (`U`). Include calculation steps for necessary derived variables *before* the final formula for `U`. Use clear mathematical notation within the <MODEL> tags.

*   **Learnable Parameters:** Your model *must* have exactly **2 learnable parameters**. Choose descriptive names suitable for Python code. Define clear, generous, finite numerical bounds (e.g., [-50.0, 50.0] or wider/narrower if appropriate). Consider the expected sign and scale, but keep bounds wide enough.

*   **Calculated Variables:** Ensure all calculated variables used in the model formula (including `sum_tokens`, `fair_perc_self_calc`, intermediate terms like `epsilon`, `fair_perc_self_calc_safe`, `R_val`, and `U`) are included in your <VARIABLES> descriptions with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`. Ensure the description for `fair_perc_self_calc` explicitly mentions the `sum_tokens > 0` condition and the default value.

*   **Model Specification:** Provide the formal mathematical formula between <MODEL> tags. Only the formula should be inside these tags.

*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables used in the <MODEL> formula (data, calculated, learnable). Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Use `split_perc_self` as the variable name for the participant's proposed percentage share percentage.

*   **Target Variable:** Specify ""accept"" as the target variable using <target_variable> tags.

*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea and, crucially, explain how its specific mathematical structure is designed to ensure parameter identifiability for *both* learnable parameters (e.g., by having them scale distinct mathematical forms like linear value and non-linear relative fairness).

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters is the absolute priority. Your goal is to find a mathematical structure where both parameters can be uniquely and reliably estimated from the data by ensuring their influences are clearly distinguishable. Previous attempts scaling different terms (linear or non-linear) within or across regimes have struggled, particularly with the fairness/penalty parameter. Explore combining a term based on absolute value with a term based on a non-linear transformation of relative fairness (like the log of the ratio), or other structures that create highly distinct roles for the two parameters. Be creative in designing the mathematical relationship between inputs, parameters, and utility to achieve this separation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
55,,,,,Error: unsupported format string passed to NoneType.__format__,,v0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
69,,,,,"sum_tokens = token_self + token_opp
fair_perc_self_calc = (token_self / sum_tokens) * 100 if sum_tokens > 0 else 50.0
deviation = fair_perc_self_calc - split_perc_self
positive_deviation = max(0.0, deviation)
safe_split_perc_self = split_perc_self + 1e-6
ratio_penalty = positive_deviation / safe_split_perc_self
U_accept = beta_value * split_perc_self - beta_unfairness * ratio_penalty","An additive utility model predicting the decision to accept or reject an offer in an ultimatum game. The utility of accepting is calculated as the product of a value sensitivity parameter (beta_value) and the percentage offered to the participant (split_perc_self), minus the product of an unfairness sensitivity parameter (beta_unfairness) and a ratio-based unfairness penalty (ratio_penalty). The ratio_penalty is defined as the positive deviation from the calculated fair share percentage, divided by the offered percentage (plus a small epsilon to prevent division by zero). This specific non-linear form of the penalty term, particularly its steep increase for low offers below fairness, is hypothesized to create a mathematically distinct influence profile for beta_unfairness compared to the linear influence of beta_value on split_perc_self within the additive structure. This distinction is intended to improve the robust parameter identifiability for both learnable parameters during model fitting and parameter recovery tests, addressing limitations observed in previous additive and ratio-based models. The model utilizes two non-negative learnable parameters: beta_value and beta_unfairness.",v0,"Okay, here are the improved instructions for the next run (Run 69), building on the analysis of previous attempts and maintaining the intense focus on achieving robust parameter identifiability for *both* learnable parameters.

Instructions for Run 69 of 100:

The **unwavering primary objective remains achieving parameter recovery >= 0.7 for *all* learnable parameters**. This is the critical benchmark for model usability. While improved BIC (lower is better) and accuracy (higher is better) are valuable secondary goals, they must not come at the expense of robust parameter recovery.

Run 68 explored a ratio structure (`U = (beta_value * split_perc_self) / (1.0 + beta_unfairness * NonLinearPenaltyTerm)`) using a squared positive difference as the non-linear penalty (`max(0.0, fair_perc_self_calc - split_perc_self)^2`). While this model showed a relatively good average BIC (44.58) and moderate accuracy (0.673), it failed the primary objective, yielding parameter recoveries of only r=0.497 for `beta_value` and r=0.445 for `beta_unfairness`. This result, combined with the history of other ratio models struggling with `beta_unfairness` recovery (e.g., Run 61, 63, 67), strongly suggests that putting the `beta_unfairness` term scaling a penalty in the denominator of a ratio structure, alongside `beta_value` scaling `split_perc_self` in the numerator, might inherently lead to identifiability issues due to correlated influences across offer ranges.

The challenge persists: how to mathematically disentangle the influence of sensitivity to the *value* of the offer (`beta_value` scaling `split_perc_self`) from the influence of sensitivity to *unfairness* (`beta_unfairness` scaling some measure of deviation from fairness)? Previous attempts using additive models (like Run 66) also failed parameter recovery when using relatively simple penalty forms.

For Run 69, let's return to an **additive utility structure**, but critically, use a **new and distinct functional form for the unfairness penalty term** that we hope will create a unique parameter landscape for `beta_unfairness`, allowing it to be disentangled from `beta_value`. Instead of a simple difference or the previous ratio penalties (`(Fair/Split) - 1`), let's define the penalty based on the **positive deviation from fairness *relative to the offered percentage***.

Proposed Model Structure: An additive model where utility is the value of the offer minus a penalty proportional to unfairness.
`U = beta_value * split_perc_self - beta_unfairness * RatioBasedPenalty`

Here, `RatioBasedPenalty` is calculated as the positive difference between the fair share and the proposed share, divided by the proposed share (plus a small epsilon to prevent division by zero). Specifically:
Calculate `fair_perc_self_calc` handling the `sum_tokens == 0` case (defaulting to 50.0).
Calculate `deviation = fair_perc_self_calc - split_perc_self`.
Define `positive_deviation = max(0.0, deviation)`. This ensures the penalty is only applied for offers below the fair share.
Define `safe_split_perc_self = split_perc_self + 1e-6` (using a small constant like 1e-6 to avoid division by zero, especially when `split_perc_self` is 0).
Define the `RatioBasedPenalty = positive_deviation / safe_split_perc_self`.
Finally, `U_accept = beta_value * split_perc_self - beta_unfairness * RatioBasedPenalty`.

The theoretical rationale for this specific penalty term (`(Fair - Split) / Split`) is that it grows non-linearly and increasingly steeply as the offer (`split_perc_self`) gets smaller, when it's below the fair share. This creates a potentially different influence profile for `beta_unfairness` (scaling this rapidly changing penalty) compared to `beta_value` (scaling the linearly changing `split_perc_self`) across the range of possible offers. This is an attempt to find a penalty form whose impact, when scaled by `beta_unfairness` in an additive model, is less correlated with the scaled value term than previous attempts.

Design the model using the additive structure `U_accept = beta_value * split_perc_self - beta_unfairness * RatioBasedPenalty`, where `RatioBasedPenalty` is calculated as specified above.

Focus on clearly defining the `RatioBasedPenalty` and articulating in the summary how its specific functional form (positive deviation divided by the offered percentage) within an additive utility structure is intended to help disentangle the influences of `beta_value` (scaling the linear value) and `beta_unfairness` (scaling this non-linear ratio penalty), thereby significantly improving robust parameter identifiability for *both* learnable parameters. Explain why this approach represents a departure from previous additive models and the recent ratio models.

*   **Key Inputs & Calculated Variables:** Use `split_perc_self`, `token_self`, `token_opp` as data variables. Calculate `sum_tokens`, `fair_perc_self_calc` (handling `sum_tokens == 0`), `deviation`, `positive_deviation`, `safe_split_perc_self`, `ratio_penalty`, and `U_accept`. Include any other necessary intermediate variables.
*   **Model Formula:** Define `U_accept` using the additive structure: `U_accept = beta_value * split_perc_self - beta_unfairness * ratio_penalty`. Include calculation steps for derived variables *before* the formula for `U_accept`. Use clear mathematical notation within the <MODEL> tags (only the formula). Ensure you handle potential division by zero explicitly with the small epsilon (e.g., 1e-6).
*   **Learnable Parameters:** Exactly **2 learnable parameters**: `beta_value` and `beta_unfairness`. Choose descriptive, Python-safe names. Define clear, generous, finite numerical bounds. Non-negative bounds (e.g., `[0.0, 100.0]` for both) are theoretically appropriate and may aid recovery, but consider if allowing negative values for `beta_value` (risk-seeking for unfair offers) or `beta_unfairness` (liking unfairness) is theoretically plausible in this context. Let's stick with non-negative bounds for now unless the model strongly suggests otherwise.
*   **Calculated Variables:** Include descriptions for *all* calculated variables used in the model (including `U_accept`, `sum_tokens`, `fair_perc_self_calc`, `deviation`, `positive_deviation`, `safe_split_perc_self`, and `ratio_penalty`) with `source: ""calculated""`. Include descriptions for all data variables used as inputs (`token_self`, `token_opp`, `split_perc_self`) with `source: ""data""`.
*   **Variable Descriptions:** Provide variable descriptions in JSON format between <VARIABLES> tags for *all* variables in the <MODEL> formula. Include source, range (with inclusive flags), distribution (if applicable), and learnable status. Remember variable names must exactly match the model.
*   **Target Variable:** Specify ""accept"" using <target_variable> tags.
*   **Summary:** Provide a concise summary between <SUMMARY> tags. Describe the model's core idea (additive structure, value term, ratio-based penalty term). Crucially, explain *how the definition of the penalty term as the positive deviation from fair share percentage divided by the offered percentage* within the additive structure is specifically intended to improve robust parameter identifiability for *both* learnable parameters by creating mathematically distinct non-linear influences for `beta_unfairness` compared to the linear influence of `beta_value`. Explain how this builds on previous attempts by exploring a new combination of additive structure and a distinctly shaped penalty term.

Available Data Variables:
- trial_type, trial_role, token_opp, token_self, combined_earning, split_opp, split_self, splitperc_opp, split_perc_self, accept, accepted_amount, accepted_perc, proposed_perc, proposed_amount.
(Note: Use `split_perc_self` for the participant's proposed percentage share percentage).

Please think through this step by step, then provide your model specification and variable descriptions. Remember that robust parameter identifiability for *both* learnable parameters (>= 0.7 recovery for both) is the absolute priority. Your goal is to find a structure where both parameters' influences are clearly distinguishable across the range of possible offers, leading to reliable estimates. Be creative in how you define and utilize the ratio-based penalty term or alternative interactions within an additive structure to maximize this distinction while remaining theoretically plausible. Find a model structure that truly shines in parameter recovery.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
69,,,,,Error: unsupported format string passed to NoneType.__format__,,v0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
