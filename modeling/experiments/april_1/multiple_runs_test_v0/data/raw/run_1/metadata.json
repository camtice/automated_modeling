{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility function should be such that a negative utility leads to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The main target is \"accept\", which is binary (1 or 0). The model should predict this based on the utility of the proposed split. The variables provided include token_self, token_opp, combined_earning, split_self, split_opp, etc. Since the task involves fairness considerations, the model should account for how the participant perceives the fairness of the split relative to their own contribution and the opponent's contribution.\n\nIn the experiment, each trial's combined earning is the total money from both participants' tokens. Each token is \u00a33, so combined_earning = 3*(token_self + token_opp). The split_self and split_opp are the proposed shares. However, the note mentions that equal split_self values don't represent the same fairness because the combined earning varies. So, fairness should be evaluated relative to contributions, not just absolute splits.\n\nThe key psychological mechanisms here are likely fairness considerations and self-interest. Participants might compare the proposed split to what they consider a fair share based on their contribution. If the proposed split is below their fair share, they might be more likely to reject it, even if it means getting nothing (as in ultimatum games). But self-interest could also play a role\u2014maybe participants have a threshold of what's acceptable, considering both their own gain and the fairness.\n\nSo, the utility function should incorporate both the absolute amount the participant receives (split_self) and the fairness of the split relative to contributions. Let's think about how to model this.\n\nFairness can be calculated as the participant's contribution proportion. The participant's contribution is token_self, and the total contribution is token_self + token_opp. So, the fair share percentage would be (token_self / (token_self + token_opp)) * 100. The proposed split percentage is splitperc_self. The difference between these two could be a measure of unfairness. If the proposed percentage is less than the fair percentage, the participant might perceive it as unfair.\n\nAlternatively, the fair split in monetary terms would be (token_self / (token_self + token_opp)) * combined_earning. The proposed split_self can be compared to this fair amount. The difference (split_self - fair_share) could be a component in the utility. If this difference is negative (proposed is less than fair), utility decreases.\n\nBut the model also needs to account for the participant's sensitivity to unfairness. Some participants might be more tolerant of receiving less than their fair share, while others might strongly reject it. This suggests a parameter that scales the impact of unfairness on utility.\n\nAdditionally, the absolute amount of split_self might matter. Even if the split is unfair, a higher absolute amount might still be acceptable. So, the utility could have a term for the split_self (self-interest) and a term for the fairness deviation (fairness consideration).\n\nPutting this together, the utility (U) could be modeled as:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - fair_share)\n\nWhere \u03b1 is the weight on self-interest, and \u03b2 is the weight on fairness. However, this might need adjustment. Alternatively, the fairness term could be the difference between the proposed split and the fair share, scaled by a parameter. If the proposed split is less than fair, that's negative, reducing utility.\n\nWait, fair_share is (token_self / (token_self + token_opp)) * combined_earning. Since combined_earning is 3*(token_self + token_opp), this simplifies to 3*token_self. So fair_share is 3*token_self. Wait, that's interesting. Because combined_earning is 3*(sum of tokens), so the fair share based on contribution would be 3*token_self. So split_self is the proposed amount, and the fair share is 3*token_self. Therefore, the difference (split_self - 3*token_self) represents how much the proposed amount deviates from the fair share. If this is positive, the participant is getting more than their fair share; if negative, less.\n\nSo, the utility could be:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - 3*token_self)\n\nBut maybe the fairness term should be relative. For example, if the participant is getting less than their fair share, the negative term would reduce utility. Alternatively, the model could use the ratio of proposed to fair share. But using the difference might be simpler.\n\nAlternatively, the model could consider the ratio of the proposed split to the fair share. For example, (split_self / (3*token_self)) - 1. If this is negative, it's unfair. But using the difference might be more straightforward.\n\nAnother angle is to model the utility as the proposed amount minus a term that penalizes unfairness. The unfairness could be the difference between the fair share and the proposed amount, but only when the proposed amount is less than the fair share. So, something like:\n\nU = split_self - \u03b8 * max(0, fair_share - split_self)\n\nHere, \u03b8 is a parameter representing the sensitivity to being shortchanged. If the proposed split is less than the fair share, the participant experiences a penalty proportional to the deficit. This is similar to models in inequity aversion, where individuals dislike unequal outcomes.\n\nAlternatively, the Fehr and Schmidt model of inequity aversion could be adapted. In their model, utility depends on one's own payoff and the difference between one's own and others' payoffs. But in this case, the opponent's split is split_opp, which is (combined_earning - split_self). So, the participant's utility might be:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nBut this is a standard inequity aversion model. However, in this task, the opponent's contribution (token_opp) and the participant's contribution (token_self) might influence what's considered fair. The Fehr and Schmidt model doesn't account for differing contributions, only the final splits. So maybe the fairness here is based on contributions, not just final splits.\n\nSo, combining both the absolute split and the contribution-based fairness. For example, the fair split is 3*token_self, and the proposed split_self is compared to that. The utility could be:\n\nU = split_self - \u03b3 * max(3*token_self - split_self, 0)\n\nHere, \u03b3 is the sensitivity to being below the fair share. If the proposed split_self is less than the fair share (3*token_self), the utility is reduced by \u03b3 times the deficit. If it's equal or more, there's no penalty. This model captures self-interest (split_self) and fairness based on contribution.\n\nAlternatively, the model could include both advantageous and disadvantageous inequity. For example, if the participant gets more than their fair share, they might feel guilty, but perhaps that's less likely. The main driver might be when they get less. So focusing on the deficit makes sense.\n\nAnother consideration is the role of the trial_type variable, which indicates whether contributions are equal, opponent more, or participant more. But since the model is supposed to use variables from the dataset, and trial_type is a categorical variable, maybe it's better to derive the necessary information from token_self and token_opp instead of using trial_type directly. Because token_self and token_opp are continuous, they can provide a more precise measure of contribution difference.\n\nSo, the key variables to include are split_self (the proposed amount), token_self (participant's contribution), and token_opp (opponent's contribution). From these, we can compute the fair share as 3*token_self. The difference between split_self and fair_share is (split_self - 3*token_self). If this is negative, the participant is getting less than their fair share.\n\nThe utility function could then be:\n\nU = split_self + \u03b1 * (split_self - 3*token_self)\n\nBut this would mean that when split_self is less than fair share, the term is negative, reducing utility. However, the parameter \u03b1 would determine how much that affects the utility. If \u03b1 is positive, then getting more than fair share increases utility, but getting less decreases it. Wait, but in that case, the model would have U = split_self + \u03b1*(split_self - fair_share) = (1 + \u03b1)*split_self - \u03b1*fair_share. That might not capture the intended effect. Alternatively, perhaps the model should have a term that penalizes the deficit.\n\nAlternatively, U = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b1 is the penalty for being below the fair share. If split_self is less than fair_share, the penalty is \u03b1*(fair_share - split_self). Otherwise, no penalty. This way, the utility is the proposed amount minus a penalty proportional to how much they're being shortchanged.\n\nThis seems plausible. The participant's utility is their own split, but they subtract a term that represents their aversion to receiving less than their fair share. The parameter \u03b1 determines how much they care about this fairness. If \u03b1 is zero, they only care about their own split. If \u03b1 is positive, they dislike being below the fair share.\n\nAnother possible component is the absolute amount of the split. For example, even if the split is unfair, a higher absolute amount might be more acceptable. But in this model, the split_self is already part of the utility. So, the model combines self-interest (split_self) with fairness consideration (penalty for being below fair share).\n\nNow, the probability of accepting is determined by the utility through a logistic function. The probability P(accept) = 1 / (1 + exp(-U)). So, if U is positive, the probability is greater than 0.5; if negative, less than 0.5. The requirement is that when utility is less than 0, the probability is less than 50%, which aligns with the logistic function.\n\nBut wait, the problem states: \"Predict utility, where less than 0 is less than 50% likely.\" So the utility U is passed through a logistic function to get the probability. So the model's U can be any real number, and the probability is logistic(U). So when U < 0, probability < 0.5.\n\nSo the model's utility function should be designed such that when U is negative, the participant is less likely to accept. The parameters in the utility function (like \u03b1) need to be learnable.\n\nNow, considering the parameters. The model needs learnable parameters with finite bounds. In the example above, \u03b1 is a learnable parameter. What's a reasonable range for \u03b1? Since split_self and fair_share are in pounds, the penalty term is in pounds multiplied by \u03b1. If split_self is in the range of, say, 0 to combined_earning (which is up to 3*(token_self + token_opp)), but token_self and token_opp are integers. Let's say combined_earning could be up to, say, 3*(36 + 36) = 216 (if each found 36 tokens, but the actual max depends on the data). However, the split_self would be a portion of that. The fair_share is 3*token_self, so if token_self is, say, 10, fair_share is 30. If the proposed split_self is 20, then the deficit is 10, and the penalty is \u03b1*10. So \u03b1 needs to be a positive value. If \u03b1 is 0.5, then the penalty is 5, so U = 20 - 5 = 15. If \u03b1 is 2, penalty is 20, U=0. So the participant would be indifferent (50% chance) when split_self - \u03b1*(fair_share - split_self) = 0. Wait, no: in the model U = split_self - \u03b1*max(fair_share - split_self, 0). So if split_self is 20, fair_share is 30, then U = 20 - \u03b1*10. For U to be zero, \u03b1 would need to be 2. So if \u03b1 is 2, then U=0 when split_self is 20. So the participant would have 50% chance of accepting. If \u03b1 is higher than 2, then U would be negative, leading to rejection.\n\nBut how to set the bounds for \u03b1? Since split_self and fair_share can vary, \u03b1 could be any positive number. However, to prevent extreme values, we can set a generous bound. For example, \u03b1 could be between 0 and 5. If \u03b1 is 5, then even a small deficit would lead to a large penalty. But maybe the upper bound can be higher. However, for parameter recovery, it's better to have reasonable bounds. Let's say \u03b1 is between 0 and 10. That's inclusive. So the range for \u03b1 is [0, 10].\n\nBut perhaps another parameter is needed. For example, maybe participants have a different sensitivity to the deficit. Or maybe there's a baseline acceptance bias. For instance, some participants might generally be more accepting or rejecting regardless of the split. So adding an intercept term could help. Let's consider:\n\nU = \u03b20 + \u03b21 * split_self - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b20 is a bias term. If \u03b20 is positive, the participant is more likely to accept even if the split is unfavorable. If \u03b20 is negative, they are more prone to reject. \u03b21 represents the sensitivity to the proposed split. But since split_self is already in pounds, \u03b21 could be 1 (assuming linear utility in money), but perhaps participants have different marginal utilities. However, adding \u03b21 as a learnable parameter might complicate the model. Alternatively, fixing \u03b21 to 1 and learning \u03b1 and \u03b20. But the problem states that the model should have parameters with finite bounds. If we include \u03b20 and \u03b1 as learnable parameters, that's two parameters. However, the instructions suggest a slight bias towards fewer parameters. So maybe just \u03b1 and \u03b20. But let's think: if \u03b20 is included, it's an intercept term. For example, even if the split is fair, the participant might have a general tendency to accept or reject. But in the task, when the split is exactly fair, the utility would be split_self (since the penalty term is zero) plus \u03b20. If \u03b20 is positive, that increases the chance of acceptance. But maybe \u03b20 is not necessary. Let's see.\n\nIf we omit \u03b20, the model is U = split_self - \u03b1 * max(fair_share - split_self, 0). Then, when the split is exactly fair (split_self = fair_share), U = split_self. Since split_self is part of the utility, and the logistic function converts that to a probability. For example, if split_self is 30, then U=30. The probability would be almost 1. But in reality, participants might not always accept even fair splits. So including \u03b20 could capture a baseline tendency. However, adding parameters increases model complexity, which might affect parameter recovery. The user mentioned that parameter recovery is a priority, so the model should have parameters that are identifiable. If two parameters are highly correlated, recovery would be difficult. \n\nAlternatively, maybe the intercept is not necessary. Let's consider that when the split is exactly fair, the utility is split_self. If split_self is high enough, the probability is high. But maybe participants have a different baseline. For example, even if the split is fair, if the absolute amount is low, they might reject. But in the current model, the utility is split_self minus the penalty. So when the split is fair, U = split_self. So higher split_self leads to higher utility. That makes sense. For example, if two trials have the same fairness (split_self equals fair_share), but different combined_earning, the split_self would be higher in the trial with higher combined_earning, leading to higher utility. That aligns with the note that equal split_self percentages don't represent the same fairness because combined_earning varies. Wait, no: split_self in pounds is part of the utility. So even if two trials have the same split percentage but different combined_earning, the split_self in pounds would be different. For example, if combined_earning is 60, split_self 30 (50%) vs. combined_earning 120, split_self 60 (50%). The utility would be 30 vs. 60, leading to higher utility in the second case. So the model accounts for the absolute amount, which is correct.\n\nBut in the case where the split is exactly fair (split_self = 3*token_self), the utility is split_self. So participants are more likely to accept higher fair splits. That makes sense. For example, if token_self is 10, fair_share is 30. If the proposed split_self is 30, utility is 30. If token_self is 20, fair_share is 60, proposed split_self is 60, utility is 60. So higher absolute amounts lead to higher acceptance rates, which is logical.\n\nBut what if the proposed split is more than the fair share? For example, token_self is 10, fair_share 30, but split_self is 40. Then, the penalty term is zero, so U = 40. The participant would likely accept. That's reasonable.\n\nNow, considering the parameters. The model as U = split_self - \u03b1 * max(fair_share - split_self, 0) has one learnable parameter, \u03b1. But maybe we need another parameter to scale the split_self. For example, participants might not value each pound linearly. So U = \u03b2 * split_self - \u03b1 * max(fair_share - split_self, 0). Here, \u03b2 is the marginal utility of money. If \u03b2 is fixed to 1, then it's linear. But allowing \u03b2 to vary could capture diminishing returns. However, adding \u03b2 as a learnable parameter would increase model complexity. The problem states to have a slight bias towards fewer parameters. So maybe keep \u03b2 fixed at 1. But if the data suggests that participants have different sensitivities to the absolute amount, then \u03b2 should be a parameter. However, given the parameter recovery concern, perhaps it's better to have one parameter. Let's proceed with \u03b1 as the only learnable parameter. Wait, but then the model would be U = split_self - \u03b1 * deficit. But maybe participants have different baseline propensities. For example, some might require a higher utility threshold to accept. Wait, the logistic function includes a bias term inherently. Because the probability is 1/(1 + exp(-U)). So if U is split_self - \u03b1*deficit, then when U=0, the probability is 0.5. So the model's utility function's intercept is determined by the parameters. But without an additional parameter, the model might not capture individual differences in baseline acceptance. For example, two participants with the same \u03b1 but different baseline tendencies would not be captured. So perhaps adding an intercept term \u03b20 would help. Let's consider:\n\nU = \u03b20 + split_self - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b20 is a learnable parameter representing the baseline tendency to accept (if positive) or reject (if negative). This allows for individual differences in overall acceptance rates, independent of the split. However, this adds another parameter. But with two parameters (\u03b20 and \u03b1), the model might still be recoverable if they are not highly correlated. The problem allows for a few parameters, as long as they are not too many. The example had two learnable parameters. So maybe this is acceptable.\n\nAlternatively, perhaps the intercept is not necessary. Let's think: when the proposed split is exactly the fair share, U = split_self. So for a given split_self, the utility is higher when the split is fair. But if the split_self is very low, even if it's fair, the utility might be negative. For example, if token_self is 1, fair_share is 3, split_self is 3. Then U = 3. So the probability is high. But if token_self is 1 and split_self is 3 (fair), but combined_earning is 6 (so split_opp is 3), then the participant gets 3, which is fair, and the utility is 3. So they are likely to accept. But if the split_self is 3 but the fair share is 30 (token_self=10), then U=30, which is higher. So the model accounts for the absolute amount. However, in cases where the split is fair but the absolute amount is low, the participant might still reject. But according to the model, U would be split_self, which is positive, leading to P(accept) > 0.5. But maybe in reality, participants might reject low but fair offers. To capture this, the model needs a parameter that adjusts the baseline. For example, if \u03b20 is negative, even a fair split might lead to U = \u03b20 + split_self. If split_self is small, U could be negative. For example, \u03b20 = -5, split_self=3: U = -5 +3 = -2, leading to P(accept) < 0.5. So the intercept allows for a general bias towards acceptance or rejection.\n\nTherefore, including \u03b20 as a learnable parameter might improve the model's flexibility. So the model would have two parameters: \u03b20 (intercept) and \u03b1 (fairness sensitivity). Both need to have bounds. For \u03b20, perhaps a range from -10 to 10, which is generous. For \u03b1, a range from 0 to 10 (since it's a penalty, should be non-negative). \n\nSo the utility function is:\n\nU = \u03b20 + split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nThis captures the baseline tendency (\u03b20), the self-interest (split_self), and the penalty for being below the fair share (\u03b1 times the deficit). \n\nNow, mapping the variables in the dataset:\n\n- split_self is a data variable (float64)\n- token_self is a data variable (int64)\n- 3*token_self is calculated as fair_share\n- max(3*token_self - split_self, 0) is calculated as the deficit\n\nThe learnable parameters are \u03b20 and \u03b1. \n\nThe target variable is \"accept\", which is binary. The model's utility U is passed through a logistic function to get the probability of acceptance. \n\nNow, the variables section needs to describe each component:\n\n- \u03b20: learnable, with range [-10, 10]\n- \u03b1: learnable, with range [0, 10]\n- split_self: from data\n- token_self: from data\n- max(...): calculated\n\nBut in the model formula, the variables used are split_self, token_self, \u03b20, and \u03b1. The max term is part of the calculation.\n\nSo the variables in the JSON would be:\n\n- beta0: learnable, description \"Baseline acceptance tendency\"\n- alpha: learnable, description \"Sensitivity to deficit below fair share\"\n- split_self: data variable\n- token_self: data variable\n- fair_share: calculated (3 * token_self)\n- deficit: calculated (max(fair_share - split_self, 0))\n\nBut in the model formula, the max term is part of the equation. So the variables used directly in the model are beta0, split_self, alpha, and the max term. However, in the variables section, we need to list all variables that are part of the model, including those calculated.\n\nWait, the model's mathematical formula is:\n\nU = beta0 + split_self - alpha * max(3*token_self - split_self, 0)\n\nSo the variables involved are:\n\n- beta0 (learnable)\n- split_self (data)\n- alpha (learnable)\n- token_self (data)\n- 3*token_self (calculated as fair_share)\n- max(fair_share - split_self, 0) (calculated as deficit)\n\nBut in the variables section, we need to list each variable that appears in the model. However, the model's formula can include calculated terms as long as they are defined. For example, fair_share is 3*token_self, which is calculated from token_self. The deficit is max(fair_share - split_self, 0), which is calculated from fair_share and split_self.\n\nBut in the variables section, we need to list all variables that are part of the model. So:\n\nVariables:\n\n- beta0: learnable, range [-10, 10]\n- alpha: learnable, range [0, 10]\n- split_self: data\n- token_self: data\n- fair_share: calculated (3 * token_self)\n- deficit: calculated (max(fair_share - split_self, 0))\n\nBut in the model formula, these calculated variables are not explicitly named; they are part of the equation. However, the variables section should list all variables used in the model, even if they are intermediate calculations. Alternatively, the model can directly use the data variables without explicitly naming the intermediate terms. For example, the model can be written as:\n\nU = beta0 + split_self - alpha * max(3*token_self - split_self, 0)\n\nIn this case, the variables used in the formula are beta0, split_self, alpha, token_self. The terms 3*token_self and max(...) are calculated on the fly. Therefore, in the variables section, we need to list beta0, alpha, split_self, and token_self. The other terms are derived from these and don't need to be listed as separate variables.\n\nSo the variables section would include:\n\n- beta0: learnable, description \"Baseline utility intercept\"\n- alpha: learnable, description \"Sensitivity to receiving less than fair share\"\n- split_self: data variable\n- token_self: data variable\n\nThe other terms (fair_share and deficit) are calculated within the model formula but are not separate variables in the JSON.\n\nTherefore, the variables JSON would be:\n\n{\n  \"variables\": {\n    \"beta0\": {\n      \"description\": \"Baseline utility intercept, capturing general acceptance tendency\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity parameter for the penalty when receiving less than the fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant, determining their fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThis covers all variables used in the model. The calculated terms (fair_share and deficit) are part of the model's formula but are not listed as separate variables since they are derived from token_self and split_self.\n\nNow, checking if this model meets the requirements:\n\n1. Predicts responder behavior (targeting \"accept\"): Yes, the utility is used in a logistic model to predict the probability of acceptance.\n\n2. Uses variables from the dataset: Uses split_self and token_self, which are provided.\n\n3. Learnable parameters have finite bounds: beta0 is [-10, 10], alpha is [0, 10]. These are generous but finite.\n\n4. Utility can be unbounded, which it is since split_self and token_self can vary.\n\nThe model's utility function combines baseline tendency, self-interest (split_self), and a penalty for being below the fair share. The parameters beta0 and alpha are learnable, with specified bounds. This should allow for parameter recovery as long as the parameters are identifiable, which they should be since beta0 affects the baseline and alpha scales the penalty term.\n\nAnother consideration: Does the model account for the opponent's contribution (token_opp)? The fair share is based on the participant's own contribution (token_self), but the opponent's contribution affects the combined_earning. However, in the model, the fair share is 3*token_self, which is independent of the opponent's contribution. Wait, but combined_earning is 3*(token_self + token_opp). The fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning = 3*token_self. So the model correctly uses the participant's contribution to determine the fair share, regardless of the opponent's contribution. This aligns with the note that two equal split_self values don't represent the same fairness because combined_earning (and thus token_opp) can vary. However, in the model, the fair share is 3*token_self, which is fixed based on the participant's own tokens. So even if the opponent's tokens vary, the fair share is determined solely by the participant's tokens. Is this correct?\n\nWait, the fair share should be based on the participant's contribution relative to the total. For example, if the participant contributed 10 tokens and the opponent 20, the total is 30 tokens. The participant's fair share would be (10/30)*combined_earning = (1/3)*combined_earning. But combined_earning is 3*(10+20) = 90. So fair_share is 30. But according to the model, fair_share is 3*token_self = 3*10=30. So in this case, it's the same. However, if the participant contributed 10 and the opponent 10, combined_earning is 60. Fair share would be (10/20)*60 = 30, which is again 3*10=30. So in all cases, the fair share based on contribution is 3*token_self, regardless of the opponent's tokens. Therefore, the model correctly captures the fair share as 3*token_self. This is a neat simplification, as it avoids needing to use token_opp in the model. Therefore, the model only needs token_self and split_self, which are available in the dataset.\n\nThis is a crucial insight. The fair share can be computed as 3*token_self, which is the participant's contribution in pounds. This simplifies the model and avoids needing to use token_opp or combined_earning directly. Therefore, the model is correctly specified.\n\nIn summary, the model uses the participant's proposed split (split_self) and their own contribution (token_self) to compute a utility that combines self-interest and fairness considerations. The utility is passed through a logistic function to predict the probability of acceptance. The learnable parameters are beta0 (baseline tendency) and alpha (sensitivity to unfairness), both with reasonable bounds to aid parameter recovery.\n</think>\n\n<MODEL>\nU = beta0 + split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"beta0\": {\n      \"description\": \"Baseline utility intercept capturing general acceptance/rejection tendency\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to receiving less than the fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token count used to compute fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA logistic choice model where utility combines baseline tendency, self-interest (split_self), and a penalty for receiving less than the contribution-based fair share (3 * token_self). The penalty scales with sensitivity parameter \u03b1. Learnable parameters \u03b20 (baseline) and \u03b1 (fairness sensitivity) have bounded ranges to ensure identifiability. The model directly uses observable split amounts and token contributions without requiring opponent-specific variables.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['beta0', 'alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = beta0 + split_self - alpha * max(3 * token_self - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"beta0\": {\n      \"description\": \"Baseline utility intercept capturing general acceptance/rejection tendency\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to receiving less than the fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token count used to compute fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, beta0=0.0, alpha=0.0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0.0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        fair_share = 3 * token_self\n        deficit = max(fair_share - split_self, 0)\n        \n        utility = beta0 + split_self - alpha * deficit\n        \n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"A\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 12, \"token_self\": 12, \"combined_earning\": 72, \"split_opp\": 36.0, \"split_self\": 36.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 36.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 36.0}, {\"ID\": 1, \"group\": \"A\", \"trial_number\": 2, \"trial_type\": 2, \"trial_role\": 1, \"token_opp\": 16, \"token_self\": 8, \"combined_earning\": 72, \"split_opp\": 47.0, \"split_self\": 25.0, \"splitperc_opp\": 65, \"splitperc_self\": 35, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 35.0, \"proposed_amount\": 25.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1743512882.940182
    }
  ],
  "learnable_parameters": {
    "beta0": {
      "description": "Baseline utility intercept capturing general acceptance/rejection tendency",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to receiving less than the fair share (3 * token_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = beta0 + split_self - alpha * max(3 * token_self - split_self, 0)",
  "variable_descriptions": {
    "beta0": {
      "description": "Baseline utility intercept capturing general acceptance/rejection tendency",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to receiving less than the fair share (3 * token_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed monetary amount for the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Participant's token count used to compute fair share (3 * token_self)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A logistic choice model where utility combines baseline tendency, self-interest (split_self), and a penalty for receiving less than the contribution-based fair share (3 * token_self). The penalty scales with sensitivity parameter \u03b1. Learnable parameters \u03b20 (baseline) and \u03b1 (fairness sensitivity) have bounded ranges to ensure identifiability. The model directly uses observable split amounts and token contributions without requiring opponent-specific variables.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility function should be such that a negative utility leads to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The main target is \"accept\", which is binary (1 or 0). The model should predict this based on the utility of the proposed split. The variables provided include token_self, token_opp, combined_earning, split_self, split_opp, etc. Since the task involves fairness considerations, the model should account for how the participant perceives the fairness of the split relative to their own contribution and the opponent's contribution.\n\nIn the experiment, each trial's combined earning is the total money from both participants' tokens. Each token is \u00a33, so combined_earning = 3*(token_self + token_opp). The split_self and split_opp are the proposed shares. However, the note mentions that equal split_self values don't represent the same fairness because the combined earning varies. So, fairness should be evaluated relative to contributions, not just absolute splits.\n\nThe key psychological mechanisms here are likely fairness considerations and self-interest. Participants might compare the proposed split to what they consider a fair share based on their contribution. If the proposed split is below their fair share, they might be more likely to reject it, even if it means getting nothing (as in ultimatum games). But self-interest could also play a role\u2014maybe participants have a threshold of what's acceptable, considering both their own gain and the fairness.\n\nSo, the utility function should incorporate both the absolute amount the participant receives (split_self) and the fairness of the split relative to contributions. Let's think about how to model this.\n\nFairness can be calculated as the participant's contribution proportion. The participant's contribution is token_self, and the total contribution is token_self + token_opp. So, the fair share percentage would be (token_self / (token_self + token_opp)) * 100. The proposed split percentage is splitperc_self. The difference between these two could be a measure of unfairness. If the proposed percentage is less than the fair percentage, the participant might perceive it as unfair.\n\nAlternatively, the fair split in monetary terms would be (token_self / (token_self + token_opp)) * combined_earning. The proposed split_self can be compared to this fair amount. The difference (split_self - fair_share) could be a component in the utility. If this difference is negative (proposed is less than fair), utility decreases.\n\nBut the model also needs to account for the participant's sensitivity to unfairness. Some participants might be more tolerant of receiving less than their fair share, while others might strongly reject it. This suggests a parameter that scales the impact of unfairness on utility.\n\nAdditionally, the absolute amount of split_self might matter. Even if the split is unfair, a higher absolute amount might still be acceptable. So, the utility could have a term for the split_self (self-interest) and a term for the fairness deviation (fairness consideration).\n\nPutting this together, the utility (U) could be modeled as:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - fair_share)\n\nWhere \u03b1 is the weight on self-interest, and \u03b2 is the weight on fairness. However, this might need adjustment. Alternatively, the fairness term could be the difference between the proposed split and the fair share, scaled by a parameter. If the proposed split is less than fair, that's negative, reducing utility.\n\nWait, fair_share is (token_self / (token_self + token_opp)) * combined_earning. Since combined_earning is 3*(token_self + token_opp), this simplifies to 3*token_self. So fair_share is 3*token_self. Wait, that's interesting. Because combined_earning is 3*(sum of tokens), so the fair share based on contribution would be 3*token_self. So split_self is the proposed amount, and the fair share is 3*token_self. Therefore, the difference (split_self - 3*token_self) represents how much the proposed amount deviates from the fair share. If this is positive, the participant is getting more than their fair share; if negative, less.\n\nSo, the utility could be:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - 3*token_self)\n\nBut maybe the fairness term should be relative. For example, if the participant is getting less than their fair share, the negative term would reduce utility. Alternatively, the model could use the ratio of proposed to fair share. But using the difference might be simpler.\n\nAlternatively, the model could consider the ratio of the proposed split to the fair share. For example, (split_self / (3*token_self)) - 1. If this is negative, it's unfair. But using the difference might be more straightforward.\n\nAnother angle is to model the utility as the proposed amount minus a term that penalizes unfairness. The unfairness could be the difference between the fair share and the proposed amount, but only when the proposed amount is less than the fair share. So, something like:\n\nU = split_self - \u03b8 * max(0, fair_share - split_self)\n\nHere, \u03b8 is a parameter representing the sensitivity to being shortchanged. If the proposed split is less than the fair share, the participant experiences a penalty proportional to the deficit. This is similar to models in inequity aversion, where individuals dislike unequal outcomes.\n\nAlternatively, the Fehr and Schmidt model of inequity aversion could be adapted. In their model, utility depends on one's own payoff and the difference between one's own and others' payoffs. But in this case, the opponent's split is split_opp, which is (combined_earning - split_self). So, the participant's utility might be:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nBut this is a standard inequity aversion model. However, in this task, the opponent's contribution (token_opp) and the participant's contribution (token_self) might influence what's considered fair. The Fehr and Schmidt model doesn't account for differing contributions, only the final splits. So maybe the fairness here is based on contributions, not just final splits.\n\nSo, combining both the absolute split and the contribution-based fairness. For example, the fair split is 3*token_self, and the proposed split_self is compared to that. The utility could be:\n\nU = split_self - \u03b3 * max(3*token_self - split_self, 0)\n\nHere, \u03b3 is the sensitivity to being below the fair share. If the proposed split_self is less than the fair share (3*token_self), the utility is reduced by \u03b3 times the deficit. If it's equal or more, there's no penalty. This model captures self-interest (split_self) and fairness based on contribution.\n\nAlternatively, the model could include both advantageous and disadvantageous inequity. For example, if the participant gets more than their fair share, they might feel guilty, but perhaps that's less likely. The main driver might be when they get less. So focusing on the deficit makes sense.\n\nAnother consideration is the role of the trial_type variable, which indicates whether contributions are equal, opponent more, or participant more. But since the model is supposed to use variables from the dataset, and trial_type is a categorical variable, maybe it's better to derive the necessary information from token_self and token_opp instead of using trial_type directly. Because token_self and token_opp are continuous, they can provide a more precise measure of contribution difference.\n\nSo, the key variables to include are split_self (the proposed amount), token_self (participant's contribution), and token_opp (opponent's contribution). From these, we can compute the fair share as 3*token_self. The difference between split_self and fair_share is (split_self - 3*token_self). If this is negative, the participant is getting less than their fair share.\n\nThe utility function could then be:\n\nU = split_self + \u03b1 * (split_self - 3*token_self)\n\nBut this would mean that when split_self is less than fair share, the term is negative, reducing utility. However, the parameter \u03b1 would determine how much that affects the utility. If \u03b1 is positive, then getting more than fair share increases utility, but getting less decreases it. Wait, but in that case, the model would have U = split_self + \u03b1*(split_self - fair_share) = (1 + \u03b1)*split_self - \u03b1*fair_share. That might not capture the intended effect. Alternatively, perhaps the model should have a term that penalizes the deficit.\n\nAlternatively, U = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b1 is the penalty for being below the fair share. If split_self is less than fair_share, the penalty is \u03b1*(fair_share - split_self). Otherwise, no penalty. This way, the utility is the proposed amount minus a penalty proportional to how much they're being shortchanged.\n\nThis seems plausible. The participant's utility is their own split, but they subtract a term that represents their aversion to receiving less than their fair share. The parameter \u03b1 determines how much they care about this fairness. If \u03b1 is zero, they only care about their own split. If \u03b1 is positive, they dislike being below the fair share.\n\nAnother possible component is the absolute amount of the split. For example, even if the split is unfair, a higher absolute amount might be more acceptable. But in this model, the split_self is already part of the utility. So, the model combines self-interest (split_self) with fairness consideration (penalty for being below fair share).\n\nNow, the probability of accepting is determined by the utility through a logistic function. The probability P(accept) = 1 / (1 + exp(-U)). So, if U is positive, the probability is greater than 0.5; if negative, less than 0.5. The requirement is that when utility is less than 0, the probability is less than 50%, which aligns with the logistic function.\n\nBut wait, the problem states: \"Predict utility, where less than 0 is less than 50% likely.\" So the utility U is passed through a logistic function to get the probability. So the model's U can be any real number, and the probability is logistic(U). So when U < 0, probability < 0.5.\n\nSo the model's utility function should be designed such that when U is negative, the participant is less likely to accept. The parameters in the utility function (like \u03b1) need to be learnable.\n\nNow, considering the parameters. The model needs learnable parameters with finite bounds. In the example above, \u03b1 is a learnable parameter. What's a reasonable range for \u03b1? Since split_self and fair_share are in pounds, the penalty term is in pounds multiplied by \u03b1. If split_self is in the range of, say, 0 to combined_earning (which is up to 3*(token_self + token_opp)), but token_self and token_opp are integers. Let's say combined_earning could be up to, say, 3*(36 + 36) = 216 (if each found 36 tokens, but the actual max depends on the data). However, the split_self would be a portion of that. The fair_share is 3*token_self, so if token_self is, say, 10, fair_share is 30. If the proposed split_self is 20, then the deficit is 10, and the penalty is \u03b1*10. So \u03b1 needs to be a positive value. If \u03b1 is 0.5, then the penalty is 5, so U = 20 - 5 = 15. If \u03b1 is 2, penalty is 20, U=0. So the participant would be indifferent (50% chance) when split_self - \u03b1*(fair_share - split_self) = 0. Wait, no: in the model U = split_self - \u03b1*max(fair_share - split_self, 0). So if split_self is 20, fair_share is 30, then U = 20 - \u03b1*10. For U to be zero, \u03b1 would need to be 2. So if \u03b1 is 2, then U=0 when split_self is 20. So the participant would have 50% chance of accepting. If \u03b1 is higher than 2, then U would be negative, leading to rejection.\n\nBut how to set the bounds for \u03b1? Since split_self and fair_share can vary, \u03b1 could be any positive number. However, to prevent extreme values, we can set a generous bound. For example, \u03b1 could be between 0 and 5. If \u03b1 is 5, then even a small deficit would lead to a large penalty. But maybe the upper bound can be higher. However, for parameter recovery, it's better to have reasonable bounds. Let's say \u03b1 is between 0 and 10. That's inclusive. So the range for \u03b1 is [0, 10].\n\nBut perhaps another parameter is needed. For example, maybe participants have a different sensitivity to the deficit. Or maybe there's a baseline acceptance bias. For instance, some participants might generally be more accepting or rejecting regardless of the split. So adding an intercept term could help. Let's consider:\n\nU = \u03b20 + \u03b21 * split_self - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b20 is a bias term. If \u03b20 is positive, the participant is more likely to accept even if the split is unfavorable. If \u03b20 is negative, they are more prone to reject. \u03b21 represents the sensitivity to the proposed split. But since split_self is already in pounds, \u03b21 could be 1 (assuming linear utility in money), but perhaps participants have different marginal utilities. However, adding \u03b21 as a learnable parameter might complicate the model. Alternatively, fixing \u03b21 to 1 and learning \u03b1 and \u03b20. But the problem states that the model should have parameters with finite bounds. If we include \u03b20 and \u03b1 as learnable parameters, that's two parameters. However, the instructions suggest a slight bias towards fewer parameters. So maybe just \u03b1 and \u03b20. But let's think: if \u03b20 is included, it's an intercept term. For example, even if the split is fair, the participant might have a general tendency to accept or reject. But in the task, when the split is exactly fair, the utility would be split_self (since the penalty term is zero) plus \u03b20. If \u03b20 is positive, that increases the chance of acceptance. But maybe \u03b20 is not necessary. Let's see.\n\nIf we omit \u03b20, the model is U = split_self - \u03b1 * max(fair_share - split_self, 0). Then, when the split is exactly fair (split_self = fair_share), U = split_self. Since split_self is part of the utility, and the logistic function converts that to a probability. For example, if split_self is 30, then U=30. The probability would be almost 1. But in reality, participants might not always accept even fair splits. So including \u03b20 could capture a baseline tendency. However, adding parameters increases model complexity, which might affect parameter recovery. The user mentioned that parameter recovery is a priority, so the model should have parameters that are identifiable. If two parameters are highly correlated, recovery would be difficult. \n\nAlternatively, maybe the intercept is not necessary. Let's consider that when the split is exactly fair, the utility is split_self. If split_self is high enough, the probability is high. But maybe participants have a different baseline. For example, even if the split is fair, if the absolute amount is low, they might reject. But in the current model, the utility is split_self minus the penalty. So when the split is fair, U = split_self. So higher split_self leads to higher utility. That makes sense. For example, if two trials have the same fairness (split_self equals fair_share), but different combined_earning, the split_self would be higher in the trial with higher combined_earning, leading to higher utility. That aligns with the note that equal split_self percentages don't represent the same fairness because combined_earning varies. Wait, no: split_self in pounds is part of the utility. So even if two trials have the same split percentage but different combined_earning, the split_self in pounds would be different. For example, if combined_earning is 60, split_self 30 (50%) vs. combined_earning 120, split_self 60 (50%). The utility would be 30 vs. 60, leading to higher utility in the second case. So the model accounts for the absolute amount, which is correct.\n\nBut in the case where the split is exactly fair (split_self = 3*token_self), the utility is split_self. So participants are more likely to accept higher fair splits. That makes sense. For example, if token_self is 10, fair_share is 30. If the proposed split_self is 30, utility is 30. If token_self is 20, fair_share is 60, proposed split_self is 60, utility is 60. So higher absolute amounts lead to higher acceptance rates, which is logical.\n\nBut what if the proposed split is more than the fair share? For example, token_self is 10, fair_share 30, but split_self is 40. Then, the penalty term is zero, so U = 40. The participant would likely accept. That's reasonable.\n\nNow, considering the parameters. The model as U = split_self - \u03b1 * max(fair_share - split_self, 0) has one learnable parameter, \u03b1. But maybe we need another parameter to scale the split_self. For example, participants might not value each pound linearly. So U = \u03b2 * split_self - \u03b1 * max(fair_share - split_self, 0). Here, \u03b2 is the marginal utility of money. If \u03b2 is fixed to 1, then it's linear. But allowing \u03b2 to vary could capture diminishing returns. However, adding \u03b2 as a learnable parameter would increase model complexity. The problem states to have a slight bias towards fewer parameters. So maybe keep \u03b2 fixed at 1. But if the data suggests that participants have different sensitivities to the absolute amount, then \u03b2 should be a parameter. However, given the parameter recovery concern, perhaps it's better to have one parameter. Let's proceed with \u03b1 as the only learnable parameter. Wait, but then the model would be U = split_self - \u03b1 * deficit. But maybe participants have different baseline propensities. For example, some might require a higher utility threshold to accept. Wait, the logistic function includes a bias term inherently. Because the probability is 1/(1 + exp(-U)). So if U is split_self - \u03b1*deficit, then when U=0, the probability is 0.5. So the model's utility function's intercept is determined by the parameters. But without an additional parameter, the model might not capture individual differences in baseline acceptance. For example, two participants with the same \u03b1 but different baseline tendencies would not be captured. So perhaps adding an intercept term \u03b20 would help. Let's consider:\n\nU = \u03b20 + split_self - \u03b1 * max(fair_share - split_self, 0)\n\nHere, \u03b20 is a learnable parameter representing the baseline tendency to accept (if positive) or reject (if negative). This allows for individual differences in overall acceptance rates, independent of the split. However, this adds another parameter. But with two parameters (\u03b20 and \u03b1), the model might still be recoverable if they are not highly correlated. The problem allows for a few parameters, as long as they are not too many. The example had two learnable parameters. So maybe this is acceptable.\n\nAlternatively, perhaps the intercept is not necessary. Let's think: when the proposed split is exactly the fair share, U = split_self. So for a given split_self, the utility is higher when the split is fair. But if the split_self is very low, even if it's fair, the utility might be negative. For example, if token_self is 1, fair_share is 3, split_self is 3. Then U = 3. So the probability is high. But if token_self is 1 and split_self is 3 (fair), but combined_earning is 6 (so split_opp is 3), then the participant gets 3, which is fair, and the utility is 3. So they are likely to accept. But if the split_self is 3 but the fair share is 30 (token_self=10), then U=30, which is higher. So the model accounts for the absolute amount. However, in cases where the split is fair but the absolute amount is low, the participant might still reject. But according to the model, U would be split_self, which is positive, leading to P(accept) > 0.5. But maybe in reality, participants might reject low but fair offers. To capture this, the model needs a parameter that adjusts the baseline. For example, if \u03b20 is negative, even a fair split might lead to U = \u03b20 + split_self. If split_self is small, U could be negative. For example, \u03b20 = -5, split_self=3: U = -5 +3 = -2, leading to P(accept) < 0.5. So the intercept allows for a general bias towards acceptance or rejection.\n\nTherefore, including \u03b20 as a learnable parameter might improve the model's flexibility. So the model would have two parameters: \u03b20 (intercept) and \u03b1 (fairness sensitivity). Both need to have bounds. For \u03b20, perhaps a range from -10 to 10, which is generous. For \u03b1, a range from 0 to 10 (since it's a penalty, should be non-negative). \n\nSo the utility function is:\n\nU = \u03b20 + split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nThis captures the baseline tendency (\u03b20), the self-interest (split_self), and the penalty for being below the fair share (\u03b1 times the deficit). \n\nNow, mapping the variables in the dataset:\n\n- split_self is a data variable (float64)\n- token_self is a data variable (int64)\n- 3*token_self is calculated as fair_share\n- max(3*token_self - split_self, 0) is calculated as the deficit\n\nThe learnable parameters are \u03b20 and \u03b1. \n\nThe target variable is \"accept\", which is binary. The model's utility U is passed through a logistic function to get the probability of acceptance. \n\nNow, the variables section needs to describe each component:\n\n- \u03b20: learnable, with range [-10, 10]\n- \u03b1: learnable, with range [0, 10]\n- split_self: from data\n- token_self: from data\n- max(...): calculated\n\nBut in the model formula, the variables used are split_self, token_self, \u03b20, and \u03b1. The max term is part of the calculation.\n\nSo the variables in the JSON would be:\n\n- beta0: learnable, description \"Baseline acceptance tendency\"\n- alpha: learnable, description \"Sensitivity to deficit below fair share\"\n- split_self: data variable\n- token_self: data variable\n- fair_share: calculated (3 * token_self)\n- deficit: calculated (max(fair_share - split_self, 0))\n\nBut in the model formula, the max term is part of the equation. So the variables used directly in the model are beta0, split_self, alpha, and the max term. However, in the variables section, we need to list all variables that are part of the model, including those calculated.\n\nWait, the model's mathematical formula is:\n\nU = beta0 + split_self - alpha * max(3*token_self - split_self, 0)\n\nSo the variables involved are:\n\n- beta0 (learnable)\n- split_self (data)\n- alpha (learnable)\n- token_self (data)\n- 3*token_self (calculated as fair_share)\n- max(fair_share - split_self, 0) (calculated as deficit)\n\nBut in the variables section, we need to list each variable that appears in the model. However, the model's formula can include calculated terms as long as they are defined. For example, fair_share is 3*token_self, which is calculated from token_self. The deficit is max(fair_share - split_self, 0), which is calculated from fair_share and split_self.\n\nBut in the variables section, we need to list all variables that are part of the model. So:\n\nVariables:\n\n- beta0: learnable, range [-10, 10]\n- alpha: learnable, range [0, 10]\n- split_self: data\n- token_self: data\n- fair_share: calculated (3 * token_self)\n- deficit: calculated (max(fair_share - split_self, 0))\n\nBut in the model formula, these calculated variables are not explicitly named; they are part of the equation. However, the variables section should list all variables used in the model, even if they are intermediate calculations. Alternatively, the model can directly use the data variables without explicitly naming the intermediate terms. For example, the model can be written as:\n\nU = beta0 + split_self - alpha * max(3*token_self - split_self, 0)\n\nIn this case, the variables used in the formula are beta0, split_self, alpha, token_self. The terms 3*token_self and max(...) are calculated on the fly. Therefore, in the variables section, we need to list beta0, alpha, split_self, and token_self. The other terms are derived from these and don't need to be listed as separate variables.\n\nSo the variables section would include:\n\n- beta0: learnable, description \"Baseline utility intercept\"\n- alpha: learnable, description \"Sensitivity to receiving less than fair share\"\n- split_self: data variable\n- token_self: data variable\n\nThe other terms (fair_share and deficit) are calculated within the model formula but are not separate variables in the JSON.\n\nTherefore, the variables JSON would be:\n\n{\n  \"variables\": {\n    \"beta0\": {\n      \"description\": \"Baseline utility intercept, capturing general acceptance tendency\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity parameter for the penalty when receiving less than the fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant, determining their fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThis covers all variables used in the model. The calculated terms (fair_share and deficit) are part of the model's formula but are not listed as separate variables since they are derived from token_self and split_self.\n\nNow, checking if this model meets the requirements:\n\n1. Predicts responder behavior (targeting \"accept\"): Yes, the utility is used in a logistic model to predict the probability of acceptance.\n\n2. Uses variables from the dataset: Uses split_self and token_self, which are provided.\n\n3. Learnable parameters have finite bounds: beta0 is [-10, 10], alpha is [0, 10]. These are generous but finite.\n\n4. Utility can be unbounded, which it is since split_self and token_self can vary.\n\nThe model's utility function combines baseline tendency, self-interest (split_self), and a penalty for being below the fair share. The parameters beta0 and alpha are learnable, with specified bounds. This should allow for parameter recovery as long as the parameters are identifiable, which they should be since beta0 affects the baseline and alpha scales the penalty term.\n\nAnother consideration: Does the model account for the opponent's contribution (token_opp)? The fair share is based on the participant's own contribution (token_self), but the opponent's contribution affects the combined_earning. However, in the model, the fair share is 3*token_self, which is independent of the opponent's contribution. Wait, but combined_earning is 3*(token_self + token_opp). The fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning = 3*token_self. So the model correctly uses the participant's contribution to determine the fair share, regardless of the opponent's contribution. This aligns with the note that two equal split_self values don't represent the same fairness because combined_earning (and thus token_opp) can vary. However, in the model, the fair share is 3*token_self, which is fixed based on the participant's own tokens. So even if the opponent's tokens vary, the fair share is determined solely by the participant's tokens. Is this correct?\n\nWait, the fair share should be based on the participant's contribution relative to the total. For example, if the participant contributed 10 tokens and the opponent 20, the total is 30 tokens. The participant's fair share would be (10/30)*combined_earning = (1/3)*combined_earning. But combined_earning is 3*(10+20) = 90. So fair_share is 30. But according to the model, fair_share is 3*token_self = 3*10=30. So in this case, it's the same. However, if the participant contributed 10 and the opponent 10, combined_earning is 60. Fair share would be (10/20)*60 = 30, which is again 3*10=30. So in all cases, the fair share based on contribution is 3*token_self, regardless of the opponent's tokens. Therefore, the model correctly captures the fair share as 3*token_self. This is a neat simplification, as it avoids needing to use token_opp in the model. Therefore, the model only needs token_self and split_self, which are available in the dataset.\n\nThis is a crucial insight. The fair share can be computed as 3*token_self, which is the participant's contribution in pounds. This simplifies the model and avoids needing to use token_opp or combined_earning directly. Therefore, the model is correctly specified.\n\nIn summary, the model uses the participant's proposed split (split_self) and their own contribution (token_self) to compute a utility that combines self-interest and fairness considerations. The utility is passed through a logistic function to predict the probability of acceptance. The learnable parameters are beta0 (baseline tendency) and alpha (sensitivity to unfairness), both with reasonable bounds to aid parameter recovery.\n</think>\n\n<MODEL>\nU = beta0 + split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"beta0\": {\n      \"description\": \"Baseline utility intercept capturing general acceptance/rejection tendency\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to receiving less than the fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token count used to compute fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA logistic choice model where utility combines baseline tendency, self-interest (split_self), and a penalty for receiving less than the contribution-based fair share (3 * token_self). The penalty scales with sensitivity parameter \u03b1. Learnable parameters \u03b20 (baseline) and \u03b1 (fairness sensitivity) have bounded ranges to ensure identifiability. The model directly uses observable split amounts and token contributions without requiring opponent-specific variables.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = beta0 + split_self - alpha * max(3 * token_self - split_self, 0)\nSummary: A logistic choice model where utility combines baseline tendency, self-interest (split_self), and a penalty for receiving less than the contribution-based fair share (3 * token_self). The penalty scales with sensitivity parameter \u03b1. Learnable parameters \u03b20 (baseline) and \u03b1 (fairness sensitivity) have bounded ranges to ensure identifiability. The model directly uses observable split amounts and token contributions without requiring opponent-specific variables.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, beta0=0.0, alpha=0.0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0.0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        fair_share = 3 * token_self\n        deficit = max(fair_share - split_self, 0)\n        \n        utility = beta0 + split_self - alpha * deficit\n        \n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8586691086691086,
  "fitting_results": [
    {
      "beta0": -5.24994777932786,
      "alpha": 3.812441277631178,
      "success": true,
      "log_likelihood": -38.02695649622573,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 4.5861268330344025,
      "alpha": 5.321921152708393,
      "success": true,
      "log_likelihood": -116.51628303737067,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -0.2664742608158939,
      "alpha": 1.3451261467029425,
      "success": true,
      "log_likelihood": -34.434280375448836,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.2361199886813794,
      "alpha": 0.7961598173594078,
      "success": true,
      "log_likelihood": -39.85783187050828,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 0.9881853666761092,
      "alpha": 2.366246978602045,
      "success": true,
      "log_likelihood": -39.91406187371189,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -0.7156153119238865,
      "alpha": 0.7950154135633902,
      "success": true,
      "log_likelihood": -41.99593792954397,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 7.274542712203181,
      "alpha": 3.1865434606495957,
      "success": true,
      "log_likelihood": -53.04268443619396,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.4618896509279884,
      "alpha": 1.3340806427682494,
      "success": true,
      "log_likelihood": -51.41511854730886,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -0.2981602431176257,
      "alpha": 1.4868035018937462,
      "success": true,
      "log_likelihood": -0.5613873349265998,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -5.250572646280689,
      "alpha": 6.684820592714664,
      "success": true,
      "log_likelihood": -61.052807443798656,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 0.4806104849061529,
      "alpha": 2.333865294460866,
      "success": true,
      "log_likelihood": -23.058840451445583,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -115.12925464660226,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 0.8084511643952212,
      "alpha": 0.6293291622738241,
      "success": true,
      "log_likelihood": -18.86490876272404,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 9.350568791649946,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 0.1755890596571436,
      "alpha": 1.5339238259736585,
      "success": true,
      "log_likelihood": -59.10310260860098,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -115.12925464660225,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -3.471981762999189,
      "alpha": 0.3742714223984696,
      "success": true,
      "log_likelihood": -65.89146769925884,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.6608986896881497,
      "alpha": 0.8817509173971765,
      "success": true,
      "log_likelihood": -75.63056918595457,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 1.3967615429818074,
      "alpha": 2.502979334109042,
      "success": true,
      "log_likelihood": -62.37521855032749,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -0.469625747270719,
      "alpha": 1.2051359515635718,
      "success": true,
      "log_likelihood": -63.94187362687336,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.5873726557437837,
      "alpha": 0.6772689505363915,
      "success": true,
      "log_likelihood": -43.05145280909613,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 4.425962994102488,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -23.026373400236835,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -92.10340371656179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 4.628452584187954,
      "alpha": 1.34697969156717,
      "success": true,
      "log_likelihood": -134.83528001525193,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -5.249367061851434,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -38.02695651859767,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.5976710994645495,
      "alpha": 0.4091974238647622,
      "success": true,
      "log_likelihood": -99.8334666080233,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.991355733991139,
      "alpha": 0.8818797698911329,
      "success": true,
      "log_likelihood": -67.98327120326239,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 8.652733781252245,
      "alpha": 0.47188768775221823,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.299767350002066,
      "alpha": 0.15683666654656608,
      "success": true,
      "log_likelihood": -17.34196072524584,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -0.9452867323662277,
      "alpha": 0.5800458511994931,
      "success": true,
      "log_likelihood": -15.135685183517037,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -0.37613347958050763,
      "alpha": 1.3870557206794625,
      "success": true,
      "log_likelihood": -69.8789003578087,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 3.9936184140768796,
      "success": true,
      "log_likelihood": -122.78549689854614,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652135,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 0.5995226062309329,
      "alpha": 1.923211053002915,
      "success": true,
      "log_likelihood": -134.16505720756257,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 1.2683878836741695,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -46.05193553827011,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -23.02585092644049,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.8073267641071669,
      "alpha": 0.45035071690617784,
      "success": true,
      "log_likelihood": -70.9363209508736,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.1183642585405267,
      "alpha": 0.6526257376293919,
      "success": true,
      "log_likelihood": -54.71132009932224,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.37484702795092,
      "alpha": 0.00808112420048125,
      "success": true,
      "log_likelihood": -54.222269949682754,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.3849388524556179,
      "alpha": 1.007993150542759,
      "success": true,
      "log_likelihood": -76.3870021139077,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.7042836575555294,
      "alpha": 0.7221229937301538,
      "success": true,
      "log_likelihood": -74.62325006862758,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 1.121025639618164,
      "alpha": 2.364465780182282,
      "success": true,
      "log_likelihood": -59.8991430818093,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.819835405052309,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -137.2733905309397,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -253.28436022684522,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -5.250361747035888,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -84.07865836310066,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.3717797188968133,
      "alpha": 0.2743718031696431,
      "success": true,
      "log_likelihood": -65.59771637749857,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -345.3877639470071,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 9.846511631734613,
      "success": true,
      "log_likelihood": -185.66343622025207,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.2023272393943314,
      "alpha": 1.6742128883704208,
      "success": true,
      "log_likelihood": -5.75774682661454,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -0.9913207549800862,
      "alpha": 5.4678757083757805,
      "success": true,
      "log_likelihood": -91.2384610068859,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.3447267981683138,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -64.88595092414624,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -4.881405776441579,
      "alpha": 0.3676615727998204,
      "success": true,
      "log_likelihood": -32.87986583582722,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.868882586493365,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -40.0418770138732,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.2385617830537594,
      "alpha": 0.1459912886609993,
      "success": true,
      "log_likelihood": -120.38869623272045,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 1.069567227901846,
      "alpha": 2.3345544200365707,
      "success": true,
      "log_likelihood": -57.974598763057884,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -5.249765600700755,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -38.026956499233584,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.399726644853921,
      "alpha": 0.5202947579768992,
      "success": true,
      "log_likelihood": -83.76451595042606,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -3.4461846411205843,
      "alpha": 0.7700543483910584,
      "success": true,
      "log_likelihood": -33.26055387746952,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.223354928903153,
      "alpha": 1.01627622004044,
      "success": true,
      "log_likelihood": -50.80321686151216,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652131,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -115.1292546466023,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.4895603559441035,
      "alpha": 0.21280356413356122,
      "success": true,
      "log_likelihood": -34.903191087905945,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652137,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.175696408981093,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -6.486365516501073e-07,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -4.921612754244438,
      "alpha": 0.18122212091355927,
      "success": true,
      "log_likelihood": -52.629575923146774,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.5360262889045337,
      "alpha": 0.9467813775407955,
      "success": true,
      "log_likelihood": -61.20218685877041,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -184.20680743672386,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 2.9999315550947654,
      "alpha": 3.9999533265978826,
      "success": true,
      "log_likelihood": -186.97939647472523,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 2.4060656903886257,
      "alpha": 1.523478417159139,
      "success": true,
      "log_likelihood": -66.10439677859702,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -46.051701856480875,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -7.873319539491032,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -23.025863724216254,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.8434170314436216,
      "alpha": 1.3193542205482032,
      "success": true,
      "log_likelihood": -204.25106069696855,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -5.250147126142733,
      "alpha": 2.2153583998748405,
      "success": true,
      "log_likelihood": -38.02695659817908,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.5865594498610234,
      "alpha": 0.08560508595622468,
      "success": true,
      "log_likelihood": -154.64415879067968,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.8862231906003115,
      "success": true,
      "log_likelihood": -69.07755278652131,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -6.982514352993708,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -69.07760686153912,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.2342905212544992,
      "alpha": 0.5316938454412029,
      "success": true,
      "log_likelihood": -5.534258360093617,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 9.846508879411582,
      "success": true,
      "log_likelihood": -277.7679412129943,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.075989536055023,
      "alpha": 7.018473831846354,
      "success": true,
      "log_likelihood": -46.05170283469658,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -1.6191861295442334,
      "alpha": 0.2828079886348745,
      "success": true,
      "log_likelihood": -106.18162693162171,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 0.32880831458912874,
      "alpha": 4.365644223608659,
      "success": true,
      "log_likelihood": -77.70317910119256,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -46.05170185648088,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": -2.6680281742933345,
      "alpha": 0.3704514261672101,
      "success": true,
      "log_likelihood": -8.90214132613137,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta0": 10.0,
      "alpha": 2.590155561514142,
      "success": true,
      "log_likelihood": -232.23225284724558,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "beta0": {
        "r": 0.9320427696198911,
        "p": 5.364104812305116e-45
      },
      "alpha": {
        "r": 0.6320846132546013,
        "p": 1.7546125011072648e-12
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 147.7657450487978,
  "bic_results": {
    "average_bic": 147.7657450487978,
    "individual_bics": [
      83.22095086936368,
      240.19960395165356,
      76.03559862780989,
      86.88270161792877,
      86.99516162433599,
      91.15891373600016,
      7.167037869712219,
      113.25240674930014,
      7.167037869712219,
      109.99727497152993,
      8.28981254676542,
      129.27265276450953,
      53.284718779803384,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      237.42554717011674,
      44.896855402360295,
      7.167037869712219,
      125.37324309411417,
      237.42554717011672,
      138.9499732754299,
      158.42817624882136,
      131.9174749775672,
      135.05078513065894,
      93.26994349510449,
      53.21978467738589,
      191.3738453100358,
      276.83759790741607,
      7.167037869712219,
      83.22095091410756,
      206.83397109295882,
      143.133580283437,
      7.167037869712219,
      41.8509593274039,
      37.43840824394629,
      146.92483859252962,
      252.7380316740045,
      145.32214344995492,
      275.49715229203736,
      99.27090895345243,
      53.2187397297932,
      149.0396797786594,
      116.5896780755567,
      115.61157777627773,
      159.94104210472761,
      156.4135380141674,
      126.96532404053082,
      281.71381893879163,
      513.7357583306027,
      175.32435460311353,
      138.36247063190936,
      697.9425657709264,
      378.49391031741635,
      18.6825315301413,
      189.64395989068402,
      136.9389397252047,
      72.92676954856665,
      87.25079190465861,
      247.94443034235312,
      123.11623540302799,
      83.22095087537939,
      174.69606977776434,
      73.68814563185126,
      108.77347159993654,
      145.32214344995484,
      237.42554717011683,
      76.97342005272411,
      145.32214344995495,
      7.167039174185323,
      112.42618972320577,
      129.57141159445305,
      375.58065275035995,
      381.1258308263627,
      139.37583143410626,
      99.27044158987397,
      53.21876532534473,
      415.6691592708493,
      83.22095107327038,
      316.4553554582716,
      145.32214344995484,
      145.32225159999047,
      18.23555459709945,
      562.7029203029008,
      99.27044354630537,
      219.53029174015563,
      162.57339607929734,
      99.27044158987398,
      24.97132052917496,
      471.63154357140337
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- beta0: r = 0.932\n- alpha: r = 0.632",
  "previous_models": [
    "Specification: U = beta0 + split_self - alpha * max(3 * token_self - split_self, 0)\nSummary: A logistic choice model where utility combines baseline tendency, self-interest (split_self), and a penalty for receiving less than the contribution-based fair share (3 * token_self). The penalty scales with sensitivity parameter \u03b1. Learnable parameters \u03b20 (baseline) and \u03b1 (fairness sensitivity) have bounded ranges to ensure identifiability. The model directly uses observable split amounts and token contributions without requiring opponent-specific variables.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 147.7657450487978\nOverall Accuracy: 0.8587\n\nParameter Recovery:\n- beta0: r = 0.932\n- alpha: r = 0.632"
  ]
}