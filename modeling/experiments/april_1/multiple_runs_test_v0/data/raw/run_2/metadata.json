{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure that parameters can be recovered, which is as important as accuracy. Also, the utility function should be such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on the utility of accepting the proposal. The utility function will be converted to a probability via a logistic function.\n\nLooking at the dataset variables, the important ones for the responder's decision might include the proposed shares (split_self and split_opp), the combined earnings, and possibly the tokens found by both the participant and the opponent (token_self and token_opp). The trial_type indicates whether contributions are equal, opponent more, or participant more, which might influence fairness perceptions.\n\nIn responder role (trial_role=1), the participant decides to accept or reject the split proposed by the opponent. The key factors here are likely fairness considerations and the monetary outcome. The utility of accepting could depend on how fair the proposed split is relative to some reference point, such as the participant's own contribution (token_self) versus the opponent's (token_opp). \n\nA common approach in behavioral economics is the Fehr-Schmidt model of inequity aversion, where individuals care about both their own payoff and the difference between their payoff and others'. However, since this is a responder model, maybe the utility is a function of the proposed split compared to what's fair based on contributions.\n\nThe combined_earning is the total pot, which is split between the two. The split_self is the amount offered to the participant. The fair split might be proportional to the tokens each contributed. For example, if the participant contributed token_self tokens and the opponent token_opp, the fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. \n\nSo, the model could compare the proposed split_self to this fair share. The difference between the proposed and fair share could be a key component of the utility. Additionally, there might be a parameter representing the participant's sensitivity to unfairness (alpha). If the proposed split is less than the fair share, the utility decreases, and vice versa. However, since the responder can only accept or reject, perhaps the disutility comes from being offered less than fair, and the utility from the absolute amount.\n\nAnother consideration is the efficiency: the total combined earning. Even if the split is unfair, a higher total might make acceptance more likely. So, the utility could have a term for the proposed amount (split_self) and a term for the fairness deviation.\n\nWait, but the problem states that two equal split_self variables don't represent the same fairness because the combined earning varies. So, the fairness should be relative to the total. For example, getting \u00a35 out of \u00a310 is different from \u00a35 out of \u00a320. So, perhaps the percentage split (splitperc_self) is important. But splitperc_self is already provided. However, maybe the fair percentage is based on the tokens. The fair percentage would be (token_self / (token_self + token_opp)) * 100. The difference between splitperc_self and this fair percentage could be a factor.\n\nAlternatively, the model could compute the fair split as (token_self / (token_self + token_opp)) * combined_earning, then compare split_self to this value. The difference (split_self - fair_share) could be part of the utility. If the split is more than fair, positive utility; if less, negative. But how to model this?\n\nLet me formalize this. Let's compute the fair share as:\n\nfair_share = (token_self / (token_self + token_opp)) * combined_earning\n\nThen, the difference between the proposed split_self and fair_share is delta = split_self - fair_share.\n\nThe utility could be a function of delta. If delta is positive, the participant is getting more than fair, which is good. If negative, less than fair, which is bad. The utility might be:\n\nU = beta * split_self + alpha * delta\n\nBut beta and alpha are parameters. However, beta could represent the sensitivity to the absolute amount offered, while alpha represents the sensitivity to fairness (deviation from fair share). Alternatively, maybe the utility is split_self plus a term that penalizes unfairness. For example:\n\nU = split_self - alpha * max(fair_share - split_self, 0)\n\nHere, alpha is the aversion to being undercompensated relative to the fair share. If the split_self is less than fair_share, the participant experiences a disutility proportional to the difference. If it's more, there's no penalty. This is similar to loss aversion or inequity aversion models.\n\nAlternatively, the model could use the ratio of split_self to fair_share. For example:\n\nU = split_self + gamma * (split_self / fair_share - 1)\n\nBut this might complicate things. Let's think about the parameters. The model needs learnable parameters with finite bounds. Let's consider alpha as the fairness sensitivity, and beta as the weight on the absolute split. But perhaps the key is to model the utility as a combination of the monetary gain and the fairness deviation.\n\nAnother angle: the participant's utility could depend on both their own share and the opponent's share, adjusted by their contributions. For example, the participant might compare their share to the opponent's share in light of their respective contributions. If the opponent contributed more, maybe the participant expects a larger share for the opponent. So, the ratio of contributions (token_self to token_opp) could influence the perceived fairness of the split.\n\nAlternatively, the utility could be:\n\nU = split_self + theta * (split_self - (token_self / (token_self + token_opp)) * combined_earning)\n\nHere, theta is a parameter that scales how much deviations from the fair share affect utility. If theta is positive, participants prefer more than their fair share; if negative, they might be willing to accept less. Wait, but theta would be a learnable parameter. If theta is positive, then getting more than fair share increases utility. If theta is negative, getting more than fair share decreases utility, which might not make sense. Hmm.\n\nAlternatively, perhaps the model should have two parameters: one for the sensitivity to advantageous inequality (getting more than fair) and one for disadvantageous inequality (getting less). But that might complicate the model with more parameters. However, the problem states to have a slight bias towards fewer parameters. So maybe a single parameter for fairness.\n\nWait, the Fehr-Schmidt model has two parameters: alpha for disadvantageous inequality and beta for advantageous. But since we need to keep the number of parameters low, perhaps combine them into one. Alternatively, focus only on disadvantageous inequality, as that's more likely to affect rejection decisions. If the participant is offered less than fair, they might reject to punish, even at a cost. So, the utility could be:\n\nU = split_self - alpha * max(fair_share - split_self, 0)\n\nIn this case, alpha represents the aversion to being below the fair share. If the offered split is below fair, the utility decreases by alpha times the deficit. The higher alpha, the more the participant dislikes being shortchanged.\n\nBut then, how does the total combined earning play in? For example, if the combined earning is higher, even a small percentage could be a larger absolute amount. But the model should account for the fact that the same split percentage can correspond to different absolute amounts depending on combined_earning. However, the split_self is already in absolute terms (in \u00a3), so maybe that's already captured. The fair_share is calculated based on tokens and combined_earning, so the delta is in \u00a3 terms.\n\nSo, the utility function could be:\n\nU = split_self + alpha * (split_self - fair_share)\n\nWait, but if split_self is less than fair_share, then (split_self - fair_share) is negative, so the utility would decrease. Alternatively, perhaps:\n\nU = split_self - alpha * max(fair_share - split_self, 0) + beta * max(split_self - fair_share, 0)\n\nHere, alpha is the penalty for being below fair, and beta is the reward for being above. But this introduces two parameters. However, the problem says to have a slight bias towards fewer parameters. Maybe combine them into a single parameter, assuming that the sensitivity to advantageous and disadvantageous inequality is the same. But that's a strong assumption. Alternatively, just model disadvantageous inequality, as that's more relevant for rejection.\n\nAlternatively, the utility could be:\n\nU = split_self + gamma * (split_self - fair_share)\n\nHere, gamma is a parameter. If gamma is positive, participants value getting more than fair; if gamma is negative, they dislike inequality. But this might not capture the asymmetric response. However, given the need for simplicity, this could be a starting point.\n\nBut how does this relate to the probability of acceptance? The logistic function converts utility to a probability. So, P(accept) = 1 / (1 + exp(-U)). If U is positive, the probability is above 50%; if negative, below.\n\nBut the problem states that the utility should be such that less than 0 corresponds to less than 50% chance. That's exactly how the logistic function works, so that's covered.\n\nNow, considering the parameters. The model needs learnable parameters. Let's say gamma is a learnable parameter. But perhaps there's also a baseline term. For example:\n\nU = beta * split_self + gamma * (split_self - fair_share)\n\nBut this introduces two parameters. Alternatively, maybe just gamma. Let's think.\n\nIf the utility is U = split_self + gamma * (split_self - fair_share), that can be rewritten as U = split_self * (1 + gamma) - gamma * fair_share. But this combines the terms. Alternatively, maybe:\n\nU = split_self + gamma * (split_self - fair_share) + intercept\n\nBut intercept could be another parameter. However, the intercept might not be necessary if the model is properly specified. Alternatively, the intercept could be fixed at zero, but that's a strong assumption.\n\nAlternatively, perhaps the utility is:\n\nU = theta * split_self + alpha * (split_self - fair_share)\n\nBut theta and alpha are parameters. Wait, this would be U = (theta + alpha) * split_self - alpha * fair_share. But this might lead to identifiability issues if theta and alpha are both free parameters. For example, different combinations of theta and alpha could lead to the same U. So, parameter recovery might be difficult. To avoid this, perhaps combine them into a single parameter. For example, set theta = 1 (fixed), and have alpha as a learnable parameter. Then:\n\nU = split_self + alpha * (split_self - fair_share)\n\nBut this is equivalent to U = split_self * (1 + alpha) - alpha * fair_share. Hmm. Alternatively, maybe the model should have a single parameter that captures the trade-off between the absolute offer and the fairness.\n\nAlternatively, the utility could be:\n\nU = split_self - alpha * (fair_share - split_self)\n\nWhich simplifies to U = split_self * (1 + alpha) - alpha * fair_share. Here, alpha is the weight on the difference between fair_share and split_self. If alpha is positive, then when split_self is less than fair_share, the utility decreases. If alpha is zero, the utility is just split_self.\n\nBut this introduces a single parameter, alpha. Let's see. The fair_share is calculated as (token_self / (token_self + token_opp)) * combined_earning. So, if the participant contributed more tokens, their fair_share is higher. If the split_self is less than that, the utility is reduced by alpha times the deficit.\n\nThis seems plausible. The parameter alpha represents how much the participant cares about receiving less than their fair share. A higher alpha means they are more sensitive to unfairness (being offered less than fair). If alpha is zero, they only care about the absolute split_self.\n\nBut what if the split_self is more than fair_share? Then, (fair_share - split_self) is negative, so the term becomes positive, increasing utility. So participants might actually get a boost in utility if they are offered more than fair. But is that realistic? In some models, people might not care as much about advantageous inequality, but in others, they might. However, in the responder's case, if they are offered more than fair, they are likely to accept, so the model should reflect that. But the question is whether the parameter alpha should have a different effect when the offer is above or below fair. \n\nIf we want to differentiate between advantageous and disadvantageous inequality, we need two parameters. But given the instruction to have fewer parameters, perhaps a single parameter is better. However, parameter recovery might be an issue if the model doesn't capture the asymmetry. But let's proceed with a single parameter for simplicity.\n\nSo, the model would be:\n\nU = split_self + alpha * (split_self - fair_share)\n\nBut wait, that's U = split_self + alpha*(split_self - fair_share) = split_self*(1 + alpha) - alpha*fair_share.\n\nAlternatively, perhaps:\n\nU = split_self - alpha * max(fair_share - split_self, 0) + beta * max(split_self - fair_share, 0)\n\nBut this introduces two parameters. However, the problem states that parameter recovery is as important as accuracy, so having two parameters might lead to higher covariance. Maybe better to have one parameter that captures the effect of unfairness in either direction. But that's a simplification.\n\nAlternatively, use the difference between split_self and fair_share, scaled by a single parameter. So:\n\nU = split_self + gamma * (split_self - fair_share)\n\nHere, gamma is the parameter. If gamma is positive, then participants derive more utility when they get more than fair, and less when they get less. If gamma is negative, the opposite. But this might not capture the asymmetric response typically seen in experiments (people are more sensitive to disadvantageous inequality). However, given the constraints, this might be the best approach.\n\nAlternatively, use a non-linear function, like a piecewise term. For example:\n\nU = split_self + alpha * max(fair_share - split_self, 0) + beta * max(split_self - fair_share, 0)\n\nBut this introduces two parameters. However, the problem allows for multiple parameters as long as they are justified. But the instruction says to have a slight bias towards fewer parameters. So maybe start with one parameter.\n\nAlternatively, think of the utility as a function that compares the proposed split to the fair split, with a single parameter governing the sensitivity to this difference. For example:\n\nU = split_self + theta * (split_self / fair_share - 1)\n\nThis way, theta scales the proportional difference. If split_self equals fair_share, the term is zero. If it's higher, the term is positive; lower, negative. Theta determines how much this proportional difference affects utility. But this introduces a non-linearity. However, the model needs to be linear in parameters for easier estimation.\n\nAlternatively, use the ratio directly. For example:\n\nU = theta * (split_self / fair_share) + (1 - theta) * split_self\n\nBut this complicates the interpretation. Maybe not.\n\nAnother angle: the participant's decision could depend on both the absolute amount they receive (split_self) and the relative share compared to their contribution. The relative share can be split_self / (token_self * 3), since each token is \u00a33. Wait, token_self is the number of tokens found by the participant, so their contribution to the combined_earning is token_self * \u00a33. Similarly for token_opp. So combined_earning is (token_self + token_opp) * \u00a33. Therefore, the fair_share is (token_self / (token_self + token_opp)) * combined_earning = token_self * \u00a33. So split_self should be compared to token_self * \u00a33. Wait, that's interesting. Because combined_earning is (token_self + token_opp) * 3. So the fair_share for the participant is exactly token_self * 3. Because their contribution is token_self tokens, each worth \u00a33. So the fair_share is simply their own contribution. The opponent's contribution is token_opp * 3. So the total combined_earning is (token_self + token_opp) * 3. Therefore, the fair split would be each keeping their own contribution. Any split where the participant gets more than their contribution is advantageous, less is disadvantageous.\n\nWait, this is a key insight. The fair_share is actually token_self * 3. Because each token is \u00a33. So if the participant found 5 tokens, their contribution is \u00a315. The opponent's contribution is token_opp * 3. The combined_earning is (5 + token_opp) * 3. So the fair split would be each keeping their own contribution. So the fair_share for the participant is token_self * 3. Therefore, the split_self should be compared to token_self * 3. If split_self is less than token_self *3, it's disadvantageous; if more, advantageous.\n\nThis simplifies the model. Because the fair_share is simply token_self *3. So the difference between split_self and fair_share is split_self - (token_self *3). This difference can be a key variable in the utility function.\n\nSo, the utility could be:\n\nU = split_self + alpha * (split_self - token_self *3)\n\nBut this is equivalent to U = split_self*(1 + alpha) - alpha * token_self *3. Alternatively, maybe:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nHere, alpha is the sensitivity to being offered less than their own contribution. If the split_self is less than their contribution, the utility decreases by alpha times the deficit. If it's more, there's no effect. This captures disadvantageous inequality aversion.\n\nAlternatively, the utility could be:\n\nU = split_self - alpha * (token_self *3 - split_self) if split_self < token_self *3\nU = split_self + beta * (split_self - token_self *3) if split_self >= token_self *3\n\nBut this introduces two parameters. However, given the need for simplicity, perhaps model only the disadvantageous case. Because when the offer is below their contribution, the participant might be more likely to reject, whereas offers above are accepted. So the key parameter is the aversion to offers below their contribution.\n\nSo, the utility function could be:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nIn this case, alpha represents the penalty for each pound below their fair share (own contribution). If alpha is high, even a small deficit leads to a large drop in utility, increasing the likelihood of rejection.\n\nAdditionally, the participant might also care about the absolute amount they receive, so split_self is already in the utility. But in this model, the utility is split_self minus alpha times any deficit. So if the offer is exactly their contribution, U = split_self. If it's more, U = split_self. If it's less, U is reduced.\n\nBut wait, if the offer is more than their contribution, the max term is zero, so U = split_self. So in that case, higher offers increase utility linearly. For offers below, utility is split_self minus the penalty. This could work.\n\nAnother consideration: the participant's decision might also depend on the opponent's share. For example, if the opponent is keeping a lot more than their contribution, the participant might feel it's unfair. But in the responder role, the participant is responding to the opponent's proposal. The split_opp is the opponent's proposed share. But split_opp + split_self = combined_earning. So split_opp = combined_earning - split_self. So including split_opp might be redundant, but perhaps the participant compares their split to the opponent's. For example, if the opponent is taking more than their contribution, the participant might reject even if their own split is fair. But this complicates the model.\n\nAlternatively, the utility could include a term comparing the participant's split to the opponent's split relative to their contributions. For example:\n\nparticipant's fair share: token_self *3\nopponent's fair share: token_opp *3\nparticipant's received: split_self\nopponent's received: split_opp = combined_earning - split_self\n\nThe participant might compare (split_self - token_self*3) and (split_opp - token_opp*3). If both are zero, it's perfectly fair. If the participant is below and the opponent is above, that's unfair. But modeling this would require more parameters. For example, the utility could be:\n\nU = split_self - alpha * max(token_self*3 - split_self, 0) - beta * max(split_opp - token_opp*3, 0)\n\nHere, alpha is the penalty for being below own contribution, and beta is the penalty for the opponent being above their contribution. But this adds another parameter. However, given the need for parameter recovery, adding more parameters might be problematic. So perhaps stick to the simpler model with only the participant's deficit.\n\nAnother angle: the participant's utility could depend on the ratio of their received amount to their contribution. For example, U = split_self + gamma * (split_self / (token_self *3) - 1). This way, gamma scales the proportional gain or loss relative to their contribution. If split_self is equal to their contribution, the term is zero. If it's higher, the term is positive; lower, negative. Gamma determines how much this proportion affects utility.\n\nBut this introduces a non-linear term. However, in terms of parameters, it's still one parameter (gamma). The utility function would be:\n\nU = split_self + gamma * (split_self / (token_self *3) - 1)\n\nThis can be rewritten as U = split_self * (1 + gamma/(token_self *3)) - gamma. But this complicates the interpretation. Alternatively, maybe it's better to keep it in the original form.\n\nAlternatively, use the difference rather than the ratio. The difference is split_self - token_self*3, which is in pounds. The ratio is unitless. Depending on the participant's psychology, they might care more about absolute differences or relative ratios. For example, someone might be upset if they get \u00a35 less than their contribution, regardless of the total. Others might care about the percentage. But the problem states that two equal split_self values don't represent the same fairness because combined_earning varies. Wait, but in our previous analysis, split_self is compared to token_self*3, which is fixed based on their contribution, not the combined_earning. Wait, token_self is fixed per trial, so token_self*3 is fixed. The combined_earning is (token_self + token_opp)*3. So split_self can vary independently of token_self*3. For example, if combined_earning is higher (because opponent contributed more), the participant's fair share is still token_self*3. So even if the split_self is the same in absolute terms, if the combined_earning is higher, the opponent's share is higher, which might affect the participant's decision. But according to the model we're considering, the participant only cares about their own split relative to their contribution. So in this case, the model would not account for the opponent's share beyond what's implied by split_self and combined_earning.\n\nBut according to the problem statement, the same split_self can correspond to different fairness levels because combined_earning varies. For example, split_self=\u00a35 when combined_earning=\u00a310 (participant gets 50%) versus split_self=\u00a35 when combined_earning=\u00a320 (25%). But in our model, the fairness is based on the participant's contribution (token_self*3), not the combined_earning. So if the participant's contribution is, say, \u00a36 (token_self=2), then split_self=\u00a35 is below their contribution, leading to a penalty. If their contribution is \u00a33 (token_self=1), split_self=\u00a35 is above, no penalty. So the model accounts for the participant's own contribution, not the total. Therefore, the same split_self can be fair or unfair depending on their own contribution, which is captured in the model.\n\nThis seems to align with the problem statement's note that two equal split_self variables don't represent the same fairness. Because token_self (and thus their contribution) can vary, leading to different fair shares.\n\nSo, the model comparing split_self to token_self*3 (their contribution) addresses this issue.\n\nTherefore, the utility function could be:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nHere, alpha is the penalty per pound below the participant's contribution. If the offer is below their contribution, utility decreases, making acceptance less likely. If the offer is equal or above, utility is just split_self, so higher offers increase the chance of acceptance.\n\nThis model has one learnable parameter, alpha, which must be non-negative (since being below contribution should decrease utility). The range for alpha could be [0, some upper bound], say 0 to 5, as higher values would indicate greater sensitivity to unfairness.\n\nAdditionally, the model should include a parameter for the baseline utility of accepting, perhaps an intercept term. For example:\n\nU = beta * split_self - alpha * max(token_self *3 - split_self, 0) + intercept\n\nBut this introduces three parameters (beta, alpha, intercept), which might complicate parameter recovery. However, beta could be fixed to 1 if we assume linear utility in money. But is that a valid assumption? Maybe participants have diminishing marginal utility, but for simplicity, assume linear.\n\nAlternatively, set beta=1 and have alpha and intercept as parameters. But the intercept would represent a baseline tendency to accept or reject regardless of the offer. However, in many models, the intercept is omitted because the utility is compared to a threshold (often zero) via the logistic function. Including an intercept could account for a general bias towards accepting or rejecting.\n\nBut the problem states that the utility should be such that less than zero corresponds to less than 50% probability. If we include an intercept, it could shift the utility. For example, if the intercept is positive, even offers slightly below fair might still have U>0. But without the intercept, the model is more parsimonious.\n\nGiven the need for simplicity and parameter recovery, perhaps omit the intercept and set beta=1. So the utility is:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nThis model has one learnable parameter, alpha, which must be >=0. The range for alpha could be [0, 10], for example, to allow for a generous bound.\n\nAnother consideration: the participant's decision might also depend on the trial type (equal, opponent more, participant more). For example, in trials where the participant contributed more, they might be more sensitive to unfair offers. However, including trial_type would require additional parameters, increasing model complexity. Since the instruction prefers fewer parameters, perhaps omit this unless necessary.\n\nAlternatively, the model could allow alpha to vary by trial_type, but that would introduce multiple parameters. For example, three alphas for each trial type. But this complicates the model. Given the dataset has 36 trials per participant, it's possible to estimate individual parameters, but parameter recovery might be challenging with more parameters.\n\nTherefore, sticking to a single alpha parameter for all trial types is preferable for simplicity and parameter recovery.\n\nNow, the utility function is:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nThe probability of accepting is logistic(U) = 1 / (1 + exp(-U)). If U >=0, P(accept) >=0.5; else, <0.5.\n\nThis model uses split_self and token_self from the dataset. The fair_share is calculated as token_self *3. The max function ensures that only deficits relative to fair_share are penalized.\n\nNow, checking if all variables used are in the dataset. The variables needed are split_self and token_self, which are present. The combined_earning is not directly used, but token_self and token_opp are used to compute fair_share. Wait, no: in this model, fair_share is token_self *3, which is derived from token_self. So combined_earning is not directly used, but token_self is part of the dataset.\n\nYes, the dataset includes token_self and split_self. So the model is feasible with the provided variables.\n\nNow, the parameters:\n\n- alpha: learnable parameter, range [0, 10] (inclusive). Represents the penalty per pound below the participant's contribution.\n\nThis is a single parameter, which is good for parameter recovery. The bounds are generous but finite.\n\nNow, the variable descriptions:\n\nVariables in the model:\n\n- U: utility, calculated as split_self - alpha * max(token_self*3 - split_self, 0). Not learnable, calculated.\n\n- split_self: from data, type float64.\n\n- token_self: from data, type int64.\n\n- alpha: learnable parameter, range [0, 10], inclusive.\n\nThe max term can be represented as deficit = max(token_self*3 - split_self, 0), which is calculated.\n\nSo the model formula is:\n\nU = split_self - alpha * deficit\n\nwhere deficit = max(token_self *3 - split_self, 0)\n\nBut in mathematical notation, we can write:\n\nU = s - \u03b1 * max(3t_s - s, 0)\n\nwhere s = split_self, t_s = token_self.\n\nSo the model between <MODEL> tags would be:\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\nVariables:\n\n- split_self: from data.\n- token_self: from data.\n- alpha: learnable parameter with range [0, 10].\n\nNow, the variable descriptions in JSON:\n\nFor each variable in the model:\n\n- U: calculated, not learnable.\n- split_self: data variable.\n- token_self: data variable.\n- alpha: learnable parameter.\n\nSo the JSON would be:\n\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n\nThe target variable is 'accept', so enclosed in <target_variable> tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the absolute offer and a penalty for receiving less than the participant's contribution (fair share). The penalty scales linearly with the deficit, controlled by a single learnable parameter (alpha). Utility is converted to acceptance probability via logistic function, ensuring U < 0 corresponds to <50% acceptance likelihood.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                tags would be:\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \\u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        deficit = max(3 * token_self - split_self, 0)\n        U = split_self - alpha * deficit\n\n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"control\", \"trial_number\": 0, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 8, \"token_self\": 8, \"combined_earning\": 48, \"split_opp\": 24.0, \"split_self\": 24.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 24.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 24.0}, {\"ID\": 1, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 8, \"token_self\": 8, \"combined_earning\": 48, \"split_opp\": 30.0, \"split_self\": 18.0, \"splitperc_opp\": 63, \"splitperc_self\": 38, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 37.5, \"proposed_amount\": 18.0}, {\"ID\": 2, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 8, \"token_self\": 8, \"combined_earning\": 48, \"split_opp\": 36.0, \"split_self\": 12.0, \"splitperc_opp\": 75, \"splitperc_self\": 25, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 25.0, \"proposed_amount\": 12.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1743512984.5363772
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "tags would be:\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "data-dependent",
        "max": "data-dependent",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "data-dependent",
        "max": "data-dependent",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "alpha": {
      "description": "Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_summary": "A utility model where acceptance probability depends on the absolute offer and a penalty for receiving less than the participant's contribution (fair share). The penalty scales linearly with the deficit, controlled by a single learnable parameter (alpha). Utility is converted to acceptance probability via logistic function, ensuring U < 0 corresponds to <50% acceptance likelihood.",
  "target_variable": "tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure that parameters can be recovered, which is as important as accuracy. Also, the utility function should be such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on the utility of accepting the proposal. The utility function will be converted to a probability via a logistic function.\n\nLooking at the dataset variables, the important ones for the responder's decision might include the proposed shares (split_self and split_opp), the combined earnings, and possibly the tokens found by both the participant and the opponent (token_self and token_opp). The trial_type indicates whether contributions are equal, opponent more, or participant more, which might influence fairness perceptions.\n\nIn responder role (trial_role=1), the participant decides to accept or reject the split proposed by the opponent. The key factors here are likely fairness considerations and the monetary outcome. The utility of accepting could depend on how fair the proposed split is relative to some reference point, such as the participant's own contribution (token_self) versus the opponent's (token_opp). \n\nA common approach in behavioral economics is the Fehr-Schmidt model of inequity aversion, where individuals care about both their own payoff and the difference between their payoff and others'. However, since this is a responder model, maybe the utility is a function of the proposed split compared to what's fair based on contributions.\n\nThe combined_earning is the total pot, which is split between the two. The split_self is the amount offered to the participant. The fair split might be proportional to the tokens each contributed. For example, if the participant contributed token_self tokens and the opponent token_opp, the fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. \n\nSo, the model could compare the proposed split_self to this fair share. The difference between the proposed and fair share could be a key component of the utility. Additionally, there might be a parameter representing the participant's sensitivity to unfairness (alpha). If the proposed split is less than the fair share, the utility decreases, and vice versa. However, since the responder can only accept or reject, perhaps the disutility comes from being offered less than fair, and the utility from the absolute amount.\n\nAnother consideration is the efficiency: the total combined earning. Even if the split is unfair, a higher total might make acceptance more likely. So, the utility could have a term for the proposed amount (split_self) and a term for the fairness deviation.\n\nWait, but the problem states that two equal split_self variables don't represent the same fairness because the combined earning varies. So, the fairness should be relative to the total. For example, getting \u00a35 out of \u00a310 is different from \u00a35 out of \u00a320. So, perhaps the percentage split (splitperc_self) is important. But splitperc_self is already provided. However, maybe the fair percentage is based on the tokens. The fair percentage would be (token_self / (token_self + token_opp)) * 100. The difference between splitperc_self and this fair percentage could be a factor.\n\nAlternatively, the model could compute the fair split as (token_self / (token_self + token_opp)) * combined_earning, then compare split_self to this value. The difference (split_self - fair_share) could be part of the utility. If the split is more than fair, positive utility; if less, negative. But how to model this?\n\nLet me formalize this. Let's compute the fair share as:\n\nfair_share = (token_self / (token_self + token_opp)) * combined_earning\n\nThen, the difference between the proposed split_self and fair_share is delta = split_self - fair_share.\n\nThe utility could be a function of delta. If delta is positive, the participant is getting more than fair, which is good. If negative, less than fair, which is bad. The utility might be:\n\nU = beta * split_self + alpha * delta\n\nBut beta and alpha are parameters. However, beta could represent the sensitivity to the absolute amount offered, while alpha represents the sensitivity to fairness (deviation from fair share). Alternatively, maybe the utility is split_self plus a term that penalizes unfairness. For example:\n\nU = split_self - alpha * max(fair_share - split_self, 0)\n\nHere, alpha is the aversion to being undercompensated relative to the fair share. If the split_self is less than fair_share, the participant experiences a disutility proportional to the difference. If it's more, there's no penalty. This is similar to loss aversion or inequity aversion models.\n\nAlternatively, the model could use the ratio of split_self to fair_share. For example:\n\nU = split_self + gamma * (split_self / fair_share - 1)\n\nBut this might complicate things. Let's think about the parameters. The model needs learnable parameters with finite bounds. Let's consider alpha as the fairness sensitivity, and beta as the weight on the absolute split. But perhaps the key is to model the utility as a combination of the monetary gain and the fairness deviation.\n\nAnother angle: the participant's utility could depend on both their own share and the opponent's share, adjusted by their contributions. For example, the participant might compare their share to the opponent's share in light of their respective contributions. If the opponent contributed more, maybe the participant expects a larger share for the opponent. So, the ratio of contributions (token_self to token_opp) could influence the perceived fairness of the split.\n\nAlternatively, the utility could be:\n\nU = split_self + theta * (split_self - (token_self / (token_self + token_opp)) * combined_earning)\n\nHere, theta is a parameter that scales how much deviations from the fair share affect utility. If theta is positive, participants prefer more than their fair share; if negative, they might be willing to accept less. Wait, but theta would be a learnable parameter. If theta is positive, then getting more than fair share increases utility. If theta is negative, getting more than fair share decreases utility, which might not make sense. Hmm.\n\nAlternatively, perhaps the model should have two parameters: one for the sensitivity to advantageous inequality (getting more than fair) and one for disadvantageous inequality (getting less). But that might complicate the model with more parameters. However, the problem states to have a slight bias towards fewer parameters. So maybe a single parameter for fairness.\n\nWait, the Fehr-Schmidt model has two parameters: alpha for disadvantageous inequality and beta for advantageous. But since we need to keep the number of parameters low, perhaps combine them into one. Alternatively, focus only on disadvantageous inequality, as that's more likely to affect rejection decisions. If the participant is offered less than fair, they might reject to punish, even at a cost. So, the utility could be:\n\nU = split_self - alpha * max(fair_share - split_self, 0)\n\nIn this case, alpha represents the aversion to being below the fair share. If the offered split is below fair, the utility decreases by alpha times the deficit. The higher alpha, the more the participant dislikes being shortchanged.\n\nBut then, how does the total combined earning play in? For example, if the combined earning is higher, even a small percentage could be a larger absolute amount. But the model should account for the fact that the same split percentage can correspond to different absolute amounts depending on combined_earning. However, the split_self is already in absolute terms (in \u00a3), so maybe that's already captured. The fair_share is calculated based on tokens and combined_earning, so the delta is in \u00a3 terms.\n\nSo, the utility function could be:\n\nU = split_self + alpha * (split_self - fair_share)\n\nWait, but if split_self is less than fair_share, then (split_self - fair_share) is negative, so the utility would decrease. Alternatively, perhaps:\n\nU = split_self - alpha * max(fair_share - split_self, 0) + beta * max(split_self - fair_share, 0)\n\nHere, alpha is the penalty for being below fair, and beta is the reward for being above. But this introduces two parameters. However, the problem says to have a slight bias towards fewer parameters. Maybe combine them into a single parameter, assuming that the sensitivity to advantageous and disadvantageous inequality is the same. But that's a strong assumption. Alternatively, just model disadvantageous inequality, as that's more relevant for rejection.\n\nAlternatively, the utility could be:\n\nU = split_self + gamma * (split_self - fair_share)\n\nHere, gamma is a parameter. If gamma is positive, participants value getting more than fair; if gamma is negative, they dislike inequality. But this might not capture the asymmetric response. However, given the need for simplicity, this could be a starting point.\n\nBut how does this relate to the probability of acceptance? The logistic function converts utility to a probability. So, P(accept) = 1 / (1 + exp(-U)). If U is positive, the probability is above 50%; if negative, below.\n\nBut the problem states that the utility should be such that less than 0 corresponds to less than 50% chance. That's exactly how the logistic function works, so that's covered.\n\nNow, considering the parameters. The model needs learnable parameters. Let's say gamma is a learnable parameter. But perhaps there's also a baseline term. For example:\n\nU = beta * split_self + gamma * (split_self - fair_share)\n\nBut this introduces two parameters. Alternatively, maybe just gamma. Let's think.\n\nIf the utility is U = split_self + gamma * (split_self - fair_share), that can be rewritten as U = split_self * (1 + gamma) - gamma * fair_share. But this combines the terms. Alternatively, maybe:\n\nU = split_self + gamma * (split_self - fair_share) + intercept\n\nBut intercept could be another parameter. However, the intercept might not be necessary if the model is properly specified. Alternatively, the intercept could be fixed at zero, but that's a strong assumption.\n\nAlternatively, perhaps the utility is:\n\nU = theta * split_self + alpha * (split_self - fair_share)\n\nBut theta and alpha are parameters. Wait, this would be U = (theta + alpha) * split_self - alpha * fair_share. But this might lead to identifiability issues if theta and alpha are both free parameters. For example, different combinations of theta and alpha could lead to the same U. So, parameter recovery might be difficult. To avoid this, perhaps combine them into a single parameter. For example, set theta = 1 (fixed), and have alpha as a learnable parameter. Then:\n\nU = split_self + alpha * (split_self - fair_share)\n\nBut this is equivalent to U = split_self * (1 + alpha) - alpha * fair_share. Hmm. Alternatively, maybe the model should have a single parameter that captures the trade-off between the absolute offer and the fairness.\n\nAlternatively, the utility could be:\n\nU = split_self - alpha * (fair_share - split_self)\n\nWhich simplifies to U = split_self * (1 + alpha) - alpha * fair_share. Here, alpha is the weight on the difference between fair_share and split_self. If alpha is positive, then when split_self is less than fair_share, the utility decreases. If alpha is zero, the utility is just split_self.\n\nBut this introduces a single parameter, alpha. Let's see. The fair_share is calculated as (token_self / (token_self + token_opp)) * combined_earning. So, if the participant contributed more tokens, their fair_share is higher. If the split_self is less than that, the utility is reduced by alpha times the deficit.\n\nThis seems plausible. The parameter alpha represents how much the participant cares about receiving less than their fair share. A higher alpha means they are more sensitive to unfairness (being offered less than fair). If alpha is zero, they only care about the absolute split_self.\n\nBut what if the split_self is more than fair_share? Then, (fair_share - split_self) is negative, so the term becomes positive, increasing utility. So participants might actually get a boost in utility if they are offered more than fair. But is that realistic? In some models, people might not care as much about advantageous inequality, but in others, they might. However, in the responder's case, if they are offered more than fair, they are likely to accept, so the model should reflect that. But the question is whether the parameter alpha should have a different effect when the offer is above or below fair. \n\nIf we want to differentiate between advantageous and disadvantageous inequality, we need two parameters. But given the instruction to have fewer parameters, perhaps a single parameter is better. However, parameter recovery might be an issue if the model doesn't capture the asymmetry. But let's proceed with a single parameter for simplicity.\n\nSo, the model would be:\n\nU = split_self + alpha * (split_self - fair_share)\n\nBut wait, that's U = split_self + alpha*(split_self - fair_share) = split_self*(1 + alpha) - alpha*fair_share.\n\nAlternatively, perhaps:\n\nU = split_self - alpha * max(fair_share - split_self, 0) + beta * max(split_self - fair_share, 0)\n\nBut this introduces two parameters. However, the problem states that parameter recovery is as important as accuracy, so having two parameters might lead to higher covariance. Maybe better to have one parameter that captures the effect of unfairness in either direction. But that's a simplification.\n\nAlternatively, use the difference between split_self and fair_share, scaled by a single parameter. So:\n\nU = split_self + gamma * (split_self - fair_share)\n\nHere, gamma is the parameter. If gamma is positive, then participants derive more utility when they get more than fair, and less when they get less. If gamma is negative, the opposite. But this might not capture the asymmetric response typically seen in experiments (people are more sensitive to disadvantageous inequality). However, given the constraints, this might be the best approach.\n\nAlternatively, use a non-linear function, like a piecewise term. For example:\n\nU = split_self + alpha * max(fair_share - split_self, 0) + beta * max(split_self - fair_share, 0)\n\nBut this introduces two parameters. However, the problem allows for multiple parameters as long as they are justified. But the instruction says to have a slight bias towards fewer parameters. So maybe start with one parameter.\n\nAlternatively, think of the utility as a function that compares the proposed split to the fair split, with a single parameter governing the sensitivity to this difference. For example:\n\nU = split_self + theta * (split_self / fair_share - 1)\n\nThis way, theta scales the proportional difference. If split_self equals fair_share, the term is zero. If it's higher, the term is positive; lower, negative. Theta determines how much this proportional difference affects utility. But this introduces a non-linearity. However, the model needs to be linear in parameters for easier estimation.\n\nAlternatively, use the ratio directly. For example:\n\nU = theta * (split_self / fair_share) + (1 - theta) * split_self\n\nBut this complicates the interpretation. Maybe not.\n\nAnother angle: the participant's decision could depend on both the absolute amount they receive (split_self) and the relative share compared to their contribution. The relative share can be split_self / (token_self * 3), since each token is \u00a33. Wait, token_self is the number of tokens found by the participant, so their contribution to the combined_earning is token_self * \u00a33. Similarly for token_opp. So combined_earning is (token_self + token_opp) * \u00a33. Therefore, the fair_share is (token_self / (token_self + token_opp)) * combined_earning = token_self * \u00a33. So split_self should be compared to token_self * \u00a33. Wait, that's interesting. Because combined_earning is (token_self + token_opp) * 3. So the fair_share for the participant is exactly token_self * 3. Because their contribution is token_self tokens, each worth \u00a33. So the fair_share is simply their own contribution. The opponent's contribution is token_opp * 3. So the total combined_earning is (token_self + token_opp) * 3. Therefore, the fair split would be each keeping their own contribution. Any split where the participant gets more than their contribution is advantageous, less is disadvantageous.\n\nWait, this is a key insight. The fair_share is actually token_self * 3. Because each token is \u00a33. So if the participant found 5 tokens, their contribution is \u00a315. The opponent's contribution is token_opp * 3. The combined_earning is (5 + token_opp) * 3. So the fair split would be each keeping their own contribution. So the fair_share for the participant is token_self * 3. Therefore, the split_self should be compared to token_self * 3. If split_self is less than token_self *3, it's disadvantageous; if more, advantageous.\n\nThis simplifies the model. Because the fair_share is simply token_self *3. So the difference between split_self and fair_share is split_self - (token_self *3). This difference can be a key variable in the utility function.\n\nSo, the utility could be:\n\nU = split_self + alpha * (split_self - token_self *3)\n\nBut this is equivalent to U = split_self*(1 + alpha) - alpha * token_self *3. Alternatively, maybe:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nHere, alpha is the sensitivity to being offered less than their own contribution. If the split_self is less than their contribution, the utility decreases by alpha times the deficit. If it's more, there's no effect. This captures disadvantageous inequality aversion.\n\nAlternatively, the utility could be:\n\nU = split_self - alpha * (token_self *3 - split_self) if split_self < token_self *3\nU = split_self + beta * (split_self - token_self *3) if split_self >= token_self *3\n\nBut this introduces two parameters. However, given the need for simplicity, perhaps model only the disadvantageous case. Because when the offer is below their contribution, the participant might be more likely to reject, whereas offers above are accepted. So the key parameter is the aversion to offers below their contribution.\n\nSo, the utility function could be:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nIn this case, alpha represents the penalty for each pound below their fair share (own contribution). If alpha is high, even a small deficit leads to a large drop in utility, increasing the likelihood of rejection.\n\nAdditionally, the participant might also care about the absolute amount they receive, so split_self is already in the utility. But in this model, the utility is split_self minus alpha times any deficit. So if the offer is exactly their contribution, U = split_self. If it's more, U = split_self. If it's less, U is reduced.\n\nBut wait, if the offer is more than their contribution, the max term is zero, so U = split_self. So in that case, higher offers increase utility linearly. For offers below, utility is split_self minus the penalty. This could work.\n\nAnother consideration: the participant's decision might also depend on the opponent's share. For example, if the opponent is keeping a lot more than their contribution, the participant might feel it's unfair. But in the responder role, the participant is responding to the opponent's proposal. The split_opp is the opponent's proposed share. But split_opp + split_self = combined_earning. So split_opp = combined_earning - split_self. So including split_opp might be redundant, but perhaps the participant compares their split to the opponent's. For example, if the opponent is taking more than their contribution, the participant might reject even if their own split is fair. But this complicates the model.\n\nAlternatively, the utility could include a term comparing the participant's split to the opponent's split relative to their contributions. For example:\n\nparticipant's fair share: token_self *3\nopponent's fair share: token_opp *3\nparticipant's received: split_self\nopponent's received: split_opp = combined_earning - split_self\n\nThe participant might compare (split_self - token_self*3) and (split_opp - token_opp*3). If both are zero, it's perfectly fair. If the participant is below and the opponent is above, that's unfair. But modeling this would require more parameters. For example, the utility could be:\n\nU = split_self - alpha * max(token_self*3 - split_self, 0) - beta * max(split_opp - token_opp*3, 0)\n\nHere, alpha is the penalty for being below own contribution, and beta is the penalty for the opponent being above their contribution. But this adds another parameter. However, given the need for parameter recovery, adding more parameters might be problematic. So perhaps stick to the simpler model with only the participant's deficit.\n\nAnother angle: the participant's utility could depend on the ratio of their received amount to their contribution. For example, U = split_self + gamma * (split_self / (token_self *3) - 1). This way, gamma scales the proportional gain or loss relative to their contribution. If split_self is equal to their contribution, the term is zero. If it's higher, the term is positive; lower, negative. Gamma determines how much this proportion affects utility.\n\nBut this introduces a non-linear term. However, in terms of parameters, it's still one parameter (gamma). The utility function would be:\n\nU = split_self + gamma * (split_self / (token_self *3) - 1)\n\nThis can be rewritten as U = split_self * (1 + gamma/(token_self *3)) - gamma. But this complicates the interpretation. Alternatively, maybe it's better to keep it in the original form.\n\nAlternatively, use the difference rather than the ratio. The difference is split_self - token_self*3, which is in pounds. The ratio is unitless. Depending on the participant's psychology, they might care more about absolute differences or relative ratios. For example, someone might be upset if they get \u00a35 less than their contribution, regardless of the total. Others might care about the percentage. But the problem states that two equal split_self values don't represent the same fairness because combined_earning varies. Wait, but in our previous analysis, split_self is compared to token_self*3, which is fixed based on their contribution, not the combined_earning. Wait, token_self is fixed per trial, so token_self*3 is fixed. The combined_earning is (token_self + token_opp)*3. So split_self can vary independently of token_self*3. For example, if combined_earning is higher (because opponent contributed more), the participant's fair share is still token_self*3. So even if the split_self is the same in absolute terms, if the combined_earning is higher, the opponent's share is higher, which might affect the participant's decision. But according to the model we're considering, the participant only cares about their own split relative to their contribution. So in this case, the model would not account for the opponent's share beyond what's implied by split_self and combined_earning.\n\nBut according to the problem statement, the same split_self can correspond to different fairness levels because combined_earning varies. For example, split_self=\u00a35 when combined_earning=\u00a310 (participant gets 50%) versus split_self=\u00a35 when combined_earning=\u00a320 (25%). But in our model, the fairness is based on the participant's contribution (token_self*3), not the combined_earning. So if the participant's contribution is, say, \u00a36 (token_self=2), then split_self=\u00a35 is below their contribution, leading to a penalty. If their contribution is \u00a33 (token_self=1), split_self=\u00a35 is above, no penalty. So the model accounts for the participant's own contribution, not the total. Therefore, the same split_self can be fair or unfair depending on their own contribution, which is captured in the model.\n\nThis seems to align with the problem statement's note that two equal split_self variables don't represent the same fairness. Because token_self (and thus their contribution) can vary, leading to different fair shares.\n\nSo, the model comparing split_self to token_self*3 (their contribution) addresses this issue.\n\nTherefore, the utility function could be:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nHere, alpha is the penalty per pound below the participant's contribution. If the offer is below their contribution, utility decreases, making acceptance less likely. If the offer is equal or above, utility is just split_self, so higher offers increase the chance of acceptance.\n\nThis model has one learnable parameter, alpha, which must be non-negative (since being below contribution should decrease utility). The range for alpha could be [0, some upper bound], say 0 to 5, as higher values would indicate greater sensitivity to unfairness.\n\nAdditionally, the model should include a parameter for the baseline utility of accepting, perhaps an intercept term. For example:\n\nU = beta * split_self - alpha * max(token_self *3 - split_self, 0) + intercept\n\nBut this introduces three parameters (beta, alpha, intercept), which might complicate parameter recovery. However, beta could be fixed to 1 if we assume linear utility in money. But is that a valid assumption? Maybe participants have diminishing marginal utility, but for simplicity, assume linear.\n\nAlternatively, set beta=1 and have alpha and intercept as parameters. But the intercept would represent a baseline tendency to accept or reject regardless of the offer. However, in many models, the intercept is omitted because the utility is compared to a threshold (often zero) via the logistic function. Including an intercept could account for a general bias towards accepting or rejecting.\n\nBut the problem states that the utility should be such that less than zero corresponds to less than 50% probability. If we include an intercept, it could shift the utility. For example, if the intercept is positive, even offers slightly below fair might still have U>0. But without the intercept, the model is more parsimonious.\n\nGiven the need for simplicity and parameter recovery, perhaps omit the intercept and set beta=1. So the utility is:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nThis model has one learnable parameter, alpha, which must be >=0. The range for alpha could be [0, 10], for example, to allow for a generous bound.\n\nAnother consideration: the participant's decision might also depend on the trial type (equal, opponent more, participant more). For example, in trials where the participant contributed more, they might be more sensitive to unfair offers. However, including trial_type would require additional parameters, increasing model complexity. Since the instruction prefers fewer parameters, perhaps omit this unless necessary.\n\nAlternatively, the model could allow alpha to vary by trial_type, but that would introduce multiple parameters. For example, three alphas for each trial type. But this complicates the model. Given the dataset has 36 trials per participant, it's possible to estimate individual parameters, but parameter recovery might be challenging with more parameters.\n\nTherefore, sticking to a single alpha parameter for all trial types is preferable for simplicity and parameter recovery.\n\nNow, the utility function is:\n\nU = split_self - alpha * max(token_self *3 - split_self, 0)\n\nThe probability of accepting is logistic(U) = 1 / (1 + exp(-U)). If U >=0, P(accept) >=0.5; else, <0.5.\n\nThis model uses split_self and token_self from the dataset. The fair_share is calculated as token_self *3. The max function ensures that only deficits relative to fair_share are penalized.\n\nNow, checking if all variables used are in the dataset. The variables needed are split_self and token_self, which are present. The combined_earning is not directly used, but token_self and token_opp are used to compute fair_share. Wait, no: in this model, fair_share is token_self *3, which is derived from token_self. So combined_earning is not directly used, but token_self is part of the dataset.\n\nYes, the dataset includes token_self and split_self. So the model is feasible with the provided variables.\n\nNow, the parameters:\n\n- alpha: learnable parameter, range [0, 10] (inclusive). Represents the penalty per pound below the participant's contribution.\n\nThis is a single parameter, which is good for parameter recovery. The bounds are generous but finite.\n\nNow, the variable descriptions:\n\nVariables in the model:\n\n- U: utility, calculated as split_self - alpha * max(token_self*3 - split_self, 0). Not learnable, calculated.\n\n- split_self: from data, type float64.\n\n- token_self: from data, type int64.\n\n- alpha: learnable parameter, range [0, 10], inclusive.\n\nThe max term can be represented as deficit = max(token_self*3 - split_self, 0), which is calculated.\n\nSo the model formula is:\n\nU = split_self - alpha * deficit\n\nwhere deficit = max(token_self *3 - split_self, 0)\n\nBut in mathematical notation, we can write:\n\nU = s - \u03b1 * max(3t_s - s, 0)\n\nwhere s = split_self, t_s = token_self.\n\nSo the model between <MODEL> tags would be:\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\nVariables:\n\n- split_self: from data.\n- token_self: from data.\n- alpha: learnable parameter with range [0, 10].\n\nNow, the variable descriptions in JSON:\n\nFor each variable in the model:\n\n- U: calculated, not learnable.\n- split_self: data variable.\n- token_self: data variable.\n- alpha: learnable parameter.\n\nSo the JSON would be:\n\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n\nThe target variable is 'accept', so enclosed in <target_variable> tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the absolute offer and a penalty for receiving less than the participant's contribution (fair share). The penalty scales linearly with the deficit, controlled by a single learnable parameter (alpha). Utility is converted to acceptance probability via logistic function, ensuring U < 0 corresponds to <50% acceptance likelihood.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: tags would be:\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\nSummary: A utility model where acceptance probability depends on the absolute offer and a penalty for receiving less than the participant's contribution (fair share). The penalty scales linearly with the deficit, controlled by a single learnable parameter (alpha). Utility is converted to acceptance probability via logistic function, ensuring U < 0 corresponds to <50% acceptance likelihood.\nTarget Variable: tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        deficit = max(3 * token_self - split_self, 0)\n        U = split_self - alpha * deficit\n\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "fitting_warnings": [
    "Error fitting participant 1001: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1002: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1003: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1004: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1005: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1006: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1007: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1008: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1009: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1010: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1011: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1012: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1013: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1015: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1016: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1017: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1018: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1019: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1020: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1021: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1022: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1023: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1024: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1025: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1026: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1027: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1028: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1029: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1030: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1031: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1032: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1033: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1034: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1035: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1036: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1037: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1038: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1039: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1040: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1041: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1042: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1043: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 1044: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2001: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2002: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2003: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2004: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2005: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2006: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2007: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2008: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2009: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2010: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2011: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2012: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2013: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2014: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2015: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2016: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2017: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2018: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2019: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2020: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2021: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2022: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2023: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2024: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2025: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2026: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2027: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2028: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2029: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2030: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2031: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2032: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2033: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2034: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2035: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2036: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2037: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2038: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2039: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2040: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2041: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2042: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2043: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2044: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2045: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2046: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2047: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data",
    "Error fitting participant 2048: Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
  ],
  "skipped_participants": [
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: Missing 'tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept' column in data"
    }
  ],
  "num_skipped_participants": 91,
  "fitting_error": "No successful parameter fits obtained",
  "recovery_error": "No fitting results found",
  "previous_models": [
    "Specification: tags would be:\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\nSummary: A utility model where acceptance probability depends on the absolute offer and a penalty for receiving less than the participant's contribution (fair share). The penalty scales linearly with the deficit, controlled by a single learnable parameter (alpha). Utility is converted to acceptance probability via logistic function, ensuring U < 0 corresponds to <50% acceptance likelihood.\nTarget Variable: tags.\n\nSummary: The model assumes participants evaluate offers based on their absolute value and a penalty for receiving less than their contribution (fair share). The penalty is proportional to the deficit, with sensitivity controlled by alpha. Utility is converted to acceptance probability via logistic function.\n\nThis model has one learnable parameter (alpha), which is bounded between 0 and 10. It uses split_self and token_self from the data. The parameter recovery should be manageable with 36 trials per participant, and the model is parsimonious.\n</think>\n\n<MODEL>\nU = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"data-dependent\",\n        \"max\": \"data-dependent\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (penalty per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept\nPrediction Type: utility"
  ]
}