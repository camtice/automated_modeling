{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and have parameters that can be recovered through testing. Also, the utility function should predict the probability of acceptance, with a logistic function converting utility to a probability.\n\nFirst, I need to understand the key variables involved. The main target is 'accept', which is binary (1 or 0). The model should predict this based on the utility of the proposed split. The variables provided include token contributions (self and opponent), combined earnings, split amounts, percentages, etc. Since the task involves fairness considerations, the model should account for how participants perceive fairness based on their contributions and the proposed splits.\n\nThe challenge is to model the responder's decision-making process. A common approach in behavioral economics is the Fehr-Schmidt model of inequity aversion, where individuals care about both their own payoff and the difference between their payoff and others'. However, in this case, the contributions (tokens) by both the participant and the opponent might influence what is considered a fair split. The combined earnings depend on the tokens, so the fairness judgment could be based on the proportion each contributed versus the proposed split percentages.\n\nLet me think about the variables. The combined_earning is the total money, which is 3*(token_self + token_opp). The split_self is the amount offered to the participant, and split_opp is the opponent's proposed share. But since the trial_role is responder, we only consider when the participant is the responder, so split_self is what the participant is offered. The participant's own contribution is token_self, and the opponent's is token_opp. \n\nFairness here might be determined by comparing the proposed split percentage (splitperc_self) to what would be considered fair based on contributions. The fair percentage would be (token_self / (token_self + token_opp)) * 100. If the proposed percentage is lower than this, the participant might feel the offer is unfair. The utility could then be a function of the monetary gain (split_self) minus some disutility from unfairness.\n\nSo, the utility function could have two components: the monetary utility from the proposed split and the disutility from inequity. The monetary part is straightforward\u2014participants prefer higher amounts. The inequity part could be the difference between the proposed percentage and the fair percentage (based on contributions). \n\nLet me formalize this. The fair percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these is (splitperc_self - fair_percentage). If this difference is positive, the offer is more than fair; if negative, it's less than fair. The disutility could be weighted by a parameter (alpha) that represents sensitivity to unfairness. \n\nAdditionally, participants might have a baseline acceptance threshold (beta) that represents their general willingness to accept offers, regardless of fairness. For example, even if an offer is slightly unfair, a participant with a high beta might still accept it if the monetary amount is sufficient.\n\nSo the utility function could be:\n\nUtility = beta * split_self + alpha * (splitperc_self - fair_percentage)\n\nBut wait, split_self is in pounds, and splitperc_self is a percentage. To make the terms compatible, maybe we should normalize the monetary term. Alternatively, since the split_self is part of the combined_earning, which is 3*(token_self + token_opp), perhaps the split_self is 3*(token_self + token_opp) * splitperc_self / 100. But maybe the model should use the percentage directly, as the splitperc_self is already a percentage of the combined earnings. Alternatively, the participant might care about the absolute amount (split_self) and the relative fairness (splitperc_self vs fair percentage).\n\nAlternatively, the utility could be a combination of the absolute offer (split_self) and the deviation from the fair split. The fair split in monetary terms would be (token_self / (token_self + token_opp)) * combined_earning. The difference between split_self and this fair amount could be the inequity. So:\n\nfair_self = (token_self / (token_self + token_opp)) * combined_earning\ninequity = split_self - fair_self\n\nThen, the utility could be:\n\nUtility = split_self + alpha * inequity\n\nBut if inequity is negative (split_self < fair_self), this would reduce the utility. However, participants might be more sensitive to negative inequity (being undercompensated) than positive. So maybe the model should have separate parameters for advantageous and disadvantageous inequity, similar to Fehr-Schmidt. But to keep the number of parameters low (as per the instruction to have a slight bias for fewer parameters), perhaps we can use a single parameter for inequity aversion, considering only disadvantageous inequity (since advantageous inequity, where the participant gets more than fair, might not affect rejection rates as much).\n\nAlternatively, the utility could be:\n\nUtility = split_self - alpha * max(fair_self - split_self, 0)\n\nHere, alpha represents the sensitivity to being offered less than the fair amount. If the offer is less than fair, the utility decreases by alpha times the difference. If it's equal or more, there's no penalty. This way, the model only penalizes unfair offers where the participant is getting less than their fair share.\n\nBut how do we compute fair_self? It's (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so fair_self would be 3*token_self. Because combined_earning is 3*(tokens), so fair_self is (token_self / total_tokens) * combined_earning = 3*token_self. Similarly, split_self is (splitperc_self / 100) * combined_earning. So split_self = (splitperc_self / 100) * 3*(token_self + token_opp). But maybe the participant's perception of fairness is based on their own contribution (token_self) versus the opponent's (token_opp). So the fair split would be proportional to their contribution. \n\nWait, the fair_self in monetary terms is 3*token_self, because each token is worth \u00a33, and the combined_earning is 3*(token_self + token_opp). So if the participant contributed token_self tokens, their fair share would be 3*token_self. The opponent's fair share is 3*token_opp. So the split_self is the amount offered to the participant. If split_self is less than 3*token_self, that's unfair. The difference is (3*token_self - split_self). If this is positive, the participant is getting less than their fair share. \n\nSo the utility could be:\n\nUtility = split_self - alpha * max(3*token_self - split_self, 0)\n\nBut split_self is the amount offered. So if the offer is equal to or more than 3*token_self, the second term is zero, and utility is just split_self. If it's less, then utility is reduced by alpha times the deficit. \n\nAlternatively, maybe participants compare the percentage split to their contribution percentage. For example, if they contributed 40% of the tokens, a 40% split would be fair. The difference between splitperc_self and 40% would be the unfairness. So:\n\nfair_perc = (token_self / (token_self + token_opp)) * 100\nunfairness = fair_perc - splitperc_self\n\nIf unfairness is positive (splitperc_self < fair_perc), the participant is getting less than fair. The utility could then be:\n\nUtility = split_self + beta * split_self - alpha * max(unfairness, 0)\n\nWait, but split_self is already part of the utility. Maybe the utility is a combination of the absolute offer and the perceived unfairness. For example:\n\nUtility = split_self - alpha * max(fair_perc - splitperc_self, 0) * combined_earning / 100\n\nBecause (fair_perc - splitperc_self) is the percentage point difference, and multiplying by combined_earning/100 converts that to the monetary difference. So max(fair_perc - splitperc_self, 0) * combined_earning /100 is the monetary amount that the participant is shortchanged. Then, alpha scales how much that affects utility.\n\nAlternatively, the utility could be:\n\nUtility = split_self - alpha * (fair_self - split_self) if split_self < fair_self else split_self\n\nWhich is similar to the previous approach. \n\nBut perhaps combining both the absolute offer and the relative fairness. For example, participants might have a baseline acceptance based on the split percentage, adjusted by their contribution. \n\nAnother angle: the participant's decision could depend on whether the offer meets their minimum acceptable percentage, which could be influenced by their contribution. For instance, if they contributed more, they might expect a higher percentage. \n\nBut how to model this with parameters. Let's think of parameters that need to be learned. The model needs learnable parameters with finite bounds. \n\nSuppose we have two parameters:\n\n1. alpha: the sensitivity to unfairness (when the offer is less than the fair share)\n2. beta: the baseline acceptance bias (could be a intercept term)\n\nThe utility function could be:\n\nUtility = beta + (split_self) - alpha * max(fair_self - split_self, 0)\n\nBut split_self is in pounds, and fair_self is 3*token_self. So the term (fair_self - split_self) is the deficit. If the offer is less than fair, this term is positive, reducing utility. \n\nAlternatively, to normalize, maybe use percentages. For example:\n\nfair_perc = (token_self / (token_self + token_opp)) * 100\nunfairness = max(fair_perc - splitperc_self, 0)\n\nUtility = beta * splitperc_self - alpha * unfairness\n\nHere, beta represents the utility per percentage point offered, and alpha is the disutility per percentage point of unfairness. \n\nBut splitperc_self is the percentage offered to the participant. So higher splitperc_self increases utility, but if it's below fair_perc, there's a penalty. \n\nThis model would have two parameters: beta and alpha. Both need to be learned. \n\nBut how does this relate to the combined earnings? For example, a 50% split when the combined earnings are high vs low. But in the problem statement, it's noted that two equal split_self variables don't represent the same fairness because the combined earning varies. Wait, split_self is in pounds, so if combined_earning is higher, a given split_self could be a smaller percentage. But the model needs to account for the fact that the same split_self in different trials may have different fairness implications. \n\nWait, the note says: \"two equal split_self variables do not represent the same 'fairness'\". So split_self alone isn't enough; the fairness depends on the combined_earning. For example, if split_self is \u00a310, but combined_earning is \u00a320 (so 50% split), versus split_self \u00a310 with combined_earning \u00a3100 (10% split). So the same split_self can be fair or unfair depending on the context. Therefore, the model must consider the split percentage (splitperc_self) rather than the absolute split_self. \n\nTherefore, the utility should be based on the percentage split, adjusted by the fair percentage based on contributions. \n\nSo, the fair percentage is (token_self / (token_self + token_opp)) * 100. The offered percentage is splitperc_self. The difference is (splitperc_self - fair_perc). If this is negative, the offer is unfair. \n\nThe utility could then be:\n\nUtility = beta * splitperc_self + alpha * (splitperc_self - fair_perc)\n\nBut if splitperc_self is less than fair_perc, the second term is negative, reducing utility. However, participants might only be sensitive to negative differences (unfairness against them), not positive. So maybe:\n\nUtility = beta * splitperc_self + alpha * max(fair_perc - splitperc_self, 0)\n\nWait, no. If splitperc_self is less than fair_perc, then (fair_perc - splitperc_self) is positive, so alpha would be a penalty parameter. So:\n\nUtility = beta * splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nHere, beta is the utility per percentage point offered, and alpha is the penalty per percentage point that the offer is below the fair percentage. \n\nAlternatively, the utility could be:\n\nUtility = splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nBut this assumes that each percentage point is valued linearly, and the penalty is alpha times the deficit. \n\nBut perhaps participants have a non-linear response, but for simplicity, a linear model is better for parameter recovery. \n\nAnother consideration: the participant's own contribution (token_self) might influence their sense of entitlement. For example, if they contributed more tokens, they might expect a higher percentage. The fair_perc variable captures this, but maybe there's an additional parameter that scales the importance of their own contribution. \n\nAlternatively, the model could include a parameter that represents the weight given to the fair percentage. For example:\n\nUtility = beta * splitperc_self + alpha * (splitperc_self - gamma * fair_perc)\n\nBut this introduces another parameter (gamma), which might complicate the model. Since the instruction prefers fewer parameters, maybe stick to two parameters. \n\nSo, the model could be:\n\nUtility = beta * splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nHere, beta is the utility gained per percentage point offered, and alpha is the disutility per percentage point that the offer is below the fair percentage. \n\nThe probability of accepting is then a logistic function of the utility:\n\nP(accept=1) = 1 / (1 + exp(-Utility))\n\nThis way, when Utility is positive, the probability is above 0.5; when negative, below 0.5. \n\nBut the problem states that the utility variable may be unbounded, but the model must predict utility where less than 0 corresponds to less than 50% likelihood. The logistic function naturally does this. \n\nNow, the parameters beta and alpha need to be learnable. What are their plausible ranges? \n\nBeta should be positive because a higher split percentage should increase utility. So beta's range could be [0, some upper bound]. Let's say [0, 10], with inclusive bounds. \n\nAlpha represents the sensitivity to unfairness. It should also be positive because unfairness (offer below fair) reduces utility. So alpha's range could be [0, 10], inclusive. \n\nBut maybe beta can be negative? If beta is negative, higher split percentages decrease utility, which doesn't make sense. So beta should be constrained to be non-negative. \n\nAlternatively, if the model includes an intercept term, like:\n\nUtility = beta0 + beta1 * splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nHere, beta0 is a bias term. For example, even if the offer is fair, a participant might have a general tendency to accept or reject. But adding another parameter (beta0) increases model complexity. The instruction says to have a slight bias for fewer parameters, so maybe omit the intercept. \n\nBut without an intercept, the model assumes that when splitperc_self is zero, the utility is -alpha * fair_perc, which might not be realistic. However, in practice, splitperc_self can't be zero because the participant is deciding to accept or reject, and a zero offer would likely be rejected. But the intercept might capture other factors not accounted for, like the participant's general willingness to accept any offer. \n\nAlternatively, maybe the intercept is necessary. Let's think: if the offer is exactly fair (splitperc_self = fair_perc), then the utility is beta * splitperc_self. For example, if beta is 0.1 and splitperc_self is 50, utility is 5. The probability of acceptance would be high. But if beta is very low, even a fair offer might have low utility. \n\nBut perhaps the intercept is needed to account for the baseline probability of acceptance. For example, some participants might have a general tendency to accept offers unless they're very unfair. So adding an intercept term beta0 could help. \n\nBut this adds another parameter. The instruction says to have a slight bias for fewer parameters. Let's see: with two parameters (beta and alpha), the model can capture sensitivity to the offer's percentage and the unfairness. Adding a third parameter (beta0) might improve fit but could complicate parameter recovery. \n\nGiven that the model needs to pass parameter recovery tests, having fewer parameters is better. So perhaps start with two parameters: beta (sensitivity to offer percentage) and alpha (sensitivity to unfairness). \n\nWait, but in the example given, the model had parameters beta and epsilon. The example's model had two learnable parameters. So maybe two parameters are acceptable here. \n\nAlternatively, maybe a single parameter model. For example, the utility is splitperc_self - alpha * max(fair_perc - splitperc_self, 0). Here, beta is fixed at 1, and alpha is the only learnable parameter. But this assumes that each percentage point is valued at 1, which might not be flexible enough. \n\nBut the problem states that the model should have parameters with finite bounds. So perhaps the two-parameter model is better. \n\nSo, the model would be:\n\nUtility = beta * splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nWhere:\n\n- fair_perc = (token_self / (token_self + token_opp)) * 100\n\nVariables needed:\n\n- splitperc_self (from data)\n- token_self (from data)\n- token_opp (from data)\n\nParameters:\n\n- beta: sensitivity to the offered percentage (learnable, >=0)\n- alpha: sensitivity to unfairness (learnable, >=0)\n\nThe probability of acceptance is logistic(Utility). \n\nNow, check if this model meets the requirements. The model uses variables available in the dataset (splitperc_self, token_self, token_opp). It predicts 'accept' via the utility function. The parameters beta and alpha are learnable with finite bounds (e.g., [0, 10]). \n\nAnother consideration: the combined_earning is 3*(token_self + token_opp), but the model uses token_self and token_opp to compute fair_perc. So the model correctly accounts for the total contribution. \n\nTesting edge cases: if the participant contributed all tokens (token_opp=0), then fair_perc is 100%, so any splitperc_self less than 100% would incur a penalty. If the offer is 100%, utility is beta*100, which should be high. \n\nIf the participant contributed nothing (token_self=0), fair_perc is 0%, so any splitperc_self is above 0% and no penalty. So the utility is beta*splitperc_self, which increases with higher offers. \n\nThis makes sense. \n\nAnother case: token_self=5, token_opp=5. fair_perc=50%. If splitperc_self=40%, then penalty is 10% * alpha. Utility=beta*40 - alpha*10. If beta is 0.1 and alpha is 0.5, utility=4 -5= -1. Probability of acceptance is 1/(1+exp(1)) \u2248 0.27, which is less than 50%. \n\nIf splitperc_self=60%, utility=0.1*60 -0=6. Probability \u2248 0.997. \n\nThis seems reasonable. \n\nNow, the parameters beta and alpha need to be bounded. Let's set their ranges. Since they are scaling factors, they can be positive. Let's set beta in [0, 10], alpha in [0, 10]. These are generous bounds, assuming that in most cases, the parameters won't exceed these values. \n\nThe variables in the model are:\n\n- splitperc_self: from data\n- token_self: from data\n- token_opp: from data\n- fair_perc: calculated as (token_self / (token_self + token_opp)) * 100\n- max(fair_perc - splitperc_self, 0): calculated\n\nThe utility is computed as beta*splitperc_self - alpha*max(fair_perc - splitperc_self, 0).\n\nThe target variable is 'accept', which is binary. \n\nThis model should meet the requirements. It uses the necessary variables, has two learnable parameters with finite bounds, and the utility is converted to a probability via logistic function. \n\nNow, let's formalize this into the required tags.\n</think>\n\n<MODEL>\nUtility = beta * splitperc_self - alpha * max((token_self / (token_self + token_opp) * 100) - splitperc_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"beta\": {\n      \"description\": \"Sensitivity to the percentage offered to the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to unfairness (penalty when offered percentage is below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage of combined earnings offered to the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant (self-contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the offered percentage and its deviation from a contribution-based fairness benchmark. The utility combines a linear term for the offered percentage (beta) and a penalty term for offers below the fair share (alpha). The fair share is computed as the participant's token contribution proportion scaled to a percentage. Logistic transformation converts utility to acceptance probability.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['beta', 'alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                Utility = beta * splitperc_self - alpha * max((token_self / (token_self + token_opp) * 100) - splitperc_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"beta\": {\n      \"description\": \"Sensitivity to the percentage offered to the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to unfairness (penalty when offered percentage is below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage of combined earnings offered to the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant (self-contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, beta=1, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        total_tokens = token_self + token_opp\n        if total_tokens == 0:\n            fair_perc = 0  # Avoid division by zero if both are zero\n        else:\n            fair_perc = (token_self / total_tokens) * 100\n\n        unfairness = max(fair_perc - splitperc_self, 0)\n        utility = beta * splitperc_self - alpha * unfairness\n\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n",
      "timestamp": 1743513040.3656168
    }
  ],
  "learnable_parameters": {
    "beta": {
      "description": "Sensitivity to the percentage offered to the participant",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to unfairness (penalty when offered percentage is below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "Utility = beta * splitperc_self - alpha * max((token_self / (token_self + token_opp) * 100) - splitperc_self, 0)",
  "variable_descriptions": {
    "beta": {
      "description": "Sensitivity to the percentage offered to the participant",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to unfairness (penalty when offered percentage is below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "splitperc_self": {
      "description": "Percentage of combined earnings offered to the participant",
      "range": {
        "min": 0,
        "max": 100,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant (self-contribution)",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model where acceptance probability depends on the offered percentage and its deviation from a contribution-based fairness benchmark. The utility combines a linear term for the offered percentage (beta) and a penalty term for offers below the fair share (alpha). The fair share is computed as the participant's token contribution proportion scaled to a percentage. Logistic transformation converts utility to acceptance probability.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and have parameters that can be recovered through testing. Also, the utility function should predict the probability of acceptance, with a logistic function converting utility to a probability.\n\nFirst, I need to understand the key variables involved. The main target is 'accept', which is binary (1 or 0). The model should predict this based on the utility of the proposed split. The variables provided include token contributions (self and opponent), combined earnings, split amounts, percentages, etc. Since the task involves fairness considerations, the model should account for how participants perceive fairness based on their contributions and the proposed splits.\n\nThe challenge is to model the responder's decision-making process. A common approach in behavioral economics is the Fehr-Schmidt model of inequity aversion, where individuals care about both their own payoff and the difference between their payoff and others'. However, in this case, the contributions (tokens) by both the participant and the opponent might influence what is considered a fair split. The combined earnings depend on the tokens, so the fairness judgment could be based on the proportion each contributed versus the proposed split percentages.\n\nLet me think about the variables. The combined_earning is the total money, which is 3*(token_self + token_opp). The split_self is the amount offered to the participant, and split_opp is the opponent's proposed share. But since the trial_role is responder, we only consider when the participant is the responder, so split_self is what the participant is offered. The participant's own contribution is token_self, and the opponent's is token_opp. \n\nFairness here might be determined by comparing the proposed split percentage (splitperc_self) to what would be considered fair based on contributions. The fair percentage would be (token_self / (token_self + token_opp)) * 100. If the proposed percentage is lower than this, the participant might feel the offer is unfair. The utility could then be a function of the monetary gain (split_self) minus some disutility from unfairness.\n\nSo, the utility function could have two components: the monetary utility from the proposed split and the disutility from inequity. The monetary part is straightforward\u2014participants prefer higher amounts. The inequity part could be the difference between the proposed percentage and the fair percentage (based on contributions). \n\nLet me formalize this. The fair percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these is (splitperc_self - fair_percentage). If this difference is positive, the offer is more than fair; if negative, it's less than fair. The disutility could be weighted by a parameter (alpha) that represents sensitivity to unfairness. \n\nAdditionally, participants might have a baseline acceptance threshold (beta) that represents their general willingness to accept offers, regardless of fairness. For example, even if an offer is slightly unfair, a participant with a high beta might still accept it if the monetary amount is sufficient.\n\nSo the utility function could be:\n\nUtility = beta * split_self + alpha * (splitperc_self - fair_percentage)\n\nBut wait, split_self is in pounds, and splitperc_self is a percentage. To make the terms compatible, maybe we should normalize the monetary term. Alternatively, since the split_self is part of the combined_earning, which is 3*(token_self + token_opp), perhaps the split_self is 3*(token_self + token_opp) * splitperc_self / 100. But maybe the model should use the percentage directly, as the splitperc_self is already a percentage of the combined earnings. Alternatively, the participant might care about the absolute amount (split_self) and the relative fairness (splitperc_self vs fair percentage).\n\nAlternatively, the utility could be a combination of the absolute offer (split_self) and the deviation from the fair split. The fair split in monetary terms would be (token_self / (token_self + token_opp)) * combined_earning. The difference between split_self and this fair amount could be the inequity. So:\n\nfair_self = (token_self / (token_self + token_opp)) * combined_earning\ninequity = split_self - fair_self\n\nThen, the utility could be:\n\nUtility = split_self + alpha * inequity\n\nBut if inequity is negative (split_self < fair_self), this would reduce the utility. However, participants might be more sensitive to negative inequity (being undercompensated) than positive. So maybe the model should have separate parameters for advantageous and disadvantageous inequity, similar to Fehr-Schmidt. But to keep the number of parameters low (as per the instruction to have a slight bias for fewer parameters), perhaps we can use a single parameter for inequity aversion, considering only disadvantageous inequity (since advantageous inequity, where the participant gets more than fair, might not affect rejection rates as much).\n\nAlternatively, the utility could be:\n\nUtility = split_self - alpha * max(fair_self - split_self, 0)\n\nHere, alpha represents the sensitivity to being offered less than the fair amount. If the offer is less than fair, the utility decreases by alpha times the difference. If it's equal or more, there's no penalty. This way, the model only penalizes unfair offers where the participant is getting less than their fair share.\n\nBut how do we compute fair_self? It's (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so fair_self would be 3*token_self. Because combined_earning is 3*(tokens), so fair_self is (token_self / total_tokens) * combined_earning = 3*token_self. Similarly, split_self is (splitperc_self / 100) * combined_earning. So split_self = (splitperc_self / 100) * 3*(token_self + token_opp). But maybe the participant's perception of fairness is based on their own contribution (token_self) versus the opponent's (token_opp). So the fair split would be proportional to their contribution. \n\nWait, the fair_self in monetary terms is 3*token_self, because each token is worth \u00a33, and the combined_earning is 3*(token_self + token_opp). So if the participant contributed token_self tokens, their fair share would be 3*token_self. The opponent's fair share is 3*token_opp. So the split_self is the amount offered to the participant. If split_self is less than 3*token_self, that's unfair. The difference is (3*token_self - split_self). If this is positive, the participant is getting less than their fair share. \n\nSo the utility could be:\n\nUtility = split_self - alpha * max(3*token_self - split_self, 0)\n\nBut split_self is the amount offered. So if the offer is equal to or more than 3*token_self, the second term is zero, and utility is just split_self. If it's less, then utility is reduced by alpha times the deficit. \n\nAlternatively, maybe participants compare the percentage split to their contribution percentage. For example, if they contributed 40% of the tokens, a 40% split would be fair. The difference between splitperc_self and 40% would be the unfairness. So:\n\nfair_perc = (token_self / (token_self + token_opp)) * 100\nunfairness = fair_perc - splitperc_self\n\nIf unfairness is positive (splitperc_self < fair_perc), the participant is getting less than fair. The utility could then be:\n\nUtility = split_self + beta * split_self - alpha * max(unfairness, 0)\n\nWait, but split_self is already part of the utility. Maybe the utility is a combination of the absolute offer and the perceived unfairness. For example:\n\nUtility = split_self - alpha * max(fair_perc - splitperc_self, 0) * combined_earning / 100\n\nBecause (fair_perc - splitperc_self) is the percentage point difference, and multiplying by combined_earning/100 converts that to the monetary difference. So max(fair_perc - splitperc_self, 0) * combined_earning /100 is the monetary amount that the participant is shortchanged. Then, alpha scales how much that affects utility.\n\nAlternatively, the utility could be:\n\nUtility = split_self - alpha * (fair_self - split_self) if split_self < fair_self else split_self\n\nWhich is similar to the previous approach. \n\nBut perhaps combining both the absolute offer and the relative fairness. For example, participants might have a baseline acceptance based on the split percentage, adjusted by their contribution. \n\nAnother angle: the participant's decision could depend on whether the offer meets their minimum acceptable percentage, which could be influenced by their contribution. For instance, if they contributed more, they might expect a higher percentage. \n\nBut how to model this with parameters. Let's think of parameters that need to be learned. The model needs learnable parameters with finite bounds. \n\nSuppose we have two parameters:\n\n1. alpha: the sensitivity to unfairness (when the offer is less than the fair share)\n2. beta: the baseline acceptance bias (could be a intercept term)\n\nThe utility function could be:\n\nUtility = beta + (split_self) - alpha * max(fair_self - split_self, 0)\n\nBut split_self is in pounds, and fair_self is 3*token_self. So the term (fair_self - split_self) is the deficit. If the offer is less than fair, this term is positive, reducing utility. \n\nAlternatively, to normalize, maybe use percentages. For example:\n\nfair_perc = (token_self / (token_self + token_opp)) * 100\nunfairness = max(fair_perc - splitperc_self, 0)\n\nUtility = beta * splitperc_self - alpha * unfairness\n\nHere, beta represents the utility per percentage point offered, and alpha is the disutility per percentage point of unfairness. \n\nBut splitperc_self is the percentage offered to the participant. So higher splitperc_self increases utility, but if it's below fair_perc, there's a penalty. \n\nThis model would have two parameters: beta and alpha. Both need to be learned. \n\nBut how does this relate to the combined earnings? For example, a 50% split when the combined earnings are high vs low. But in the problem statement, it's noted that two equal split_self variables don't represent the same fairness because the combined earning varies. Wait, split_self is in pounds, so if combined_earning is higher, a given split_self could be a smaller percentage. But the model needs to account for the fact that the same split_self in different trials may have different fairness implications. \n\nWait, the note says: \"two equal split_self variables do not represent the same 'fairness'\". So split_self alone isn't enough; the fairness depends on the combined_earning. For example, if split_self is \u00a310, but combined_earning is \u00a320 (so 50% split), versus split_self \u00a310 with combined_earning \u00a3100 (10% split). So the same split_self can be fair or unfair depending on the context. Therefore, the model must consider the split percentage (splitperc_self) rather than the absolute split_self. \n\nTherefore, the utility should be based on the percentage split, adjusted by the fair percentage based on contributions. \n\nSo, the fair percentage is (token_self / (token_self + token_opp)) * 100. The offered percentage is splitperc_self. The difference is (splitperc_self - fair_perc). If this is negative, the offer is unfair. \n\nThe utility could then be:\n\nUtility = beta * splitperc_self + alpha * (splitperc_self - fair_perc)\n\nBut if splitperc_self is less than fair_perc, the second term is negative, reducing utility. However, participants might only be sensitive to negative differences (unfairness against them), not positive. So maybe:\n\nUtility = beta * splitperc_self + alpha * max(fair_perc - splitperc_self, 0)\n\nWait, no. If splitperc_self is less than fair_perc, then (fair_perc - splitperc_self) is positive, so alpha would be a penalty parameter. So:\n\nUtility = beta * splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nHere, beta is the utility per percentage point offered, and alpha is the penalty per percentage point that the offer is below the fair percentage. \n\nAlternatively, the utility could be:\n\nUtility = splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nBut this assumes that each percentage point is valued linearly, and the penalty is alpha times the deficit. \n\nBut perhaps participants have a non-linear response, but for simplicity, a linear model is better for parameter recovery. \n\nAnother consideration: the participant's own contribution (token_self) might influence their sense of entitlement. For example, if they contributed more tokens, they might expect a higher percentage. The fair_perc variable captures this, but maybe there's an additional parameter that scales the importance of their own contribution. \n\nAlternatively, the model could include a parameter that represents the weight given to the fair percentage. For example:\n\nUtility = beta * splitperc_self + alpha * (splitperc_self - gamma * fair_perc)\n\nBut this introduces another parameter (gamma), which might complicate the model. Since the instruction prefers fewer parameters, maybe stick to two parameters. \n\nSo, the model could be:\n\nUtility = beta * splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nHere, beta is the utility gained per percentage point offered, and alpha is the disutility per percentage point that the offer is below the fair percentage. \n\nThe probability of accepting is then a logistic function of the utility:\n\nP(accept=1) = 1 / (1 + exp(-Utility))\n\nThis way, when Utility is positive, the probability is above 0.5; when negative, below 0.5. \n\nBut the problem states that the utility variable may be unbounded, but the model must predict utility where less than 0 corresponds to less than 50% likelihood. The logistic function naturally does this. \n\nNow, the parameters beta and alpha need to be learnable. What are their plausible ranges? \n\nBeta should be positive because a higher split percentage should increase utility. So beta's range could be [0, some upper bound]. Let's say [0, 10], with inclusive bounds. \n\nAlpha represents the sensitivity to unfairness. It should also be positive because unfairness (offer below fair) reduces utility. So alpha's range could be [0, 10], inclusive. \n\nBut maybe beta can be negative? If beta is negative, higher split percentages decrease utility, which doesn't make sense. So beta should be constrained to be non-negative. \n\nAlternatively, if the model includes an intercept term, like:\n\nUtility = beta0 + beta1 * splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nHere, beta0 is a bias term. For example, even if the offer is fair, a participant might have a general tendency to accept or reject. But adding another parameter (beta0) increases model complexity. The instruction says to have a slight bias for fewer parameters, so maybe omit the intercept. \n\nBut without an intercept, the model assumes that when splitperc_self is zero, the utility is -alpha * fair_perc, which might not be realistic. However, in practice, splitperc_self can't be zero because the participant is deciding to accept or reject, and a zero offer would likely be rejected. But the intercept might capture other factors not accounted for, like the participant's general willingness to accept any offer. \n\nAlternatively, maybe the intercept is necessary. Let's think: if the offer is exactly fair (splitperc_self = fair_perc), then the utility is beta * splitperc_self. For example, if beta is 0.1 and splitperc_self is 50, utility is 5. The probability of acceptance would be high. But if beta is very low, even a fair offer might have low utility. \n\nBut perhaps the intercept is needed to account for the baseline probability of acceptance. For example, some participants might have a general tendency to accept offers unless they're very unfair. So adding an intercept term beta0 could help. \n\nBut this adds another parameter. The instruction says to have a slight bias for fewer parameters. Let's see: with two parameters (beta and alpha), the model can capture sensitivity to the offer's percentage and the unfairness. Adding a third parameter (beta0) might improve fit but could complicate parameter recovery. \n\nGiven that the model needs to pass parameter recovery tests, having fewer parameters is better. So perhaps start with two parameters: beta (sensitivity to offer percentage) and alpha (sensitivity to unfairness). \n\nWait, but in the example given, the model had parameters beta and epsilon. The example's model had two learnable parameters. So maybe two parameters are acceptable here. \n\nAlternatively, maybe a single parameter model. For example, the utility is splitperc_self - alpha * max(fair_perc - splitperc_self, 0). Here, beta is fixed at 1, and alpha is the only learnable parameter. But this assumes that each percentage point is valued at 1, which might not be flexible enough. \n\nBut the problem states that the model should have parameters with finite bounds. So perhaps the two-parameter model is better. \n\nSo, the model would be:\n\nUtility = beta * splitperc_self - alpha * max(fair_perc - splitperc_self, 0)\n\nWhere:\n\n- fair_perc = (token_self / (token_self + token_opp)) * 100\n\nVariables needed:\n\n- splitperc_self (from data)\n- token_self (from data)\n- token_opp (from data)\n\nParameters:\n\n- beta: sensitivity to the offered percentage (learnable, >=0)\n- alpha: sensitivity to unfairness (learnable, >=0)\n\nThe probability of acceptance is logistic(Utility). \n\nNow, check if this model meets the requirements. The model uses variables available in the dataset (splitperc_self, token_self, token_opp). It predicts 'accept' via the utility function. The parameters beta and alpha are learnable with finite bounds (e.g., [0, 10]). \n\nAnother consideration: the combined_earning is 3*(token_self + token_opp), but the model uses token_self and token_opp to compute fair_perc. So the model correctly accounts for the total contribution. \n\nTesting edge cases: if the participant contributed all tokens (token_opp=0), then fair_perc is 100%, so any splitperc_self less than 100% would incur a penalty. If the offer is 100%, utility is beta*100, which should be high. \n\nIf the participant contributed nothing (token_self=0), fair_perc is 0%, so any splitperc_self is above 0% and no penalty. So the utility is beta*splitperc_self, which increases with higher offers. \n\nThis makes sense. \n\nAnother case: token_self=5, token_opp=5. fair_perc=50%. If splitperc_self=40%, then penalty is 10% * alpha. Utility=beta*40 - alpha*10. If beta is 0.1 and alpha is 0.5, utility=4 -5= -1. Probability of acceptance is 1/(1+exp(1)) \u2248 0.27, which is less than 50%. \n\nIf splitperc_self=60%, utility=0.1*60 -0=6. Probability \u2248 0.997. \n\nThis seems reasonable. \n\nNow, the parameters beta and alpha need to be bounded. Let's set their ranges. Since they are scaling factors, they can be positive. Let's set beta in [0, 10], alpha in [0, 10]. These are generous bounds, assuming that in most cases, the parameters won't exceed these values. \n\nThe variables in the model are:\n\n- splitperc_self: from data\n- token_self: from data\n- token_opp: from data\n- fair_perc: calculated as (token_self / (token_self + token_opp)) * 100\n- max(fair_perc - splitperc_self, 0): calculated\n\nThe utility is computed as beta*splitperc_self - alpha*max(fair_perc - splitperc_self, 0).\n\nThe target variable is 'accept', which is binary. \n\nThis model should meet the requirements. It uses the necessary variables, has two learnable parameters with finite bounds, and the utility is converted to a probability via logistic function. \n\nNow, let's formalize this into the required tags.\n</think>\n\n<MODEL>\nUtility = beta * splitperc_self - alpha * max((token_self / (token_self + token_opp) * 100) - splitperc_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"beta\": {\n      \"description\": \"Sensitivity to the percentage offered to the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to unfairness (penalty when offered percentage is below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"splitperc_self\": {\n      \"description\": \"Percentage of combined earnings offered to the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 100,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant (self-contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the offered percentage and its deviation from a contribution-based fairness benchmark. The utility combines a linear term for the offered percentage (beta) and a penalty term for offers below the fair share (alpha). The fair share is computed as the participant's token contribution proportion scaled to a percentage. Logistic transformation converts utility to acceptance probability.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: Utility = beta * splitperc_self - alpha * max((token_self / (token_self + token_opp) * 100) - splitperc_self, 0)\nSummary: A utility model where acceptance probability depends on the offered percentage and its deviation from a contribution-based fairness benchmark. The utility combines a linear term for the offered percentage (beta) and a penalty term for offers below the fair share (alpha). The fair share is computed as the participant's token contribution proportion scaled to a percentage. Logistic transformation converts utility to acceptance probability.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, beta=1, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        splitperc_self = trial.get(\"splitperc_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n\n        total_tokens = token_self + token_opp\n        if total_tokens == 0:\n            fair_perc = 0  # Avoid division by zero if both are zero\n        else:\n            fair_perc = (token_self / total_tokens) * 100\n\n        unfairness = max(fair_perc - splitperc_self, 0)\n        utility = beta * splitperc_self - alpha * unfairness\n\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.7924297924297925,
  "fitting_results": [
    {
      "beta": 8.816362612087241,
      "alpha": 9.54480820106568,
      "success": true,
      "log_likelihood": -437.49116766716884,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.7642320112650236,
      "alpha": 1.9726086126808369,
      "success": true,
      "log_likelihood": -138.1551055766462,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.4249762143709495,
      "alpha": 4.0289194609803,
      "success": true,
      "log_likelihood": -115.12925464660226,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.610910276801381,
      "alpha": 8.795827511724731,
      "success": true,
      "log_likelihood": -207.23265836676427,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.8521879295509676,
      "alpha": 2.876304372928745,
      "success": true,
      "log_likelihood": -253.28436022684525,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 8.530999966421,
      "alpha": 9.37415678525219,
      "success": true,
      "log_likelihood": -115.12925464660229,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.4831786347638,
      "alpha": 0.9074381807835019,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.531497274338004,
      "alpha": 2.658779866577465,
      "success": true,
      "log_likelihood": -115.12925464660225,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.9639533587511337,
      "alpha": 4.782336195260322,
      "success": true,
      "log_likelihood": -184.20680743672375,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.144091575801079,
      "alpha": 4.691743772581459,
      "success": true,
      "log_likelihood": -46.05170185648092,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.598142319497104,
      "alpha": 9.27248731683496,
      "success": true,
      "log_likelihood": -230.25850953730702,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.9351931259580266,
      "alpha": 7.2971013575829415,
      "success": true,
      "log_likelihood": -69.07755278652134,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.936847558410833,
      "alpha": 1.928135064457911,
      "success": true,
      "log_likelihood": -23.025850926621803,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.664073239483229,
      "alpha": 1.5039097555866898,
      "success": true,
      "log_likelihood": -23.025850926440462,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.128327191399389,
      "alpha": 0.6985927775363721,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.9346502027490382,
      "alpha": 0.9341353326184375,
      "success": true,
      "log_likelihood": -23.02585264176486,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -92.10340371656177,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.6347695577250683,
      "alpha": 0.4415596951314882,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.882642810954905,
      "alpha": 5.123014792016521,
      "success": true,
      "log_likelihood": -138.15510557664282,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.769191931911964,
      "alpha": 5.9799225596033985,
      "success": true,
      "log_likelihood": -207.23265836676416,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.24901375328043707,
      "alpha": 3.5879711549230917,
      "success": true,
      "log_likelihood": -207.23265836676427,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.7396364310719266,
      "alpha": 8.073542761626088,
      "success": true,
      "log_likelihood": -161.18095650668317,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.269105406398406,
      "alpha": 9.405253480412568,
      "success": true,
      "log_likelihood": -94.82308009602414,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.994050613374424,
      "alpha": 2.26615586333081,
      "success": true,
      "log_likelihood": -115.12925464660228,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.800320438465112,
      "alpha": 4.072098891940979,
      "success": true,
      "log_likelihood": -207.23265836676393,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 8.42992155516914,
      "alpha": 7.039616358610733,
      "success": true,
      "log_likelihood": -345.387763947007,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.066653576418002,
      "alpha": 5.884226978727398,
      "success": true,
      "log_likelihood": -276.3102111568485,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -184.20680743672378,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.169714214214333,
      "alpha": 2.265760353062931,
      "success": true,
      "log_likelihood": -46.05170185648092,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.141162629853049,
      "alpha": 5.670869557123856,
      "success": true,
      "log_likelihood": -437.49082130757824,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0027622092868526756,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -278.4356979734058,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.008315700696733,
      "alpha": 5.735224505928001,
      "success": true,
      "log_likelihood": -138.15510557667542,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 8.751911339703074,
      "alpha": 8.524197037978412,
      "success": true,
      "log_likelihood": -184.2068074367239,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.468281918407586,
      "alpha": 4.014495151593364,
      "success": true,
      "log_likelihood": -115.12925464660226,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.829663501877156,
      "alpha": 9.20003145558695,
      "success": true,
      "log_likelihood": -184.2068074367237,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 9.612322207560366,
      "alpha": 3.4971844582442535,
      "success": true,
      "log_likelihood": -207.23265836676438,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 9.165323958638984,
      "alpha": 0.7854405456069657,
      "success": true,
      "log_likelihood": -69.07755278652135,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.1851585823073145,
      "alpha": 4.238143686862658,
      "success": true,
      "log_likelihood": -276.31021115688554,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6508489165492977,
      "alpha": 5.377967715876641,
      "success": true,
      "log_likelihood": -46.051702084021514,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -23.02585092644049,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.352803520874779,
      "alpha": 3.6982662079070443,
      "success": true,
      "log_likelihood": -184.20680743672364,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.068284629194233,
      "alpha": 4.877805263043013,
      "success": true,
      "log_likelihood": -161.1809565066832,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.670615553657248,
      "alpha": 3.1856736052069414,
      "success": true,
      "log_likelihood": -138.15510557664282,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.6478038532655095,
      "alpha": 6.209860352112698,
      "success": true,
      "log_likelihood": -161.1809565066832,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.442911954078052,
      "alpha": 7.323704643577271,
      "success": true,
      "log_likelihood": -162.46486338692472,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.5525716262373233,
      "alpha": 9.448708407644158,
      "success": true,
      "log_likelihood": -161.18095650668337,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.278354462192711,
      "alpha": 4.683479666339392,
      "success": true,
      "log_likelihood": -322.3619130169665,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 9.425667368217733,
      "alpha": 1.774962872204885,
      "success": true,
      "log_likelihood": -253.28436022684522,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.7304712931179145,
      "alpha": 8.351196847595073,
      "success": true,
      "log_likelihood": -391.439465807088,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.795201396549425,
      "alpha": 3.778815455210054,
      "success": true,
      "log_likelihood": -276.31021115688554,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 8.876421298754163,
      "alpha": 3.233942955187908,
      "success": true,
      "log_likelihood": -276.3102111568857,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.097431294425919,
      "alpha": 6.1539691228392845,
      "success": true,
      "log_likelihood": -209.07449829367295,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.673060207975797,
      "alpha": 6.52537165271454,
      "success": true,
      "log_likelihood": -69.07755278652138,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.7279513915930975,
      "alpha": 9.40592062500703,
      "success": true,
      "log_likelihood": -138.1551055766427,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -92.1034037165618,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.7583677373308615,
      "alpha": 8.705174077881866,
      "success": true,
      "log_likelihood": -184.20680743672372,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.322709566342727,
      "alpha": 2.9880060092582266,
      "success": true,
      "log_likelihood": -115.12925464660226,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.688441814992041,
      "alpha": 4.882657076216751,
      "success": true,
      "log_likelihood": -207.2326583667642,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.2569086105660583,
      "alpha": 8.081183406478022,
      "success": true,
      "log_likelihood": -184.2068074367238,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.259604742227851,
      "alpha": 6.622203025498025,
      "success": true,
      "log_likelihood": -437.49116766716895,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.494995787312397,
      "alpha": 3.9297925029399963,
      "success": true,
      "log_likelihood": -253.2843602268452,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.1143268266526107,
      "alpha": 6.62337808110101,
      "success": true,
      "log_likelihood": -138.15510557664288,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.807286284846701,
      "alpha": 4.116646626971656,
      "success": true,
      "log_likelihood": -138.15510615674665,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652131,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.698981627473062,
      "alpha": 2.455169438004934,
      "success": true,
      "log_likelihood": -115.12922134046619,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.739880720196927,
      "alpha": 4.121990185757387,
      "success": true,
      "log_likelihood": -184.20680743672372,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652137,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.04766360790136259,
      "alpha": 6.856960972792204,
      "success": true,
      "log_likelihood": -6.9850765438600225e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0004899080003759018,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -71.82569904204138,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.42102939923691,
      "alpha": 4.091854908531747,
      "success": true,
      "log_likelihood": -161.18095650668326,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -184.20680743672386,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.1159901925777727,
      "alpha": 6.8233397684543355,
      "success": true,
      "log_likelihood": -322.36191301696647,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.5087186363699945,
      "alpha": 0.8004900122988001,
      "success": true,
      "log_likelihood": -138.1551055863318,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -46.051701856480875,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.5803301461268293,
      "alpha": 9.746530869135125,
      "success": true,
      "log_likelihood": -184.20680743672364,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.17224517068528,
      "alpha": 9.916422229824795,
      "success": true,
      "log_likelihood": -299.33606208692595,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.745833088736065,
      "alpha": 6.60310536277413,
      "success": true,
      "log_likelihood": -506.56471612154024,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.9505241722685627,
      "alpha": 8.806628126512376,
      "success": true,
      "log_likelihood": -299.33606208692595,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.282148391913898,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652131,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.802823351566438,
      "alpha": 8.427029554743473,
      "success": true,
      "log_likelihood": -253.2843602268451,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6302273983955944,
      "alpha": 0.7516777525845855,
      "success": true,
      "log_likelihood": -92.0441373575365,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.270255839479821,
      "alpha": 3.2837453220034387,
      "success": true,
      "log_likelihood": -276.3102111568856,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 9.847325656941612,
      "alpha": 5.041372037309685,
      "success": true,
      "log_likelihood": -621.6979751074925,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.25,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.0304040945775,
      "alpha": 8.957981632771805,
      "success": true,
      "log_likelihood": -207.23265836676418,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.0710658971604365,
      "alpha": 8.276599007170914,
      "success": true,
      "log_likelihood": -184.2068074367258,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.9804776246881106,
      "alpha": 0.032305811665198636,
      "success": true,
      "log_likelihood": -46.05170185648088,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.248598284141584,
      "alpha": 7.865010199344167,
      "success": true,
      "log_likelihood": -161.18095650668332,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 8.316319528887885,
      "alpha": 8.618218196036164,
      "success": true,
      "log_likelihood": -253.2843602268451,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "beta": {
        "r": 0.40961990852059693,
        "p": 2.3168234599430904e-05
      },
      "alpha": {
        "r": 0.46628485545296644,
        "p": 1.011545892573284e-06
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 347.4753287195863,
  "bic_results": {
    "average_bic": 347.4753287195863,
    "individual_bics": [
      882.1493732112499,
      283.47724903020463,
      237.42554717011674,
      421.63235461044076,
      513.7357583306027,
      237.4255471701168,
      7.167037869712219,
      237.42554717011672,
      7.167037869712219,
      375.5806527503597,
      99.27044158987405,
      467.68405695152626,
      145.3221434499549,
      53.218739730155825,
      53.21873972979314,
      7.167037869712219,
      7.167037869712219,
      53.21874316044194,
      191.37384531003576,
      7.167037869712219,
      283.47724903019787,
      421.63235461044053,
      421.63235461044076,
      329.52895089027857,
      196.8131980689605,
      237.42554717011677,
      421.6323546104401,
      697.9425657709262,
      559.7874601906092,
      375.5806527503598,
      99.27044158987405,
      882.1486804920687,
      564.0384338237238,
      283.47724903026307,
      7.167037869712219,
      375.58065275036,
      237.42554717011674,
      375.5806527503596,
      421.632354610441,
      145.32214344995492,
      559.7874601906833,
      99.27044204495525,
      53.2187397297932,
      375.5806527503595,
      329.5289508902786,
      283.47724903019787,
      329.5289508902786,
      332.09676465076166,
      329.52895089027896,
      651.8908639108453,
      513.7357583306027,
      790.0459694910883,
      559.7874601906833,
      559.7874601906836,
      425.3160344642581,
      145.32214344995498,
      283.47724903019764,
      191.37384531003582,
      375.58065275035966,
      237.42554717011674,
      421.63235461044064,
      375.58065275035983,
      882.1493732112501,
      513.7357583306026,
      283.477249030198,
      283.4772501904055,
      145.32214344995484,
      237.4254805578446,
      375.58065275035966,
      145.32214344995495,
      7.167037890882373,
      150.81843596099498,
      329.52895089027874,
      375.58065275035995,
      651.8908639108452,
      283.47724904957585,
      99.27044158987397,
      375.5806527503595,
      605.8391620507641,
      1020.2964701199927,
      605.8391620507641,
      145.32214344995484,
      513.7357583306025,
      191.25531259198522,
      559.7874601906834,
      1250.5629880918973,
      421.6323546104406,
      375.5806527503638,
      99.27044158987398,
      329.52895089027885,
      513.7357583306025
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- beta: r = 0.410\n- alpha: r = 0.466",
  "previous_models": [
    "Specification: Utility = beta * splitperc_self - alpha * max((token_self / (token_self + token_opp) * 100) - splitperc_self, 0)\nSummary: A utility model where acceptance probability depends on the offered percentage and its deviation from a contribution-based fairness benchmark. The utility combines a linear term for the offered percentage (beta) and a penalty term for offers below the fair share (alpha). The fair share is computed as the participant's token contribution proportion scaled to a percentage. Logistic transformation converts utility to acceptance probability.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 347.4753287195863\nOverall Accuracy: 0.7924\n\nParameter Recovery:\n- beta: r = 0.410\n- alpha: r = 0.466"
  ]
}