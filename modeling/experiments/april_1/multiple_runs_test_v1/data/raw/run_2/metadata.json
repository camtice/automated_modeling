{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility function should be designed such that a utility less than 0 corresponds to less than 50% probability of acceptance, which suggests using a logistic function to convert utility to probability.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on the utility of the proposal from the responder's perspective. The dataset includes variables like token contributions (token_self and token_opp), combined earnings, split amounts, and percentages. \n\nThe main psychological mechanisms here likely involve fairness considerations and self-interest. Participants might compare their proposed share to what they perceive as fair based on their own and the opponent's contributions. The utility could be a function of the difference between the proposed split and a fair reference point, possibly adjusted by parameters that capture sensitivity to unfairness or self-interest.\n\nLet's think about how to model fairness. A common approach in behavioral economics is the inequity aversion model, where individuals dislike unequal outcomes. However, since the combined earnings vary, the fairness might be relative to contributions. For example, if the participant contributed more tokens, they might expect a larger share. The fair split could be proportional to their contribution relative to the total. \n\nThe variables token_self and token_opp represent each party's contribution. Each token is worth \u00a33, so the total combined earning is 3*(token_self + token_opp). The split_self is the amount offered to the participant. The fair share would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair share could be a key factor in the utility.\n\nSo, the utility might be calculated as the proposed split minus the fair share, adjusted by some parameters. But perhaps participants have different sensitivities to being under-benefited (receiving less than fair) versus over-benefited (receiving more). However, since the task is about accepting or rejecting, over-benefiting might not lead to rejection, but under-benefiting would. So maybe the model should focus on the negative deviation from the fair share.\n\nAlternatively, the utility could be a linear combination of the monetary gain and the fairness term. For example, utility = \u03b1*(split_self) + \u03b2*(fairness_deviation). But how to quantify fairness_deviation?\n\nWait, the split_self is in \u00a3, and the fair share would be (token_self / (token_self + token_opp)) * combined_earning. Let's compute that. Let's call this fair_self. Then, the difference between split_self and fair_self is the deviation from fairness. If split_self is less than fair_self, the participant might feel treated unfairly, leading to a lower utility. \n\nBut the utility could be split_self minus some term that penalizes unfairness. For example, if the split is less than fair, the penalty is higher. Let's model the utility as:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b8 is a learnable parameter representing the sensitivity to being under-benefited. If the proposed split is less than the fair share, the participant experiences a disutility proportional to the difference, scaled by \u03b8. If the split is equal or more, there's no penalty. This way, the utility decreases when the offer is below the fair share, making acceptance less likely.\n\nBut wait, the combined_earning is 3*(token_self + token_opp), so fair_self is 3*token_self. Because each token is \u00a33, so the participant's contribution is worth 3*token_self, and the opponent's is 3*token_opp. The total combined is 3*(token_self + token_opp). So the fair split for the participant would be exactly 3*token_self. Because that's their contribution. So if the split_self is less than 3*token_self, that's unfair. If it's more, they're getting a surplus.\n\nWait, but the split_self is part of the combined_earning. So the split_self + split_opp = combined_earning. So the fair split would be each getting their own contribution. So split_self should be 3*token_self, and split_opp 3*token_opp. But in reality, the proposer might offer a different split. So the fair_self is 3*token_self. The difference between split_self and fair_self is (split_self - 3*token_self). If this is negative, the participant is getting less than fair. If positive, more.\n\nSo the utility could be:\n\nUtility = split_self + \u03b1 * (split_self - fair_self)\n\nBut that might not capture the asymmetry. Alternatively, participants might only be averse to disadvantageous inequality. So they dislike when split_self < fair_self, but not when split_self > fair_self. So the disutility is only when split_self is less than fair_self.\n\nSo, the utility could be:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b8 is the sensitivity to being under-benefited. If \u03b8 is positive, then the utility decreases when the offer is below the fair share. The higher \u03b8, the more the participant dislikes being underpaid.\n\nAlternatively, maybe participants also care about the absolute difference, not just the direction. But given that the model needs to predict accept/reject, and the example suggests that utility < 0 leads to <50% probability, perhaps focusing on the disadvantageous inequality is sufficient.\n\nAnother consideration is the participant's own contribution (token_self) and the opponent's (token_opp). The trial_type variable indicates whether contributions are equal, opponent more, or participant more. This might influence the participant's perception of fairness. However, since token_self and token_opp are available, the model can directly compute the fair share without needing trial_type.\n\nSo, the key variables to include are split_self (the offer), token_self, and token_opp. From these, compute fair_self = 3 * token_self. Then, compute the difference between split_self and fair_self. If split_self < fair_self, the participant is getting less than their contribution, which might lead to rejection. The parameter \u03b8 scales this disutility.\n\nAdditionally, there might be a baseline acceptance tendency. For example, even if the offer is fair, there could be a general propensity to accept or reject, captured by an intercept term. So the utility could be:\n\nUtility = \u03b20 + \u03b21 * split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b20 is a bias term. If \u03b20 is positive, it increases the utility, making acceptance more likely. \u03b21 represents the sensitivity to the monetary amount offered. However, since split_self is already in the model, perhaps \u03b21 is redundant if \u03b8 is capturing the effect of the split relative to the fair share. Alternatively, \u03b21 could be fixed to 1, assuming that utility increases linearly with the monetary gain, and \u03b8 captures the disutility from unfairness.\n\nWait, if we set \u03b21=1, then Utility = split_self - \u03b8 * max(fair_self - split_self, 0) + \u03b20. But maybe \u03b20 is not necessary. Let's think: when the offer is exactly fair (split_self = fair_self), then the utility is split_self. If split_self is higher than fair_self, utility is split_self. If lower, utility is split_self - \u03b8*(fair_self - split_self). So, the utility is always split_self minus a penalty when the offer is below fair. But perhaps participants have a baseline tendency to accept or reject regardless of the offer. For example, \u03b20 could represent this. If \u03b20 is positive, they are more likely to accept even if the offer is slightly unfair. If negative, they might reject even fair offers.\n\nAlternatively, maybe the intercept is not needed, and the utility is directly as split_self minus the penalty. Then, the probability of acceptance is logistic(Utility). If Utility > 0, probability > 0.5. So, the model would predict acceptance when split_self - \u03b8*(fair_self - split_self) > 0 (if split_self < fair_self). But when split_self >= fair_self, utility is split_self, so as long as split_self > 0, which it always is, the utility is positive, leading to higher probability of acceptance. But maybe participants have a different baseline. For example, even if the offer is fair, they might have a general tendency to reject, which would require an intercept.\n\nHmm. Let's consider an example. Suppose \u03b8 is 0.5. If fair_self is 10, and split_self is 8, then utility is 8 - 0.5*(10-8) = 8 -1 =7. Wait, that's higher than split_self. That doesn't make sense. Wait, no: the formula is split_self - \u03b8*(fair_self - split_self). So if split_self is less than fair_self, the penalty is \u03b8*(fair_self - split_self). So in this case, split_self is 8, fair_self is 10. The penalty is 0.5*(2) =1. So utility is 8 -1=7. But if split_self is 12, then utility is 12. So higher offers lead to higher utility, which makes sense. But in the case where the offer is below fair, the utility is split_self minus the penalty. But in this example, even with a penalty, the utility is still positive (7), which would predict a high probability of acceptance. But maybe the participant would reject because it's unfair. So perhaps the model needs to have the penalty term outweigh the split_self. Wait, but how?\n\nAlternatively, maybe the utility should be split_self - \u03b8*(fair_self - split_self). Let's compute that. If split_self is 8, fair_self is 10: utility is 8 - \u03b8*(2). If \u03b8 is 2, then utility is 8-4=4. Still positive. If \u03b8 is 5, utility is 8-10= -2. Then, the probability would be less than 0.5. So the parameter \u03b8 determines how much the participant penalizes unfairness. If \u03b8 >1, then even a small unfairness can lead to negative utility. For example, if \u03b8=1.5, then split_self=8, fair_self=10: utility=8 -1.5*2=5. Still positive. Hmm. Maybe the model needs another parameter.\n\nAlternatively, perhaps the utility is (split_self - \u03b8*(fair_self - split_self)). But this can be rewritten as split_self*(1 + \u03b8) - \u03b8*fair_self. Which is equivalent to \u03b8*(split_self - fair_self) + split_self. Wait, that's the same as split_self + \u03b8*(split_self - fair_self) = split_self*(1 + \u03b8) - \u03b8*fair_self. Hmm. Not sure if that's helpful.\n\nAlternatively, maybe the utility is split_self compared to a reference point, which could be the fair_self. So the utility is split_self - reference_point, and the reference_point is fair_self adjusted by some parameter. But I'm not sure.\n\nAnother angle: the participant's decision may depend on both the absolute amount they receive (split_self) and the relative fairness compared to their contribution. So the utility could be a combination of these two factors. For example:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self / fair_self - 1)\n\nHere, \u03b1 is the weight on the absolute amount, and \u03b2 is the weight on the relative fairness. If split_self is less than fair_self, the relative term is negative, reducing utility. But this introduces two parameters, \u03b1 and \u03b2. However, the problem states to have a slight bias towards fewer parameters. So maybe combining them into a single parameter.\n\nAlternatively, the utility could be:\n\nUtility = split_self - \u03b8 * (fair_self - split_self)\n\nWhich simplifies to split_self*(1 + \u03b8) - \u03b8*fair_self. Here, \u03b8 represents the trade-off between the absolute amount and the fairness. If \u03b8 is 0, the participant only cares about the absolute split_self. If \u03b8 is positive, they care about both the split and how it compares to the fair share. But this might lead to issues if \u03b8 is large. For example, if \u03b8 is 2, then even if split_self is slightly below fair_self, the utility could drop significantly.\n\nBut how does this model handle cases where split_self is greater than fair_self? In that case, (fair_self - split_self) is negative, so the term becomes positive, increasing utility. But maybe participants don't care about advantageous inequality (getting more than fair). So perhaps the model should only penalize when split_self is less than fair_self. Hence, using max(fair_self - split_self, 0) as the penalty term.\n\nSo, revising the utility function:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nIn this case, when split_self >= fair_self, the max term is zero, so utility is just split_self. When split_self < fair_self, utility is split_self minus \u03b8 times the deficit. If \u03b8 is greater than 1, the penalty is more than the deficit, leading to lower utility. For example, if \u03b8=2 and split_self is 8 (fair_self=10), utility is 8 - 2*2=4. Still positive. If \u03b8=3, utility is 8-6=2. Hmm, but even with \u03b8=3, the utility is positive. To get a negative utility, \u03b8 needs to be greater than (split_self)/(fair_self - split_self). Wait, but split_self can vary. For example, if split_self is 5 and fair_self is 10, then \u03b8 needs to be greater than 5/5=1 to make utility negative (5 - \u03b8*5). So if \u03b8>1, then 5 -5\u03b8 <0. So \u03b8=1.2 would give 5-6= -1, leading to less than 50% chance of acceptance.\n\nThis seems plausible. So the utility function would be:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nThen, the probability of acceptance is logistic(Utility). If Utility >0, probability >0.5; else, <0.5.\n\nBut wait, the logistic function is 1/(1 + exp(-Utility)). So when Utility is 0, probability is 0.5. So the model's prediction is based on whether the utility is positive or negative.\n\nBut in this model, when the offer is exactly fair (split_self = fair_self), utility is split_self, which is 3*token_self. Since token_self is at least 0, split_self is non-negative. So in fair offers, utility is positive, leading to higher probability of acceptance. But maybe participants have a different baseline. For example, even if the offer is fair, they might have a general tendency to reject, which would require an intercept term. So perhaps adding an intercept \u03b20:\n\nUtility = \u03b20 + split_self - \u03b8 * max(fair_self - split_self, 0)\n\nBut this introduces another parameter. However, the problem states to have a slight bias towards fewer parameters. So maybe the intercept is necessary. Let's think: without an intercept, when the offer is exactly fair, the utility is split_self (3*token_self). If token_self is, say, 10, then split_self is 30, leading to a very high utility (30), which would predict near-certain acceptance. But maybe in reality, participants might have other considerations, so an intercept could capture a general bias. However, adding \u03b20 might complicate parameter recovery, especially if there's collinearity between \u03b20 and \u03b8. \n\nAlternatively, perhaps the intercept is not needed. Let's proceed without it for simplicity, and see if that works. If the model without intercept can capture the necessary behavior, that's better. If not, then adding \u03b20 would be necessary.\n\nAnother consideration: the combined_earning is 3*(token_self + token_opp). The split_self and split_opp must sum to this. But in the responder's role, the split_self is the amount offered to them. The proposer's split is split_opp. But the responder's decision is based on their own split and their perception of fairness.\n\nWait, the fair_self is 3*token_self. The split_self is the amount offered. So the difference is split_self - 3*token_self. If this is negative, the responder is getting less than their contribution. The parameter \u03b8 scales how much this affects the utility.\n\nSo the utility function is:\n\nUtility = split_self - \u03b8 * max(3*token_self - split_self, 0)\n\nThis model has one learnable parameter, \u03b8, which must be positive. The range for \u03b8 could be [0, 5], for example, to allow for sufficient flexibility.\n\nBut wait, what if the participant also cares about the opponent's share? For example, if the opponent is getting more than their fair share, the participant might feel envious. But the problem states to focus on responder behavior, and the model should use the provided variables. However, including the opponent's fair share might complicate things. But given the variables available, perhaps it's better to stick with the participant's own fair share.\n\nAnother variable to consider is trial_type, which indicates whether contributions are equal, opponent more, or participant more. But since token_self and token_opp are available, trial_type can be derived from them, so including it might be redundant. The model can implicitly capture the trial_type through token_self and token_opp.\n\nNow, considering parameter recovery. The model should have parameters that are identifiable. If we have \u03b8 and \u03b20, but they are correlated, that could hinder recovery. So perhaps a model with just \u03b8 is better. But without an intercept, maybe the model can't capture a general bias. For example, if participants have a general tendency to reject offers even when fair, the model without an intercept can't capture that. In that case, adding \u03b20 would be necessary.\n\nAlternatively, maybe the intercept is part of the utility function. For example, participants might require a certain premium over the fair share. But that's more complex.\n\nLet's consider an example. Suppose a participant contributed 10 tokens (fair_self=30). If the offer is 30 (fair), utility is 30. If \u03b8=0.5, then any offer below 30 would reduce utility. For example, an offer of 25: utility=25 -0.5*(5)=22.5. Still positive, so probability >0.5. But if \u03b8=2, then 25 -2*5=15, still positive. Only when \u03b8> (split_self)/(fair_self - split_self) would the utility become negative. For example, if split_self=20, fair_self=30: \u03b8 needs to be >20/10=2. So \u03b8=3 would make utility=20 -3*10= -10, leading to rejection.\n\nBut in reality, participants might reject offers that are significantly below fair, even if the absolute amount is positive. So \u03b8 needs to be a parameter that scales the penalty for unfairness. The higher \u03b8, the more sensitive the participant is to being underpaid.\n\nSo the model with \u03b8 as the only learnable parameter might be sufficient. However, without an intercept, the model assumes that when the offer is exactly fair, the utility is split_self, which is 3*token_self. If token_self varies across trials, the utility for fair offers would vary. For example, if token_self is 10 (fair_self=30), utility is 30. If token_self is 5 (fair_self=15), utility is 15. But the probability of acceptance in both cases would be high (logistic(30) \u22481, logistic(15)\u22481). So the model assumes that fair offers are almost always accepted, which might not be the case if participants have varying baseline tendencies.\n\nAlternatively, adding an intercept \u03b20 could allow the model to account for a general bias. For example, if \u03b20 is negative, even fair offers might have lower utility. Let's see:\n\nUtility = \u03b20 + split_self - \u03b8 * max(fair_self - split_self, 0)\n\nIf \u03b20 is negative, say -5, then even if split_self is 30 (fair), utility is 30 -5 =25, which is still positive. So maybe the intercept isn't necessary. Alternatively, if \u03b20 is part of the utility function in a different way.\n\nAlternatively, perhaps the utility should be relative to the fair share. For example:\n\nUtility = (split_self - fair_self) - \u03b8 * max(fair_self - split_self, 0)\n\nWait, that would be split_self - fair_self - \u03b8*(fair_self - split_self) when split_self < fair_self. Which simplifies to (split_self - fair_self) - \u03b8*(fair_self - split_self) = (1 + \u03b8)(split_self - fair_self). But this is the same as (split_self - fair_self)*(1 + \u03b8). So when split_self < fair_self, this term is negative, scaled by (1 + \u03b8). But this seems redundant. Maybe this approach isn't helpful.\n\nAnother angle: the participant's utility could depend on the ratio of the split to the fair share. For example:\n\nUtility = split_self / fair_self - \u03b8 * max(1 - (split_self / fair_self), 0)\n\nBut this complicates the interpretation. Alternatively, using logarithms, but that might not be necessary.\n\nAlternatively, the utility could be the monetary amount minus a fairness term. For example:\n\nUtility = split_self - \u03b8 * (fair_self - split_self)\n\nBut this is the same as split_self*(1 + \u03b8) - \u03b8*fair_self. Which can be rewritten as \u03b8*(split_self - fair_self) + split_self. This combines the monetary gain and the fairness term. However, this introduces \u03b8 as a multiplier on both terms. If \u03b8 is positive, then when split_self > fair_self, the utility increases more than linearly. But when split_self < fair_self, the utility decreases. However, this model doesn't differentiate between advantageous and disadvantageous inequality. It just scales the difference. But in the context of responder behavior, perhaps only disadvantageous inequality matters. Hence, the max function is better.\n\nBack to the original model:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nThis model has one learnable parameter, \u03b8, which must be \u22650. The range for \u03b8 could be [0, 5], with inclusive bounds. This allows \u03b8 to be zero (no fairness concern) up to a maximum of 5, which would heavily penalize any deficit.\n\nThe probability of acceptance is then logistic(Utility). So when Utility is positive, the probability is greater than 0.5. When Utility is negative, less than 0.5.\n\nBut wait, the problem states that the utility variable may be unbounded, but the model must predict utility where less than 0 corresponds to less than 50% probability. The logistic function ensures this, as logistic(0) = 0.5. So this model satisfies that requirement.\n\nNow, checking if this model uses the provided variables appropriately. The variables needed are split_self, token_self. From token_self, we compute fair_self = 3 * token_self. The model doesn't use token_opp directly, but combined_earning is 3*(token_self + token_opp), but split_self + split_opp = combined_earning. However, the model as described doesn't require token_opp, since fair_self is based on the participant's own contribution. But is this appropriate?\n\nWait, the fair_self is 3*token_self, which is the participant's contribution. The opponent's contribution is 3*token_opp. The total is combined_earning = 3*(token_self + token_opp). The fair split would be each getting their own contribution. So split_self should be 3*token_self, split_opp 3*token_opp. If the proposer offers a different split, the responder evaluates based on their own fair share. This seems reasonable.\n\nBut perhaps the responder also considers the opponent's fair share. For example, if the opponent is offering a split that gives the responder more than their fair share, but the opponent is taking less than their own fair share, the responder might feel guilty. However, including this would complicate the model and add more parameters. Given the instruction to have a slight bias towards fewer parameters, it's better to omit this.\n\nThus, the model with \u03b8 as the only learnable parameter seems appropriate. It captures the responder's sensitivity to receiving less than their fair share, based on their own contribution. The utility is a function of the offered amount and the penalty for unfairness.\n\nNow, checking the variables in the dataset. The variables used in the model are split_self and token_self. These are available in the dataset. The model doesn't require any other variables, so it's compatible with the provided dataset structure.\n\nNext, parameter bounds. \u03b8 should be \u22650, as a negative \u03b8 would imply that participants derive utility from being underpaid, which doesn't make sense. The upper bound can be set to a reasonably high value, say 5, to allow for strong aversion to unfairness. So \u03b8 \u2208 [0, 5], inclusive.\n\nThe variables section would then include:\n\n- split_self: from data, float64.\n- token_self: from data, int64.\n- fair_self: calculated as 3 * token_self.\n- \u03b8: learnable parameter, range [0,5].\n\nThe utility is calculated as split_self - \u03b8 * max(fair_self - split_self, 0).\n\nThe target variable is 'accept', which is binary. The model's utility is passed through a logistic function to get the probability of acceptance.\n\nNow, ensuring that all variables in the model are described in the VARIABLES section. The variables used in the model are:\n\n- split_self: data variable.\n- token_self: data variable.\n- fair_self: calculated variable (3 * token_self).\n- \u03b8: learnable parameter.\n\nThe mathematical model is:\n\nUtility = split_self - theta * max(fair_self - split_self, 0)\n\nWhich can be written as:\n\nU = split_self - \u03b8 * max(3*token_self - split_self, 0)\n\nSo in the MODEL tags:\n\n<MODEL>\nU = split_self - theta * max(3 * token_self - split_self, 0)\n</MODEL>\n\nIn the VARIABLES JSON:\n\n- split_self: source data, not learnable.\n- token_self: source data, not learnable.\n- theta: learnable, range [0,5].\n- U: calculated, range (-inf, inf).\n\nThe summary should describe the model concisely, mentioning the fairness consideration based on own contribution and the penalty parameter.\n\nPotential issues: Does this model account for cases where the combined earning is different due to different token contributions? Yes, because fair_self is based on the participant's own tokens, which vary per trial. The model dynamically adjusts the fair share based on each trial's token_self.\n\nAnother consideration: participants might have different baseline propensities to accept or reject, which could be captured by an intercept. However, adding an intercept would introduce another parameter. Given the instruction to prefer fewer parameters, and the need for parameter recovery, a single parameter model is better. If the data shows a general bias towards acceptance or rejection not explained by the fairness term, the model might underfit. But without seeing the data, it's hard to say. However, the problem states that parameter recovery is roughly equal in priority to accuracy, so a simpler model is preferable.\n\nThus, the final model is as described, with theta as the only learnable parameter.\n</think>\n\n<MODEL>\nU = split_self - theta * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity parameter for disadvantageous inequality (penalty for receiving less than fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where responders evaluate offers based on their monetary gain and a penalty for receiving less than their fair share (3 * token_self). The penalty is scaled by a learnable parameter (theta) representing sensitivity to disadvantageous inequality. Utility is converted to acceptance probability via a logistic function, with negative utilities predicting rejection.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['theta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = split_self - theta * max(3 * token_self - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity parameter for disadvantageous inequality (penalty for receiving less than fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, theta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        fair_self = 3 * token_self\n        utility = split_self - theta * max(fair_self - split_self, 0)\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n",
      "timestamp": 1743515093.4054499
    }
  ],
  "learnable_parameters": {
    "theta": {
      "description": "Sensitivity parameter for disadvantageous inequality (penalty for receiving less than fair share)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = split_self - theta * max(3 * token_self - split_self, 0)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "theta": {
      "description": "Sensitivity parameter for disadvantageous inequality (penalty for receiving less than fair share)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_summary": "A utility model where responders evaluate offers based on their monetary gain and a penalty for receiving less than their fair share (3 * token_self). The penalty is scaled by a learnable parameter (theta) representing sensitivity to disadvantageous inequality. Utility is converted to acceptance probability via a logistic function, with negative utilities predicting rejection.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility function should be designed such that a utility less than 0 corresponds to less than 50% probability of acceptance, which suggests using a logistic function to convert utility to probability.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on the utility of the proposal from the responder's perspective. The dataset includes variables like token contributions (token_self and token_opp), combined earnings, split amounts, and percentages. \n\nThe main psychological mechanisms here likely involve fairness considerations and self-interest. Participants might compare their proposed share to what they perceive as fair based on their own and the opponent's contributions. The utility could be a function of the difference between the proposed split and a fair reference point, possibly adjusted by parameters that capture sensitivity to unfairness or self-interest.\n\nLet's think about how to model fairness. A common approach in behavioral economics is the inequity aversion model, where individuals dislike unequal outcomes. However, since the combined earnings vary, the fairness might be relative to contributions. For example, if the participant contributed more tokens, they might expect a larger share. The fair split could be proportional to their contribution relative to the total. \n\nThe variables token_self and token_opp represent each party's contribution. Each token is worth \u00a33, so the total combined earning is 3*(token_self + token_opp). The split_self is the amount offered to the participant. The fair share would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair share could be a key factor in the utility.\n\nSo, the utility might be calculated as the proposed split minus the fair share, adjusted by some parameters. But perhaps participants have different sensitivities to being under-benefited (receiving less than fair) versus over-benefited (receiving more). However, since the task is about accepting or rejecting, over-benefiting might not lead to rejection, but under-benefiting would. So maybe the model should focus on the negative deviation from the fair share.\n\nAlternatively, the utility could be a linear combination of the monetary gain and the fairness term. For example, utility = \u03b1*(split_self) + \u03b2*(fairness_deviation). But how to quantify fairness_deviation?\n\nWait, the split_self is in \u00a3, and the fair share would be (token_self / (token_self + token_opp)) * combined_earning. Let's compute that. Let's call this fair_self. Then, the difference between split_self and fair_self is the deviation from fairness. If split_self is less than fair_self, the participant might feel treated unfairly, leading to a lower utility. \n\nBut the utility could be split_self minus some term that penalizes unfairness. For example, if the split is less than fair, the penalty is higher. Let's model the utility as:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b8 is a learnable parameter representing the sensitivity to being under-benefited. If the proposed split is less than the fair share, the participant experiences a disutility proportional to the difference, scaled by \u03b8. If the split is equal or more, there's no penalty. This way, the utility decreases when the offer is below the fair share, making acceptance less likely.\n\nBut wait, the combined_earning is 3*(token_self + token_opp), so fair_self is 3*token_self. Because each token is \u00a33, so the participant's contribution is worth 3*token_self, and the opponent's is 3*token_opp. The total combined is 3*(token_self + token_opp). So the fair split for the participant would be exactly 3*token_self. Because that's their contribution. So if the split_self is less than 3*token_self, that's unfair. If it's more, they're getting a surplus.\n\nWait, but the split_self is part of the combined_earning. So the split_self + split_opp = combined_earning. So the fair split would be each getting their own contribution. So split_self should be 3*token_self, and split_opp 3*token_opp. But in reality, the proposer might offer a different split. So the fair_self is 3*token_self. The difference between split_self and fair_self is (split_self - 3*token_self). If this is negative, the participant is getting less than fair. If positive, more.\n\nSo the utility could be:\n\nUtility = split_self + \u03b1 * (split_self - fair_self)\n\nBut that might not capture the asymmetry. Alternatively, participants might only be averse to disadvantageous inequality. So they dislike when split_self < fair_self, but not when split_self > fair_self. So the disutility is only when split_self is less than fair_self.\n\nSo, the utility could be:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b8 is the sensitivity to being under-benefited. If \u03b8 is positive, then the utility decreases when the offer is below the fair share. The higher \u03b8, the more the participant dislikes being underpaid.\n\nAlternatively, maybe participants also care about the absolute difference, not just the direction. But given that the model needs to predict accept/reject, and the example suggests that utility < 0 leads to <50% probability, perhaps focusing on the disadvantageous inequality is sufficient.\n\nAnother consideration is the participant's own contribution (token_self) and the opponent's (token_opp). The trial_type variable indicates whether contributions are equal, opponent more, or participant more. This might influence the participant's perception of fairness. However, since token_self and token_opp are available, the model can directly compute the fair share without needing trial_type.\n\nSo, the key variables to include are split_self (the offer), token_self, and token_opp. From these, compute fair_self = 3 * token_self. Then, compute the difference between split_self and fair_self. If split_self < fair_self, the participant is getting less than their contribution, which might lead to rejection. The parameter \u03b8 scales this disutility.\n\nAdditionally, there might be a baseline acceptance tendency. For example, even if the offer is fair, there could be a general propensity to accept or reject, captured by an intercept term. So the utility could be:\n\nUtility = \u03b20 + \u03b21 * split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b20 is a bias term. If \u03b20 is positive, it increases the utility, making acceptance more likely. \u03b21 represents the sensitivity to the monetary amount offered. However, since split_self is already in the model, perhaps \u03b21 is redundant if \u03b8 is capturing the effect of the split relative to the fair share. Alternatively, \u03b21 could be fixed to 1, assuming that utility increases linearly with the monetary gain, and \u03b8 captures the disutility from unfairness.\n\nWait, if we set \u03b21=1, then Utility = split_self - \u03b8 * max(fair_self - split_self, 0) + \u03b20. But maybe \u03b20 is not necessary. Let's think: when the offer is exactly fair (split_self = fair_self), then the utility is split_self. If split_self is higher than fair_self, utility is split_self. If lower, utility is split_self - \u03b8*(fair_self - split_self). So, the utility is always split_self minus a penalty when the offer is below fair. But perhaps participants have a baseline tendency to accept or reject regardless of the offer. For example, \u03b20 could represent this. If \u03b20 is positive, they are more likely to accept even if the offer is slightly unfair. If negative, they might reject even fair offers.\n\nAlternatively, maybe the intercept is not needed, and the utility is directly as split_self minus the penalty. Then, the probability of acceptance is logistic(Utility). If Utility > 0, probability > 0.5. So, the model would predict acceptance when split_self - \u03b8*(fair_self - split_self) > 0 (if split_self < fair_self). But when split_self >= fair_self, utility is split_self, so as long as split_self > 0, which it always is, the utility is positive, leading to higher probability of acceptance. But maybe participants have a different baseline. For example, even if the offer is fair, they might have a general tendency to reject, which would require an intercept.\n\nHmm. Let's consider an example. Suppose \u03b8 is 0.5. If fair_self is 10, and split_self is 8, then utility is 8 - 0.5*(10-8) = 8 -1 =7. Wait, that's higher than split_self. That doesn't make sense. Wait, no: the formula is split_self - \u03b8*(fair_self - split_self). So if split_self is less than fair_self, the penalty is \u03b8*(fair_self - split_self). So in this case, split_self is 8, fair_self is 10. The penalty is 0.5*(2) =1. So utility is 8 -1=7. But if split_self is 12, then utility is 12. So higher offers lead to higher utility, which makes sense. But in the case where the offer is below fair, the utility is split_self minus the penalty. But in this example, even with a penalty, the utility is still positive (7), which would predict a high probability of acceptance. But maybe the participant would reject because it's unfair. So perhaps the model needs to have the penalty term outweigh the split_self. Wait, but how?\n\nAlternatively, maybe the utility should be split_self - \u03b8*(fair_self - split_self). Let's compute that. If split_self is 8, fair_self is 10: utility is 8 - \u03b8*(2). If \u03b8 is 2, then utility is 8-4=4. Still positive. If \u03b8 is 5, utility is 8-10= -2. Then, the probability would be less than 0.5. So the parameter \u03b8 determines how much the participant penalizes unfairness. If \u03b8 >1, then even a small unfairness can lead to negative utility. For example, if \u03b8=1.5, then split_self=8, fair_self=10: utility=8 -1.5*2=5. Still positive. Hmm. Maybe the model needs another parameter.\n\nAlternatively, perhaps the utility is (split_self - \u03b8*(fair_self - split_self)). But this can be rewritten as split_self*(1 + \u03b8) - \u03b8*fair_self. Which is equivalent to \u03b8*(split_self - fair_self) + split_self. Wait, that's the same as split_self + \u03b8*(split_self - fair_self) = split_self*(1 + \u03b8) - \u03b8*fair_self. Hmm. Not sure if that's helpful.\n\nAlternatively, maybe the utility is split_self compared to a reference point, which could be the fair_self. So the utility is split_self - reference_point, and the reference_point is fair_self adjusted by some parameter. But I'm not sure.\n\nAnother angle: the participant's decision may depend on both the absolute amount they receive (split_self) and the relative fairness compared to their contribution. So the utility could be a combination of these two factors. For example:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self / fair_self - 1)\n\nHere, \u03b1 is the weight on the absolute amount, and \u03b2 is the weight on the relative fairness. If split_self is less than fair_self, the relative term is negative, reducing utility. But this introduces two parameters, \u03b1 and \u03b2. However, the problem states to have a slight bias towards fewer parameters. So maybe combining them into a single parameter.\n\nAlternatively, the utility could be:\n\nUtility = split_self - \u03b8 * (fair_self - split_self)\n\nWhich simplifies to split_self*(1 + \u03b8) - \u03b8*fair_self. Here, \u03b8 represents the trade-off between the absolute amount and the fairness. If \u03b8 is 0, the participant only cares about the absolute split_self. If \u03b8 is positive, they care about both the split and how it compares to the fair share. But this might lead to issues if \u03b8 is large. For example, if \u03b8 is 2, then even if split_self is slightly below fair_self, the utility could drop significantly.\n\nBut how does this model handle cases where split_self is greater than fair_self? In that case, (fair_self - split_self) is negative, so the term becomes positive, increasing utility. But maybe participants don't care about advantageous inequality (getting more than fair). So perhaps the model should only penalize when split_self is less than fair_self. Hence, using max(fair_self - split_self, 0) as the penalty term.\n\nSo, revising the utility function:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nIn this case, when split_self >= fair_self, the max term is zero, so utility is just split_self. When split_self < fair_self, utility is split_self minus \u03b8 times the deficit. If \u03b8 is greater than 1, the penalty is more than the deficit, leading to lower utility. For example, if \u03b8=2 and split_self is 8 (fair_self=10), utility is 8 - 2*2=4. Still positive. If \u03b8=3, utility is 8-6=2. Hmm, but even with \u03b8=3, the utility is positive. To get a negative utility, \u03b8 needs to be greater than (split_self)/(fair_self - split_self). Wait, but split_self can vary. For example, if split_self is 5 and fair_self is 10, then \u03b8 needs to be greater than 5/5=1 to make utility negative (5 - \u03b8*5). So if \u03b8>1, then 5 -5\u03b8 <0. So \u03b8=1.2 would give 5-6= -1, leading to less than 50% chance of acceptance.\n\nThis seems plausible. So the utility function would be:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nThen, the probability of acceptance is logistic(Utility). If Utility >0, probability >0.5; else, <0.5.\n\nBut wait, the logistic function is 1/(1 + exp(-Utility)). So when Utility is 0, probability is 0.5. So the model's prediction is based on whether the utility is positive or negative.\n\nBut in this model, when the offer is exactly fair (split_self = fair_self), utility is split_self, which is 3*token_self. Since token_self is at least 0, split_self is non-negative. So in fair offers, utility is positive, leading to higher probability of acceptance. But maybe participants have a different baseline. For example, even if the offer is fair, they might have a general tendency to reject, which would require an intercept term. So perhaps adding an intercept \u03b20:\n\nUtility = \u03b20 + split_self - \u03b8 * max(fair_self - split_self, 0)\n\nBut this introduces another parameter. However, the problem states to have a slight bias towards fewer parameters. So maybe the intercept is necessary. Let's think: without an intercept, when the offer is exactly fair, the utility is split_self (3*token_self). If token_self is, say, 10, then split_self is 30, leading to a very high utility (30), which would predict near-certain acceptance. But maybe in reality, participants might have other considerations, so an intercept could capture a general bias. However, adding \u03b20 might complicate parameter recovery, especially if there's collinearity between \u03b20 and \u03b8. \n\nAlternatively, perhaps the intercept is not needed. Let's proceed without it for simplicity, and see if that works. If the model without intercept can capture the necessary behavior, that's better. If not, then adding \u03b20 would be necessary.\n\nAnother consideration: the combined_earning is 3*(token_self + token_opp). The split_self and split_opp must sum to this. But in the responder's role, the split_self is the amount offered to them. The proposer's split is split_opp. But the responder's decision is based on their own split and their perception of fairness.\n\nWait, the fair_self is 3*token_self. The split_self is the amount offered. So the difference is split_self - 3*token_self. If this is negative, the responder is getting less than their contribution. The parameter \u03b8 scales how much this affects the utility.\n\nSo the utility function is:\n\nUtility = split_self - \u03b8 * max(3*token_self - split_self, 0)\n\nThis model has one learnable parameter, \u03b8, which must be positive. The range for \u03b8 could be [0, 5], for example, to allow for sufficient flexibility.\n\nBut wait, what if the participant also cares about the opponent's share? For example, if the opponent is getting more than their fair share, the participant might feel envious. But the problem states to focus on responder behavior, and the model should use the provided variables. However, including the opponent's fair share might complicate things. But given the variables available, perhaps it's better to stick with the participant's own fair share.\n\nAnother variable to consider is trial_type, which indicates whether contributions are equal, opponent more, or participant more. But since token_self and token_opp are available, trial_type can be derived from them, so including it might be redundant. The model can implicitly capture the trial_type through token_self and token_opp.\n\nNow, considering parameter recovery. The model should have parameters that are identifiable. If we have \u03b8 and \u03b20, but they are correlated, that could hinder recovery. So perhaps a model with just \u03b8 is better. But without an intercept, maybe the model can't capture a general bias. For example, if participants have a general tendency to reject offers even when fair, the model without an intercept can't capture that. In that case, adding \u03b20 would be necessary.\n\nAlternatively, maybe the intercept is part of the utility function. For example, participants might require a certain premium over the fair share. But that's more complex.\n\nLet's consider an example. Suppose a participant contributed 10 tokens (fair_self=30). If the offer is 30 (fair), utility is 30. If \u03b8=0.5, then any offer below 30 would reduce utility. For example, an offer of 25: utility=25 -0.5*(5)=22.5. Still positive, so probability >0.5. But if \u03b8=2, then 25 -2*5=15, still positive. Only when \u03b8> (split_self)/(fair_self - split_self) would the utility become negative. For example, if split_self=20, fair_self=30: \u03b8 needs to be >20/10=2. So \u03b8=3 would make utility=20 -3*10= -10, leading to rejection.\n\nBut in reality, participants might reject offers that are significantly below fair, even if the absolute amount is positive. So \u03b8 needs to be a parameter that scales the penalty for unfairness. The higher \u03b8, the more sensitive the participant is to being underpaid.\n\nSo the model with \u03b8 as the only learnable parameter might be sufficient. However, without an intercept, the model assumes that when the offer is exactly fair, the utility is split_self, which is 3*token_self. If token_self varies across trials, the utility for fair offers would vary. For example, if token_self is 10 (fair_self=30), utility is 30. If token_self is 5 (fair_self=15), utility is 15. But the probability of acceptance in both cases would be high (logistic(30) \u22481, logistic(15)\u22481). So the model assumes that fair offers are almost always accepted, which might not be the case if participants have varying baseline tendencies.\n\nAlternatively, adding an intercept \u03b20 could allow the model to account for a general bias. For example, if \u03b20 is negative, even fair offers might have lower utility. Let's see:\n\nUtility = \u03b20 + split_self - \u03b8 * max(fair_self - split_self, 0)\n\nIf \u03b20 is negative, say -5, then even if split_self is 30 (fair), utility is 30 -5 =25, which is still positive. So maybe the intercept isn't necessary. Alternatively, if \u03b20 is part of the utility function in a different way.\n\nAlternatively, perhaps the utility should be relative to the fair share. For example:\n\nUtility = (split_self - fair_self) - \u03b8 * max(fair_self - split_self, 0)\n\nWait, that would be split_self - fair_self - \u03b8*(fair_self - split_self) when split_self < fair_self. Which simplifies to (split_self - fair_self) - \u03b8*(fair_self - split_self) = (1 + \u03b8)(split_self - fair_self). But this is the same as (split_self - fair_self)*(1 + \u03b8). So when split_self < fair_self, this term is negative, scaled by (1 + \u03b8). But this seems redundant. Maybe this approach isn't helpful.\n\nAnother angle: the participant's utility could depend on the ratio of the split to the fair share. For example:\n\nUtility = split_self / fair_self - \u03b8 * max(1 - (split_self / fair_self), 0)\n\nBut this complicates the interpretation. Alternatively, using logarithms, but that might not be necessary.\n\nAlternatively, the utility could be the monetary amount minus a fairness term. For example:\n\nUtility = split_self - \u03b8 * (fair_self - split_self)\n\nBut this is the same as split_self*(1 + \u03b8) - \u03b8*fair_self. Which can be rewritten as \u03b8*(split_self - fair_self) + split_self. This combines the monetary gain and the fairness term. However, this introduces \u03b8 as a multiplier on both terms. If \u03b8 is positive, then when split_self > fair_self, the utility increases more than linearly. But when split_self < fair_self, the utility decreases. However, this model doesn't differentiate between advantageous and disadvantageous inequality. It just scales the difference. But in the context of responder behavior, perhaps only disadvantageous inequality matters. Hence, the max function is better.\n\nBack to the original model:\n\nUtility = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nThis model has one learnable parameter, \u03b8, which must be \u22650. The range for \u03b8 could be [0, 5], with inclusive bounds. This allows \u03b8 to be zero (no fairness concern) up to a maximum of 5, which would heavily penalize any deficit.\n\nThe probability of acceptance is then logistic(Utility). So when Utility is positive, the probability is greater than 0.5. When Utility is negative, less than 0.5.\n\nBut wait, the problem states that the utility variable may be unbounded, but the model must predict utility where less than 0 corresponds to less than 50% probability. The logistic function ensures this, as logistic(0) = 0.5. So this model satisfies that requirement.\n\nNow, checking if this model uses the provided variables appropriately. The variables needed are split_self, token_self. From token_self, we compute fair_self = 3 * token_self. The model doesn't use token_opp directly, but combined_earning is 3*(token_self + token_opp), but split_self + split_opp = combined_earning. However, the model as described doesn't require token_opp, since fair_self is based on the participant's own contribution. But is this appropriate?\n\nWait, the fair_self is 3*token_self, which is the participant's contribution. The opponent's contribution is 3*token_opp. The total is combined_earning = 3*(token_self + token_opp). The fair split would be each getting their own contribution. So split_self should be 3*token_self, split_opp 3*token_opp. If the proposer offers a different split, the responder evaluates based on their own fair share. This seems reasonable.\n\nBut perhaps the responder also considers the opponent's fair share. For example, if the opponent is offering a split that gives the responder more than their fair share, but the opponent is taking less than their own fair share, the responder might feel guilty. However, including this would complicate the model and add more parameters. Given the instruction to have a slight bias towards fewer parameters, it's better to omit this.\n\nThus, the model with \u03b8 as the only learnable parameter seems appropriate. It captures the responder's sensitivity to receiving less than their fair share, based on their own contribution. The utility is a function of the offered amount and the penalty for unfairness.\n\nNow, checking the variables in the dataset. The variables used in the model are split_self and token_self. These are available in the dataset. The model doesn't require any other variables, so it's compatible with the provided dataset structure.\n\nNext, parameter bounds. \u03b8 should be \u22650, as a negative \u03b8 would imply that participants derive utility from being underpaid, which doesn't make sense. The upper bound can be set to a reasonably high value, say 5, to allow for strong aversion to unfairness. So \u03b8 \u2208 [0, 5], inclusive.\n\nThe variables section would then include:\n\n- split_self: from data, float64.\n- token_self: from data, int64.\n- fair_self: calculated as 3 * token_self.\n- \u03b8: learnable parameter, range [0,5].\n\nThe utility is calculated as split_self - \u03b8 * max(fair_self - split_self, 0).\n\nThe target variable is 'accept', which is binary. The model's utility is passed through a logistic function to get the probability of acceptance.\n\nNow, ensuring that all variables in the model are described in the VARIABLES section. The variables used in the model are:\n\n- split_self: data variable.\n- token_self: data variable.\n- fair_self: calculated variable (3 * token_self).\n- \u03b8: learnable parameter.\n\nThe mathematical model is:\n\nUtility = split_self - theta * max(fair_self - split_self, 0)\n\nWhich can be written as:\n\nU = split_self - \u03b8 * max(3*token_self - split_self, 0)\n\nSo in the MODEL tags:\n\n<MODEL>\nU = split_self - theta * max(3 * token_self - split_self, 0)\n</MODEL>\n\nIn the VARIABLES JSON:\n\n- split_self: source data, not learnable.\n- token_self: source data, not learnable.\n- theta: learnable, range [0,5].\n- U: calculated, range (-inf, inf).\n\nThe summary should describe the model concisely, mentioning the fairness consideration based on own contribution and the penalty parameter.\n\nPotential issues: Does this model account for cases where the combined earning is different due to different token contributions? Yes, because fair_self is based on the participant's own tokens, which vary per trial. The model dynamically adjusts the fair share based on each trial's token_self.\n\nAnother consideration: participants might have different baseline propensities to accept or reject, which could be captured by an intercept. However, adding an intercept would introduce another parameter. Given the instruction to prefer fewer parameters, and the need for parameter recovery, a single parameter model is better. If the data shows a general bias towards acceptance or rejection not explained by the fairness term, the model might underfit. But without seeing the data, it's hard to say. However, the problem states that parameter recovery is roughly equal in priority to accuracy, so a simpler model is preferable.\n\nThus, the final model is as described, with theta as the only learnable parameter.\n</think>\n\n<MODEL>\nU = split_self - theta * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity parameter for disadvantageous inequality (penalty for receiving less than fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where responders evaluate offers based on their monetary gain and a penalty for receiving less than their fair share (3 * token_self). The penalty is scaled by a learnable parameter (theta) representing sensitivity to disadvantageous inequality. Utility is converted to acceptance probability via a logistic function, with negative utilities predicting rejection.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = split_self - theta * max(3 * token_self - split_self, 0)\nSummary: A utility model where responders evaluate offers based on their monetary gain and a penalty for receiving less than their fair share (3 * token_self). The penalty is scaled by a learnable parameter (theta) representing sensitivity to disadvantageous inequality. Utility is converted to acceptance probability via a logistic function, with negative utilities predicting rejection.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, theta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        fair_self = 3 * token_self\n        utility = split_self - theta * max(fair_self - split_self, 0)\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8247863247863249,
  "fitting_results": [
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -123.07842402899026,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.5631685957695125,
      "success": true,
      "log_likelihood": -81.816701911661,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4340858382801107,
      "success": true,
      "log_likelihood": -35.169380798746715,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.499878363019537,
      "success": true,
      "log_likelihood": -77.37698014489436,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.9931402422030895,
      "success": true,
      "log_likelihood": -43.22378014785838,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.0017137467561308,
      "success": true,
      "log_likelihood": -44.089759962467916,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.9851105595740631,
      "success": true,
      "log_likelihood": -43.98732458123794,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.6734371183449543,
      "success": true,
      "log_likelihood": -59.982368843956564,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.6228329580876115,
      "success": true,
      "log_likelihood": -0.7929674299182946,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -108.07875087752873,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 2.1212304511238993,
      "success": true,
      "log_likelihood": -23.09789778660678,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.48830317123895856,
      "success": true,
      "log_likelihood": -3.748582142103117,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.4762121522460934,
      "success": true,
      "log_likelihood": -19.31452717549007,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4706937125761537,
      "success": true,
      "log_likelihood": -59.349035416396156,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.5140758373488747,
      "success": true,
      "log_likelihood": -25.472735713307458,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4515637141818134,
      "success": true,
      "log_likelihood": -129.83474450271396,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4151698048936103,
      "success": true,
      "log_likelihood": -90.33375836688788,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.5933802437697018,
      "success": true,
      "log_likelihood": -69.45760306146522,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.3832317950940105,
      "success": true,
      "log_likelihood": -65.73587205297524,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.3564105080973823,
      "success": true,
      "log_likelihood": -51.83178861255025,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 3.1579982620577978,
      "success": true,
      "log_likelihood": -12.836555988673224,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.4554474739312893,
      "success": true,
      "log_likelihood": -39.11081025939204,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.6130353205148418,
      "success": true,
      "log_likelihood": -91.22230591611748,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -123.07842402899024,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4405528454853427,
      "success": true,
      "log_likelihood": -152.66612489290029,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.504467446850374,
      "success": true,
      "log_likelihood": -90.13646073263821,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.8168662855427268,
      "success": true,
      "log_likelihood": -64.1740179286293,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.8708109267651577,
      "success": true,
      "log_likelihood": -17.232774792979658,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.52293471097301,
      "success": true,
      "log_likelihood": -70.71977376579574,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.9727847039190162,
      "success": true,
      "log_likelihood": -110.8299608002948,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.23590755569337457,
      "success": true,
      "log_likelihood": -38.684629828106324,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.392938871242717,
      "success": true,
      "log_likelihood": -130.77865551658755,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -76.05354319481447,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -23.025863872743603,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.0214628617502137,
      "success": true,
      "log_likelihood": -90.90803821651022,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.9876231566126286,
      "success": true,
      "log_likelihood": -63.779710785254025,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.6548377912788617,
      "success": true,
      "log_likelihood": -96.03532654392814,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4397279930913167,
      "success": true,
      "log_likelihood": -84.46905630955052,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.536206325186548,
      "success": true,
      "log_likelihood": -134.3414964024504,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.7566199482589457,
      "success": true,
      "log_likelihood": -65.59493917032079,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.0491996728052697,
      "success": true,
      "log_likelihood": -209.10272437595506,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.7785440716898495,
      "success": true,
      "log_likelihood": -109.86334757774294,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -152.10427491868813,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.3990170862386375,
      "success": true,
      "log_likelihood": -113.09880594245503,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.6998507733679961,
      "success": true,
      "log_likelihood": -177.72322312763916,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 2.236563720557637,
      "success": true,
      "log_likelihood": -158.88302913117028,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 2.371375981414513,
      "success": true,
      "log_likelihood": -11.282174529459907,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 2.357738768243198,
      "success": true,
      "log_likelihood": -69.27877965223938,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.17897922270680322,
      "success": true,
      "log_likelihood": -84.40634317801849,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 3.1850965837649983,
      "success": true,
      "log_likelihood": -134.93439278320994,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.43971511816369446,
      "success": true,
      "log_likelihood": -75.15375424721348,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4421516160775398,
      "success": true,
      "log_likelihood": -145.68400685542787,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.728831992924963,
      "success": true,
      "log_likelihood": -62.774514622788786,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -123.0784240289903,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.40453632271485,
      "success": true,
      "log_likelihood": -123.92497188327575,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -91.05290212758926,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4050945001646544,
      "success": true,
      "log_likelihood": -56.22288379806896,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.325653247324273,
      "success": true,
      "log_likelihood": -36.715037650388666,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.43893095754486977,
      "success": true,
      "log_likelihood": -45.16199733086469,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.9652973028772116,
      "success": true,
      "log_likelihood": -95.12142010348177,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.22639865948019933,
      "success": true,
      "log_likelihood": -49.76313050678907,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -54.00180878405687,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -158.104258683684,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4243316543330442,
      "success": true,
      "log_likelihood": -72.22479667277427,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.6504535436300586,
      "success": true,
      "log_likelihood": -82.03084841880131,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.487760242075284,
      "success": true,
      "log_likelihood": -158.18609171159355,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.8166139904478781,
      "success": true,
      "log_likelihood": -65.20674072852243,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.2008555308830603,
      "success": true,
      "log_likelihood": -33.38938098166416,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -146.1052102837751,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4087306063408502,
      "success": true,
      "log_likelihood": -201.10249463958087,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -123.07842402899027,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.8432098018339176,
      "success": true,
      "log_likelihood": -210.20317234675767,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -68.73866400294936,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -137.1046017672266,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.3836896873685725,
      "success": true,
      "log_likelihood": -38.539726117298486,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.6895584046415293,
      "success": true,
      "log_likelihood": -216.84527713077532,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 5.0,
      "success": true,
      "log_likelihood": -100.05257531939338,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.7095310174636466,
      "success": true,
      "log_likelihood": -116.74997195697455,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 4.033791971980327,
      "success": true,
      "log_likelihood": -77.93913360335479,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.26374842518844244,
      "success": true,
      "log_likelihood": -19.41688073945227,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.3367265077187185,
      "success": true,
      "log_likelihood": -84.06326694061882,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.9843692805304242,
      "success": true,
      "log_likelihood": -168.75714182399705,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "theta": {
        "r": 0.9869906198116075,
        "p": 1.3211729306074948e-79
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 161.57521507125742,
  "bic_results": {
    "average_bic": 161.57521507125742,
    "individual_bics": [
      249.74036699643662,
      167.2169227617781,
      73.92228053594954,
      158.33747922824483,
      90.03107923417286,
      91.76303886339194,
      3.5835448238623537,
      91.558168100932,
      3.5835448238623537,
      123.54825662636924,
      5.169453798292699,
      219.74102069351358,
      49.77931451166967,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      11.080683222662344,
      42.21257328943625,
      3.5835448238623537,
      122.28158977124842,
      54.528990365071024,
      263.25300794388403,
      184.25103567223186,
      142.49872506138655,
      135.0552630444066,
      107.2470961635566,
      29.256630915802557,
      81.80513945724019,
      186.02813077069106,
      3.5835448238623537,
      249.7403669964366,
      308.9157687242567,
      183.85644040373253,
      3.5835448238623537,
      131.9315547957147,
      38.049068524415425,
      145.02306647004758,
      225.2434405390457,
      80.95277859466876,
      265.1408299716312,
      155.69060532808504,
      49.635246683943315,
      185.39959537147655,
      131.14294050896416,
      195.65417202631238,
      172.52163155755716,
      272.2665117433569,
      134.77339727909768,
      421.78896769036623,
      223.310214093942,
      307.7920687758324,
      229.78113082336617,
      359.0299651937344,
      321.34957720079666,
      26.147867997375922,
      142.14107824293487,
      172.3962052944931,
      273.452304504876,
      153.89102743288308,
      294.95153264931184,
      129.13254818403368,
      249.7403669964367,
      251.4334627050076,
      185.68932319363464,
      116.02928653459404,
      77.01359423923344,
      93.9075136001855,
      193.82635914541964,
      103.10977995203424,
      111.58713650656985,
      319.7920363058241,
      148.03311228400466,
      167.64521577605873,
      319.9557023616432,
      133.99700039550098,
      70.36228090178443,
      295.7939395060063,
      405.78850821761785,
      249.74036699643665,
      423.98986363197145,
      141.06084694435484,
      277.7927224729093,
      80.66297117305308,
      437.27407320000674,
      203.68866957724288,
      237.0834628524052,
      159.4617861451657,
      42.41728041736065,
      171.71005281969374,
      341.0978025864502
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- theta: r = 0.987",
  "previous_models": [
    "Specification: U = split_self - theta * max(3 * token_self - split_self, 0)\nSummary: A utility model where responders evaluate offers based on their monetary gain and a penalty for receiving less than their fair share (3 * token_self). The penalty is scaled by a learnable parameter (theta) representing sensitivity to disadvantageous inequality. Utility is converted to acceptance probability via a logistic function, with negative utilities predicting rejection.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 161.57521507125742\nOverall Accuracy: 0.8248\n\nParameter Recovery:\n- theta: r = 0.987"
  ]
}