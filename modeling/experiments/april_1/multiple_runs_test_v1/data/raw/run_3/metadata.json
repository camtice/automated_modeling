{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided dataset variables appropriately, and ensure parameter recovery is feasible. Also, the utility should be modeled such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on the utility of accepting the proposal. The utility function should incorporate factors that influence the responder's decision.\n\nLooking at the dataset variables, important ones for the responder's decision might include:\n\n- split_self: The proposed share for the participant in \u00a3.\n- split_opp: The proposed share for the opponent in \u00a3.\n- token_self and token_opp: The number of tokens each contributed, which relate to their contributions to the combined earning.\n- combined_earning: Total money earned, which is 3*(token_self + token_opp) since each token is \u00a33.\n\nThe trial_type indicates whether contributions are equal, opponent more, or participant more. This might affect fairness perceptions. Trial_role is 1 for responder, which is the focus here.\n\nThe key psychological factors here are likely fairness considerations and self-interest. The responder might compare their proposed share to what they consider fair based on their own and the opponent's contributions. If the proposed split is perceived as unfair, they might reject it even if it's a positive gain.\n\nSo, the utility could be a function of the monetary gain (split_self) minus some fairness term that penalizes deviations from the fair share. The fair share could be based on the participant's contribution relative to the total. For example, if the participant contributed token_self tokens out of total tokens (token_self + token_opp), their fair share percentage would be (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between the proposed and fair share could be the fairness deviation.\n\nBut the splitperc_self is already provided as a variable. However, maybe the fair share is calculated based on contributions. Let's think: the fair share percentage would be (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these two could be a measure of unfairness. If the proposed percentage is less than the fair share, the responder might feel it's unfair, leading to a lower utility.\n\nAlternatively, the model could consider the ratio of the proposed split to the fair split. For example, if the fair amount is (token_self / total_tokens) * combined_earning, then the proposed split_self compared to that fair amount. The difference could be a term in the utility function.\n\nBut perhaps a simpler approach is to model the utility as the proposed monetary gain (split_self) minus a fairness term that depends on how much the split deviates from the equitable split based on contributions. The fairness term could be weighted by a parameter representing the participant's sensitivity to fairness.\n\nAnother consideration is the total combined earning. If the combined earning is higher, maybe the responder is more sensitive to fairness because the stakes are higher. But the split_self is already in absolute terms, so maybe the fairness term should be relative.\n\nLet me formalize this. The fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. The proposed share is split_self. The difference between split_self and the fair share is (split_self - fair_share). If this is positive, the participant is getting more than fair; if negative, less. The utility could be split_self (monetary gain) plus a term that penalizes deviations from fairness. Wait, but if the participant is getting less than fair, that's a negative term, reducing utility. Alternatively, the fairness term could be the difference between the proposed and fair share, multiplied by a parameter. For example:\n\nUtility = split_self + \u03b1 * (split_self - fair_share)\n\nBut if split_self is less than fair_share, this term becomes negative, reducing utility. However, this might not capture the idea that people might reject even positive offers if they're unfair. Alternatively, the fairness term could be the difference between the proposed and fair share, but squared to capture aversion to any deviation. Or perhaps the absolute value.\n\nAlternatively, the model could use the ratio of the proposed split to the fair split. For example, (split_self / fair_share) - 1. If this is negative, the participant is getting less than fair. The utility could then be split_self multiplied by a term that includes this ratio. But this might complicate things.\n\nAnother angle is the concept of inequity aversion, where people dislike unequal outcomes. The responder might compare their share to the opponent's share. If the opponent is getting more, that could lead to a negative utility. But in this case, the split_opp is the opponent's proposed share. However, when the participant is the responder, the proposer is the opponent. So the proposer (opponent) is offering split_self to the responder and keeping split_opp. Inequity aversion models often consider the difference between one's own payoff and the other's. So, the utility could be split_self - \u03b8 * max(split_opp - split_self, 0) - \u03b3 * max(split_self - split_opp, 0). Here, \u03b8 is aversion to disadvantageous inequality, and \u03b3 to advantageous. But this is the Fehr-Schmidt model.\n\nBut in this task, the combined earning is split, so split_self + split_opp = combined_earning. So, if the responder's share is split_self, the opponent's is split_opp = combined_earning - split_self. So, the difference is split_opp - split_self = combined_earning - 2*split_self. So, the Fehr-Schmidt utility could be:\n\nU = split_self - \u03b1 * max(combined_earning - 2*split_self, 0) - \u03b2 * max(2*split_self - combined_earning, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequality (when opponent gets more), and \u03b2 to advantageous. But maybe in this context, since the proposer is the opponent, the responder might be more sensitive to being treated unfairly, so \u03b1 would be a key parameter.\n\nAlternatively, the model could consider the fairness based on contributions. The fair share would be based on the tokens each contributed. For example, the participant's contribution is token_self, opponent's is token_opp. The fair split_self would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair value could be a term in the utility. So, the utility could be:\n\nU = split_self + \u03bb * (split_self - fair_share)\n\nBut if split_self is less than fair_share, this term is negative, reducing utility. The parameter \u03bb represents the sensitivity to fairness deviations. If \u03bb is positive, the participant values getting more than fair, but if negative, they penalize deviations from fair. Wait, maybe \u03bb should be a parameter that scales the importance of fairness. Alternatively, the utility could be:\n\nU = split_self - \u03b1 * (fair_share - split_self)\n\nIf split_self is less than fair_share, then (fair_share - split_self) is positive, so the utility decreases by \u03b1 times that difference. If split_self is more than fair_share, this term becomes negative, so utility increases. But maybe participants are only averse to being under-compensated relative to their contribution, not over. So perhaps the model should only penalize when split_self < fair_share. So:\n\nU = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nThis way, if the proposed split is less than fair, the utility is reduced by \u03b1 times the difference. If it's equal or more, no penalty. This captures the idea that participants reject offers that are below their fair share based on contribution.\n\nAdditionally, the model might include a term for the total stake. For example, if the combined_earning is high, the same absolute difference might matter more or less. But since split_self is in absolute terms, maybe the fairness term should be relative. Alternatively, the fairness term could be the percentage difference. For example, (fair_share - split_self)/fair_share. But this complicates the model.\n\nAnother consideration is the participant's own contribution (token_self). If they contributed more, they might expect a higher share. The fair_share calculation already accounts for this, as it's based on their contribution proportion.\n\nPutting this together, the utility function could be:\n\nUtility = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nWhere fair_share = (token_self / (token_self + token_opp)) * combined_earning\n\nBut combined_earning is 3*(token_self + token_opp), so fair_share simplifies to 3*token_self. Because combined_earning = 3*(token_self + token_opp), and (token_self / (token_self + token_opp)) * combined_earning = 3*token_self. So fair_share is simply 3* token_self. That's an interesting simplification. Because combined_earning is 3*(sum of tokens), so the fair share based on contribution is exactly 3* own tokens. So split_self is the proposed amount, and the fair share is 3* token_self. So the difference is split_self - 3*token_self. If this is negative, the participant is getting less than their fair share. So the utility could be:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nThis simplifies the model because we don't need to calculate fair_share each time; it's directly 3*token_self. That's a neat point. So the model can use this directly.\n\nAdditionally, the participant might have a baseline tendency to accept or reject, which could be captured by a intercept term. For example, even if the offer is fair, there might be a general bias. So:\n\nUtility = \u03b2 * split_self - \u03b1 * max(3*token_self - split_self, 0) + \u03b5\n\nHere, \u03b2 could be a parameter representing the sensitivity to monetary gain, \u03b1 the sensitivity to unfairness (under-fair offers), and \u03b5 a noise term. However, since the utility is passed through a logistic function to get the probability of acceptance, the \u03b2 and \u03b1 parameters would need to be learned. But perhaps \u03b2 can be set to 1 for identifiability, as scaling both \u03b2 and \u03b1 could lead to parameter trade-offs. Alternatively, include \u03b2 as a learnable parameter.\n\nBut to keep the number of parameters low (since the instruction says to have a slight bias for fewer parameters), maybe set \u03b2=1 and have \u03b1 as the only learnable parameter. However, this might not capture cases where participants value money more or less. Alternatively, include both \u03b2 and \u03b1 as parameters. But that's two parameters, which is manageable.\n\nWait, but the problem states that the model should predict utility where less than 0 corresponds to less than 50% chance of acceptance. The utility is passed through a logistic function. So, the probability of acceptance is 1 / (1 + exp(-Utility)). So, if Utility is positive, the probability is >0.5; if negative, <0.5.\n\nSo, the model needs to compute Utility as a function of the variables, and then the probability is derived from that.\n\nNow, considering the parameters:\n\n- \u03b1: sensitivity to unfairness (when split_self < 3*token_self)\n- \u03b2: sensitivity to the monetary gain (split_self)\n\nBut if we include both, they might be correlated. For example, if \u03b2 is high, the participant is more sensitive to monetary gains, which could compensate for unfairness. However, to avoid overparameterization, maybe fix \u03b2=1 and have \u03b1 as the only learnable parameter. Let's see:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nIf \u03b1 is positive, then when the offer is below the fair share (3*token_self), the utility is reduced. If the offer is equal or above, utility is just split_self. So, for example, if the offer is exactly fair (split_self = 3*token_self), utility is 3*token_self. If the offer is more than fair, utility is split_self (which is higher than 3*token_self). If the offer is less, utility is split_self minus \u03b1*(3*token_self - split_self).\n\nThis model has one learnable parameter, \u03b1, which must be bounded. Since \u03b1 is a scaling factor for the unfairness penalty, it should be non-negative. So the range could be [0, some upper bound]. The upper bound could be, say, 5, but needs to be generous. Maybe 0 to 10, inclusive.\n\nAlternatively, if we include \u03b2 as a learnable parameter, then:\n\nUtility = \u03b2 * split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nHere, \u03b2 could be constrained to be positive. For example, \u03b2 in [0.1, 10], and \u03b1 in [0, 10]. But this adds another parameter. However, the instruction says to have a slight bias for fewer parameters. So maybe start with one parameter.\n\nBut perhaps participants might have different sensitivities to the monetary amount. For example, some might be more willing to accept lower offers if they're not too unfair, while others might be very sensitive to any unfairness. Including \u03b2 allows the model to capture the relative importance of monetary gain versus fairness. However, with two parameters, there's a risk of covariance. But given that the task requires parameter recovery, having two parameters might be manageable if they are not perfectly correlated.\n\nAlternatively, perhaps the model can be simplified by assuming \u03b2=1, and only learn \u03b1. Let's proceed with that first.\n\nSo, the utility function would be:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nThen, the probability of acceptance is logistic(Utility). If Utility > 0, P(accept) > 0.5.\n\nBut wait, the logistic function is 1/(1 + exp(-Utility)). So, if Utility is split_self - \u03b1*(...), then when split_self is high enough, Utility is positive. But if the split_self is low and the penalty is high, Utility could be negative.\n\nThis seems reasonable. However, what if the split_self is very high, but the participant's contribution is low? For example, if token_self is 2, token_opp is 8, combined_earning is 30. The fair share would be 6 (3*2). If the split_self is 10, which is more than fair, then the utility is 10. So the participant would likely accept. If split_self is 5, which is less than fair (6), then utility is 5 - \u03b1*(6-5) = 5 - \u03b1. If \u03b1 is 3, then utility is 2, which is positive. Wait, that's not right. Wait, 5 - 3*(1) = 2. So even though the offer is below fair, the utility is still positive. That would mean the participant would accept. But maybe in reality, participants might reject offers below their fair share even if the absolute amount is positive. So perhaps the model needs to have a higher penalty.\n\nAlternatively, maybe the utility should be split_self - \u03b1*(fair_share - split_self). So, if split_self is less than fair_share, the penalty is \u03b1*(fair_share - split_self). If split_self is 5 and fair_share is 6, then penalty is \u03b1*1. So utility is 5 - \u03b1*1. If \u03b1 is 5, then utility is 0. If \u03b1 is 6, utility is -1. So in that case, the probability of acceptance would be less than 0.5.\n\nBut this depends on the value of \u03b1. So the model's ability to predict rejection when offers are unfair depends on \u03b1 being sufficiently large. However, \u03b1 is a learnable parameter, so during estimation, the model would learn the appropriate value.\n\nBut perhaps this model is too simplistic. Maybe participants also consider the proposer's share. For example, if the proposer is keeping a lot more, even if the offer is fair, the responder might still reject. But in the current model, that's not captured. Alternatively, the model could include a term that compares the proposer's share to their fair share. For example, the proposer's fair share is 3*token_opp. The proposer's actual share is split_opp = combined_earning - split_self. So, the proposer's overpayment is (split_opp - 3*token_opp). If this is positive, the proposer is keeping more than their fair share. The responder might be averse to this, leading to a lower utility. So the utility could be:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b3 * max( (split_opp - 3*token_opp), 0 )\n\nBut split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self. The proposer's fair share is 3*token_opp. So split_opp - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self. Wait, that's the same as the participant's underpayment. So this term is redundant. Therefore, including both terms might not add anything. So perhaps the model should stick to the participant's own underpayment.\n\nAlternatively, the proposer's overpayment could be split_opp - 3*token_opp. But split_opp is combined_earning - split_self. So split_opp - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self. Which is the same as the participant's underpayment. So this term is the same as the first term. Therefore, including both would be redundant. Hence, the model can stick to the participant's own underpayment.\n\nSo the initial model with one parameter \u03b1 seems plausible. But perhaps including a term for advantageous inequality (when the participant gets more than their fair share) could be considered. However, in the Fehr-Schmidt model, advantageous inequality aversion is usually weaker. But in this context, since the participant is the responder, they might not care as much about getting more than their fair share. So maybe the model can ignore advantageous inequality.\n\nThus, the model can be:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nThis captures the idea that the participant's utility is their monetary gain minus a penalty for receiving less than their fair share (based on their contribution). The parameter \u03b1 determines how much they penalize unfairness.\n\nNow, the probability of acceptance is logistic(Utility). So when Utility > 0, the probability is greater than 0.5. If the offer is fair or generous, split_self >= 3*token_self, so Utility is split_self, which is positive (since split_self is at least 3*token_self, which is a positive amount). Wait, but split_self can be any value up to combined_earning. For example, if the participant contributed 2 tokens (fair share 6), and the proposer offers 10 (split_self=10), then Utility is 10, which is positive. So the probability of acceptance is high. If the offer is 5, which is below fair share (6), then Utility is 5 - \u03b1*(1). If \u03b1 is 5, Utility is 0, so 50% chance. If \u03b1 is 6, Utility is -1, so less than 50% chance.\n\nThis seems reasonable. The model has one learnable parameter \u03b1, which is bounded between 0 and some upper limit. Since \u03b1 represents the penalty per unit of unfairness (in \u00a3), it's plausible that it can't be negative (as a negative \u03b1 would mean the participant likes being underpaid). So \u03b1's range is [0, ...]. The upper bound should be generous, say 10. So \u03b1 \u2208 [0, 10].\n\nBut wait, what if the participant's token_self is 0? Then their fair share is 0, so any split_self >=0 would not incur a penalty. But in the dataset, token_self is the number of tokens found by the participant, which I assume is at least 0. But if token_self is 0, then the fair share is 0, so any split_self >=0 would not have a penalty. But in reality, if the participant contributed nothing, they might still have some fairness considerations. However, according to the model, their fair share is 0, so any positive split_self is acceptable. But maybe the model should account for that. However, given the task description, the pretask involves finding tokens, so token_self is likely positive. But the dataset may include cases where token_self is 0. The model as designed handles that by not penalizing in such cases.\n\nAnother consideration: the combined_earning is 3*(token_self + token_opp). So split_self + split_opp = 3*(token_self + token_opp). Therefore, split_opp = 3*(token_self + token_opp) - split_self. The proposer's fair share is 3*token_opp. So the proposer's split_opp minus their fair share is (3*(token_self + token_opp) - split_self) - 3*token_opp = 3*token_self - split_self. Which is the same as the participant's underpayment. So any underpayment by the participant is exactly the proposer's overpayment. Hence, the model's penalty term captures both the participant's underpayment and the proposer's overpayment. Therefore, the model indirectly accounts for the proposer's overpayment.\n\nThus, the model with \u03b1 as the only learnable parameter seems sufficient. It captures the responder's aversion to receiving less than their contribution-based fair share, which also corresponds to the proposer keeping more than their fair share.\n\nNow, the variables in the model:\n\n- split_self: from the dataset (float64)\n- token_self: from the dataset (int64)\n- \u03b1: learnable parameter, range [0, 10]\n\nThe utility is calculated as split_self - \u03b1 * max(3*token_self - split_self, 0). The max function ensures that the penalty is only applied when split_self < 3*token_self.\n\nThe target variable is 'accept', which is binary. The probability of acceptance is 1 / (1 + exp(-Utility)). The model parameters (\u03b1) can be estimated using maximum likelihood estimation, optimizing the likelihood of the observed accept/reject decisions.\n\nNow, checking the requirements:\n\n1. Predict responder behavior (accept). The model uses trial_role=1 (responder) implicitly, as the model is only for responders. The dataset should be filtered to include only responder trials (trial_role=1) when fitting the model.\n\n2. Uses variables available in the dataset: split_self and token_self are available.\n\n3. The utility is modeled such that when Utility < 0, the probability of acceptance is <0.5. This is satisfied as the logistic function converts Utility to probabilities accordingly.\n\nLearnable parameters: \u03b1, with bounds [0, 10]. This is generous and inclusive.\n\nNow, the variable descriptions:\n\n- split_self: from data, the proposed share for the participant.\n- token_self: from data, the participant's tokens.\n- \u03b1: learnable parameter, description: \"Sensitivity to unfairness (penalty per unit \u00a3 below fair share)\", range [0,10], inclusive.\n\nThe mathematical model is:\n\nUtility = split_self - \u03b1 * max(3 * token_self - split_self, 0)\n\nThis can be written in LaTeX as:\n\nU = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0)\n\nBut in the <MODEL> tags, using variable names as given.\n\nSo, the model would be:\n\n<MODEL>\nutility = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\nThen, the variables:\n\n- split_self: data variable, source is 'data'.\n- token_self: data variable, source is 'data'.\n- alpha: learnable parameter, range [0, 10], inclusive.\n\nAdditionally, the model may include a noise term (epsilon) as part of the utility for stochasticity, but since the problem states that the utility is passed through a logistic function, which inherently includes a logit noise model. Therefore, the utility is deterministic, and the noise is captured by the logistic transformation. Hence, no need for an explicit noise parameter in the utility equation.\n\nThus, the final model is as described, with alpha as the only learnable parameter.\n</think>\n\n<MODEL>\nutility = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"utility\": {\n      \"description\": \"Calculated utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to unfairness (penalty per \u00a31 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the proposed share and a penalty for offers below the contribution-based fair share. The fair share is calculated as 3 * token_self (each token = \u00a33). The penalty scales with the deficit relative to this fair value, controlled by parameter alpha. Utility is converted to acceptance probability via logistic function.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                tags, using variable names as given.\n\nSo, the model would be:\n\n<MODEL>\nutility = split_self - alpha * max(3 * token_self - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"utility\": {\n      \"description\": \"Calculated utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to unfairness (penalty per \\u00a31 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = split_self - alpha * max(3 * token_self - split_self, 0)\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 1, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 5, \"token_self\": 5, \"combined_earning\": 30, \"split_opp\": 15.0, \"split_self\": 15.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 15.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 15.0}, {\"ID\": 2, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 2, \"trial_role\": 1, \"token_opp\": 8, \"token_self\": 2, \"combined_earning\": 30, \"split_opp\": 24.0, \"split_self\": 6.0, \"splitperc_opp\": 80, \"splitperc_self\": 20, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 20.0, \"proposed_amount\": 6.0}, {\"ID\": 3, \"group\": \"control\", \"trial_number\": 3, \"trial_type\": 3, \"trial_role\": 1, \"token_opp\": 2, \"token_self\": 8, \"combined_earning\": 30, \"split_opp\": 6.0, \"split_self\": 24.0, \"splitperc_opp\": 20, \"splitperc_self\": 80, \"accept\": 1.0, \"accepted_amount\": 24.0, \"accepted_perc\": 80.0, \"proposed_perc\": 80.0, \"proposed_amount\": 24.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1743515171.7005062
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to unfairness (penalty per \u00a31 below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "tags, using variable names as given.\n\nSo, the model would be:\n\n<MODEL>\nutility = split_self - alpha * max(3 * token_self - split_self, 0)",
  "variable_descriptions": {
    "utility": {
      "description": "Calculated utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "alpha": {
      "description": "Sensitivity to unfairness (penalty per \u00a31 below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_summary": "A utility model where acceptance probability depends on the proposed share and a penalty for offers below the contribution-based fair share. The fair share is calculated as 3 * token_self (each token = \u00a33). The penalty scales with the deficit relative to this fair value, controlled by parameter alpha. Utility is converted to acceptance probability via logistic function.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided dataset variables appropriately, and ensure parameter recovery is feasible. Also, the utility should be modeled such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on the utility of accepting the proposal. The utility function should incorporate factors that influence the responder's decision.\n\nLooking at the dataset variables, important ones for the responder's decision might include:\n\n- split_self: The proposed share for the participant in \u00a3.\n- split_opp: The proposed share for the opponent in \u00a3.\n- token_self and token_opp: The number of tokens each contributed, which relate to their contributions to the combined earning.\n- combined_earning: Total money earned, which is 3*(token_self + token_opp) since each token is \u00a33.\n\nThe trial_type indicates whether contributions are equal, opponent more, or participant more. This might affect fairness perceptions. Trial_role is 1 for responder, which is the focus here.\n\nThe key psychological factors here are likely fairness considerations and self-interest. The responder might compare their proposed share to what they consider fair based on their own and the opponent's contributions. If the proposed split is perceived as unfair, they might reject it even if it's a positive gain.\n\nSo, the utility could be a function of the monetary gain (split_self) minus some fairness term that penalizes deviations from the fair share. The fair share could be based on the participant's contribution relative to the total. For example, if the participant contributed token_self tokens out of total tokens (token_self + token_opp), their fair share percentage would be (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between the proposed and fair share could be the fairness deviation.\n\nBut the splitperc_self is already provided as a variable. However, maybe the fair share is calculated based on contributions. Let's think: the fair share percentage would be (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these two could be a measure of unfairness. If the proposed percentage is less than the fair share, the responder might feel it's unfair, leading to a lower utility.\n\nAlternatively, the model could consider the ratio of the proposed split to the fair split. For example, if the fair amount is (token_self / total_tokens) * combined_earning, then the proposed split_self compared to that fair amount. The difference could be a term in the utility function.\n\nBut perhaps a simpler approach is to model the utility as the proposed monetary gain (split_self) minus a fairness term that depends on how much the split deviates from the equitable split based on contributions. The fairness term could be weighted by a parameter representing the participant's sensitivity to fairness.\n\nAnother consideration is the total combined earning. If the combined earning is higher, maybe the responder is more sensitive to fairness because the stakes are higher. But the split_self is already in absolute terms, so maybe the fairness term should be relative.\n\nLet me formalize this. The fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. The proposed share is split_self. The difference between split_self and the fair share is (split_self - fair_share). If this is positive, the participant is getting more than fair; if negative, less. The utility could be split_self (monetary gain) plus a term that penalizes deviations from fairness. Wait, but if the participant is getting less than fair, that's a negative term, reducing utility. Alternatively, the fairness term could be the difference between the proposed and fair share, multiplied by a parameter. For example:\n\nUtility = split_self + \u03b1 * (split_self - fair_share)\n\nBut if split_self is less than fair_share, this term becomes negative, reducing utility. However, this might not capture the idea that people might reject even positive offers if they're unfair. Alternatively, the fairness term could be the difference between the proposed and fair share, but squared to capture aversion to any deviation. Or perhaps the absolute value.\n\nAlternatively, the model could use the ratio of the proposed split to the fair split. For example, (split_self / fair_share) - 1. If this is negative, the participant is getting less than fair. The utility could then be split_self multiplied by a term that includes this ratio. But this might complicate things.\n\nAnother angle is the concept of inequity aversion, where people dislike unequal outcomes. The responder might compare their share to the opponent's share. If the opponent is getting more, that could lead to a negative utility. But in this case, the split_opp is the opponent's proposed share. However, when the participant is the responder, the proposer is the opponent. So the proposer (opponent) is offering split_self to the responder and keeping split_opp. Inequity aversion models often consider the difference between one's own payoff and the other's. So, the utility could be split_self - \u03b8 * max(split_opp - split_self, 0) - \u03b3 * max(split_self - split_opp, 0). Here, \u03b8 is aversion to disadvantageous inequality, and \u03b3 to advantageous. But this is the Fehr-Schmidt model.\n\nBut in this task, the combined earning is split, so split_self + split_opp = combined_earning. So, if the responder's share is split_self, the opponent's is split_opp = combined_earning - split_self. So, the difference is split_opp - split_self = combined_earning - 2*split_self. So, the Fehr-Schmidt utility could be:\n\nU = split_self - \u03b1 * max(combined_earning - 2*split_self, 0) - \u03b2 * max(2*split_self - combined_earning, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequality (when opponent gets more), and \u03b2 to advantageous. But maybe in this context, since the proposer is the opponent, the responder might be more sensitive to being treated unfairly, so \u03b1 would be a key parameter.\n\nAlternatively, the model could consider the fairness based on contributions. The fair share would be based on the tokens each contributed. For example, the participant's contribution is token_self, opponent's is token_opp. The fair split_self would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair value could be a term in the utility. So, the utility could be:\n\nU = split_self + \u03bb * (split_self - fair_share)\n\nBut if split_self is less than fair_share, this term is negative, reducing utility. The parameter \u03bb represents the sensitivity to fairness deviations. If \u03bb is positive, the participant values getting more than fair, but if negative, they penalize deviations from fair. Wait, maybe \u03bb should be a parameter that scales the importance of fairness. Alternatively, the utility could be:\n\nU = split_self - \u03b1 * (fair_share - split_self)\n\nIf split_self is less than fair_share, then (fair_share - split_self) is positive, so the utility decreases by \u03b1 times that difference. If split_self is more than fair_share, this term becomes negative, so utility increases. But maybe participants are only averse to being under-compensated relative to their contribution, not over. So perhaps the model should only penalize when split_self < fair_share. So:\n\nU = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nThis way, if the proposed split is less than fair, the utility is reduced by \u03b1 times the difference. If it's equal or more, no penalty. This captures the idea that participants reject offers that are below their fair share based on contribution.\n\nAdditionally, the model might include a term for the total stake. For example, if the combined_earning is high, the same absolute difference might matter more or less. But since split_self is in absolute terms, maybe the fairness term should be relative. Alternatively, the fairness term could be the percentage difference. For example, (fair_share - split_self)/fair_share. But this complicates the model.\n\nAnother consideration is the participant's own contribution (token_self). If they contributed more, they might expect a higher share. The fair_share calculation already accounts for this, as it's based on their contribution proportion.\n\nPutting this together, the utility function could be:\n\nUtility = split_self - \u03b1 * max(fair_share - split_self, 0)\n\nWhere fair_share = (token_self / (token_self + token_opp)) * combined_earning\n\nBut combined_earning is 3*(token_self + token_opp), so fair_share simplifies to 3*token_self. Because combined_earning = 3*(token_self + token_opp), and (token_self / (token_self + token_opp)) * combined_earning = 3*token_self. So fair_share is simply 3* token_self. That's an interesting simplification. Because combined_earning is 3*(sum of tokens), so the fair share based on contribution is exactly 3* own tokens. So split_self is the proposed amount, and the fair share is 3* token_self. So the difference is split_self - 3*token_self. If this is negative, the participant is getting less than their fair share. So the utility could be:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nThis simplifies the model because we don't need to calculate fair_share each time; it's directly 3*token_self. That's a neat point. So the model can use this directly.\n\nAdditionally, the participant might have a baseline tendency to accept or reject, which could be captured by a intercept term. For example, even if the offer is fair, there might be a general bias. So:\n\nUtility = \u03b2 * split_self - \u03b1 * max(3*token_self - split_self, 0) + \u03b5\n\nHere, \u03b2 could be a parameter representing the sensitivity to monetary gain, \u03b1 the sensitivity to unfairness (under-fair offers), and \u03b5 a noise term. However, since the utility is passed through a logistic function to get the probability of acceptance, the \u03b2 and \u03b1 parameters would need to be learned. But perhaps \u03b2 can be set to 1 for identifiability, as scaling both \u03b2 and \u03b1 could lead to parameter trade-offs. Alternatively, include \u03b2 as a learnable parameter.\n\nBut to keep the number of parameters low (since the instruction says to have a slight bias for fewer parameters), maybe set \u03b2=1 and have \u03b1 as the only learnable parameter. However, this might not capture cases where participants value money more or less. Alternatively, include both \u03b2 and \u03b1 as parameters. But that's two parameters, which is manageable.\n\nWait, but the problem states that the model should predict utility where less than 0 corresponds to less than 50% chance of acceptance. The utility is passed through a logistic function. So, the probability of acceptance is 1 / (1 + exp(-Utility)). So, if Utility is positive, the probability is >0.5; if negative, <0.5.\n\nSo, the model needs to compute Utility as a function of the variables, and then the probability is derived from that.\n\nNow, considering the parameters:\n\n- \u03b1: sensitivity to unfairness (when split_self < 3*token_self)\n- \u03b2: sensitivity to the monetary gain (split_self)\n\nBut if we include both, they might be correlated. For example, if \u03b2 is high, the participant is more sensitive to monetary gains, which could compensate for unfairness. However, to avoid overparameterization, maybe fix \u03b2=1 and have \u03b1 as the only learnable parameter. Let's see:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nIf \u03b1 is positive, then when the offer is below the fair share (3*token_self), the utility is reduced. If the offer is equal or above, utility is just split_self. So, for example, if the offer is exactly fair (split_self = 3*token_self), utility is 3*token_self. If the offer is more than fair, utility is split_self (which is higher than 3*token_self). If the offer is less, utility is split_self minus \u03b1*(3*token_self - split_self).\n\nThis model has one learnable parameter, \u03b1, which must be bounded. Since \u03b1 is a scaling factor for the unfairness penalty, it should be non-negative. So the range could be [0, some upper bound]. The upper bound could be, say, 5, but needs to be generous. Maybe 0 to 10, inclusive.\n\nAlternatively, if we include \u03b2 as a learnable parameter, then:\n\nUtility = \u03b2 * split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nHere, \u03b2 could be constrained to be positive. For example, \u03b2 in [0.1, 10], and \u03b1 in [0, 10]. But this adds another parameter. However, the instruction says to have a slight bias for fewer parameters. So maybe start with one parameter.\n\nBut perhaps participants might have different sensitivities to the monetary amount. For example, some might be more willing to accept lower offers if they're not too unfair, while others might be very sensitive to any unfairness. Including \u03b2 allows the model to capture the relative importance of monetary gain versus fairness. However, with two parameters, there's a risk of covariance. But given that the task requires parameter recovery, having two parameters might be manageable if they are not perfectly correlated.\n\nAlternatively, perhaps the model can be simplified by assuming \u03b2=1, and only learn \u03b1. Let's proceed with that first.\n\nSo, the utility function would be:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nThen, the probability of acceptance is logistic(Utility). If Utility > 0, P(accept) > 0.5.\n\nBut wait, the logistic function is 1/(1 + exp(-Utility)). So, if Utility is split_self - \u03b1*(...), then when split_self is high enough, Utility is positive. But if the split_self is low and the penalty is high, Utility could be negative.\n\nThis seems reasonable. However, what if the split_self is very high, but the participant's contribution is low? For example, if token_self is 2, token_opp is 8, combined_earning is 30. The fair share would be 6 (3*2). If the split_self is 10, which is more than fair, then the utility is 10. So the participant would likely accept. If split_self is 5, which is less than fair (6), then utility is 5 - \u03b1*(6-5) = 5 - \u03b1. If \u03b1 is 3, then utility is 2, which is positive. Wait, that's not right. Wait, 5 - 3*(1) = 2. So even though the offer is below fair, the utility is still positive. That would mean the participant would accept. But maybe in reality, participants might reject offers below their fair share even if the absolute amount is positive. So perhaps the model needs to have a higher penalty.\n\nAlternatively, maybe the utility should be split_self - \u03b1*(fair_share - split_self). So, if split_self is less than fair_share, the penalty is \u03b1*(fair_share - split_self). If split_self is 5 and fair_share is 6, then penalty is \u03b1*1. So utility is 5 - \u03b1*1. If \u03b1 is 5, then utility is 0. If \u03b1 is 6, utility is -1. So in that case, the probability of acceptance would be less than 0.5.\n\nBut this depends on the value of \u03b1. So the model's ability to predict rejection when offers are unfair depends on \u03b1 being sufficiently large. However, \u03b1 is a learnable parameter, so during estimation, the model would learn the appropriate value.\n\nBut perhaps this model is too simplistic. Maybe participants also consider the proposer's share. For example, if the proposer is keeping a lot more, even if the offer is fair, the responder might still reject. But in the current model, that's not captured. Alternatively, the model could include a term that compares the proposer's share to their fair share. For example, the proposer's fair share is 3*token_opp. The proposer's actual share is split_opp = combined_earning - split_self. So, the proposer's overpayment is (split_opp - 3*token_opp). If this is positive, the proposer is keeping more than their fair share. The responder might be averse to this, leading to a lower utility. So the utility could be:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b3 * max( (split_opp - 3*token_opp), 0 )\n\nBut split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self. The proposer's fair share is 3*token_opp. So split_opp - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self. Wait, that's the same as the participant's underpayment. So this term is redundant. Therefore, including both terms might not add anything. So perhaps the model should stick to the participant's own underpayment.\n\nAlternatively, the proposer's overpayment could be split_opp - 3*token_opp. But split_opp is combined_earning - split_self. So split_opp - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self. Which is the same as the participant's underpayment. So this term is the same as the first term. Therefore, including both would be redundant. Hence, the model can stick to the participant's own underpayment.\n\nSo the initial model with one parameter \u03b1 seems plausible. But perhaps including a term for advantageous inequality (when the participant gets more than their fair share) could be considered. However, in the Fehr-Schmidt model, advantageous inequality aversion is usually weaker. But in this context, since the participant is the responder, they might not care as much about getting more than their fair share. So maybe the model can ignore advantageous inequality.\n\nThus, the model can be:\n\nUtility = split_self - \u03b1 * max(3*token_self - split_self, 0)\n\nThis captures the idea that the participant's utility is their monetary gain minus a penalty for receiving less than their fair share (based on their contribution). The parameter \u03b1 determines how much they penalize unfairness.\n\nNow, the probability of acceptance is logistic(Utility). So when Utility > 0, the probability is greater than 0.5. If the offer is fair or generous, split_self >= 3*token_self, so Utility is split_self, which is positive (since split_self is at least 3*token_self, which is a positive amount). Wait, but split_self can be any value up to combined_earning. For example, if the participant contributed 2 tokens (fair share 6), and the proposer offers 10 (split_self=10), then Utility is 10, which is positive. So the probability of acceptance is high. If the offer is 5, which is below fair share (6), then Utility is 5 - \u03b1*(1). If \u03b1 is 5, Utility is 0, so 50% chance. If \u03b1 is 6, Utility is -1, so less than 50% chance.\n\nThis seems reasonable. The model has one learnable parameter \u03b1, which is bounded between 0 and some upper limit. Since \u03b1 represents the penalty per unit of unfairness (in \u00a3), it's plausible that it can't be negative (as a negative \u03b1 would mean the participant likes being underpaid). So \u03b1's range is [0, ...]. The upper bound should be generous, say 10. So \u03b1 \u2208 [0, 10].\n\nBut wait, what if the participant's token_self is 0? Then their fair share is 0, so any split_self >=0 would not incur a penalty. But in the dataset, token_self is the number of tokens found by the participant, which I assume is at least 0. But if token_self is 0, then the fair share is 0, so any split_self >=0 would not have a penalty. But in reality, if the participant contributed nothing, they might still have some fairness considerations. However, according to the model, their fair share is 0, so any positive split_self is acceptable. But maybe the model should account for that. However, given the task description, the pretask involves finding tokens, so token_self is likely positive. But the dataset may include cases where token_self is 0. The model as designed handles that by not penalizing in such cases.\n\nAnother consideration: the combined_earning is 3*(token_self + token_opp). So split_self + split_opp = 3*(token_self + token_opp). Therefore, split_opp = 3*(token_self + token_opp) - split_self. The proposer's fair share is 3*token_opp. So the proposer's split_opp minus their fair share is (3*(token_self + token_opp) - split_self) - 3*token_opp = 3*token_self - split_self. Which is the same as the participant's underpayment. So any underpayment by the participant is exactly the proposer's overpayment. Hence, the model's penalty term captures both the participant's underpayment and the proposer's overpayment. Therefore, the model indirectly accounts for the proposer's overpayment.\n\nThus, the model with \u03b1 as the only learnable parameter seems sufficient. It captures the responder's aversion to receiving less than their contribution-based fair share, which also corresponds to the proposer keeping more than their fair share.\n\nNow, the variables in the model:\n\n- split_self: from the dataset (float64)\n- token_self: from the dataset (int64)\n- \u03b1: learnable parameter, range [0, 10]\n\nThe utility is calculated as split_self - \u03b1 * max(3*token_self - split_self, 0). The max function ensures that the penalty is only applied when split_self < 3*token_self.\n\nThe target variable is 'accept', which is binary. The probability of acceptance is 1 / (1 + exp(-Utility)). The model parameters (\u03b1) can be estimated using maximum likelihood estimation, optimizing the likelihood of the observed accept/reject decisions.\n\nNow, checking the requirements:\n\n1. Predict responder behavior (accept). The model uses trial_role=1 (responder) implicitly, as the model is only for responders. The dataset should be filtered to include only responder trials (trial_role=1) when fitting the model.\n\n2. Uses variables available in the dataset: split_self and token_self are available.\n\n3. The utility is modeled such that when Utility < 0, the probability of acceptance is <0.5. This is satisfied as the logistic function converts Utility to probabilities accordingly.\n\nLearnable parameters: \u03b1, with bounds [0, 10]. This is generous and inclusive.\n\nNow, the variable descriptions:\n\n- split_self: from data, the proposed share for the participant.\n- token_self: from data, the participant's tokens.\n- \u03b1: learnable parameter, description: \"Sensitivity to unfairness (penalty per unit \u00a3 below fair share)\", range [0,10], inclusive.\n\nThe mathematical model is:\n\nUtility = split_self - \u03b1 * max(3 * token_self - split_self, 0)\n\nThis can be written in LaTeX as:\n\nU = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0)\n\nBut in the <MODEL> tags, using variable names as given.\n\nSo, the model would be:\n\n<MODEL>\nutility = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\nThen, the variables:\n\n- split_self: data variable, source is 'data'.\n- token_self: data variable, source is 'data'.\n- alpha: learnable parameter, range [0, 10], inclusive.\n\nAdditionally, the model may include a noise term (epsilon) as part of the utility for stochasticity, but since the problem states that the utility is passed through a logistic function, which inherently includes a logit noise model. Therefore, the utility is deterministic, and the noise is captured by the logistic transformation. Hence, no need for an explicit noise parameter in the utility equation.\n\nThus, the final model is as described, with alpha as the only learnable parameter.\n</think>\n\n<MODEL>\nutility = split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"utility\": {\n      \"description\": \"Calculated utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to unfairness (penalty per \u00a31 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the proposed share and a penalty for offers below the contribution-based fair share. The fair share is calculated as 3 * token_self (each token = \u00a33). The penalty scales with the deficit relative to this fair value, controlled by parameter alpha. Utility is converted to acceptance probability via logistic function.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: tags, using variable names as given.\n\nSo, the model would be:\n\n<MODEL>\nutility = split_self - alpha * max(3 * token_self - split_self, 0)\nSummary: A utility model where acceptance probability depends on the proposed share and a penalty for offers below the contribution-based fair share. The fair share is calculated as 3 * token_self (each token = \u00a33). The penalty scales with the deficit relative to this fair value, controlled by parameter alpha. Utility is converted to acceptance probability via logistic function.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = split_self - alpha * max(3 * token_self - split_self, 0)\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8403540903540905,
  "fitting_results": [
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955368,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.563139807606845,
      "success": true,
      "log_likelihood": -81.81670205658847,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4340846802549745,
      "success": true,
      "log_likelihood": -35.1693808005058,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4990466251145556,
      "success": true,
      "log_likelihood": -77.37733181876025,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9931403506714205,
      "success": true,
      "log_likelihood": -43.223780147858804,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0017141238521894,
      "success": true,
      "log_likelihood": -44.0897599604572,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9855730422193212,
      "success": true,
      "log_likelihood": -43.98731018378683,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6734214443073179,
      "success": true,
      "log_likelihood": -59.982370987148414,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6228329616317487,
      "success": true,
      "log_likelihood": -0.7929674299182901,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.289184108716671,
      "success": true,
      "log_likelihood": -81.73955882715134,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.1212151842082,
      "success": true,
      "log_likelihood": -23.097897790254475,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.48830316830181475,
      "success": true,
      "log_likelihood": -3.748582142103133,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4762140259584682,
      "success": true,
      "log_likelihood": -19.314527176111614,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4707217036953752,
      "success": true,
      "log_likelihood": -59.34903566027848,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.514075988734656,
      "success": true,
      "log_likelihood": -25.472735713368788,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4543714578407638,
      "success": true,
      "log_likelihood": -129.83126860662705,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4163651397968835,
      "success": true,
      "log_likelihood": -90.33312557274587,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5933786259101972,
      "success": true,
      "log_likelihood": -69.45760305942461,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.383184161567224,
      "success": true,
      "log_likelihood": -65.7358752293074,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3564101322733937,
      "success": true,
      "log_likelihood": -51.83178861253027,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.1579928048063053,
      "success": true,
      "log_likelihood": -12.836555988546822,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4554486104982675,
      "success": true,
      "log_likelihood": -39.11081026104425,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6131842521249258,
      "success": true,
      "log_likelihood": -91.22231236703131,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955366,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4424211008354944,
      "success": true,
      "log_likelihood": -152.66483691874575,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5044006873721965,
      "success": true,
      "log_likelihood": -90.13646248183875,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8168898081289727,
      "success": true,
      "log_likelihood": -64.17401719365832,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8708119331878807,
      "success": true,
      "log_likelihood": -17.232774793138486,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.522956541941995,
      "success": true,
      "log_likelihood": -70.71977391080797,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9727554889399229,
      "success": true,
      "log_likelihood": -110.82996080074776,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.23590758170242415,
      "success": true,
      "log_likelihood": -38.684629828106985,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0214384201532714,
      "success": true,
      "log_likelihood": -127.88140001639961,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.65977107469034,
      "success": true,
      "log_likelihood": -46.10140611978637,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -23.025863872743603,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0212969380361052,
      "success": true,
      "log_likelihood": -90.90804732792583,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9922241162143902,
      "success": true,
      "log_likelihood": -63.77032253899882,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.648292473179825,
      "success": true,
      "log_likelihood": -96.06178620178689,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4396671041060194,
      "success": true,
      "log_likelihood": -84.46905499511672,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5359282761409487,
      "success": true,
      "log_likelihood": -134.3415166754923,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7540623135808644,
      "success": true,
      "log_likelihood": -65.59519451785248,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0488583234313957,
      "success": true,
      "log_likelihood": -209.10271826879654,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7769731759052074,
      "success": true,
      "log_likelihood": -109.86396924981486,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -115.22642481011445,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3990175363977453,
      "success": true,
      "log_likelihood": -113.09880594362092,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6951536027340092,
      "success": true,
      "log_likelihood": -177.72033552985891,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.99922979149286,
      "success": true,
      "log_likelihood": -162.57125844109086,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.37137646472536,
      "success": true,
      "log_likelihood": -11.28217452944004,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.887415845316445,
      "success": true,
      "log_likelihood": -94.29948071017911,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.17974183004952482,
      "success": true,
      "log_likelihood": -84.40605575598535,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -92.20057619959418,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.43985596096075924,
      "success": true,
      "log_likelihood": -75.15375917587986,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4421984445097813,
      "success": true,
      "log_likelihood": -145.6840061875211,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.728813555242977,
      "success": true,
      "log_likelihood": -62.7745146155531,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955369,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4037775370694543,
      "success": true,
      "log_likelihood": -123.92520928714558,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.0008694466369406,
      "success": true,
      "log_likelihood": -77.43639241613026,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.405094494320969,
      "success": true,
      "log_likelihood": -56.222883798069006,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.32565553694862925,
      "success": true,
      "log_likelihood": -36.71503764867489,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.43893268852585055,
      "success": true,
      "log_likelihood": -45.16199733170496,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9681368867457929,
      "success": true,
      "log_likelihood": -95.11861159661557,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.22642770898458292,
      "success": true,
      "log_likelihood": -49.76313095414278,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -0.09717469987590818,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -115.2264271296346,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.424442774468709,
      "success": true,
      "log_likelihood": -72.22478499371667,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6393419741486636,
      "success": true,
      "log_likelihood": -82.12046304242388,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4878594661779503,
      "success": true,
      "log_likelihood": -158.18607892342513,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8224127345637474,
      "success": true,
      "log_likelihood": -65.20005455571197,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.20085684078846025,
      "success": true,
      "log_likelihood": -33.38938225446673,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -92.20057619959414,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4087159746079954,
      "success": true,
      "log_likelihood": -201.1024946336861,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955369,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3935144651461964,
      "success": true,
      "log_likelihood": -222.0958091474772,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -68.73866400294936,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.000870518072753,
      "success": true,
      "log_likelihood": -123.48809205576786,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.383317935061163,
      "success": true,
      "log_likelihood": -38.53978731229208,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7091293936197288,
      "success": true,
      "log_likelihood": -216.74027610312345,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -46.148876559956825,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7096690173978699,
      "success": true,
      "log_likelihood": -116.74996116700245,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.033811738640534,
      "success": true,
      "log_likelihood": -77.93913361146852,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.26333205991504655,
      "success": true,
      "log_likelihood": -19.416833851259778,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3439012207394627,
      "success": true,
      "log_likelihood": -84.05515331188766,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9841591785352107,
      "success": true,
      "log_likelihood": -168.75718012948192,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.9810177835043199,
        "p": 1.2586421302093897e-71
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 149.58268140620058,
  "bic_results": {
    "average_bic": 149.58268140620058,
    "individual_bics": [
      141.93296947756346,
      167.21692305163305,
      73.9222805394677,
      158.33818257597662,
      90.03107923417372,
      91.76303885937051,
      3.5835448238623537,
      91.55813930602977,
      3.5835448238623537,
      123.54826091275294,
      5.16945379829269,
      167.0626365927588,
      49.77931451896506,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      11.080683222662376,
      42.21257329067934,
      3.5835448238623537,
      122.28159025901307,
      54.528990365193685,
      263.2460561517102,
      184.24977008394785,
      142.49872505730534,
      135.0552693970709,
      107.24709616351664,
      29.256630915549753,
      81.80513946054461,
      186.02814367251872,
      3.5835448238623537,
      141.93296947756343,
      308.9131927759476,
      183.8564439021336,
      3.5835448238623537,
      131.93155332577274,
      38.04906852473308,
      145.02306676007206,
      225.24344053995162,
      80.95277859467008,
      259.34631897125536,
      95.78633117802885,
      49.635246683943315,
      185.39961359430777,
      131.12416401645376,
      195.70709134202988,
      172.52162892868955,
      272.2665522894407,
      134.77390797416106,
      421.7889554760492,
      223.31145743808582,
      234.036368558685,
      229.78113082569794,
      359.02418999817394,
      328.72603582063783,
      26.14786799733619,
      192.18248035881433,
      172.3956304504268,
      187.98467133764447,
      153.89103729021582,
      294.9515313134983,
      129.1325481695623,
      141.9329694775635,
      251.43393751274726,
      158.45630377071663,
      116.02928653459412,
      77.01359423580588,
      93.90751360186603,
      193.82074213168724,
      103.10978084674167,
      3.7778683382079263,
      234.0363731977253,
      148.03308892588944,
      167.82444502330387,
      319.95567678530637,
      133.98362804988005,
      70.36228344738957,
      187.9846713376444,
      405.7885082058283,
      141.9329694775635,
      447.77513723341053,
      141.06084694435484,
      250.55970304999184,
      80.66309356304026,
      437.064071144703,
      95.88127205836976,
      237.08344127246102,
      159.46178616139315,
      42.417186640975665,
      171.69382556223144,
      341.09787919741996
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.981",
  "previous_models": [
    "Specification: tags, using variable names as given.\n\nSo, the model would be:\n\n<MODEL>\nutility = split_self - alpha * max(3 * token_self - split_self, 0)\nSummary: A utility model where acceptance probability depends on the proposed share and a penalty for offers below the contribution-based fair share. The fair share is calculated as 3 * token_self (each token = \u00a33). The penalty scales with the deficit relative to this fair value, controlled by parameter alpha. Utility is converted to acceptance probability via logistic function.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 149.58268140620058\nOverall Accuracy: 0.8404\n\nParameter Recovery:\n- alpha: r = 0.981"
  ]
}