{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\nOkay, let's look at the previous model's results. The BIC was around 124.65, which is okay, but there's room for improvement. The accuracy was 84.8%, which is decent, but parameter recovery for theta was lower (0.766) compared to beta (0.890). That suggests that theta might not be as well-identified in the model. Maybe the model needs more features or a different structure to better capture the underlying mechanisms.\n\nThe previous model used a utility function with beta (baseline) and theta (penalty for unfairness). The penalty was based on the difference between 3*token_self and split_self. But maybe there's more to fairness. For example, participants might also care about the opponent's share relative to their contribution. Or maybe the penalty should be nonlinear, like a quadratic term when the offer is very unfair.\n\nAnother thought: the current model uses max(3*token_self - split_self, 0), which only penalizes when the offer is below the fair share. But perhaps participants also have a sensitivity to advantageous inequity, where they get more than their fair share. Although, in the responder role, they might not reject such offers, but including that could help in parameter recovery if some participants are sensitive to overcompensation.\n\nAlternatively, the model could include a term for the ratio of split_self to the fair share instead of the absolute difference. For example, (split_self / (3*token_self)) - 1. This might capture proportional deviations, which could be more psychologically relevant.\n\nAlso, the current model has two parameters. Maybe adding a third parameter, like a sensitivity to the opponent's overcompensation, but that could increase covariance. Alternatively, using a nonlinear transformation of the existing terms, like a sigmoid for the penalty, might help without adding parameters.\n\nThe parameter recovery for theta was lower, so perhaps the model needs to better differentiate the effects of theta. Maybe introducing an interaction term between split_self and the fairness penalty. Or scaling the penalty by some other variable, like the total combined earnings.\n\nAnother angle: the baseline beta might not account for individual differences in response bias. Maybe including a trial-level random effect or a hierarchical structure, but that's complex. Alternatively, adding another fixed effect, like trial_type (whether contributions are equal, opponent more, etc.), could capture context effects.\n\nWait, the trial_type variable indicates if contributions were equal, opponent more, or participant more. Including trial_type in the model might help. For example, participants might be more sensitive to unfairness when they contributed more (trial_type=3). So an interaction between trial_type and the fairness penalty could improve the model.\n\nBut the instructions say to use the variables appropriately but not necessarily all. So maybe trial_type can be a moderator for theta. For example, theta could vary based on trial_type, allowing different sensitivity in different contexts. That would add parameters, but maybe improve recovery.\n\nAlternatively, the model could use the difference between split_self and split_opp, but given that split_opp is determined once split_self is set, that might not add new info. But if participants compare their split to the opponent's, that could be a factor. However, split_opp is the opponent's proposed share, which might not be directly comparable.\n\nAnother idea: the model could consider the proportion of the total combined earnings that split_self represents. For example, split_perc_self. But the current model uses absolute amounts. Maybe participants think in percentages rather than absolute values. So the utility could be based on split_perc_self compared to the fair percentage (token_self / (token_self + token_opp) * 100). But that's similar to the current approach, just in percentage terms.\n\nWait, the current model uses 3*token_self as the fair share, which is equivalent to (token_self / (token_self + token_opp)) * combined_earning, since combined_earning is 3*(token_self + token_opp). So the model already captures the fair share proportionally. But maybe participants have nonlinear utility for money, so log(split_self) instead of split_self. That could be tested.\n\nAlternatively, the model could include a term for the magnitude of the combined earnings. For example, higher combined earnings might make participants more or less sensitive to unfairness. But that would require an interaction term between combined_earning and the fairness penalty, adding another parameter.\n\nIn summary, possible improvements:\n\n1. Introduce nonlinear terms for the fairness penalty (e.g., squared term).\n2. Include trial_type as a moderator for theta.\n3. Use percentage-based fairness instead of absolute.\n4. Consider advantageous inequity (overcompensation) with a separate parameter.\n5. Add a parameter for the curvature of the utility function (e.g., log utility).\n\nBut to keep the number of parameters low, maybe try a nonlinear penalty. For example, theta * (max(3*token_self - split_self, 0))^2. This would make the penalty increase quadratically with the deficit, which might better capture strong aversion to large unfairness.\n\nAlternatively, use a sigmoid function to cap the penalty, but that's more complex. A quadratic term adds one parameter (the exponent), but if we fix it to 2, no new parameters. However, the current model uses a linear penalty. A quadratic term might improve fit without adding parameters if the exponent is fixed.\n\nAnother approach: allow theta to vary with token_self. For example, participants with higher token_self (more contribution) might be more sensitive to unfairness. But that would require an interaction term, adding a parameter.\n\nAlternatively, the model could include a dynamic baseline where beta is influenced by token_self or other variables. But that complicates the model.\n\nGiven the previous results, the main issue is theta's lower recovery. To address this, the model needs to better isolate the effect of theta. Perhaps by making the penalty term more distinct or adding another feature that theta interacts with, allowing better estimation.\n\nIn the new instructions, I should encourage exploring different functional forms for the fairness component, considering both under and over-compensation, and possibly interactions with trial context (trial_type). Also, suggest testing nonlinear transformations of the fairness term to improve parameter identifiability.\n</think>\n\n**Revised Instructions for Next Run:**\n\n1. **Expand Fairness Sensitivity:** Explore nonlinear transformations of the fairness penalty (e.g., quadratic terms, sigmoidal scaling) to better capture aversion thresholds. Consider asymmetric penalties for small vs. large deficits.\n\n2. **Contextual Modulation:** Investigate interactions between fairness sensitivity (theta) and trial context (e.g., `trial_type` where contributions are unequal). Allow parameters to adapt based on whether the participant contributed more/less than the opponent.\n\n3. **Relative Value Encoding:** Test utility formulations that compare the offer to both parties' contributions (e.g., ratio of `split_self` to opponent's `split_opp` normalized by contributions) rather than absolute fairness.\n\n4. **Dynamic Baseline:** Allow baseline acceptance tendency (beta) to vary with contextual factors like total combined earnings or contribution magnitude to capture scale-dependent risk attitudes.\n\n5. **Parameter Identifiability:** Prioritize models where parameters have distinct mechanistic roles (e.g., separate terms for monetary utility vs. social comparisons) to reduce covariance. Consider piecewise functions for different fairness regimes.\n\n6. **Alternative Utility Forms:** Experiment with prospect theory elements (e.g., reference-dependent utility where 3*token_self acts as a reference point) or loss aversion framing.\n\n**Key Focus:** Break the linear penalty paradigm while maintaining \u22643 parameters. Target theta recovery improvement through clearer separation from baseline tendencies.",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: <think>\nOkay, let's look at the previous model's results. The BIC was around 124.65, which is okay, but there's room for improvement. The accuracy was 84.8%, which is decent, but parameter recovery for theta was lower (0.766) compared to beta (0.890). That suggests that theta might not be as well-identified in the model. Maybe the model needs more features or a different structure to better capture the underlying mechanisms.\n\nThe previous model used a utility function with beta (baseline) and theta (penalty for unfairness). The penalty was based on the difference between 3*token_self and split_self. But maybe there's more to fairness. For example, participants might also care about the opponent's share relative to their contribution. Or maybe the penalty should be nonlinear, like a quadratic term when the offer is very unfair.\n\nAnother thought: the current model uses max(3*token_self - split_self, 0), which only penalizes when the offer is below the fair share. But perhaps participants also have a sensitivity to advantageous inequity, where they get more than their fair share. Although, in the responder role, they might not reject such offers, but including that could help in parameter recovery if some participants are sensitive to overcompensation.\n\nAlternatively, the model could include a term for the ratio of split_self to the fair share instead of the absolute difference. For example, (split_self / (3*token_self)) - 1. This might capture proportional deviations, which could be more psychologically relevant.\n\nAlso, the current model has two parameters. Maybe adding a third parameter, like a sensitivity to the opponent's overcompensation, but that could increase covariance. Alternatively, using a nonlinear transformation of the existing terms, like a sigmoid for the penalty, might help without adding parameters.\n\nThe parameter recovery for theta was lower, so perhaps the model needs to better differentiate the effects of theta. Maybe introducing an interaction term between split_self and the fairness penalty. Or scaling the penalty by some other variable, like the total combined earnings.\n\nAnother angle: the baseline beta might not account for individual differences in response bias. Maybe including a trial-level random effect or a hierarchical structure, but that's complex. Alternatively, adding another fixed effect, like trial_type (whether contributions are equal, opponent more, etc.), could capture context effects.\n\nWait, the trial_type variable indicates if contributions were equal, opponent more, or participant more. Including trial_type in the model might help. For example, participants might be more sensitive to unfairness when they contributed more (trial_type=3). So an interaction between trial_type and the fairness penalty could improve the model.\n\nBut the instructions say to use the variables appropriately but not necessarily all. So maybe trial_type can be a moderator for theta. For example, theta could vary based on trial_type, allowing different sensitivity in different contexts. That would add parameters, but maybe improve recovery.\n\nAlternatively, the model could use the difference between split_self and split_opp, but given that split_opp is determined once split_self is set, that might not add new info. But if participants compare their split to the opponent's, that could be a factor. However, split_opp is the opponent's proposed share, which might not be directly comparable.\n\nAnother idea: the model could consider the proportion of the total combined earnings that split_self represents. For example, split_perc_self. But the current model uses absolute amounts. Maybe participants think in percentages rather than absolute values. So the utility could be based on split_perc_self compared to the fair percentage (token_self / (token_self + token_opp) * 100). But that's similar to the current approach, just in percentage terms.\n\nWait, the current model uses 3*token_self as the fair share, which is equivalent to (token_self / (token_self + token_opp)) * combined_earning, since combined_earning is 3*(token_self + token_opp). So the model already captures the fair share proportionally. But maybe participants have nonlinear utility for money, so log(split_self) instead of split_self. That could be tested.\n\nAlternatively, the model could include a term for the magnitude of the combined earnings. For example, higher combined earnings might make participants more or less sensitive to unfairness. But that would require an interaction term between combined_earning and the fairness penalty, adding another parameter.\n\nIn summary, possible improvements:\n\n1. Introduce nonlinear terms for the fairness penalty (e.g., squared term).\n2. Include trial_type as a moderator for theta.\n3. Use percentage-based fairness instead of absolute.\n4. Consider advantageous inequity (overcompensation) with a separate parameter.\n5. Add a parameter for the curvature of the utility function (e.g., log utility).\n\nBut to keep the number of parameters low, maybe try a nonlinear penalty. For example, theta * (max(3*token_self - split_self, 0))^2. This would make the penalty increase quadratically with the deficit, which might better capture strong aversion to large unfairness.\n\nAlternatively, use a sigmoid function to cap the penalty, but that's more complex. A quadratic term adds one parameter (the exponent), but if we fix it to 2, no new parameters. However, the current model uses a linear penalty. A quadratic term might improve fit without adding parameters if the exponent is fixed.\n\nAnother approach: allow theta to vary with token_self. For example, participants with higher token_self (more contribution) might be more sensitive to unfairness. But that would require an interaction term, adding a parameter.\n\nAlternatively, the model could include a dynamic baseline where beta is influenced by token_self or other variables. But that complicates the model.\n\nGiven the previous results, the main issue is theta's lower recovery. To address this, the model needs to better isolate the effect of theta. Perhaps by making the penalty term more distinct or adding another feature that theta interacts with, allowing better estimation.\n\nIn the new instructions, I should encourage exploring different functional forms for the fairness component, considering both under and over-compensation, and possibly interactions with trial context (trial_type). Also, suggest testing nonlinear transformations of the fairness term to improve parameter identifiability.\n</think>\n\n**Revised Instructions for Next Run:**\n\n1. **Expand Fairness Sensitivity:** Explore nonlinear transformations of the fairness penalty (e.g., quadratic terms, sigmoidal scaling) to better capture aversion thresholds. Consider asymmetric penalties for small vs. large deficits.\n\n2. **Contextual Modulation:** Investigate interactions between fairness sensitivity (theta) and trial context (e.g., `trial_type` where contributions are unequal). Allow parameters to adapt based on whether the participant contributed more/less than the opponent.\n\n3. **Relative Value Encoding:** Test utility formulations that compare the offer to both parties' contributions (e.g., ratio of `split_self` to opponent's `split_opp` normalized by contributions) rather than absolute fairness.\n\n4. **Dynamic Baseline:** Allow baseline acceptance tendency (beta) to vary with contextual factors like total combined earnings or contribution magnitude to capture scale-dependent risk attitudes.\n\n5. **Parameter Identifiability:** Prioritize models where parameters have distinct mechanistic roles (e.g., separate terms for monetary utility vs. social comparisons) to reduce covariance. Consider piecewise functions for different fairness regimes.\n\n6. **Alternative Utility Forms:** Experiment with prospect theory elements (e.g., reference-dependent utility where 3*token_self acts as a reference point) or loss aversion framing.\n\n**Key Focus:** Break the linear penalty paradigm while maintaining \u22643 parameters. Target theta recovery improvement through clearer separation from baseline tendencies.\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal. The model needs to focus on responder behavior, so we'll filter the data where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). \n\nFirst, I need to think about what factors influence a responder's decision. From the variables given, the key ones seem to be the proposed share for the participant (split_self), the opponent's tokens (token_opp), the participant's own tokens (token_self), and the combined earnings. The fairness aspect is important here. The description mentions that two equal split_self values don't represent the same fairness because the combined earnings vary. So, the fairness should be relative to the contributions.\n\nThe utility model should compare the proposed split to what's considered fair. A fair split might be based on the participant's contribution. Since each token is worth \u00a33, the participant's fair share would be 3 * token_self. If the proposed split_self is less than this, the participant might perceive it as unfair. The difference between the fair share and the proposed share could be a penalty term in the utility function.\n\nSo, the utility could be modeled as the proposed split_self minus some penalty for unfairness. The penalty could be proportional to how much the proposed share is below the fair share. Let's say the penalty is a parameter (theta) multiplied by the difference between the fair share and the proposed share, but only if the proposed share is less than the fair share (to avoid rewarding over-fair offers). \n\nAdditionally, there might be a baseline tendency to accept or reject, captured by another parameter (beta). This could account for any general bias towards accepting or rejecting offers regardless of fairness.\n\nThe probability of accepting would then be a logistic function of the utility. If the utility is greater than 0, the probability is greater than 0.5.\n\nPutting this together, the utility U could be:\n\nU = beta + split_self - theta * max(3*token_self - split_self, 0)\n\nHere, beta is the baseline utility, split_self is the monetary gain, and the theta term penalizes offers that are below the fair share. The max function ensures we only penalize when the offer is below the fair share.\n\nNow, the parameters beta and theta need to be learnable. Beta can be any real number, but to keep it bounded, maybe between -10 and 10. Theta should be positive since it's a penalty, so between 0 and a large value like 20. These bounds are generous but finite.\n\nVariables in the model:\n- beta: baseline acceptance tendency (learnable)\n- theta: fairness sensitivity penalty (learnable)\n- split_self: from data\n- token_self: from data\n\nThe utility U is calculated, and then the probability of accept=1 is 1 / (1 + exp(-U)).\n\nThis model uses only the necessary variables and focuses on the responder's perspective. It should be able to predict 'accept' based on the proposed share and fairness considerations. The parameters are kept simple for better recovery, and the model structure aligns with the task description.\n</think>\n\n<MODEL>\nU = beta + split_self - theta * max(3*token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline acceptance tendency (positive values increase acceptance likelihood)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"theta\": {\n      \"description\": \"Fairness sensitivity penalty per \u00a31 deficit from fair share (3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed \u00a3 amount for participant (from data)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (from data)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance decisions depend on monetary gain (split_self) and a fairness penalty. The penalty activates when offers fall below the participant's contribution value (3*token_self), scaled by a learnable sensitivity parameter (theta). Baseline acceptance tendency (beta) captures context-independent preferences. Logistic transformation converts utility to acceptance probability.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['beta', 'theta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = beta + split_self - theta * max(3*token_self - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline acceptance tendency (positive values increase acceptance likelihood)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"theta\": {\n      \"description\": \"Fairness sensitivity penalty per \\u00a31 deficit from fair share (3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed \\u00a3 amount for participant (from data)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (from data)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, beta=0, theta=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = beta + split_self - theta * max(3*token_self - split_self, 0)\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 1, \"group\": \"A\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 7, \"token_self\": 7, \"combined_earning\": 14, \"split_opp\": 3.5, \"split_self\": 3.5, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 3.5, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 3.5}, {\"ID\": 2, \"group\": \"A\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 7, \"token_self\": 7, \"combined_earning\": 14, \"split_opp\": 2.8, \"split_self\": 4.2, \"splitperc_opp\": 40, \"splitperc_self\": 60, \"accept\": 1.0, \"accepted_amount\": 4.2, \"accepted_perc\": 60.0, \"proposed_perc\": 60.0, \"proposed_amount\": 4.2}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1743523943.43907
    }
  ],
  "learnable_parameters": {
    "beta": {
      "description": "Baseline acceptance tendency (positive values increase acceptance likelihood)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "theta": {
      "description": "Fairness sensitivity penalty per \u00a31 deficit from fair share (3*token_self)",
      "range": {
        "min": 0,
        "max": 20,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = beta + split_self - theta * max(3*token_self - split_self, 0)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the offer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "beta": {
      "description": "Baseline acceptance tendency (positive values increase acceptance likelihood)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "theta": {
      "description": "Fairness sensitivity penalty per \u00a31 deficit from fair share (3*token_self)",
      "range": {
        "min": 0,
        "max": 20,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed \u00a3 amount for participant (from data)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Participant's token contribution (from data)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model where acceptance decisions depend on monetary gain (split_self) and a fairness penalty. The penalty activates when offers fall below the participant's contribution value (3*token_self), scaled by a learnable sensitivity parameter (theta). Baseline acceptance tendency (beta) captures context-independent preferences. Logistic transformation converts utility to acceptance probability.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal. The model needs to focus on responder behavior, so we'll filter the data where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). \n\nFirst, I need to think about what factors influence a responder's decision. From the variables given, the key ones seem to be the proposed share for the participant (split_self), the opponent's tokens (token_opp), the participant's own tokens (token_self), and the combined earnings. The fairness aspect is important here. The description mentions that two equal split_self values don't represent the same fairness because the combined earnings vary. So, the fairness should be relative to the contributions.\n\nThe utility model should compare the proposed split to what's considered fair. A fair split might be based on the participant's contribution. Since each token is worth \u00a33, the participant's fair share would be 3 * token_self. If the proposed split_self is less than this, the participant might perceive it as unfair. The difference between the fair share and the proposed share could be a penalty term in the utility function.\n\nSo, the utility could be modeled as the proposed split_self minus some penalty for unfairness. The penalty could be proportional to how much the proposed share is below the fair share. Let's say the penalty is a parameter (theta) multiplied by the difference between the fair share and the proposed share, but only if the proposed share is less than the fair share (to avoid rewarding over-fair offers). \n\nAdditionally, there might be a baseline tendency to accept or reject, captured by another parameter (beta). This could account for any general bias towards accepting or rejecting offers regardless of fairness.\n\nThe probability of accepting would then be a logistic function of the utility. If the utility is greater than 0, the probability is greater than 0.5.\n\nPutting this together, the utility U could be:\n\nU = beta + split_self - theta * max(3*token_self - split_self, 0)\n\nHere, beta is the baseline utility, split_self is the monetary gain, and the theta term penalizes offers that are below the fair share. The max function ensures we only penalize when the offer is below the fair share.\n\nNow, the parameters beta and theta need to be learnable. Beta can be any real number, but to keep it bounded, maybe between -10 and 10. Theta should be positive since it's a penalty, so between 0 and a large value like 20. These bounds are generous but finite.\n\nVariables in the model:\n- beta: baseline acceptance tendency (learnable)\n- theta: fairness sensitivity penalty (learnable)\n- split_self: from data\n- token_self: from data\n\nThe utility U is calculated, and then the probability of accept=1 is 1 / (1 + exp(-U)).\n\nThis model uses only the necessary variables and focuses on the responder's perspective. It should be able to predict 'accept' based on the proposed share and fairness considerations. The parameters are kept simple for better recovery, and the model structure aligns with the task description.\n</think>\n\n<MODEL>\nU = beta + split_self - theta * max(3*token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Baseline acceptance tendency (positive values increase acceptance likelihood)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"theta\": {\n      \"description\": \"Fairness sensitivity penalty per \u00a31 deficit from fair share (3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed \u00a3 amount for participant (from data)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (from data)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance decisions depend on monetary gain (split_self) and a fairness penalty. The penalty activates when offers fall below the participant's contribution value (3*token_self), scaled by a learnable sensitivity parameter (theta). Baseline acceptance tendency (beta) captures context-independent preferences. Logistic transformation converts utility to acceptance probability.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = beta + split_self - theta * max(3*token_self - split_self, 0)\nSummary: A utility model where acceptance decisions depend on monetary gain (split_self) and a fairness penalty. The penalty activates when offers fall below the participant's contribution value (3*token_self), scaled by a learnable sensitivity parameter (theta). Baseline acceptance tendency (beta) captures context-independent preferences. Logistic transformation converts utility to acceptance probability.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, beta=0, theta=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = beta + split_self - theta * max(3*token_self - split_self, 0)\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8437118437118436,
  "fitting_results": [
    {
      "beta": -5.249604056905411,
      "theta": 14.784712009580321,
      "success": true,
      "log_likelihood": -38.026956504968126,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.2375919496625727,
      "theta": 1.1395897061732498,
      "success": true,
      "log_likelihood": -77.3199370491668,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.2614454091204508,
      "theta": 1.3472242850957812,
      "success": true,
      "log_likelihood": -34.43461727829765,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.2361386511171846,
      "theta": 0.7961546134990513,
      "success": true,
      "log_likelihood": -39.85783187419139,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.988311080840096,
      "theta": 2.3663211491428826,
      "success": true,
      "log_likelihood": -39.91406199022886,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.7150645695851141,
      "theta": 0.7952051925033089,
      "success": true,
      "log_likelihood": -41.99593688830596,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -276.31021115688566,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.728114148123814,
      "theta": 1.0502765452284688,
      "success": true,
      "log_likelihood": -49.38210328638213,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.2981585477131326,
      "theta": 1.4868042205078227,
      "success": true,
      "log_likelihood": -0.5613873349323139,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -5.249201687202814,
      "theta": 20.0,
      "success": true,
      "log_likelihood": -61.05280746182426,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.48136389040483046,
      "theta": 2.334358746474918,
      "success": true,
      "log_likelihood": -23.058840556315722,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -115.12925464660226,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -92.10340371656177,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.887668617626062,
      "theta": 3.3371491219064002,
      "success": true,
      "log_likelihood": -76.07178993456489,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -115.12925464660225,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.591681764370278,
      "theta": 0.49556297895979073,
      "success": true,
      "log_likelihood": -72.55214910399319,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.668764951835148,
      "theta": 0.8781338575177243,
      "success": true,
      "log_likelihood": -75.6312711487516,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.3966674687551877,
      "theta": 2.5029261894671753,
      "success": true,
      "log_likelihood": -62.37521851124371,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.8698643883781993,
      "theta": 0.36424246399156557,
      "success": true,
      "log_likelihood": -63.21847270683157,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.5874358317890926,
      "theta": 0.6772090046020134,
      "success": true,
      "log_likelihood": -43.051452673385214,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.8441104506022143,
      "theta": 2.483975456057201,
      "success": true,
      "log_likelihood": -11.283607180582441,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -92.10340371656179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.3289513552591616,
      "theta": 0.2769391958475338,
      "success": true,
      "log_likelihood": -83.60490335072339,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -5.2508410746182665,
      "theta": 19.38115440653872,
      "success": true,
      "log_likelihood": -38.026956534536815,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.673922678485329,
      "theta": 0.39966316446137307,
      "success": true,
      "log_likelihood": -99.69045192093593,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.0143411793630004,
      "theta": 0.8765275157916825,
      "success": true,
      "log_likelihood": -67.98247891315157,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -299.3360620869261,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -253.2843602268452,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.969104989915671,
      "theta": 5.641450358586318,
      "success": true,
      "log_likelihood": -116.51570636623462,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.086758387242225,
      "theta": 1.7573502876452187,
      "success": true,
      "log_likelihood": -123.51838408066848,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652135,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.9157458318073662,
      "theta": 0.7567596603593946,
      "success": true,
      "log_likelihood": -121.84135131316509,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.712301107375184,
      "theta": 10.805742391771975,
      "success": true,
      "log_likelihood": -46.05174029466773,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -23.02585092644049,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.4602120274098986,
      "theta": 3.5339546363634957,
      "success": true,
      "log_likelihood": -204.15479164957355,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.1607168403898864,
      "theta": 0.6373003148051919,
      "success": true,
      "log_likelihood": -54.65195533911018,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.191771627899165,
      "theta": 0.10386832845440531,
      "success": true,
      "log_likelihood": -53.26839464589796,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.3836280973489676,
      "theta": 1.0084267843749786,
      "success": true,
      "log_likelihood": -76.3870436826194,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.7043386098961197,
      "theta": 0.7220980622905555,
      "success": true,
      "log_likelihood": -74.62324993210493,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.1211308429601488,
      "theta": 2.3645588055097386,
      "success": true,
      "log_likelihood": -59.89914339131235,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.8240287372609707,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -137.27264023092653,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.07583584886773,
      "theta": 0.17421344959319957,
      "success": true,
      "log_likelihood": -85.31267152522858,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -5.250467265275678,
      "theta": 4.6188255099400894,
      "success": true,
      "log_likelihood": -84.07865836786007,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.31734013440425,
      "theta": 0.27308435965431704,
      "success": true,
      "log_likelihood": -65.500691925756,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.7903300236347874,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -89.94570008782577,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.258105462591376,
      "theta": 14.01132256423188,
      "success": true,
      "log_likelihood": -162.56688311392062,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.202327286117187,
      "theta": 1.6742128702456494,
      "success": true,
      "log_likelihood": -5.757746826614555,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.9913281470601817,
      "theta": 5.467934030110974,
      "success": true,
      "log_likelihood": -91.23846107081957,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -92.1034037165618,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -5.251170094567966,
      "theta": 10.190679963728574,
      "success": true,
      "log_likelihood": -61.05280750090177,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 5.59483103878144e-16,
      "success": true,
      "log_likelihood": -161.18095650668326,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.7826010367133494,
      "theta": 14.941999026060172,
      "success": true,
      "log_likelihood": -392.8257601531063,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.06960251710457,
      "theta": 2.334560904849875,
      "success": true,
      "log_likelihood": -57.97459878236056,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -5.2518048751149164,
      "theta": 10.462376683909385,
      "success": true,
      "log_likelihood": -38.026956674736,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.3989804755713116,
      "theta": 0.5210664341267192,
      "success": true,
      "log_likelihood": -83.76491484694779,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.3768327462812573,
      "theta": 4.958824888100208,
      "success": true,
      "log_likelihood": -67.82517288319109,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.2232903353422786,
      "theta": 1.016295746397236,
      "success": true,
      "log_likelihood": -50.8032168647235,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652131,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -115.1292546466023,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.489596366008164,
      "theta": 0.21280038288247785,
      "success": true,
      "log_likelihood": -34.90319111098365,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652137,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.4163635204810289,
      "theta": 17.87605238305585,
      "success": true,
      "log_likelihood": 3.5999005996577364e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -4.921654674380677,
      "theta": 0.18121100472755103,
      "success": true,
      "log_likelihood": -52.629575898148275,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.5360016066067343,
      "theta": 0.9467752053940572,
      "success": true,
      "log_likelihood": -61.20218691807809,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -184.20680743672386,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.9999885406784395,
      "theta": 4.0000022947139575,
      "success": true,
      "log_likelihood": -186.97939647597644,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.37199048646581656,
      "theta": 0.8325959192537808,
      "success": true,
      "log_likelihood": -65.99529021242913,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -46.051701856480875,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -7.08802678110353,
      "theta": 20.0,
      "success": true,
      "log_likelihood": -23.025869757312165,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.700066038454023,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -117.53831472884914,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -5.250149486823773,
      "theta": 3.7842902381567,
      "success": true,
      "log_likelihood": -38.02695649717208,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.0413979060411145,
      "theta": 0.3094160823867,
      "success": true,
      "log_likelihood": -158.71901016218328,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 7.403642415867157e-16,
      "success": true,
      "log_likelihood": -69.07755278652131,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -7.593087237782106,
      "theta": 18.200828307537538,
      "success": true,
      "log_likelihood": -69.0775536830997,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.2342878316554713,
      "theta": 0.5316929035602884,
      "success": true,
      "log_likelihood": -5.534258360911874,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 9.846499929549845,
      "success": true,
      "log_likelihood": -277.7679412166552,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.044621423142621,
      "theta": 20.0,
      "success": true,
      "log_likelihood": -46.05170203232599,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -1.6644755844846744,
      "theta": 0.2838744906827774,
      "success": true,
      "log_likelihood": -106.25584154372052,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.3292084718142617,
      "theta": 4.366154234657152,
      "success": true,
      "log_likelihood": -77.70317940743278,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "theta": 0.0,
      "success": true,
      "log_likelihood": -46.05170185648088,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.6680236481312947,
      "theta": 0.3704524968928417,
      "success": true,
      "log_likelihood": -8.902141326364248,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -2.2796903975572116,
      "theta": 0.037006080565761795,
      "success": true,
      "log_likelihood": -131.77707673262583,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "beta": {
        "r": 0.9062026298856498,
        "p": 2.0543754672547643e-38
      },
      "theta": {
        "r": 0.5070540086205421,
        "p": 7.30935450672932e-08
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 164.38675651583307,
  "bic_results": {
    "average_bic": 164.38675651583307,
    "individual_bics": [
      83.22095088684847,
      161.8069119752458,
      76.03627243350752,
      86.882701625295,
      86.99516185736994,
      91.15891165352414,
      7.167037869712219,
      559.7874601906835,
      7.167037869712219,
      105.93124444967648,
      8.289812546776847,
      129.27265280056073,
      53.28471898954366,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      237.42554717011674,
      191.37384531003576,
      7.167037869712219,
      159.310617746042,
      237.42554717011672,
      152.2713360848986,
      158.4295801744154,
      131.91747489939965,
      133.60398329057537,
      93.26994322368265,
      29.734252238077104,
      191.3738453100358,
      174.376844578359,
      7.167037869712219,
      83.22095094598585,
      206.54794171878407,
      143.13199570321535,
      7.167037869712219,
      605.8391620507645,
      513.7357583306026,
      240.19845060938147,
      254.20380603824918,
      145.32214344995492,
      250.8497405032424,
      99.27051846624768,
      53.2187397297932,
      415.4766211760593,
      116.47094855513258,
      113.70382716870814,
      159.94112524215103,
      156.41353774112207,
      126.96532465953692,
      281.7123183387653,
      177.79238092736938,
      175.32435461263236,
      138.16842172842422,
      187.05843805256376,
      332.30080410475347,
      18.68253153014133,
      189.64396001855135,
      191.37384531003582,
      129.27265287871577,
      329.52895089027874,
      792.8185581831248,
      123.11623544163334,
      83.22095122638422,
      174.6968675708078,
      142.8173836432944,
      108.77347160635922,
      145.32214344995484,
      237.42554717011683,
      76.97342009887952,
      145.32214344995495,
      7.167037869712419,
      112.42618967320877,
      129.57141171306841,
      375.58065275035995,
      381.1258308288651,
      139.15761830177047,
      99.27044158987397,
      53.21877739153655,
      242.2436673346105,
      83.22095087125638,
      324.6050582012788,
      145.32214344995484,
      145.3221452431116,
      18.235554598735966,
      562.7029203102226,
      99.2704419415642,
      219.67872096435326,
      162.5733966917778,
      99.27044158987398,
      24.971320529640714,
      270.7211913421639
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- beta: r = 0.906\n- theta: r = 0.507",
  "previous_models": [
    "Specification: U = beta + split_self - theta * max(3*token_self - split_self, 0)\nSummary: A utility model where acceptance decisions depend on monetary gain (split_self) and a fairness penalty. The penalty activates when offers fall below the participant's contribution value (3*token_self), scaled by a learnable sensitivity parameter (theta). Baseline acceptance tendency (beta) captures context-independent preferences. Logistic transformation converts utility to acceptance probability.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 164.38675651583307\nOverall Accuracy: 0.8437\n\nParameter Recovery:\n- beta: r = 0.906\n- theta: r = 0.507"
  ]
}