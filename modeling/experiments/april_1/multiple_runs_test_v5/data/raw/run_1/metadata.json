{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility function should be such that a utility less than 0 corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The main target is 'accept' (binary 0/1). The model should predict this based on the utility of accepting the proposal. The variables provided include token_self, token_opp, combined_earning, split_self, split_opp, etc. Since the task involves fairness considerations, the model should account for how the participant perceives the fairness of the split relative to their own contribution and the opponent's.\n\nThe participants are responders in some trials (trial_role=1). So, the model should only consider those cases. The trial_type indicates whether contributions are equal, opponent more, or participant more. This might influence fairness perceptions.\n\nThe combined_earning is the total money, which is 3*(token_self + token_opp). The split_self and split_opp are the proposed shares. The split percentages (splitperc_self and splitperc_opp) might be important. However, the note says that two equal split_self values don't represent the same fairness because the combined earning varies. So, the absolute amounts and the relative contributions matter.\n\nA common approach in such social decision-making tasks is to model utility based on fairness considerations, often using inequity aversion models. For example, Fehr and Schmidt's model considers both advantageous and disadvantageous inequity. However, since the contributions (tokens) vary, the model should account for the participant's own contribution versus the opponent's.\n\nLet me think about the utility function. The utility of accepting could depend on the monetary gain (split_self) compared to some reference point, perhaps the fair share based on contributions. The fair share could be calculated as (token_self / (token_self + token_opp)) * combined_earning. If the proposed split_self is less than this fair share, the participant might feel treated unfairly, leading to a lower utility.\n\nSo, the utility could be a function of the difference between the proposed split and the fair share. Additionally, there might be a parameter that scales this difference, representing sensitivity to unfairness. Also, there could be a baseline acceptance tendency.\n\nAnother consideration is the absolute amount of split_self. Even if the split is unfair, a higher absolute amount might still lead to acceptance. So, the utility might combine both the absolute offer and the deviation from the fair share.\n\nPutting this together, the utility (U) could be modeled as:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - fair_share)\n\nHere, \u03b1 represents the sensitivity to the absolute offer, and \u03b2 represents the sensitivity to the deviation from the fair share. If \u03b2 is positive, participants are more likely to accept when the offer is above the fair share. However, if \u03b2 is negative, it might indicate aversion to unfairness (if the offer is below fair share, utility decreases).\n\nAlternatively, the fair share could be a reference point, and the utility could be split_self minus some function of the inequity. For example:\n\nfair_share = (token_self / (token_self + token_opp)) * combined_earning\ninequity = split_self - fair_share\nU = \u03b1 * split_self + \u03b2 * inequity\n\nBut maybe the inequity term should be nonlinear. For example, people might be more sensitive to negative inequity (getting less than fair) than positive. So, perhaps separate parameters for positive and negative deviations. However, adding more parameters could complicate parameter recovery. Since the instruction prefers fewer parameters, maybe a single parameter for inequity.\n\nWait, the note says that two equal split_self values don't represent the same fairness because combined_earning varies. So, the same split_self in different trials (with different combined_earning) would have different fairness. Therefore, the model must account for the fair share based on contributions.\n\nSo, the fair share is (token_self / total_tokens) * combined_earning. Since combined_earning is 3*(token_self + token_opp), the fair_share in pounds is 3 * token_self. Because combined_earning = 3*(token_self + token_opp), and fair_share would be (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3 * token_self. Wait, that's interesting. So, the fair share in pounds is exactly 3 times the participant's own tokens. Because each token is worth \u00a33. So, if the participant contributed token_self tokens, their fair share is 3 * token_self. Similarly, the opponent's fair share is 3 * token_opp.\n\nBut the split_self is the proposed amount for the participant. So, the difference between split_self and 3*token_self is the deviation from the fair share. So, the utility could be based on this difference.\n\nBut maybe participants also consider the opponent's share. For example, if the opponent is getting more than their fair share, the participant might feel envious, or if they're getting more, they might feel guilty. But again, adding parameters for that might complicate things.\n\nAlternatively, the utility could be:\n\nU = \u03b8 * split_self + \u03b3 * (split_self - 3 * token_self)\n\nHere, \u03b8 is the weight on the absolute offer, and \u03b3 is the weight on the deviation from the fair share. If \u03b3 is positive, participants prefer offers above their fair share. If \u03b3 is negative, they dislike deviations below the fair share.\n\nBut perhaps the deviation should be normalized. For example, (split_self - fair_share) / fair_share, which would represent proportional deviation. But that might complicate the model. Alternatively, using the raw difference.\n\nAnother angle: the participant's utility could depend on both their own payoff and the difference between their payoff and the opponent's. But since the opponent's contribution (token_opp) and their proposed split (split_opp) are known, maybe the model should compare split_self and split_opp relative to their respective fair shares.\n\nWait, the split_opp is the opponent's proposed share. So, in the responder role, the participant is evaluating the proposer's (opponent's) offer. The proposer (opponent) offers split_self to the participant and keeps split_opp. The total combined_earning is split_self + split_opp. But combined_earning is fixed as 3*(token_self + token_opp). So, split_self + split_opp = 3*(token_self + token_opp). Therefore, split_opp = 3*(token_self + token_opp) - split_self.\n\nBut maybe the participant cares about how their split compares to the opponent's split. For example, if the opponent is keeping more than their fair share, the participant might reject even if their own split is acceptable. But this adds complexity. However, given the task's emphasis on fairness, this might be necessary.\n\nAlternatively, the participant's utility could be a function of their own split and the difference between their split and the opponent's split. For example:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - split_opp)\n\nBut split_opp is 3*(token_self + token_opp) - split_self. So, split_self - split_opp = 2*split_self - 3*(token_self + token_opp). Hmm, not sure if that's the right way.\n\nAlternatively, the participant might compare their split to the opponent's split. If split_self is less than split_opp, that's disadvantageous inequity. If it's more, advantageous. So, using Fehr-Schmidt model:\n\nU = split_self - \u03b4 * max(split_opp - split_self, 0) - \u03c3 * max(split_self - split_opp, 0)\n\nHere, \u03b4 is aversion to disadvantageous inequity, \u03c3 to advantageous. But this introduces two parameters. However, the problem states that parameter recovery is important, so maybe two parameters are manageable. But the instruction says to have a slight bias towards fewer parameters. So perhaps combine them into one parameter if possible.\n\nAlternatively, use a single parameter for inequity aversion, considering the difference between the two splits. For example:\n\nU = split_self + \u03bb * (split_self - split_opp)\n\nBut this assumes that participants care linearly about the difference. If \u03bb is positive, they prefer getting more than the opponent; if negative, they dislike inequality.\n\nBut maybe the fair share is a better reference. Let's go back to the fair share idea. The fair share for the participant is 3*token_self, and for the opponent is 3*token_opp. So, the fair split would be split_self = 3*token_self and split_opp = 3*token_opp. Any deviation from this could be considered unfair.\n\nSo, the participant's utility could depend on how much their split_self deviates from their fair share, and how much the opponent's split_opp deviates from their fair share. For example:\n\nU = split_self - \u03b1 * (split_self - 3*token_self) - \u03b2 * (split_opp - 3*token_opp)\n\nBut split_opp is determined once split_self is known, since split_opp = combined_earning - split_self. So, maybe this can be simplified.\n\nAlternatively, the participant's utility could be:\n\nU = split_self + \u03b3 * (split_self - 3*token_self) + \u03b4 * (split_opp - 3*token_opp)\n\nBut since split_opp = combined_earning - split_self, and combined_earning is 3*(token_self + token_opp), then split_opp - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self.\n\nSo, substituting:\n\nU = split_self + \u03b3*(split_self - 3*token_self) + \u03b4*(3*token_self - split_self)\n\nThis simplifies to:\n\nU = split_self + (\u03b3 - \u03b4)*(split_self - 3*token_self)\n\nWhich is equivalent to:\n\nU = split_self + \u03b8*(split_self - 3*token_self), where \u03b8 = \u03b3 - \u03b4.\n\nSo, this reduces to a single parameter for the deviation from the fair share. That's interesting. So, the utility is split_self plus a parameter times the difference between split_self and the fair share. If \u03b8 is positive, participants prefer offers above their fair share; if negative, they are averse to deviations (either direction? Not sure). Wait, if \u03b8 is positive, then when split_self is higher than fair share, utility increases. If \u03b8 is negative, then higher split_self than fair share would decrease utility, which doesn't make sense. Hmm, maybe this approach isn't capturing the right intuition.\n\nAlternatively, perhaps the participant's utility is influenced by both their own gain and the fairness relative to their contribution. The fairness could be the ratio of split_self to their fair share. For example:\n\nU = \u03b1 * split_self + \u03b2 * (split_self / (3 * token_self))\n\nBut if token_self is zero, this could be problematic. Alternatively, using the difference:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - 3 * token_self)\n\nHere, \u03b1 is the marginal utility of money, and \u03b2 is the sensitivity to fairness (difference from fair share). If \u03b2 is positive, getting more than fair increases utility; if negative, deviations from fair (either above or below) decrease utility. Wait, no\u2014if \u03b2 is positive, then split_self being higher than fair increases utility. If \u03b2 is negative, then higher split_self than fair would decrease utility, which might not make sense. Maybe participants dislike being overcompensated? Unlikely. More likely, they dislike being undercompensated. So perhaps the model should have a term that only penalizes when split_self is below fair.\n\nThis suggests a piecewise function. For example:\n\nIf split_self < 3*token_self:\n    U = split_self - \u03bb*(3*token_self - split_self)\nElse:\n    U = split_self\n\nHere, \u03bb is the aversion to being below the fair share. Alternatively, using a linear term with a parameter that's only active when below. But piecewise functions can complicate parameter estimation. Alternatively, model it as:\n\nU = split_self - \u03bb * max(3*token_self - split_self, 0)\n\nThis way, the participant only incurs a disutility when their split is below the fair share. The parameter \u03bb represents how much they dislike being undercompensated. This is similar to the Fehr-Schmidt model but focused on the fair share based on contribution.\n\nAlternatively, combining both absolute and relative terms. For example:\n\nU = \u03b1 * split_self - \u03b2 * max( (3*token_self - split_self)/ (3*token_self), 0 )\n\nHere, the relative shortfall is considered. Dividing by 3*token_self makes it a proportional difference. But if token_self is zero, this is undefined. However, in the dataset, token_self is an integer (int64), so maybe it's always positive? The problem states that in the pretask, participants find a number of tokens, so token_self is at least 1? Not sure, but assuming token_self is positive.\n\nSo, the utility function could be:\n\nU = \u03b1 * split_self - \u03b2 * max( (fair_share - split_self) / fair_share, 0 )\n\nWhere fair_share = 3 * token_self.\n\nThis way, \u03b2 scales the proportional deficit from the fair share. If the split is below fair, the participant experiences a disutility proportional to the percentage shortfall. \u03b1 represents the utility per pound received.\n\nBut this introduces two parameters: \u03b1 and \u03b2. However, since the model needs to predict utility which is then passed through a logistic function to get the probability of acceptance, perhaps \u03b1 can be set to 1 (assuming linear utility in money), and only \u03b2 is a learnable parameter. But the problem states that the model should have learnable parameters with finite bounds.\n\nAlternatively, include both \u03b1 and \u03b2 as learnable parameters. But the instruction says to have a slight bias towards fewer parameters. Let's see.\n\nIf we set \u03b1=1 (assuming utility is linear in money), then the model would have one parameter \u03b2. The utility becomes:\n\nU = split_self - \u03b2 * max( (fair_share - split_self) / fair_share, 0 )\n\nBut this is equivalent to:\n\nIf split_self >= fair_share:\n    U = split_self\nElse:\n    U = split_self - \u03b2 * (fair_share - split_self)/fair_share\n\nBut scaling \u03b2 here might be tricky. Alternatively, without normalizing:\n\nU = split_self - \u03b2 * max(fair_share - split_self, 0 )\n\nHere, \u03b2 represents the disutility per pound below the fair share. So, for each pound below fair, utility decreases by \u03b2. This is simpler and has one parameter.\n\nBut then, the utility is split_self minus \u03b2 times the deficit. If the deficit is large, the utility could become negative even if split_self is positive. For example, if split_self is \u00a35, fair_share is \u00a310, and \u03b2 is 2, then U = 5 - 2*5 = 5 -10 = -5. The probability of acceptance would be 1/(1 + exp(-U)), but wait, the logistic function maps U to [0,1]. Wait, no: the standard logistic function is 1/(1 + exp(-U)), so if U is negative, the probability is less than 0.5. The problem states that \"less than 0 is less than 50% likely\", which aligns with this.\n\nSo, the model could be:\n\nU = split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nHere, \u03b2 is a learnable parameter representing the sensitivity to being below the fair share. If \u03b2 is high, even small deficits lead to lower utility and lower probability of acceptance.\n\nBut maybe participants also care about how their split compares to the opponent's split in absolute terms. For example, if the opponent is getting a lot more, even if the participant's split is above their fair share, they might reject. But adding that would require another parameter.\n\nAlternatively, the model could include both the deficit from fair share and the ratio of the splits. But again, more parameters.\n\nGiven the need for parameter recovery and simplicity, perhaps starting with one parameter is better. Let's proceed with the model:\n\nU = split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nThis model has one learnable parameter, \u03b2, which must be non-negative (since being below fair share should decrease utility). The utility is split_self minus \u03b2 times any deficit below the fair share. If the offer is equal to or above the fair share, U is just split_self. If below, U decreases.\n\nThe probability of acceptance is logistic(U) = 1 / (1 + exp(-U)). So when U > 0, probability > 0.5; when U < 0, probability < 0.5.\n\nBut wait, the utility here is in pounds. If split_self is \u00a310, and \u03b2 is 2, then a \u00a35 deficit (fair share \u00a315) would give U = 10 - 2*5 = 0. The probability would be 0.5. If split_self is \u00a314, fair share \u00a315: U =14 -2*1=12 \u2192 probability ~1. So, this might not capture cases where the offer is close to fair but still rejected.\n\nAlternatively, perhaps the model should include a term that compares the proposed split to the fair share proportionally. For example:\n\nU = split_self - \u03b2 * ( (3*token_self - split_self) / (3*token_self) )\n\nThis way, the deficit is normalized by the fair share. So, if the fair share is \u00a315 and the offer is \u00a310, the deficit is \u00a35, which is 1/3 of the fair share. \u03b2 would then scale this proportional deficit. This might make \u03b2 more interpretable as a proportional sensitivity.\n\nBut this introduces division by token_self, which could be zero. However, in the dataset, token_self is the number of tokens found by the participant, which is likely at least 1 (since they had to find some tokens in the pretask). So, division by zero is not an issue.\n\nSo, the model becomes:\n\nU = split_self - \u03b2 * max( (3*token_self - split_self) / (3*token_self), 0 )\n\nThis can be simplified to:\n\nU = split_self - \u03b2 * max( (fair_share - split_self) / fair_share, 0 )\n\nWhich is the proportional deficit below fair share.\n\nAlternatively, combining both absolute and proportional terms. But again, more parameters.\n\nAnother consideration: the participant's own contribution (token_self) might influence their sense of entitlement. For example, if they contributed more, they might expect a higher split. The model already accounts for this via the fair share.\n\nBut perhaps the model should also include a term that reflects the opponent's contribution. For example, if the opponent contributed less but is offering a split that's unfair, the participant might be more likely to reject. However, this would require additional parameters.\n\nAlternatively, the trial_type variable indicates whether the contribution was equal, opponent more, or participant more. This could interact with the fairness evaluation. For example, in trials where the participant contributed more (trial_type=3), they might be more sensitive to receiving less than their fair share. But including trial_type would require additional parameters, perhaps as interaction terms with \u03b2. However, trial_type is a categorical variable with three levels, which complicates the model. Since the instruction says to use the variables appropriately but not necessarily all, maybe trial_type can be omitted unless necessary.\n\nGiven the dataset's variables, the key ones for the responder's decision are split_self, token_self, and token_opp (since combined_earning is derived from tokens). The model should use these to compute the fair share and the proposed split.\n\nPutting it all together, the model with one parameter \u03b2 seems manageable. The utility is based on the proposed split and the proportional deficit below the fair share. The parameter \u03b2 determines how sensitive the participant is to this deficit.\n\nNow, considering the parameter bounds. \u03b2 should be non-negative because a higher \u03b2 means more penalty for being below fair. So, the range for \u03b2 could be [0, some upper bound]. The upper bound can be generous, say 10, which would mean that even a small proportional deficit (e.g., 10% below fair) would lead to a significant decrease in utility. But the exact value can be determined during estimation.\n\nAnother possible parameter is the baseline acceptance tendency. For example, some participants might have a general tendency to accept or reject offers regardless of the terms. This could be modeled with an intercept term \u03b1. So:\n\nU = \u03b1 + split_self - \u03b2 * max( (fair_share - split_self)/fair_share, 0 )\n\nHere, \u03b1 represents the baseline utility. If \u03b1 is positive, the participant is more likely to accept even if the offer is slightly unfair. However, adding \u03b1 introduces another parameter. But the problem allows for a few parameters as long as they are manageable. Since parameter recovery is a priority, two parameters (\u03b1 and \u03b2) might be acceptable.\n\nBut let's think: if \u03b1 is included, it could capture the general tendency to accept, while \u03b2 captures the sensitivity to unfairness. For example, even if the offer is fair (split_self = fair_share), the utility would be \u03b1 + split_self. So, participants with higher \u03b1 are more likely to accept fair offers. But in this case, when the offer is exactly fair, U = \u03b1 + fair_share. The probability of acceptance would be logistic(\u03b1 + fair_share). But fair_share varies across trials (since token_self varies), so this might complicate things. For example, a participant with \u03b1=0 would have U = split_self when the offer is fair. But split_self can vary, leading to varying acceptance probabilities even for fair offers. However, including \u03b1 allows the model to account for a general bias towards accepting or rejecting.\n\nAlternatively, perhaps \u03b1 is not necessary. If the model is U = split_self - \u03b2 * deficit, then when the offer is fair, U = split_self = fair_share. The utility in this case is fair_share, which is 3*token_self. So, for higher token_self, the utility is higher, leading to higher probability of acceptance. But in reality, even if the offer is fair, the absolute amount (split_self) might influence acceptance. For example, a fair offer of \u00a33 (token_self=1) might be rejected more often than a fair offer of \u00a330 (token_self=10), because \u00a33 is a small amount. So, the model without \u03b1 would naturally account for this because the utility is higher for higher split_self, leading to higher acceptance probability. Thus, including \u03b1 might not be necessary.\n\nBut if we include \u03b1, it could capture any additional bias beyond the monetary value. For example, some participants might have a general tendency to reject offers (negative \u03b1) or accept (positive \u03b1). However, without strong theoretical justification, adding \u03b1 might complicate the model. Since the instruction prefers fewer parameters, perhaps omitting \u03b1 is better.\n\nThus, the model with one parameter \u03b2:\n\nU = split_self - \u03b2 * max( (fair_share - split_self)/fair_share, 0 )\n\nBut wait, the fair_share is 3*token_self. So, substituting:\n\nU = split_self - \u03b2 * max( (3*token_self - split_self)/(3*token_self), 0 )\n\nThis can be simplified to:\n\nU = split_self - \u03b2 * max(1 - (split_self)/(3*token_self), 0 )\n\nWhich is:\n\nIf split_self >= 3*token_self:\n    U = split_self\nElse:\n    U = split_self - \u03b2 * (1 - split_self/(3*token_self))\n\nBut this introduces a non-linear term. Alternatively, using the absolute difference:\n\nU = split_self - \u03b2 * (3*token_self - split_self)\n\nBut this would be:\n\nU = split_self + \u03b2*(split_self - 3*token_self) = (1 + \u03b2)*split_self - \u03b2*3*token_self\n\nThis is a linear model in split_self and token_self. However, this would penalize offers above the fair share if \u03b2 is positive. For example, if split_self is higher than fair_share, the term (split_self - 3*token_self) is positive, so U increases. But this might not be desired, as getting more than fair share might not be a disutility. Wait, in this case, if \u03b2 is positive, then offers above fair_share increase utility, which is reasonable. But if \u03b2 is negative, offers above fair_share decrease utility. But since \u03b2 is a learnable parameter, the model can learn the appropriate sign. However, we need to set bounds on \u03b2. If we expect that participants dislike being below fair_share, then \u03b2 should be positive. So, setting \u03b2's lower bound to 0.\n\nBut this model would have U = split_self + \u03b2*(split_self - 3*token_self) = (1 + \u03b2)split_self - \u03b2*3*token_self.\n\nThis can be rewritten as:\n\nU = (1 + \u03b2)split_self - 3\u03b2 token_self\n\nBut this is a linear combination of split_self and token_self. However, this might not capture the non-linear aversion to being below fair_share. For example, if the offer is slightly below fair_share, the disutility is linear. But participants might have a non-linear response (e.g., more sensitive to larger deficits). However, with one parameter, we can't capture non-linearity. So, a linear model is a starting point.\n\nAlternatively, using the max function to only apply the penalty when below fair_share:\n\nU = split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nHere, \u03b2 is the penalty per unit deficit. This is simpler and only activates the penalty when the offer is below fair_share. This seems more aligned with the idea that participants are averse to being shortchanged.\n\nSo, the utility function is:\n\nU = split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nThis model has one learnable parameter \u03b2, which must be non-negative. The range for \u03b2 can be [0, a large number], say 10, to allow for strong aversion.\n\nNow, checking if this model meets the requirements:\n\n1. Predict responder behavior (accept) using the given variables. The model uses split_self and token_self, which are available in the dataset.\n\n2. Applicable to the dataset: Yes, as the variables are present.\n\n3. Utility < 0 implies <50% probability: Yes, because the logistic function converts U to probability, and U < 0 gives p < 0.5.\n\nParameter recovery: With one parameter, recovery should be feasible. The parameter \u03b2 has a clear interpretation (sensitivity to deficit below fair share), and the bounds are [0, 10] or similar.\n\nNow, variable descriptions:\n\n- U: Utility of accepting the offer. Calculated, not learnable.\n- split_self: From data, the proposed share for the participant.\n- token_self: From data, the participant's contribution.\n- \u03b2: Learnable parameter, range [0, 10], inclusive.\n\nAnother consideration: the combined_earning is 3*(token_self + token_opp), but split_self + split_opp = combined_earning. However, in the responder role, the participant's decision is based on their own split and their contribution. The opponent's contribution (token_opp) might influence the fair share, but in this model, the fair share is based solely on the participant's own tokens. However, the opponent's tokens determine the total combined_earning, which affects split_opp. But since the model focuses on the participant's own fair share, token_opp isn't directly used, except in calculating split_opp. But split_opp isn't used in the model. So, the model doesn't require token_opp, only token_self and split_self.\n\nWait, but the fair share is 3*token_self, which is part of the model. So, token_self is necessary. The model uses split_self and token_self, both of which are in the dataset.\n\nThus, the model is feasible.\n\nNow, writing the mathematical model:\n\n<MODEL>\nU = split_self - beta * max(3 * token_self - split_self, 0)\n</MODEL>\n\nVariables:\n\n- U: utility, calculated.\n- split_self: from data.\n- token_self: from data.\n- beta: learnable parameter, range [0, 10].\n\nThe variable descriptions in JSON:\n\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to deficit below fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n\nThis model has one learnable parameter, beta, with a range [0,10]. The utility is calculated as split_self minus beta times any deficit below the fair share (3*token_self). The probability of acceptance is logistic(U), which meets the requirement that U < 0 implies less than 50% chance.\n\nThe summary should concisely describe the model:\n\n<SUMMARY>\nA utility model where acceptance probability depends on the proposed share and a penalty for offers below the participant's fair share (3 times their tokens). The penalty is scaled by a learnable sensitivity parameter (beta). Utility is the proposed amount minus beta times any deficit below the fair share, passed through a logistic function to determine acceptance likelihood.\n</SUMMARY>\n</think>\n\n<MODEL>\nU = split_self - beta * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to deficit below fair share (3 * token_self). Penalty per pound below fair share.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nUtility depends linearly on the proposed amount and penalizes deficits below the participant's fair contribution value (3 * token_self). A single learnable parameter (beta) scales the penalty for unfairness, with higher values increasing rejection likelihood for sub-fair offers. The logistic transform of utility predicts acceptance probability.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = split_self - beta * max(3 * token_self - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to deficit below fair share (3 * token_self). Penalty per pound below fair share.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        fair_share = 3 * token_self\n        deficit = max(fair_share - split_self, 0)\n        \n        U = split_self - beta * deficit\n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"control\", \"trial_number\": 0, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 6, \"token_self\": 6, \"combined_earning\": 36, \"split_opp\": 17.0, \"split_self\": 19.0, \"splitperc_opp\": 47, \"splitperc_self\": 53, \"accept\": 1.0, \"accepted_amount\": 19.0, \"accepted_perc\": 53.0, \"proposed_perc\": 53.0, \"proposed_amount\": 19.0}, {\"ID\": 1, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 6, \"token_self\": 6, \"combined_earning\": 36, \"split_opp\": 17.0, \"split_self\": 19.0, \"splitperc_opp\": 47, \"splitperc_self\": 53, \"accept\": 1.0, \"accepted_amount\": 19.0, \"accepted_perc\": 53.0, \"proposed_perc\": 53.0, \"proposed_amount\": 19.0}, {\"ID\": 2, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 6, \"token_self\": 6, \"combined_earning\": 36, \"split_opp\": 17.0, \"split_self\": 19.0, \"splitperc_opp\": 47, \"splitperc_self\": 53, \"accept\": 1.0, \"accepted_amount\": 19.0, \"accepted_perc\": 53.0, \"proposed_perc\": 53.0, \"proposed_amount\": 19.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1743516580.501549
    }
  ],
  "learnable_parameters": {
    "beta": {
      "description": "Sensitivity to deficit below fair share (3 * token_self). Penalty per pound below fair share.",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = split_self - beta * max(3 * token_self - split_self, 0)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "split_self": {
      "description": "Proposed share for the participant in pounds",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "beta": {
      "description": "Sensitivity to deficit below fair share (3 * token_self). Penalty per pound below fair share.",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_summary": "A utility model where acceptance probability depends on the proposed share and a penalty for offers below the participant's fair share (3 times their tokens). The penalty is scaled by a learnable sensitivity parameter (beta). Utility is the proposed amount minus beta times any deficit below the fair share, passed through a logistic function to determine acceptance likelihood.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility function should be such that a utility less than 0 corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The main target is 'accept' (binary 0/1). The model should predict this based on the utility of accepting the proposal. The variables provided include token_self, token_opp, combined_earning, split_self, split_opp, etc. Since the task involves fairness considerations, the model should account for how the participant perceives the fairness of the split relative to their own contribution and the opponent's.\n\nThe participants are responders in some trials (trial_role=1). So, the model should only consider those cases. The trial_type indicates whether contributions are equal, opponent more, or participant more. This might influence fairness perceptions.\n\nThe combined_earning is the total money, which is 3*(token_self + token_opp). The split_self and split_opp are the proposed shares. The split percentages (splitperc_self and splitperc_opp) might be important. However, the note says that two equal split_self values don't represent the same fairness because the combined earning varies. So, the absolute amounts and the relative contributions matter.\n\nA common approach in such social decision-making tasks is to model utility based on fairness considerations, often using inequity aversion models. For example, Fehr and Schmidt's model considers both advantageous and disadvantageous inequity. However, since the contributions (tokens) vary, the model should account for the participant's own contribution versus the opponent's.\n\nLet me think about the utility function. The utility of accepting could depend on the monetary gain (split_self) compared to some reference point, perhaps the fair share based on contributions. The fair share could be calculated as (token_self / (token_self + token_opp)) * combined_earning. If the proposed split_self is less than this fair share, the participant might feel treated unfairly, leading to a lower utility.\n\nSo, the utility could be a function of the difference between the proposed split and the fair share. Additionally, there might be a parameter that scales this difference, representing sensitivity to unfairness. Also, there could be a baseline acceptance tendency.\n\nAnother consideration is the absolute amount of split_self. Even if the split is unfair, a higher absolute amount might still lead to acceptance. So, the utility might combine both the absolute offer and the deviation from the fair share.\n\nPutting this together, the utility (U) could be modeled as:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - fair_share)\n\nHere, \u03b1 represents the sensitivity to the absolute offer, and \u03b2 represents the sensitivity to the deviation from the fair share. If \u03b2 is positive, participants are more likely to accept when the offer is above the fair share. However, if \u03b2 is negative, it might indicate aversion to unfairness (if the offer is below fair share, utility decreases).\n\nAlternatively, the fair share could be a reference point, and the utility could be split_self minus some function of the inequity. For example:\n\nfair_share = (token_self / (token_self + token_opp)) * combined_earning\ninequity = split_self - fair_share\nU = \u03b1 * split_self + \u03b2 * inequity\n\nBut maybe the inequity term should be nonlinear. For example, people might be more sensitive to negative inequity (getting less than fair) than positive. So, perhaps separate parameters for positive and negative deviations. However, adding more parameters could complicate parameter recovery. Since the instruction prefers fewer parameters, maybe a single parameter for inequity.\n\nWait, the note says that two equal split_self values don't represent the same fairness because combined_earning varies. So, the same split_self in different trials (with different combined_earning) would have different fairness. Therefore, the model must account for the fair share based on contributions.\n\nSo, the fair share is (token_self / total_tokens) * combined_earning. Since combined_earning is 3*(token_self + token_opp), the fair_share in pounds is 3 * token_self. Because combined_earning = 3*(token_self + token_opp), and fair_share would be (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3 * token_self. Wait, that's interesting. So, the fair share in pounds is exactly 3 times the participant's own tokens. Because each token is worth \u00a33. So, if the participant contributed token_self tokens, their fair share is 3 * token_self. Similarly, the opponent's fair share is 3 * token_opp.\n\nBut the split_self is the proposed amount for the participant. So, the difference between split_self and 3*token_self is the deviation from the fair share. So, the utility could be based on this difference.\n\nBut maybe participants also consider the opponent's share. For example, if the opponent is getting more than their fair share, the participant might feel envious, or if they're getting more, they might feel guilty. But again, adding parameters for that might complicate things.\n\nAlternatively, the utility could be:\n\nU = \u03b8 * split_self + \u03b3 * (split_self - 3 * token_self)\n\nHere, \u03b8 is the weight on the absolute offer, and \u03b3 is the weight on the deviation from the fair share. If \u03b3 is positive, participants prefer offers above their fair share. If \u03b3 is negative, they dislike deviations below the fair share.\n\nBut perhaps the deviation should be normalized. For example, (split_self - fair_share) / fair_share, which would represent proportional deviation. But that might complicate the model. Alternatively, using the raw difference.\n\nAnother angle: the participant's utility could depend on both their own payoff and the difference between their payoff and the opponent's. But since the opponent's contribution (token_opp) and their proposed split (split_opp) are known, maybe the model should compare split_self and split_opp relative to their respective fair shares.\n\nWait, the split_opp is the opponent's proposed share. So, in the responder role, the participant is evaluating the proposer's (opponent's) offer. The proposer (opponent) offers split_self to the participant and keeps split_opp. The total combined_earning is split_self + split_opp. But combined_earning is fixed as 3*(token_self + token_opp). So, split_self + split_opp = 3*(token_self + token_opp). Therefore, split_opp = 3*(token_self + token_opp) - split_self.\n\nBut maybe the participant cares about how their split compares to the opponent's split. For example, if the opponent is keeping more than their fair share, the participant might reject even if their own split is acceptable. But this adds complexity. However, given the task's emphasis on fairness, this might be necessary.\n\nAlternatively, the participant's utility could be a function of their own split and the difference between their split and the opponent's split. For example:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - split_opp)\n\nBut split_opp is 3*(token_self + token_opp) - split_self. So, split_self - split_opp = 2*split_self - 3*(token_self + token_opp). Hmm, not sure if that's the right way.\n\nAlternatively, the participant might compare their split to the opponent's split. If split_self is less than split_opp, that's disadvantageous inequity. If it's more, advantageous. So, using Fehr-Schmidt model:\n\nU = split_self - \u03b4 * max(split_opp - split_self, 0) - \u03c3 * max(split_self - split_opp, 0)\n\nHere, \u03b4 is aversion to disadvantageous inequity, \u03c3 to advantageous. But this introduces two parameters. However, the problem states that parameter recovery is important, so maybe two parameters are manageable. But the instruction says to have a slight bias towards fewer parameters. So perhaps combine them into one parameter if possible.\n\nAlternatively, use a single parameter for inequity aversion, considering the difference between the two splits. For example:\n\nU = split_self + \u03bb * (split_self - split_opp)\n\nBut this assumes that participants care linearly about the difference. If \u03bb is positive, they prefer getting more than the opponent; if negative, they dislike inequality.\n\nBut maybe the fair share is a better reference. Let's go back to the fair share idea. The fair share for the participant is 3*token_self, and for the opponent is 3*token_opp. So, the fair split would be split_self = 3*token_self and split_opp = 3*token_opp. Any deviation from this could be considered unfair.\n\nSo, the participant's utility could depend on how much their split_self deviates from their fair share, and how much the opponent's split_opp deviates from their fair share. For example:\n\nU = split_self - \u03b1 * (split_self - 3*token_self) - \u03b2 * (split_opp - 3*token_opp)\n\nBut split_opp is determined once split_self is known, since split_opp = combined_earning - split_self. So, maybe this can be simplified.\n\nAlternatively, the participant's utility could be:\n\nU = split_self + \u03b3 * (split_self - 3*token_self) + \u03b4 * (split_opp - 3*token_opp)\n\nBut since split_opp = combined_earning - split_self, and combined_earning is 3*(token_self + token_opp), then split_opp - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self.\n\nSo, substituting:\n\nU = split_self + \u03b3*(split_self - 3*token_self) + \u03b4*(3*token_self - split_self)\n\nThis simplifies to:\n\nU = split_self + (\u03b3 - \u03b4)*(split_self - 3*token_self)\n\nWhich is equivalent to:\n\nU = split_self + \u03b8*(split_self - 3*token_self), where \u03b8 = \u03b3 - \u03b4.\n\nSo, this reduces to a single parameter for the deviation from the fair share. That's interesting. So, the utility is split_self plus a parameter times the difference between split_self and the fair share. If \u03b8 is positive, participants prefer offers above their fair share; if negative, they are averse to deviations (either direction? Not sure). Wait, if \u03b8 is positive, then when split_self is higher than fair share, utility increases. If \u03b8 is negative, then higher split_self than fair share would decrease utility, which doesn't make sense. Hmm, maybe this approach isn't capturing the right intuition.\n\nAlternatively, perhaps the participant's utility is influenced by both their own gain and the fairness relative to their contribution. The fairness could be the ratio of split_self to their fair share. For example:\n\nU = \u03b1 * split_self + \u03b2 * (split_self / (3 * token_self))\n\nBut if token_self is zero, this could be problematic. Alternatively, using the difference:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - 3 * token_self)\n\nHere, \u03b1 is the marginal utility of money, and \u03b2 is the sensitivity to fairness (difference from fair share). If \u03b2 is positive, getting more than fair increases utility; if negative, deviations from fair (either above or below) decrease utility. Wait, no\u2014if \u03b2 is positive, then split_self being higher than fair increases utility. If \u03b2 is negative, then higher split_self than fair would decrease utility, which might not make sense. Maybe participants dislike being overcompensated? Unlikely. More likely, they dislike being undercompensated. So perhaps the model should have a term that only penalizes when split_self is below fair.\n\nThis suggests a piecewise function. For example:\n\nIf split_self < 3*token_self:\n    U = split_self - \u03bb*(3*token_self - split_self)\nElse:\n    U = split_self\n\nHere, \u03bb is the aversion to being below the fair share. Alternatively, using a linear term with a parameter that's only active when below. But piecewise functions can complicate parameter estimation. Alternatively, model it as:\n\nU = split_self - \u03bb * max(3*token_self - split_self, 0)\n\nThis way, the participant only incurs a disutility when their split is below the fair share. The parameter \u03bb represents how much they dislike being undercompensated. This is similar to the Fehr-Schmidt model but focused on the fair share based on contribution.\n\nAlternatively, combining both absolute and relative terms. For example:\n\nU = \u03b1 * split_self - \u03b2 * max( (3*token_self - split_self)/ (3*token_self), 0 )\n\nHere, the relative shortfall is considered. Dividing by 3*token_self makes it a proportional difference. But if token_self is zero, this is undefined. However, in the dataset, token_self is an integer (int64), so maybe it's always positive? The problem states that in the pretask, participants find a number of tokens, so token_self is at least 1? Not sure, but assuming token_self is positive.\n\nSo, the utility function could be:\n\nU = \u03b1 * split_self - \u03b2 * max( (fair_share - split_self) / fair_share, 0 )\n\nWhere fair_share = 3 * token_self.\n\nThis way, \u03b2 scales the proportional deficit from the fair share. If the split is below fair, the participant experiences a disutility proportional to the percentage shortfall. \u03b1 represents the utility per pound received.\n\nBut this introduces two parameters: \u03b1 and \u03b2. However, since the model needs to predict utility which is then passed through a logistic function to get the probability of acceptance, perhaps \u03b1 can be set to 1 (assuming linear utility in money), and only \u03b2 is a learnable parameter. But the problem states that the model should have learnable parameters with finite bounds.\n\nAlternatively, include both \u03b1 and \u03b2 as learnable parameters. But the instruction says to have a slight bias towards fewer parameters. Let's see.\n\nIf we set \u03b1=1 (assuming utility is linear in money), then the model would have one parameter \u03b2. The utility becomes:\n\nU = split_self - \u03b2 * max( (fair_share - split_self) / fair_share, 0 )\n\nBut this is equivalent to:\n\nIf split_self >= fair_share:\n    U = split_self\nElse:\n    U = split_self - \u03b2 * (fair_share - split_self)/fair_share\n\nBut scaling \u03b2 here might be tricky. Alternatively, without normalizing:\n\nU = split_self - \u03b2 * max(fair_share - split_self, 0 )\n\nHere, \u03b2 represents the disutility per pound below the fair share. So, for each pound below fair, utility decreases by \u03b2. This is simpler and has one parameter.\n\nBut then, the utility is split_self minus \u03b2 times the deficit. If the deficit is large, the utility could become negative even if split_self is positive. For example, if split_self is \u00a35, fair_share is \u00a310, and \u03b2 is 2, then U = 5 - 2*5 = 5 -10 = -5. The probability of acceptance would be 1/(1 + exp(-U)), but wait, the logistic function maps U to [0,1]. Wait, no: the standard logistic function is 1/(1 + exp(-U)), so if U is negative, the probability is less than 0.5. The problem states that \"less than 0 is less than 50% likely\", which aligns with this.\n\nSo, the model could be:\n\nU = split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nHere, \u03b2 is a learnable parameter representing the sensitivity to being below the fair share. If \u03b2 is high, even small deficits lead to lower utility and lower probability of acceptance.\n\nBut maybe participants also care about how their split compares to the opponent's split in absolute terms. For example, if the opponent is getting a lot more, even if the participant's split is above their fair share, they might reject. But adding that would require another parameter.\n\nAlternatively, the model could include both the deficit from fair share and the ratio of the splits. But again, more parameters.\n\nGiven the need for parameter recovery and simplicity, perhaps starting with one parameter is better. Let's proceed with the model:\n\nU = split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nThis model has one learnable parameter, \u03b2, which must be non-negative (since being below fair share should decrease utility). The utility is split_self minus \u03b2 times any deficit below the fair share. If the offer is equal to or above the fair share, U is just split_self. If below, U decreases.\n\nThe probability of acceptance is logistic(U) = 1 / (1 + exp(-U)). So when U > 0, probability > 0.5; when U < 0, probability < 0.5.\n\nBut wait, the utility here is in pounds. If split_self is \u00a310, and \u03b2 is 2, then a \u00a35 deficit (fair share \u00a315) would give U = 10 - 2*5 = 0. The probability would be 0.5. If split_self is \u00a314, fair share \u00a315: U =14 -2*1=12 \u2192 probability ~1. So, this might not capture cases where the offer is close to fair but still rejected.\n\nAlternatively, perhaps the model should include a term that compares the proposed split to the fair share proportionally. For example:\n\nU = split_self - \u03b2 * ( (3*token_self - split_self) / (3*token_self) )\n\nThis way, the deficit is normalized by the fair share. So, if the fair share is \u00a315 and the offer is \u00a310, the deficit is \u00a35, which is 1/3 of the fair share. \u03b2 would then scale this proportional deficit. This might make \u03b2 more interpretable as a proportional sensitivity.\n\nBut this introduces division by token_self, which could be zero. However, in the dataset, token_self is the number of tokens found by the participant, which is likely at least 1 (since they had to find some tokens in the pretask). So, division by zero is not an issue.\n\nSo, the model becomes:\n\nU = split_self - \u03b2 * max( (3*token_self - split_self) / (3*token_self), 0 )\n\nThis can be simplified to:\n\nU = split_self - \u03b2 * max( (fair_share - split_self) / fair_share, 0 )\n\nWhich is the proportional deficit below fair share.\n\nAlternatively, combining both absolute and proportional terms. But again, more parameters.\n\nAnother consideration: the participant's own contribution (token_self) might influence their sense of entitlement. For example, if they contributed more, they might expect a higher split. The model already accounts for this via the fair share.\n\nBut perhaps the model should also include a term that reflects the opponent's contribution. For example, if the opponent contributed less but is offering a split that's unfair, the participant might be more likely to reject. However, this would require additional parameters.\n\nAlternatively, the trial_type variable indicates whether the contribution was equal, opponent more, or participant more. This could interact with the fairness evaluation. For example, in trials where the participant contributed more (trial_type=3), they might be more sensitive to receiving less than their fair share. But including trial_type would require additional parameters, perhaps as interaction terms with \u03b2. However, trial_type is a categorical variable with three levels, which complicates the model. Since the instruction says to use the variables appropriately but not necessarily all, maybe trial_type can be omitted unless necessary.\n\nGiven the dataset's variables, the key ones for the responder's decision are split_self, token_self, and token_opp (since combined_earning is derived from tokens). The model should use these to compute the fair share and the proposed split.\n\nPutting it all together, the model with one parameter \u03b2 seems manageable. The utility is based on the proposed split and the proportional deficit below the fair share. The parameter \u03b2 determines how sensitive the participant is to this deficit.\n\nNow, considering the parameter bounds. \u03b2 should be non-negative because a higher \u03b2 means more penalty for being below fair. So, the range for \u03b2 could be [0, some upper bound]. The upper bound can be generous, say 10, which would mean that even a small proportional deficit (e.g., 10% below fair) would lead to a significant decrease in utility. But the exact value can be determined during estimation.\n\nAnother possible parameter is the baseline acceptance tendency. For example, some participants might have a general tendency to accept or reject offers regardless of the terms. This could be modeled with an intercept term \u03b1. So:\n\nU = \u03b1 + split_self - \u03b2 * max( (fair_share - split_self)/fair_share, 0 )\n\nHere, \u03b1 represents the baseline utility. If \u03b1 is positive, the participant is more likely to accept even if the offer is slightly unfair. However, adding \u03b1 introduces another parameter. But the problem allows for a few parameters as long as they are manageable. Since parameter recovery is a priority, two parameters (\u03b1 and \u03b2) might be acceptable.\n\nBut let's think: if \u03b1 is included, it could capture the general tendency to accept, while \u03b2 captures the sensitivity to unfairness. For example, even if the offer is fair (split_self = fair_share), the utility would be \u03b1 + split_self. So, participants with higher \u03b1 are more likely to accept fair offers. But in this case, when the offer is exactly fair, U = \u03b1 + fair_share. The probability of acceptance would be logistic(\u03b1 + fair_share). But fair_share varies across trials (since token_self varies), so this might complicate things. For example, a participant with \u03b1=0 would have U = split_self when the offer is fair. But split_self can vary, leading to varying acceptance probabilities even for fair offers. However, including \u03b1 allows the model to account for a general bias towards accepting or rejecting.\n\nAlternatively, perhaps \u03b1 is not necessary. If the model is U = split_self - \u03b2 * deficit, then when the offer is fair, U = split_self = fair_share. The utility in this case is fair_share, which is 3*token_self. So, for higher token_self, the utility is higher, leading to higher probability of acceptance. But in reality, even if the offer is fair, the absolute amount (split_self) might influence acceptance. For example, a fair offer of \u00a33 (token_self=1) might be rejected more often than a fair offer of \u00a330 (token_self=10), because \u00a33 is a small amount. So, the model without \u03b1 would naturally account for this because the utility is higher for higher split_self, leading to higher acceptance probability. Thus, including \u03b1 might not be necessary.\n\nBut if we include \u03b1, it could capture any additional bias beyond the monetary value. For example, some participants might have a general tendency to reject offers (negative \u03b1) or accept (positive \u03b1). However, without strong theoretical justification, adding \u03b1 might complicate the model. Since the instruction prefers fewer parameters, perhaps omitting \u03b1 is better.\n\nThus, the model with one parameter \u03b2:\n\nU = split_self - \u03b2 * max( (fair_share - split_self)/fair_share, 0 )\n\nBut wait, the fair_share is 3*token_self. So, substituting:\n\nU = split_self - \u03b2 * max( (3*token_self - split_self)/(3*token_self), 0 )\n\nThis can be simplified to:\n\nU = split_self - \u03b2 * max(1 - (split_self)/(3*token_self), 0 )\n\nWhich is:\n\nIf split_self >= 3*token_self:\n    U = split_self\nElse:\n    U = split_self - \u03b2 * (1 - split_self/(3*token_self))\n\nBut this introduces a non-linear term. Alternatively, using the absolute difference:\n\nU = split_self - \u03b2 * (3*token_self - split_self)\n\nBut this would be:\n\nU = split_self + \u03b2*(split_self - 3*token_self) = (1 + \u03b2)*split_self - \u03b2*3*token_self\n\nThis is a linear model in split_self and token_self. However, this would penalize offers above the fair share if \u03b2 is positive. For example, if split_self is higher than fair_share, the term (split_self - 3*token_self) is positive, so U increases. But this might not be desired, as getting more than fair share might not be a disutility. Wait, in this case, if \u03b2 is positive, then offers above fair_share increase utility, which is reasonable. But if \u03b2 is negative, offers above fair_share decrease utility. But since \u03b2 is a learnable parameter, the model can learn the appropriate sign. However, we need to set bounds on \u03b2. If we expect that participants dislike being below fair_share, then \u03b2 should be positive. So, setting \u03b2's lower bound to 0.\n\nBut this model would have U = split_self + \u03b2*(split_self - 3*token_self) = (1 + \u03b2)split_self - \u03b2*3*token_self.\n\nThis can be rewritten as:\n\nU = (1 + \u03b2)split_self - 3\u03b2 token_self\n\nBut this is a linear combination of split_self and token_self. However, this might not capture the non-linear aversion to being below fair_share. For example, if the offer is slightly below fair_share, the disutility is linear. But participants might have a non-linear response (e.g., more sensitive to larger deficits). However, with one parameter, we can't capture non-linearity. So, a linear model is a starting point.\n\nAlternatively, using the max function to only apply the penalty when below fair_share:\n\nU = split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nHere, \u03b2 is the penalty per unit deficit. This is simpler and only activates the penalty when the offer is below fair_share. This seems more aligned with the idea that participants are averse to being shortchanged.\n\nSo, the utility function is:\n\nU = split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nThis model has one learnable parameter \u03b2, which must be non-negative. The range for \u03b2 can be [0, a large number], say 10, to allow for strong aversion.\n\nNow, checking if this model meets the requirements:\n\n1. Predict responder behavior (accept) using the given variables. The model uses split_self and token_self, which are available in the dataset.\n\n2. Applicable to the dataset: Yes, as the variables are present.\n\n3. Utility < 0 implies <50% probability: Yes, because the logistic function converts U to probability, and U < 0 gives p < 0.5.\n\nParameter recovery: With one parameter, recovery should be feasible. The parameter \u03b2 has a clear interpretation (sensitivity to deficit below fair share), and the bounds are [0, 10] or similar.\n\nNow, variable descriptions:\n\n- U: Utility of accepting the offer. Calculated, not learnable.\n- split_self: From data, the proposed share for the participant.\n- token_self: From data, the participant's contribution.\n- \u03b2: Learnable parameter, range [0, 10], inclusive.\n\nAnother consideration: the combined_earning is 3*(token_self + token_opp), but split_self + split_opp = combined_earning. However, in the responder role, the participant's decision is based on their own split and their contribution. The opponent's contribution (token_opp) might influence the fair share, but in this model, the fair share is based solely on the participant's own tokens. However, the opponent's tokens determine the total combined_earning, which affects split_opp. But since the model focuses on the participant's own fair share, token_opp isn't directly used, except in calculating split_opp. But split_opp isn't used in the model. So, the model doesn't require token_opp, only token_self and split_self.\n\nWait, but the fair share is 3*token_self, which is part of the model. So, token_self is necessary. The model uses split_self and token_self, both of which are in the dataset.\n\nThus, the model is feasible.\n\nNow, writing the mathematical model:\n\n<MODEL>\nU = split_self - beta * max(3 * token_self - split_self, 0)\n</MODEL>\n\nVariables:\n\n- U: utility, calculated.\n- split_self: from data.\n- token_self: from data.\n- beta: learnable parameter, range [0, 10].\n\nThe variable descriptions in JSON:\n\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to deficit below fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n\nThis model has one learnable parameter, beta, with a range [0,10]. The utility is calculated as split_self minus beta times any deficit below the fair share (3*token_self). The probability of acceptance is logistic(U), which meets the requirement that U < 0 implies less than 50% chance.\n\nThe summary should concisely describe the model:\n\n<SUMMARY>\nA utility model where acceptance probability depends on the proposed share and a penalty for offers below the participant's fair share (3 times their tokens). The penalty is scaled by a learnable sensitivity parameter (beta). Utility is the proposed amount minus beta times any deficit below the fair share, passed through a logistic function to determine acceptance likelihood.\n</SUMMARY>\n</think>\n\n<MODEL>\nU = split_self - beta * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to deficit below fair share (3 * token_self). Penalty per pound below fair share.\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nUtility depends linearly on the proposed amount and penalizes deficits below the participant's fair contribution value (3 * token_self). A single learnable parameter (beta) scales the penalty for unfairness, with higher values increasing rejection likelihood for sub-fair offers. The logistic transform of utility predicts acceptance probability.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = split_self - beta * max(3 * token_self - split_self, 0)\nSummary: A utility model where acceptance probability depends on the proposed share and a penalty for offers below the participant's fair share (3 times their tokens). The penalty is scaled by a learnable sensitivity parameter (beta). Utility is the proposed amount minus beta times any deficit below the fair share, passed through a logistic function to determine acceptance likelihood.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        fair_share = 3 * token_self\n        deficit = max(fair_share - split_self, 0)\n        \n        U = split_self - beta * deficit\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "fitting_warnings": [
    "Error fitting participant 1010: Error in parameter fitting: All optimization attempts failed. Errors: []"
  ],
  "skipped_participants": [
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: []"
    }
  ],
  "num_skipped_participants": 1,
  "overall_accuracy": 0.8416666666666667,
  "fitting_results": [
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955368,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.5631429516095439,
      "success": true,
      "log_likelihood": -81.81670202880834,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4340570076541665,
      "success": true,
      "log_likelihood": -35.16938113651017,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.5000104339807288,
      "success": true,
      "log_likelihood": -77.37696314826702,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.9931402606829864,
      "success": true,
      "log_likelihood": -43.22378014785828,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.0017188116798252,
      "success": true,
      "log_likelihood": -44.08975995109146,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9856634059492024,
      "success": true,
      "log_likelihood": -43.98732936130089,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.6228329762831437,
      "success": true,
      "log_likelihood": -0.7929674299182974,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.289521877360489,
      "success": true,
      "log_likelihood": -81.73955824288922,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.121247551108228,
      "success": true,
      "log_likelihood": -23.097897792593255,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.4883031671638144,
      "success": true,
      "log_likelihood": -3.7485821421031416,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.47621260570380286,
      "success": true,
      "log_likelihood": -19.314527175396755,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4706626778992236,
      "success": true,
      "log_likelihood": -59.34903571885347,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.5140761119857434,
      "success": true,
      "log_likelihood": -25.472735713438762,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4502636033334868,
      "success": true,
      "log_likelihood": -129.8380700074698,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4164295717682034,
      "success": true,
      "log_likelihood": -90.33312254515967,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.593370464389964,
      "success": true,
      "log_likelihood": -69.45760305774171,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.3834577385233462,
      "success": true,
      "log_likelihood": -65.73587922453052,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.356410505919733,
      "success": true,
      "log_likelihood": -51.831788612549886,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.1579957781186585,
      "success": true,
      "log_likelihood": -12.8365559885152,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.45544645130528405,
      "success": true,
      "log_likelihood": -39.110810258503285,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6130329968472714,
      "success": true,
      "log_likelihood": -91.22230590601812,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955366,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4398963089582273,
      "success": true,
      "log_likelihood": -152.66708379830098,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.5043191123388373,
      "success": true,
      "log_likelihood": -90.13646556137094,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.8169153016804607,
      "success": true,
      "log_likelihood": -64.17401672472796,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.8708109357543924,
      "success": true,
      "log_likelihood": -17.232774792979473,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.5229309886970905,
      "success": true,
      "log_likelihood": -70.71977376547521,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.8055133596879257,
      "success": true,
      "log_likelihood": -111.42088905797478,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.23590725575739582,
      "success": true,
      "log_likelihood": -38.684629828149134,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.392939093337843,
      "success": true,
      "log_likelihood": -130.7786555165927,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.6597273737799085,
      "success": true,
      "log_likelihood": -46.101406118089265,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -23.025863872743603,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.0215113589094311,
      "success": true,
      "log_likelihood": -90.90803977681163,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9919370565379677,
      "success": true,
      "log_likelihood": -63.7703672698431,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6560054976372637,
      "success": true,
      "log_likelihood": -96.03293985200936,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4397653258115637,
      "success": true,
      "log_likelihood": -84.4690582549517,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.534257143922721,
      "success": true,
      "log_likelihood": -134.3423736065809,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.7549032916040943,
      "success": true,
      "log_likelihood": -65.59507103817909,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.0490976468777695,
      "success": true,
      "log_likelihood": -209.10271640677414,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.7844286653395636,
      "success": true,
      "log_likelihood": -109.86797697686583,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -115.22642481011445,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.3990132794979373,
      "success": true,
      "log_likelihood": -113.09880593929994,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6969229057754586,
      "success": true,
      "log_likelihood": -177.7192931406941,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.533290379561401,
      "success": true,
      "log_likelihood": -157.09122705551965,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.37137580621943,
      "success": true,
      "log_likelihood": -11.282174529477437,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.887408641599965,
      "success": true,
      "log_likelihood": -94.29948071221668,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.17977518923294966,
      "success": true,
      "log_likelihood": -84.40605500006632,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -92.20057619959418,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.4397339705190316,
      "success": true,
      "log_likelihood": -75.15375433489304,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4421736765211575,
      "success": true,
      "log_likelihood": -145.68400674252044,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.7288193492763444,
      "success": true,
      "log_likelihood": -62.77451461417868,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955369,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4044943828672942,
      "success": true,
      "log_likelihood": -123.92497248234947,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.0008704253153615,
      "success": true,
      "log_likelihood": -77.43639241612915,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4050944940115444,
      "success": true,
      "log_likelihood": -56.222883798069006,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.32565476646911656,
      "success": true,
      "log_likelihood": -36.7150376489288,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.438932371823316,
      "success": true,
      "log_likelihood": -45.16199733148708,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9681171820002922,
      "success": true,
      "log_likelihood": -95.11861334527264,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.2263751640908786,
      "success": true,
      "log_likelihood": -49.763130785807185,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.09717469987590818,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -115.2264271296346,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4242942394173999,
      "success": true,
      "log_likelihood": -72.22480244878315,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6591908976889154,
      "success": true,
      "log_likelihood": -82.00444791779076,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.487800385904638,
      "success": true,
      "log_likelihood": -158.18608595669846,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.8118971097600762,
      "success": true,
      "log_likelihood": -65.22219019705693,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.20087712580709327,
      "success": true,
      "log_likelihood": -33.38938053751631,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -92.20057619959414,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4087253296688478,
      "success": true,
      "log_likelihood": -201.10249461795283,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955369,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.3938521065535945,
      "success": true,
      "log_likelihood": -222.0957592348858,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "success": true,
      "log_likelihood": -68.73866400294936,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 7.000871226679958,
      "success": true,
      "log_likelihood": -123.48809205578563,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.3815725842351947,
      "success": true,
      "log_likelihood": -38.54142167302597,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.703114591902984,
      "success": true,
      "log_likelihood": -216.75126936953603,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "success": true,
      "log_likelihood": -46.148876559956825,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.7094872905069783,
      "success": true,
      "log_likelihood": -116.74997983520011,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.033792376937503,
      "success": true,
      "log_likelihood": -77.93913360335053,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.2633591812741478,
      "success": true,
      "log_likelihood": -19.41683353271703,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.3394207766020534,
      "success": true,
      "log_likelihood": -84.05849396646673,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9839989894442359,
      "success": true,
      "log_likelihood": -168.7572341442413,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "beta": {
        "r": 0.9763063215291679,
        "p": 5.878556859573184e-67
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 149.82558902601426,
  "bic_results": {
    "average_bic": 149.82558902601426,
    "individual_bics": [
      141.93296947756346,
      167.21692299607278,
      73.92228121147645,
      158.33744523499016,
      90.03107923417267,
      91.76303884063903,
      3.5835448238623537,
      91.55817766105788,
      3.5835448238623537,
      5.169453798292705,
      167.06263542423454,
      49.77931452364262,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      11.080683222662394,
      42.21257328924962,
      3.5835448238623537,
      122.28159037616305,
      54.528990365333634,
      263.25965895339573,
      184.24976402877545,
      142.49872505393952,
      135.05527738751715,
      107.24709616355588,
      29.256630915486507,
      81.80513945546268,
      186.02813075049235,
      3.5835448238623537,
      141.93296947756343,
      308.91768653505807,
      183.856450061198,
      3.5835448238623537,
      131.93155238791203,
      38.049068524415055,
      145.02306646940653,
      226.42529705440566,
      80.95277859475438,
      265.1408299716415,
      95.78633117463464,
      49.635246683943315,
      185.39959849207938,
      131.12425347814232,
      195.64939864247484,
      172.52163544835952,
      272.2682661516179,
      134.77366101481428,
      421.7889517520044,
      223.31947289218778,
      234.036368558685,
      229.78113081705598,
      359.0221052198443,
      317.7659730494954,
      26.147867997410984,
      192.18248036288946,
      172.39562893858874,
      187.98467133764447,
      153.89102760824218,
      294.951532423497,
      129.13254816681348,
      141.9329694775635,
      251.43346390315506,
      158.4563037707144,
      116.02928653459412,
      77.01359423631371,
      93.90751360143027,
      193.82074562900138,
      103.10978051007048,
      3.7778683382079263,
      234.0363731977253,
      148.0331238360224,
      167.59241477403762,
      319.95569085185303,
      134.02789933256997,
      70.36228001348873,
      187.9846713376444,
      405.78850817436177,
      141.9329694775635,
      447.7750374082277,
      141.06084694435484,
      250.55970305002737,
      80.66636228450805,
      437.0860576775282,
      95.88127205836976,
      237.08347860885632,
      159.46178614515716,
      42.417186003890166,
      171.70050687138956,
      341.0979872269387
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- beta: r = 0.976",
  "previous_models": [
    "Specification: U = split_self - beta * max(3 * token_self - split_self, 0)\nSummary: A utility model where acceptance probability depends on the proposed share and a penalty for offers below the participant's fair share (3 times their tokens). The penalty is scaled by a learnable sensitivity parameter (beta). Utility is the proposed amount minus beta times any deficit below the fair share, passed through a logistic function to determine acceptance likelihood.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 149.82558902601426\nOverall Accuracy: 0.8417\n\nParameter Recovery:\n- beta: r = 0.976"
  ]
}