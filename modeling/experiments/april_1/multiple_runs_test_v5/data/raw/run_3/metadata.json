{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\nAlright, the user provided the previous results and the new instructions they want me to generate. Let me start by understanding the previous run's outcomes. The average BIC improved to 142.18, which is lower than the previous 149.83, indicating a better model fit. The accuracy slightly decreased from 0.89 to 0.844, but that might be a trade-off for fewer parameters. The parameter recovery for beta dropped from 0.976 to 0.707, and alpha and gamma are around 0.56-0.57, which isn't great.\n\nThe previous instructions emphasized strict parameter parsimony, composite terms, covariance analysis, and validation. The new results show that while BIC improved, parameter recovery suffered, especially for beta. This suggests that maybe the model became too simplistic, leading to parameters being conflated or not identifiable. \n\nSo, the next instructions need to balance parsimony with ensuring parameters are recoverable. The user's goal is to maintain low BIC but improve parameter recovery. The previous instructions might have been too aggressive in reducing parameters, causing important mechanisms to be oversimplified. \n\nI should adjust the instructions to allow necessary parameters but with rigorous checks. Maybe instead of fixing parameters, find a middle ground where each parameter is justified with empirical evidence and identifiability. Also, suggest using hierarchical modeling if individual differences are significant. The covariance threshold was set at 0.7, but maybe that's too lenient. Lowering it could help reduce redundancy. Additionally, cross-validation during model design might help in assessing parameter necessity without overfitting. \n\nIncluding a step to test parameter recovery through simulated data before finalizing the model could preemptively catch issues. Also, considering interactions between parameters might help in designing composite terms that don't sacrifice recoverability. The new instructions should stress empirical justification, identifiability checks with stricter covariance thresholds, simulation-based validation, and perhaps hierarchical structures if needed.\n</think>\n\nPrioritize identifiability-optimized parsimony through these steps:\n\n1) Implement hierarchical parameterization where appropriate:\n   - Individual-level parameters ONLY for dimensions with empirical evidence of between-subject variance\n   - Shared population-level parameters for stable cognitive operations\n\n2) Enforce geometric constraint checking:\n   - Reject any parameter configuration where pairwise correlations > |0.6| in recovery simulations\n   - Require orthogonal parameter effects (\u0394BIC > 5 when individually varied)\n\n3) Use nonlinear compression for related mechanisms:\n   - Implement sigmoidal transforms instead of separate threshold parameters\n   - Replace additive terms with normalized products where theoretically justified\n\n4) Conduct sequential identifiability testing:\n   a) Simulate recovery with ground truth parameters\n   b) Prune parameters showing posterior overlap (R-hat > 1.05)\n   c) Validate through cross-parameter prediction tests (r > 0.7 required)\n\n5) Maintain explicit biological plausibility anchors:\n   - Fix parameters to literature-derived values when experimental manipulations exist\n   - Constrain parameter ranges using neural/physiological measurement boundaries",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: <think>\nAlright, the user provided the previous results and the new instructions they want me to generate. Let me start by understanding the previous run's outcomes. The average BIC improved to 142.18, which is lower than the previous 149.83, indicating a better model fit. The accuracy slightly decreased from 0.89 to 0.844, but that might be a trade-off for fewer parameters. The parameter recovery for beta dropped from 0.976 to 0.707, and alpha and gamma are around 0.56-0.57, which isn't great.\n\nThe previous instructions emphasized strict parameter parsimony, composite terms, covariance analysis, and validation. The new results show that while BIC improved, parameter recovery suffered, especially for beta. This suggests that maybe the model became too simplistic, leading to parameters being conflated or not identifiable. \n\nSo, the next instructions need to balance parsimony with ensuring parameters are recoverable. The user's goal is to maintain low BIC but improve parameter recovery. The previous instructions might have been too aggressive in reducing parameters, causing important mechanisms to be oversimplified. \n\nI should adjust the instructions to allow necessary parameters but with rigorous checks. Maybe instead of fixing parameters, find a middle ground where each parameter is justified with empirical evidence and identifiability. Also, suggest using hierarchical modeling if individual differences are significant. The covariance threshold was set at 0.7, but maybe that's too lenient. Lowering it could help reduce redundancy. Additionally, cross-validation during model design might help in assessing parameter necessity without overfitting. \n\nIncluding a step to test parameter recovery through simulated data before finalizing the model could preemptively catch issues. Also, considering interactions between parameters might help in designing composite terms that don't sacrifice recoverability. The new instructions should stress empirical justification, identifiability checks with stricter covariance thresholds, simulation-based validation, and perhaps hierarchical structures if needed.\n</think>\n\nPrioritize identifiability-optimized parsimony through these steps:\n\n1) Implement hierarchical parameterization where appropriate:\n   - Individual-level parameters ONLY for dimensions with empirical evidence of between-subject variance\n   - Shared population-level parameters for stable cognitive operations\n\n2) Enforce geometric constraint checking:\n   - Reject any parameter configuration where pairwise correlations > |0.6| in recovery simulations\n   - Require orthogonal parameter effects (\u0394BIC > 5 when individually varied)\n\n3) Use nonlinear compression for related mechanisms:\n   - Implement sigmoidal transforms instead of separate threshold parameters\n   - Replace additive terms with normalized products where theoretically justified\n\n4) Conduct sequential identifiability testing:\n   a) Simulate recovery with ground truth parameters\n   b) Prune parameters showing posterior overlap (R-hat > 1.05)\n   c) Validate through cross-parameter prediction tests (r > 0.7 required)\n\n5) Maintain explicit biological plausibility anchors:\n   - Fix parameters to literature-derived values when experimental manipulations exist\n   - Constrain parameter ranges using neural/physiological measurement boundaries\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible.\n\nFirst, I need to identify which variables are relevant for the responder's decision. The target variable is \"accept\", which is binary (1 for accept, 0 for reject). The model should predict this based on the utility of the proposal from the responder's perspective.\n\nKey variables to consider:\n- token_self and token_opp: These represent the contributions of the participant and opponent to the shared pot. Since combined_earning is the total money (each token is \u00a33), it's calculated as 3*(token_self + token_opp). But since combined_earning is already a variable, maybe we can use that directly.\n- split_self and split_opp: The proposed shares in \u00a3. The responder's share is split_self, and the opponent's is split_opp.\n- trial_type: Indicates if contributions are equal, opponent more, or participant more. This might affect fairness perceptions.\n- trial_role: We only consider when the participant is the responder (trial_role=1).\n\nThe utility function should capture the fairness of the split relative to contributions. A common approach is to model utility as a combination of monetary gain and inequity aversion. Inequity aversion suggests that people dislike unequal outcomes, even if it means sacrificing some personal gain.\n\nSo, the utility (U) could be the participant's monetary gain (split_self) minus some term that penalizes inequality. The inequality could be the difference between the participant's share and what's considered a fair share based on contributions.\n\nWhat's a fair share? If contributions matter, the fair split might be proportional to tokens contributed. The participant's fair share would be (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so the fair share simplifies to 3*token_self. However, the proposed split_self might differ from this.\n\nAlternatively, the fair share percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these could be a factor.\n\nBut the problem states that two equal split_self values don't represent the same fairness because combined_earning varies. So, the model should account for the proportion of the split relative to the contribution.\n\nLet me formalize this:\n\nFair share (F) = token_self / (token_self + token_opp) * combined_earning\nBut combined_earning = 3*(token_self + token_opp), so F = 3*token_self.\n\nThe proposed share is split_self. The difference between split_self and F could be a key factor. If split_self < F, the participant is getting less than their contribution, which might lead to rejection.\n\nBut inequity aversion also considers the opponent's share. Another approach is to use the Fehr-Schmidt model, where utility is:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequality (opponent gets more), and \u03b2 is aversion to advantageous inequality (participant gets more). But in this task, since the participant is the responder, maybe only disadvantageous inequality matters. Or both.\n\nHowever, the problem mentions that the model should predict based on the provided variables. The Fehr-Schmidt model uses split_self and split_opp, which are available.\n\nBut perhaps we need to incorporate the contribution-based fairness. Another way is to compute the fair split percentage as (token_self / (token_self + token_opp)) * 100, which is splitperc_self_fair = (token_self / (token_self + token_opp)) * 100. The proposed split percentage is splitperc_self. The difference (splitperc_self - splitperc_self_fair) could be a parameter. If this is negative, the participant is getting less than fair, which might decrease acceptance likelihood.\n\nAlternatively, the utility could be a linear combination of the monetary gain and the difference between the proposed split and the fair split.\n\nLet me try combining these ideas:\n\nUtility = \u03b8 * split_self + \u03b3 * (split_self - F) + \u03b4 * (split_opp - split_self)\n\nWhere F is the fair share (3*token_self). \u03b8 represents the sensitivity to absolute monetary gain, \u03b3 is sensitivity to deviation from fair share, and \u03b4 is sensitivity to inequality (similar to \u03b1 in Fehr-Schmidt). However, these parameters need to be learnable with bounds.\n\nBut maybe it's better to normalize the terms. For example, the deviation from fair share could be (split_self - F)/F to make it a percentage difference. Similarly, inequality could be (split_opp - split_self)/combined_earning.\n\nAlternatively, using the percentage variables provided: splitperc_self (proposed percentage) and the fair percentage would be (token_self / (token_self + token_opp)) * 100. Let's call this fair_perc. Then, the difference is (splitperc_self - fair_perc). This captures how much the proposed percentage deviates from the fair percentage.\n\nSo, the utility could be:\n\nU = \u03b2 * split_self + \u03b1 * (splitperc_self - fair_perc) + \u03b3 * (split_opp - split_self)\n\nBut split_opp is the opponent's proposed share, which in the responder's case is the proposer's share. Wait, when the participant is the responder, the proposer is the opponent, so split_opp is what the opponent proposes to keep for themselves, and split_self is what they offer to the participant.\n\nWait, the variables are:\n\n- split_opp: the proposed share of the opponent (in \u00a3)\n- split_self: the proposed share for the participant (in \u00a3)\n\nSo, in the responder role, the participant is being offered split_self, and the opponent would keep split_opp. The total combined_earning = split_self + split_opp.\n\nBut combined_earning is fixed as 3*(token_self + token_opp). So, the sum of split_self and split_opp must equal combined_earning. Therefore, split_opp = combined_earning - split_self. So, including both split_self and split_opp might be redundant since one is determined by the other. However, in the dataset, both are provided, so perhaps it's better to use split_self directly.\n\nBut for the model, since combined_earning is known, split_opp is redundant. So, the model can just use split_self and combined_earning.\n\nBut the fair share based on contributions is 3*token_self (since each token is \u00a33). So, the fair share for the participant is 3*token_self, and the opponent's fair share is 3*token_opp. The total fair would be 3*(token_self + token_opp) = combined_earning, which checks out.\n\nSo, the proposed split_self is compared to 3*token_self. The difference (split_self - 3*token_self) could be a term. If positive, the participant is getting more than their fair share; if negative, less.\n\nAdditionally, the model might consider the absolute amount offered (split_self) and the relative unfairness.\n\nSo, utility could be:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - 3*token_self) + \u03b3 * (split_self - split_opp)\n\nWait, but split_opp is combined_earning - split_self. So, split_self - split_opp = 2*split_self - combined_earning. Maybe that's a way to represent the inequality.\n\nAlternatively, using the percentage terms: splitperc_self is the percentage offered to the participant, and the fair percentage is (token_self / (token_self + token_opp)) * 100. The difference between these percentages could be a factor.\n\nLet me define fair_perc = (token_self / (token_self + token_opp)) * 100. Then, the proposed percentage is splitperc_self. The difference is (splitperc_self - fair_perc). If this is positive, the offer is more than fair; if negative, less.\n\nSo, the utility could be:\n\nU = \u03b2 * split_self + \u03b1 * (splitperc_self - fair_perc) + \u03b3 * (split_self - split_opp)\n\nBut split_self - split_opp = 2*split_self - combined_earning. Since combined_earning is fixed, this term represents how much more the participant is getting compared to the opponent. Alternatively, using the percentage difference.\n\nAlternatively, the model could use the difference between the proposed split and the fair split in monetary terms: (split_self - 3*token_self). This represents the deviation from the contribution-based fair share.\n\nSo, combining these elements:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - 3*token_self) + \u03b3 * (split_self - (combined_earning - split_self))\n\nSimplifying the third term: split_self - (combined_earning - split_self) = 2*split_self - combined_earning.\n\nBut combined_earning is 3*(token_self + token_opp), so:\n\n2*split_self - 3*(token_self + token_opp)\n\nThis might not be the best way. Alternatively, the third term could be the difference between the participant's share and the opponent's share: split_self - split_opp. But since split_opp = combined_earning - split_self, this becomes split_self - (combined_earning - split_self) = 2*split_self - combined_earning.\n\nAlternatively, the relative difference: (split_self - split_opp)/combined_earning. This normalizes the inequality by the total amount.\n\nSo, putting it all together:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - 3*token_self) + \u03b3 * ((split_self - split_opp)/combined_earning)\n\nBut split_opp is redundant because it's combined_earning - split_self. So, (split_self - (combined_earning - split_self))/combined_earning = (2*split_self - combined_earning)/combined_earning = 2*(split_self/combined_earning) - 1.\n\nAlternatively, using the percentage terms:\n\nU = \u03b2 * split_self + \u03b1 * (splitperc_self - fair_perc) + \u03b3 * (splitperc_self - splitperc_opp)\n\nBut splitperc_opp is 100 - splitperc_self, so this becomes splitperc_self - (100 - splitperc_self) = 2*splitperc_self - 100. Which is similar to the previous idea.\n\nAlternatively, the model could focus on two main factors: the monetary gain (split_self) and the deviation from the fair share (split_self - fair_share), where fair_share = 3*token_self.\n\nAdditionally, considering the trial_type, which indicates whether the participant contributed more, less, or equal. This might modulate the fairness perception. For example, if the participant contributed more (trial_type=3), they might expect a higher share, so the deviation from fair share would have a larger impact.\n\nBut including trial_type as a moderator would introduce interaction terms, which might complicate the model. Since the model needs to be simple for parameter recovery, perhaps it's better to avoid that unless necessary.\n\nSo, a parsimonious model might include:\n\n1. Monetary utility: split_self (\u03b2 parameter)\n2. Fairness utility: difference between proposed split and fair share (\u03b1 parameter)\n3. Inequality aversion: difference between self and opponent's share (\u03b3 parameter)\n\nBut to simplify, maybe combine fairness and inequality into one term. Alternatively, use the deviation from fair share as the primary fairness component.\n\nAnother approach is to model the utility as the proposed share minus the fair share, scaled by some parameter, plus the absolute share.\n\nWait, the utility function should be such that if U > 0, the participant is more likely to accept. So, the probability of accepting is logistic(U).\n\nLet me try this:\n\nU = \u03b2 * (split_self - \u03b8 * fair_share) + \u03b3 * (split_self - split_opp)\n\nWhere \u03b8 is a parameter that represents the weight of the fair share. But this might not be the best way.\n\nAlternatively, using a ratio:\n\nU = \u03b2 * (split_self / fair_share) + \u03b3 * (split_self - split_opp)\n\nBut normalizing by fair_share could help. If split_self is equal to fair_share, the ratio is 1. If it's higher, the ratio is >1, contributing positively.\n\nBut perhaps a linear combination is better for interpretability.\n\nGoing back to the Fehr-Schmidt model, which is a standard in inequity aversion:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequality (when opponent gets more), and \u03b2 is aversion to advantageous inequality (when participant gets more). However, in many cases, \u03b2 is found to be smaller than \u03b1.\n\nBut in this task, since the participant is the responder, and the proposer is the opponent, the focus might be on disadvantageous inequality. If the offer is less than fair, the participant might reject due to negative utility from unfairness.\n\nBut the Fehr-Schmidt model doesn't explicitly account for the contribution-based fairness, only the outcome inequality. So, perhaps modifying it to include contribution-based fairness.\n\nA hybrid model could be:\n\nU = split_self + \u03b1 * (split_self - fair_share) - \u03b2 * max(split_opp - split_self, 0)\n\nHere, \u03b1 represents the sensitivity to deviation from the fair share (could be positive or negative), and \u03b2 is the aversion to disadvantageous inequality.\n\nAlternatively, the fair_share term could replace the inequality term. For example:\n\nU = split_self + \u03b1 * (split_self - fair_share)\n\nIf split_self > fair_share, utility increases; if less, it decreases. This captures the idea that participants evaluate offers relative to their contribution.\n\nBut maybe participants also care about the absolute amount. So, the model needs to capture both absolute and relative aspects.\n\nAnother consideration is that the utility should be designed such that when U >= 0, the probability of accepting is >= 0.5. The logistic function converts U to a probability: P(accept) = 1 / (1 + exp(-U)).\n\nSo, the model needs to define U in terms of the variables, with learnable parameters.\n\nPutting this together, a possible model is:\n\nU = \u03b2 * (split_self - fair_share) + \u03b3 * split_self\n\nHere, \u03b2 captures the sensitivity to the difference between the offer and the fair share, and \u03b3 captures the sensitivity to the absolute offer. However, these two terms might be collinear since split_self is part of both. Alternatively, combining them:\n\nU = \u03b8 * split_self + \u03b1 * (split_self - fair_share)\n\nWhich simplifies to U = (\u03b8 + \u03b1) * split_self - \u03b1 * fair_share\n\nBut this might not be ideal because \u03b8 and \u03b1 could be confounded. Perhaps better to have one parameter for the absolute term and another for the relative term.\n\nAlternatively, U = \u03b2 * split_self + \u03b1 * (split_self / fair_share - 1)\n\nThis way, the second term is the percentage deviation from the fair share. If split_self equals fair_share, the term is zero. If it's higher, positive; lower, negative.\n\nBut this introduces a non-linear term. However, since the model is linear in parameters, it's manageable.\n\nAlternatively, using the percentage difference:\n\nU = \u03b2 * split_self + \u03b1 * ((split_self - fair_share) / fair_share)\n\nThis represents the proportional deviation from the fair share.\n\nBut perhaps the simplest way is to have:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - fair_share)\n\nWhich combines the absolute and relative terms. However, this can be rewritten as U = (\u03b2 + \u03b1) * split_self - \u03b1 * fair_share. This might make it hard to distinguish \u03b2 and \u03b1 unless there's variation in fair_share.\n\nAlternatively, since fair_share = 3 * token_self, we can substitute:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - 3 * token_self)\n\nThis way, the parameters \u03b2 and \u03b1 can be learned. If \u03b1 is positive, participants derive positive utility when the offer exceeds their contribution-based fair share. If \u03b1 is negative, offers below the fair share reduce utility.\n\nAdditionally, considering the trial_type, which indicates whether the participant contributed more, less, or equal. For example, if trial_type is 3 (participant more), the fair_share is higher, so the same offer might be perceived as less fair. But including trial_type as a moderator would require interaction terms, complicating the model. Since the model needs to be simple for parameter recovery, perhaps it's better to exclude it unless necessary.\n\nAnother factor is the total combined_earning. However, since split_self + split_opp = combined_earning, and combined_earning is fixed per trial, it's already accounted for in split_self and split_opp.\n\nWait, but the problem statement mentions that two equal split_self values don't represent the same fairness because combined_earning varies. For example, a split_self of \u00a310 when combined_earning is \u00a320 (50%) is different from split_self \u00a310 when combined_earning is \u00a330 (33.3%). So, the model should account for the proportion of the total, not just the absolute amount.\n\nThis suggests that the percentage terms (splitperc_self) might be more relevant than the absolute split_self. Alternatively, the ratio of split_self to combined_earning.\n\nBut the fair share percentage is (token_self / (token_self + token_opp)) * 100. The proposed splitperc_self is given. The difference between these percentages could be a key factor.\n\nSo, defining fair_perc = (token_self / (token_self + token_opp)) * 100. Then, the proposed splitperc_self minus fair_perc indicates how fair the offer is relative to contributions.\n\nThe utility could then be:\n\nU = \u03b2 * splitperc_self + \u03b1 * (splitperc_self - fair_perc)\n\nHere, \u03b2 captures the sensitivity to the percentage offered, and \u03b1 captures the sensitivity to the deviation from the fair percentage. If the proposed percentage is higher than fair, it adds to utility; if lower, it subtracts.\n\nAlternatively, using the absolute split_self and the percentage deviation:\n\nU = \u03b2 * split_self + \u03b1 * (splitperc_self - fair_perc)\n\nThis combines absolute and relative measures.\n\nBut perhaps using the percentage terms is better since the problem emphasizes that the same split_self can have different fairness based on combined_earning.\n\nAnother approach is to model the utility as a function of the offer's deviation from the fair share, scaled by the total amount. For example:\n\nU = (split_self - fair_share) / combined_earning + \u03b2 * split_self\n\nThis normalizes the deviation by the total, making it a proportion. But this might complicate interpretation.\n\nAlternatively, the utility could be:\n\nU = \u03b2 * (split_self / combined_earning) + \u03b1 * ((split_self - fair_share) / combined_earning)\n\nThis captures both the proportion of the total and the deviation from fair share, scaled by the total.\n\nBut again, this introduces more complexity.\n\nGiven the need for simplicity and parameter recovery, perhaps the best approach is to use the Fehr-Schmidt model augmented with the contribution-based fairness.\n\nSo, combining the two ideas:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) + \u03b2 * (split_self - fair_share)\n\nHere, \u03b1 is the aversion to disadvantageous inequality, and \u03b2 is the sensitivity to deviation from the fair share. If the offer is below the fair share, the term (split_self - fair_share) is negative, reducing utility.\n\nAlternatively, U = split_self + \u03b2 * (split_self - fair_share) - \u03b1 * max(split_opp - split_self, 0)\n\nThis way, the utility increases with the offer amount, increases if the offer is above the fair share, and decreases if the opponent is getting more (disadvantageous inequality).\n\nThis model has two parameters: \u03b1 and \u03b2. Both need to be learnable with appropriate bounds.\n\nThe probability of accepting is then logistic(U). The model needs to predict the binary 'accept' variable based on this utility.\n\nNow, considering parameter bounds. \u03b1 and \u03b2 should be positive because:\n\n- \u03b1: Aversion to disadvantageous inequality. Higher \u03b1 means more penalty when the opponent gets more.\n- \u03b2: Sensitivity to deviation from fair share. If \u03b2 is positive, offers above fair share increase utility; if negative, the opposite. But the problem states that the utility should be such that less than 0 utility means less than 50% chance to accept. So, if the model's U is designed such that higher values mean more likely to accept, then \u03b2 should be positive to reflect that getting more than fair increases acceptance likelihood.\n\nBut wait, in the current formulation, U = split_self + \u03b2*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nsplit_self is the absolute amount offered. \u03b2*(split_self - fair_share) adds to utility if the offer is above fair share. The max term subtracts \u03b1 times the disadvantageous inequality.\n\nSo, all terms are such that higher values increase utility. The parameters \u03b1 and \u03b2 should be positive.\n\nBut what if the participant cares about fairness in a different way? For example, if the offer is exactly fair, that's ideal. Offers above or below might be less desirable. But that would require a quadratic term, which complicates the model. Given the need for simplicity, linear terms are better.\n\nThus, the model would have:\n\nU = split_self + \u03b2*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nBut split_self - fair_share can be positive or negative. If \u03b2 is positive, then when the offer is above fair share, utility increases; when below, it decreases. This seems reasonable.\n\nAlternatively, to avoid collinearity between split_self and (split_self - fair_share), perhaps combine them:\n\nU = \u03b3*split_self + \u03b2*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nBut this would be equivalent to U = (\u03b3 + \u03b2)*split_self - \u03b2*fair_share - \u03b1*max(...). This might make \u03b3 and \u03b2 hard to distinguish. So, perhaps better to keep \u03b3 as 1 (fixed) and have \u03b2 and \u03b1 as learnable parameters. But the problem requires learnable parameters, so we can't fix \u03b3.\n\nAlternatively, the model can have:\n\nU = \u03b21*split_self + \u03b22*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nBut this introduces two parameters (\u03b21 and \u03b22) for the split_self terms, which might be redundant. Maybe better to combine them into a single parameter.\n\nAlternatively, U = \u03b2*split_self + \u03b1*(split_self - fair_share) - \u03b3*max(split_opp - split_self, 0)\n\nBut this has three parameters. However, the problem allows for multiple parameters as long as they are recoverable.\n\nBut the user mentioned that parameter recovery is roughly equal in priority to accuracy. So, the model should have a minimal number of parameters to ensure good recovery.\n\nGiven that, perhaps the best approach is to use two parameters: one for the deviation from fair share and one for disadvantageous inequality.\n\nSo, U = \u03b2*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nHere, \u03b2 represents the sensitivity to the offer relative to the fair share. If \u03b2 is positive, higher offers (relative to fair) increase utility. \u03b1 is the aversion to disadvantageous inequality.\n\nThis model has two parameters, which should be recoverable. The utility is then passed through a logistic function to get the probability of acceptance.\n\nNow, checking the variables:\n\n- split_self: from data\n- fair_share: calculated as 3*token_self\n- split_opp: from data, but since split_opp = combined_earning - split_self, and combined_earning is 3*(token_self + token_opp), it's redundant. However, the dataset includes split_opp, so we can use it directly.\n\nBut in the model, max(split_opp - split_self, 0) is the same as max((combined_earning - split_self) - split_self, 0) = max(combined_earning - 2*split_self, 0). But using split_opp directly is straightforward.\n\nSo, the model uses split_self, split_opp, and token_self (to compute fair_share).\n\nBut token_self is a given variable, so it's allowed.\n\nNow, specifying the mathematical model:\n\nU = \u03b2*(split_self - 3*token_self) - \u03b1*max(split_opp - split_self, 0)\n\nThis captures the deviation from the fair share and the disadvantageous inequality.\n\nThe parameters are \u03b2 and \u03b1, both learnable. They should have bounds. Since they represent sensitivity and aversion, they should be positive. Let's set bounds as [0, 5] for both, which is generous but finite.\n\nThe utility U can be any real number, which when passed through the logistic function, gives the probability of acceptance.\n\nNow, variable descriptions:\n\n- \u03b2: Sensitivity to deviation from fair share. Range [0, 5], learnable.\n- \u03b1: Aversion to disadvantageous inequality. Range [0, 5], learnable.\n- split_self: from data.\n- token_self: from data.\n- split_opp: from data.\n- max(split_opp - split_self, 0): calculated.\n\nThe target variable is 'accept'.\n\nThis model should predict the responder's decision based on the utility, which combines fairness and inequality aversion.\n</think>\n\n<MODEL>\nU = beta * (split_self - 3 * token_self) - alpha * max(split_opp - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Aversion to disadvantageous inequality (opponent receives more)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's contribution in tokens\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for opponent in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-component utility model combining contribution-based fairness evaluation and disadvantageous inequality aversion. The first term compares the offered amount to the participant's token-based fair share (3\u00d7token_self). The second term penalizes offers where the opponent receives more than the participant. Parameters control sensitivity to fair share deviations (beta) and inequality aversion strength (alpha).\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['beta', 'alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = beta * (split_self - 3 * token_self) - alpha * max(split_opp - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Aversion to disadvantageous inequality (opponent receives more)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's contribution in tokens\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for opponent in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, beta=1, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n\n        fair_share = 3 * token_self\n        inequality = max(split_opp - split_self, 0)\n        \n        utility = beta * (split_self - fair_share) - alpha * inequality\n        \n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"group1\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 6, \"token_self\": 6, \"combined_earning\": 36, \"split_opp\": 18.0, \"split_self\": 18.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 18.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 18.0}, {\"ID\": 1, \"group\": \"group1\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 6, \"token_self\": 6, \"combined_earning\": 36, \"split_opp\": 26.0, \"split_self\": 10.0, \"splitperc_opp\": 72, \"splitperc_self\": 28, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 28.0, \"proposed_amount\": 10.0}]'\n\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1743516759.228984
    }
  ],
  "learnable_parameters": {
    "beta": {
      "description": "Sensitivity to deviation from contribution-based fair share",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Aversion to disadvantageous inequality (opponent receives more)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = beta * (split_self - 3 * token_self) - alpha * max(split_opp - split_self, 0)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "beta": {
      "description": "Sensitivity to deviation from contribution-based fair share",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Aversion to disadvantageous inequality (opponent receives more)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Participant's contribution in tokens",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_opp": {
      "description": "Proposed share for opponent in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A dual-component utility model combining contribution-based fairness evaluation and disadvantageous inequality aversion. The first term compares the offered amount to the participant's token-based fair share (3\u00d7token_self). The second term penalizes offers where the opponent receives more than the participant. Parameters control sensitivity to fair share deviations (beta) and inequality aversion strength (alpha).",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible.\n\nFirst, I need to identify which variables are relevant for the responder's decision. The target variable is \"accept\", which is binary (1 for accept, 0 for reject). The model should predict this based on the utility of the proposal from the responder's perspective.\n\nKey variables to consider:\n- token_self and token_opp: These represent the contributions of the participant and opponent to the shared pot. Since combined_earning is the total money (each token is \u00a33), it's calculated as 3*(token_self + token_opp). But since combined_earning is already a variable, maybe we can use that directly.\n- split_self and split_opp: The proposed shares in \u00a3. The responder's share is split_self, and the opponent's is split_opp.\n- trial_type: Indicates if contributions are equal, opponent more, or participant more. This might affect fairness perceptions.\n- trial_role: We only consider when the participant is the responder (trial_role=1).\n\nThe utility function should capture the fairness of the split relative to contributions. A common approach is to model utility as a combination of monetary gain and inequity aversion. Inequity aversion suggests that people dislike unequal outcomes, even if it means sacrificing some personal gain.\n\nSo, the utility (U) could be the participant's monetary gain (split_self) minus some term that penalizes inequality. The inequality could be the difference between the participant's share and what's considered a fair share based on contributions.\n\nWhat's a fair share? If contributions matter, the fair split might be proportional to tokens contributed. The participant's fair share would be (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so the fair share simplifies to 3*token_self. However, the proposed split_self might differ from this.\n\nAlternatively, the fair share percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these could be a factor.\n\nBut the problem states that two equal split_self values don't represent the same fairness because combined_earning varies. So, the model should account for the proportion of the split relative to the contribution.\n\nLet me formalize this:\n\nFair share (F) = token_self / (token_self + token_opp) * combined_earning\nBut combined_earning = 3*(token_self + token_opp), so F = 3*token_self.\n\nThe proposed share is split_self. The difference between split_self and F could be a key factor. If split_self < F, the participant is getting less than their contribution, which might lead to rejection.\n\nBut inequity aversion also considers the opponent's share. Another approach is to use the Fehr-Schmidt model, where utility is:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequality (opponent gets more), and \u03b2 is aversion to advantageous inequality (participant gets more). But in this task, since the participant is the responder, maybe only disadvantageous inequality matters. Or both.\n\nHowever, the problem mentions that the model should predict based on the provided variables. The Fehr-Schmidt model uses split_self and split_opp, which are available.\n\nBut perhaps we need to incorporate the contribution-based fairness. Another way is to compute the fair split percentage as (token_self / (token_self + token_opp)) * 100, which is splitperc_self_fair = (token_self / (token_self + token_opp)) * 100. The proposed split percentage is splitperc_self. The difference (splitperc_self - splitperc_self_fair) could be a parameter. If this is negative, the participant is getting less than fair, which might decrease acceptance likelihood.\n\nAlternatively, the utility could be a linear combination of the monetary gain and the difference between the proposed split and the fair split.\n\nLet me try combining these ideas:\n\nUtility = \u03b8 * split_self + \u03b3 * (split_self - F) + \u03b4 * (split_opp - split_self)\n\nWhere F is the fair share (3*token_self). \u03b8 represents the sensitivity to absolute monetary gain, \u03b3 is sensitivity to deviation from fair share, and \u03b4 is sensitivity to inequality (similar to \u03b1 in Fehr-Schmidt). However, these parameters need to be learnable with bounds.\n\nBut maybe it's better to normalize the terms. For example, the deviation from fair share could be (split_self - F)/F to make it a percentage difference. Similarly, inequality could be (split_opp - split_self)/combined_earning.\n\nAlternatively, using the percentage variables provided: splitperc_self (proposed percentage) and the fair percentage would be (token_self / (token_self + token_opp)) * 100. Let's call this fair_perc. Then, the difference is (splitperc_self - fair_perc). This captures how much the proposed percentage deviates from the fair percentage.\n\nSo, the utility could be:\n\nU = \u03b2 * split_self + \u03b1 * (splitperc_self - fair_perc) + \u03b3 * (split_opp - split_self)\n\nBut split_opp is the opponent's proposed share, which in the responder's case is the proposer's share. Wait, when the participant is the responder, the proposer is the opponent, so split_opp is what the opponent proposes to keep for themselves, and split_self is what they offer to the participant.\n\nWait, the variables are:\n\n- split_opp: the proposed share of the opponent (in \u00a3)\n- split_self: the proposed share for the participant (in \u00a3)\n\nSo, in the responder role, the participant is being offered split_self, and the opponent would keep split_opp. The total combined_earning = split_self + split_opp.\n\nBut combined_earning is fixed as 3*(token_self + token_opp). So, the sum of split_self and split_opp must equal combined_earning. Therefore, split_opp = combined_earning - split_self. So, including both split_self and split_opp might be redundant since one is determined by the other. However, in the dataset, both are provided, so perhaps it's better to use split_self directly.\n\nBut for the model, since combined_earning is known, split_opp is redundant. So, the model can just use split_self and combined_earning.\n\nBut the fair share based on contributions is 3*token_self (since each token is \u00a33). So, the fair share for the participant is 3*token_self, and the opponent's fair share is 3*token_opp. The total fair would be 3*(token_self + token_opp) = combined_earning, which checks out.\n\nSo, the proposed split_self is compared to 3*token_self. The difference (split_self - 3*token_self) could be a term. If positive, the participant is getting more than their fair share; if negative, less.\n\nAdditionally, the model might consider the absolute amount offered (split_self) and the relative unfairness.\n\nSo, utility could be:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - 3*token_self) + \u03b3 * (split_self - split_opp)\n\nWait, but split_opp is combined_earning - split_self. So, split_self - split_opp = 2*split_self - combined_earning. Maybe that's a way to represent the inequality.\n\nAlternatively, using the percentage terms: splitperc_self is the percentage offered to the participant, and the fair percentage is (token_self / (token_self + token_opp)) * 100. The difference between these percentages could be a factor.\n\nLet me define fair_perc = (token_self / (token_self + token_opp)) * 100. Then, the proposed percentage is splitperc_self. The difference is (splitperc_self - fair_perc). If this is positive, the offer is more than fair; if negative, less.\n\nSo, the utility could be:\n\nU = \u03b2 * split_self + \u03b1 * (splitperc_self - fair_perc) + \u03b3 * (split_self - split_opp)\n\nBut split_self - split_opp = 2*split_self - combined_earning. Since combined_earning is fixed, this term represents how much more the participant is getting compared to the opponent. Alternatively, using the percentage difference.\n\nAlternatively, the model could use the difference between the proposed split and the fair split in monetary terms: (split_self - 3*token_self). This represents the deviation from the contribution-based fair share.\n\nSo, combining these elements:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - 3*token_self) + \u03b3 * (split_self - (combined_earning - split_self))\n\nSimplifying the third term: split_self - (combined_earning - split_self) = 2*split_self - combined_earning.\n\nBut combined_earning is 3*(token_self + token_opp), so:\n\n2*split_self - 3*(token_self + token_opp)\n\nThis might not be the best way. Alternatively, the third term could be the difference between the participant's share and the opponent's share: split_self - split_opp. But since split_opp = combined_earning - split_self, this becomes split_self - (combined_earning - split_self) = 2*split_self - combined_earning.\n\nAlternatively, the relative difference: (split_self - split_opp)/combined_earning. This normalizes the inequality by the total amount.\n\nSo, putting it all together:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - 3*token_self) + \u03b3 * ((split_self - split_opp)/combined_earning)\n\nBut split_opp is redundant because it's combined_earning - split_self. So, (split_self - (combined_earning - split_self))/combined_earning = (2*split_self - combined_earning)/combined_earning = 2*(split_self/combined_earning) - 1.\n\nAlternatively, using the percentage terms:\n\nU = \u03b2 * split_self + \u03b1 * (splitperc_self - fair_perc) + \u03b3 * (splitperc_self - splitperc_opp)\n\nBut splitperc_opp is 100 - splitperc_self, so this becomes splitperc_self - (100 - splitperc_self) = 2*splitperc_self - 100. Which is similar to the previous idea.\n\nAlternatively, the model could focus on two main factors: the monetary gain (split_self) and the deviation from the fair share (split_self - fair_share), where fair_share = 3*token_self.\n\nAdditionally, considering the trial_type, which indicates whether the participant contributed more, less, or equal. This might modulate the fairness perception. For example, if the participant contributed more (trial_type=3), they might expect a higher share, so the deviation from fair share would have a larger impact.\n\nBut including trial_type as a moderator would introduce interaction terms, which might complicate the model. Since the model needs to be simple for parameter recovery, perhaps it's better to avoid that unless necessary.\n\nSo, a parsimonious model might include:\n\n1. Monetary utility: split_self (\u03b2 parameter)\n2. Fairness utility: difference between proposed split and fair share (\u03b1 parameter)\n3. Inequality aversion: difference between self and opponent's share (\u03b3 parameter)\n\nBut to simplify, maybe combine fairness and inequality into one term. Alternatively, use the deviation from fair share as the primary fairness component.\n\nAnother approach is to model the utility as the proposed share minus the fair share, scaled by some parameter, plus the absolute share.\n\nWait, the utility function should be such that if U > 0, the participant is more likely to accept. So, the probability of accepting is logistic(U).\n\nLet me try this:\n\nU = \u03b2 * (split_self - \u03b8 * fair_share) + \u03b3 * (split_self - split_opp)\n\nWhere \u03b8 is a parameter that represents the weight of the fair share. But this might not be the best way.\n\nAlternatively, using a ratio:\n\nU = \u03b2 * (split_self / fair_share) + \u03b3 * (split_self - split_opp)\n\nBut normalizing by fair_share could help. If split_self is equal to fair_share, the ratio is 1. If it's higher, the ratio is >1, contributing positively.\n\nBut perhaps a linear combination is better for interpretability.\n\nGoing back to the Fehr-Schmidt model, which is a standard in inequity aversion:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequality (when opponent gets more), and \u03b2 is aversion to advantageous inequality (when participant gets more). However, in many cases, \u03b2 is found to be smaller than \u03b1.\n\nBut in this task, since the participant is the responder, and the proposer is the opponent, the focus might be on disadvantageous inequality. If the offer is less than fair, the participant might reject due to negative utility from unfairness.\n\nBut the Fehr-Schmidt model doesn't explicitly account for the contribution-based fairness, only the outcome inequality. So, perhaps modifying it to include contribution-based fairness.\n\nA hybrid model could be:\n\nU = split_self + \u03b1 * (split_self - fair_share) - \u03b2 * max(split_opp - split_self, 0)\n\nHere, \u03b1 represents the sensitivity to deviation from the fair share (could be positive or negative), and \u03b2 is the aversion to disadvantageous inequality.\n\nAlternatively, the fair_share term could replace the inequality term. For example:\n\nU = split_self + \u03b1 * (split_self - fair_share)\n\nIf split_self > fair_share, utility increases; if less, it decreases. This captures the idea that participants evaluate offers relative to their contribution.\n\nBut maybe participants also care about the absolute amount. So, the model needs to capture both absolute and relative aspects.\n\nAnother consideration is that the utility should be designed such that when U >= 0, the probability of accepting is >= 0.5. The logistic function converts U to a probability: P(accept) = 1 / (1 + exp(-U)).\n\nSo, the model needs to define U in terms of the variables, with learnable parameters.\n\nPutting this together, a possible model is:\n\nU = \u03b2 * (split_self - fair_share) + \u03b3 * split_self\n\nHere, \u03b2 captures the sensitivity to the difference between the offer and the fair share, and \u03b3 captures the sensitivity to the absolute offer. However, these two terms might be collinear since split_self is part of both. Alternatively, combining them:\n\nU = \u03b8 * split_self + \u03b1 * (split_self - fair_share)\n\nWhich simplifies to U = (\u03b8 + \u03b1) * split_self - \u03b1 * fair_share\n\nBut this might not be ideal because \u03b8 and \u03b1 could be confounded. Perhaps better to have one parameter for the absolute term and another for the relative term.\n\nAlternatively, U = \u03b2 * split_self + \u03b1 * (split_self / fair_share - 1)\n\nThis way, the second term is the percentage deviation from the fair share. If split_self equals fair_share, the term is zero. If it's higher, positive; lower, negative.\n\nBut this introduces a non-linear term. However, since the model is linear in parameters, it's manageable.\n\nAlternatively, using the percentage difference:\n\nU = \u03b2 * split_self + \u03b1 * ((split_self - fair_share) / fair_share)\n\nThis represents the proportional deviation from the fair share.\n\nBut perhaps the simplest way is to have:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - fair_share)\n\nWhich combines the absolute and relative terms. However, this can be rewritten as U = (\u03b2 + \u03b1) * split_self - \u03b1 * fair_share. This might make it hard to distinguish \u03b2 and \u03b1 unless there's variation in fair_share.\n\nAlternatively, since fair_share = 3 * token_self, we can substitute:\n\nU = \u03b2 * split_self + \u03b1 * (split_self - 3 * token_self)\n\nThis way, the parameters \u03b2 and \u03b1 can be learned. If \u03b1 is positive, participants derive positive utility when the offer exceeds their contribution-based fair share. If \u03b1 is negative, offers below the fair share reduce utility.\n\nAdditionally, considering the trial_type, which indicates whether the participant contributed more, less, or equal. For example, if trial_type is 3 (participant more), the fair_share is higher, so the same offer might be perceived as less fair. But including trial_type as a moderator would require interaction terms, complicating the model. Since the model needs to be simple for parameter recovery, perhaps it's better to exclude it unless necessary.\n\nAnother factor is the total combined_earning. However, since split_self + split_opp = combined_earning, and combined_earning is fixed per trial, it's already accounted for in split_self and split_opp.\n\nWait, but the problem statement mentions that two equal split_self values don't represent the same fairness because combined_earning varies. For example, a split_self of \u00a310 when combined_earning is \u00a320 (50%) is different from split_self \u00a310 when combined_earning is \u00a330 (33.3%). So, the model should account for the proportion of the total, not just the absolute amount.\n\nThis suggests that the percentage terms (splitperc_self) might be more relevant than the absolute split_self. Alternatively, the ratio of split_self to combined_earning.\n\nBut the fair share percentage is (token_self / (token_self + token_opp)) * 100. The proposed splitperc_self is given. The difference between these percentages could be a key factor.\n\nSo, defining fair_perc = (token_self / (token_self + token_opp)) * 100. Then, the proposed splitperc_self minus fair_perc indicates how fair the offer is relative to contributions.\n\nThe utility could then be:\n\nU = \u03b2 * splitperc_self + \u03b1 * (splitperc_self - fair_perc)\n\nHere, \u03b2 captures the sensitivity to the percentage offered, and \u03b1 captures the sensitivity to the deviation from the fair percentage. If the proposed percentage is higher than fair, it adds to utility; if lower, it subtracts.\n\nAlternatively, using the absolute split_self and the percentage deviation:\n\nU = \u03b2 * split_self + \u03b1 * (splitperc_self - fair_perc)\n\nThis combines absolute and relative measures.\n\nBut perhaps using the percentage terms is better since the problem emphasizes that the same split_self can have different fairness based on combined_earning.\n\nAnother approach is to model the utility as a function of the offer's deviation from the fair share, scaled by the total amount. For example:\n\nU = (split_self - fair_share) / combined_earning + \u03b2 * split_self\n\nThis normalizes the deviation by the total, making it a proportion. But this might complicate interpretation.\n\nAlternatively, the utility could be:\n\nU = \u03b2 * (split_self / combined_earning) + \u03b1 * ((split_self - fair_share) / combined_earning)\n\nThis captures both the proportion of the total and the deviation from fair share, scaled by the total.\n\nBut again, this introduces more complexity.\n\nGiven the need for simplicity and parameter recovery, perhaps the best approach is to use the Fehr-Schmidt model augmented with the contribution-based fairness.\n\nSo, combining the two ideas:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) + \u03b2 * (split_self - fair_share)\n\nHere, \u03b1 is the aversion to disadvantageous inequality, and \u03b2 is the sensitivity to deviation from the fair share. If the offer is below the fair share, the term (split_self - fair_share) is negative, reducing utility.\n\nAlternatively, U = split_self + \u03b2 * (split_self - fair_share) - \u03b1 * max(split_opp - split_self, 0)\n\nThis way, the utility increases with the offer amount, increases if the offer is above the fair share, and decreases if the opponent is getting more (disadvantageous inequality).\n\nThis model has two parameters: \u03b1 and \u03b2. Both need to be learnable with appropriate bounds.\n\nThe probability of accepting is then logistic(U). The model needs to predict the binary 'accept' variable based on this utility.\n\nNow, considering parameter bounds. \u03b1 and \u03b2 should be positive because:\n\n- \u03b1: Aversion to disadvantageous inequality. Higher \u03b1 means more penalty when the opponent gets more.\n- \u03b2: Sensitivity to deviation from fair share. If \u03b2 is positive, offers above fair share increase utility; if negative, the opposite. But the problem states that the utility should be such that less than 0 utility means less than 50% chance to accept. So, if the model's U is designed such that higher values mean more likely to accept, then \u03b2 should be positive to reflect that getting more than fair increases acceptance likelihood.\n\nBut wait, in the current formulation, U = split_self + \u03b2*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nsplit_self is the absolute amount offered. \u03b2*(split_self - fair_share) adds to utility if the offer is above fair share. The max term subtracts \u03b1 times the disadvantageous inequality.\n\nSo, all terms are such that higher values increase utility. The parameters \u03b1 and \u03b2 should be positive.\n\nBut what if the participant cares about fairness in a different way? For example, if the offer is exactly fair, that's ideal. Offers above or below might be less desirable. But that would require a quadratic term, which complicates the model. Given the need for simplicity, linear terms are better.\n\nThus, the model would have:\n\nU = split_self + \u03b2*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nBut split_self - fair_share can be positive or negative. If \u03b2 is positive, then when the offer is above fair share, utility increases; when below, it decreases. This seems reasonable.\n\nAlternatively, to avoid collinearity between split_self and (split_self - fair_share), perhaps combine them:\n\nU = \u03b3*split_self + \u03b2*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nBut this would be equivalent to U = (\u03b3 + \u03b2)*split_self - \u03b2*fair_share - \u03b1*max(...). This might make \u03b3 and \u03b2 hard to distinguish. So, perhaps better to keep \u03b3 as 1 (fixed) and have \u03b2 and \u03b1 as learnable parameters. But the problem requires learnable parameters, so we can't fix \u03b3.\n\nAlternatively, the model can have:\n\nU = \u03b21*split_self + \u03b22*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nBut this introduces two parameters (\u03b21 and \u03b22) for the split_self terms, which might be redundant. Maybe better to combine them into a single parameter.\n\nAlternatively, U = \u03b2*split_self + \u03b1*(split_self - fair_share) - \u03b3*max(split_opp - split_self, 0)\n\nBut this has three parameters. However, the problem allows for multiple parameters as long as they are recoverable.\n\nBut the user mentioned that parameter recovery is roughly equal in priority to accuracy. So, the model should have a minimal number of parameters to ensure good recovery.\n\nGiven that, perhaps the best approach is to use two parameters: one for the deviation from fair share and one for disadvantageous inequality.\n\nSo, U = \u03b2*(split_self - fair_share) - \u03b1*max(split_opp - split_self, 0)\n\nHere, \u03b2 represents the sensitivity to the offer relative to the fair share. If \u03b2 is positive, higher offers (relative to fair) increase utility. \u03b1 is the aversion to disadvantageous inequality.\n\nThis model has two parameters, which should be recoverable. The utility is then passed through a logistic function to get the probability of acceptance.\n\nNow, checking the variables:\n\n- split_self: from data\n- fair_share: calculated as 3*token_self\n- split_opp: from data, but since split_opp = combined_earning - split_self, and combined_earning is 3*(token_self + token_opp), it's redundant. However, the dataset includes split_opp, so we can use it directly.\n\nBut in the model, max(split_opp - split_self, 0) is the same as max((combined_earning - split_self) - split_self, 0) = max(combined_earning - 2*split_self, 0). But using split_opp directly is straightforward.\n\nSo, the model uses split_self, split_opp, and token_self (to compute fair_share).\n\nBut token_self is a given variable, so it's allowed.\n\nNow, specifying the mathematical model:\n\nU = \u03b2*(split_self - 3*token_self) - \u03b1*max(split_opp - split_self, 0)\n\nThis captures the deviation from the fair share and the disadvantageous inequality.\n\nThe parameters are \u03b2 and \u03b1, both learnable. They should have bounds. Since they represent sensitivity and aversion, they should be positive. Let's set bounds as [0, 5] for both, which is generous but finite.\n\nThe utility U can be any real number, which when passed through the logistic function, gives the probability of acceptance.\n\nNow, variable descriptions:\n\n- \u03b2: Sensitivity to deviation from fair share. Range [0, 5], learnable.\n- \u03b1: Aversion to disadvantageous inequality. Range [0, 5], learnable.\n- split_self: from data.\n- token_self: from data.\n- split_opp: from data.\n- max(split_opp - split_self, 0): calculated.\n\nThe target variable is 'accept'.\n\nThis model should predict the responder's decision based on the utility, which combines fairness and inequality aversion.\n</think>\n\n<MODEL>\nU = beta * (split_self - 3 * token_self) - alpha * max(split_opp - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Aversion to disadvantageous inequality (opponent receives more)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's contribution in tokens\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for opponent in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-component utility model combining contribution-based fairness evaluation and disadvantageous inequality aversion. The first term compares the offered amount to the participant's token-based fair share (3\u00d7token_self). The second term penalizes offers where the opponent receives more than the participant. Parameters control sensitivity to fair share deviations (beta) and inequality aversion strength (alpha).\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = beta * (split_self - 3 * token_self) - alpha * max(split_opp - split_self, 0)\nSummary: A dual-component utility model combining contribution-based fairness evaluation and disadvantageous inequality aversion. The first term compares the offered amount to the participant's token-based fair share (3\u00d7token_self). The second term penalizes offers where the opponent receives more than the participant. Parameters control sensitivity to fair share deviations (beta) and inequality aversion strength (alpha).\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, beta=1, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n\n        fair_share = 3 * token_self\n        inequality = max(split_opp - split_self, 0)\n        \n        utility = beta * (split_self - fair_share) - alpha * inequality\n        \n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.594017094017094,
  "fitting_results": [
    {
      "beta": 0.0,
      "alpha": 5.0,
      "success": true,
      "log_likelihood": -2.0794415377798354,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.02627006626525309,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -22.431437028314363,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.029298129938702427,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -21.931647438521804,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.03771447785034531,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -20.480649975897734,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.04988068917054338,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -18.394471712456212,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.011434263543546938,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.402921678448955,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.009604451849874188,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.5606655741006,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.054699972989950205,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -17.61093021952364,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.048118665449341624,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -18.688423734478132,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 1.959564154819538,
      "success": true,
      "log_likelihood": -25.105292467881668,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.05418784485722375,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -17.69259515688311,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.930584403526622,
      "alpha": 2.8573665653314246,
      "success": true,
      "log_likelihood": -668.442804216552,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.19444444444444445,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 4.005254223913676,
      "success": true,
      "log_likelihood": -761.9325222291019,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.027777777777777776,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.023439231227766096,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -22.87855188619049,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.023318547136526677,
      "success": true,
      "log_likelihood": -19.462811335077802,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.01803501060020438,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -23.654360845309988,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.03739158330746964,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -20.53697905843707,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.014455878216089471,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.092498574857654,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.01220969487548644,
      "alpha": 0.004803091014055299,
      "success": true,
      "log_likelihood": -23.289172609100266,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.11510952029713595,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -10.884737753753837,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 4.818306378440827,
      "success": true,
      "log_likelihood": -2.0794415377798354,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.01809406479757653,
      "success": true,
      "log_likelihood": -21.19329758463164,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.030451221401253153,
      "alpha": 0.007100613906367959,
      "success": true,
      "log_likelihood": -19.55055000738815,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.2925076556459922e-07,
      "alpha": 0.0006085723280370024,
      "success": true,
      "log_likelihood": -24.94782174505417,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 4.717844515948445,
      "success": true,
      "log_likelihood": -508.6481619986701,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3611111111111111,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.352927747698492,
      "alpha": 4.32748619877912,
      "success": true,
      "log_likelihood": -369.1067620575072,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.004824017895388339,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.852215956672577,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.013688058926881122,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.176925959469898,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.09152644521279588,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -12.87820423001542,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.009045203972888431,
      "success": true,
      "log_likelihood": -23.82780626146957,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 4.884515948176244,
      "success": true,
      "log_likelihood": -439.5706092085487,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.000604751715116998,
      "alpha": 0.013669491664101802,
      "success": true,
      "log_likelihood": -22.898552734595427,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.027044865565755673,
      "alpha": 0.002903667049658176,
      "success": true,
      "log_likelihood": -21.45567404181441,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.057935945682713194,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -17.10428691054779,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.002135286540489234,
      "success": true,
      "log_likelihood": -24.88612128886767,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.07225064895785839,
      "success": true,
      "log_likelihood": -8.379935298535036,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.005449805058461176,
      "alpha": 0.00662220181271618,
      "success": true,
      "log_likelihood": -24.097678523668343,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3333333333333333,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.00030424128369246483,
      "success": true,
      "log_likelihood": -24.951929424078198,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.039033959802984984,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -20.25042969006156,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.08177536621397033,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -13.915564895374443,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.08634562587534234,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -13.411480246839028,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 4.986381636156903,
      "success": true,
      "log_likelihood": -25.10529246782031,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.006046417874429514,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.795150532979736,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.03200257569086726,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -21.472117164157023,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.6137780596423124,
      "alpha": 1.9486656193556602,
      "success": true,
      "log_likelihood": -23.71899801676104,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.2008492899606495,
      "alpha": 2.8794092793107198,
      "success": true,
      "log_likelihood": -323.055014031204,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.05550841826623156,
      "alpha": 0.022703884663805074,
      "success": true,
      "log_likelihood": -12.047299707749307,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.011835991728432426,
      "alpha": 0.00282154786458645,
      "success": true,
      "log_likelihood": -23.818981210426298,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3333333333333333,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.512467203825874e-07,
      "alpha": 0.005903460156946029,
      "success": true,
      "log_likelihood": -24.453851615810333,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 4.5728503494853365,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.3862965609526277,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.08223450172236808,
      "success": true,
      "log_likelihood": -7.225706986508839,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.021206333330410863,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -23.21326213895949,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.178219184501284e-07,
      "alpha": 0.00612018545552453,
      "success": true,
      "log_likelihood": -24.417779779890456,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.004650793127572686,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.859322493517375,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 5.0,
      "success": true,
      "log_likelihood": -25.10529246782033,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 5.0,
      "success": true,
      "log_likelihood": -2.0794415377798354,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.59954832778252e-07,
      "alpha": 0.005580553787734245,
      "success": true,
      "log_likelihood": -24.505530457455446,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.07225060868844944,
      "success": true,
      "log_likelihood": -8.37993529853377,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.02298422933015168,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -22.94818574542964,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.08807565261651085,
      "alpha": 0.08361975473330627,
      "success": true,
      "log_likelihood": -4.334243059678733,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.07673297443776866,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -14.509544313686897,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.02226800596904391,
      "alpha": 0.0010709149160919049,
      "success": true,
      "log_likelihood": -23.370986214087495,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.9052070953848763e-16,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -24.953298492958048,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "beta": {
        "r": 0.3041499530812834,
        "p": 0.0020945148273886246
      },
      "alpha": {
        "r": 0.41823944900215587,
        "p": 1.4909430196511632e-05
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 113.96229617869788,
  "bic_results": {
    "average_bic": 113.96229617869788,
    "individual_bics": [
      11.32592095247189,
      52.029911933540944,
      51.030332753955825,
      48.128337828707686,
      43.95598130182464,
      55.97288123381013,
      57.073634862828236,
      56.28836902511342,
      57.073634862828236,
      42.3888983159595,
      44.54388534586848,
      57.377622812675554,
      42.55222819067844,
      57.073634862828236,
      57.073634862828236,
      57.073634862828236,
      57.073634862828236,
      1344.0526463100161,
      57.073634862828236,
      1531.032082335116,
      52.9241416492932,
      57.073634862828236,
      46.09266054706782,
      54.475759567532194,
      48.240995993786356,
      55.352035026627526,
      53.74538309511275,
      28.936513384419897,
      57.073634862828236,
      57.073634862828236,
      57.073634862828236,
      11.32592095247189,
      49.5536330461755,
      46.26813789168852,
      57.073634862828236,
      57.06268136702056,
      1024.4633618742525,
      745.3805619919266,
      56.87146979025737,
      57.073634862828236,
      55.520889795852014,
      32.92344633694306,
      57.073634862828236,
      54.822650399851355,
      886.3082562940097,
      57.073634862828236,
      52.96414334610307,
      50.07838596054104,
      41.375611698007795,
      56.93928045464756,
      57.073634862828236,
      23.926908473982294,
      55.3623949242489,
      57.070896725068614,
      47.667897257035335,
      34.99816766766111,
      33.98999837059028,
      57.073634862828236,
      57.377622812552836,
      57.073634862828236,
      56.75733894287169,
      50.111272205226264,
      54.6050339104343,
      653.2770659393202,
      31.261637292410832,
      54.80500029776481,
      57.073634862828236,
      57.073634862828236,
      56.07474110853288,
      57.073634862828236,
      9.939630998817474,
      21.618451849929897,
      53.5935621548312,
      57.073634862828236,
      56.00259743669313,
      56.88568286394697,
      57.073634862828236,
      57.37762281255288,
      57.073634862828236,
      11.32592095247189,
      56.17809879182311,
      57.073634862828236,
      23.926908473979758,
      53.0634093677715,
      57.073634862828236,
      15.835523996269686,
      57.073634862828236,
      36.186126504286015,
      57.073634862828236,
      53.90901030508721,
      57.073634862828314
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- beta: r = 0.304\n- alpha: r = 0.418",
  "previous_models": [
    "Specification: U = beta * (split_self - 3 * token_self) - alpha * max(split_opp - split_self, 0)\nSummary: A dual-component utility model combining contribution-based fairness evaluation and disadvantageous inequality aversion. The first term compares the offered amount to the participant's token-based fair share (3\u00d7token_self). The second term penalizes offers where the opponent receives more than the participant. Parameters control sensitivity to fair share deviations (beta) and inequality aversion strength (alpha).\nTarget Variable: accept\nPrediction Type: utility\nBIC: 113.96229617869788\nOverall Accuracy: 0.5940\n\nParameter Recovery:\n- beta: r = 0.304\n- alpha: r = 0.418"
  ]
}