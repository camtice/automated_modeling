{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). The model should predict utility, which will then be converted to a probability via a logistic function. Also, parameter recovery is important, so we need to keep the number of learnable parameters manageable to avoid overfitting and high covariance.\n\nFirst, let's think about the key factors influencing a responder's decision. The main variables provided are token_self (participant's contribution), token_opp (opponent's contribution), split_self (proposed share in \u00a3), split_opp (opponent's proposed share), and combined_earning. The trial_type indicates whether contributions are equal, opponent more, or participant more. \n\nThe responder's decision likely depends on fairness considerations. But fairness here isn't just about the split percentages (splitperc_self and splitperc_opp) because the combined_earning varies. For example, a 50% split when both contributed equally might be acceptable, but if the combined amount is low, maybe not. Alternatively, if the participant contributed more tokens, they might expect a larger share.\n\nSo, the utility function should compare the proposed split to some notion of a fair share. The fair share could be based on the participant's contribution relative to the total. For instance, if the participant contributed token_self tokens out of (token_self + token_opp), their fair share proportion would be token_self / (token_self + token_opp). The proposed split (splitperc_self) is given, so the difference between the proposed and fair share could be a key factor.\n\nAnother aspect is the absolute amount offered. Even if the split is unfair, a higher absolute amount might still be accepted. So, the model might need parameters to weigh both the proportional fairness and the absolute offer.\n\nLet's outline possible components:\n\n1. Fairness Component: The difference between the proposed percentage (splitperc_self) and the fair percentage based on contribution. Let's call this \u0394fair = splitperc_self - (token_self / (token_self + token_opp)) * 100. If \u0394fair is positive, the offer is more than fair; if negative, less. This could be multiplied by a parameter \u03b1 to represent sensitivity to fairness.\n\n2. Absolute Offer Component: The actual amount offered (split_self) could be weighted by a parameter \u03b2. People might accept higher amounts even if unfair.\n\n3. Disadvantageous Inequality Aversion: If the participant's split is less than the opponent's, they might have negative utility. But splitperc_opp is the opponent's proposed percentage. Wait, in the responder role, the split_opp is the opponent's proposed share (since the proposer is the opponent). So, when the participant is responder, the proposer is the opponent, so split_opp is what the opponent is taking, and split_self is what's offered to the participant. So, the split percentages sum to 100. So, if splitperc_self is 40%, splitperc_opp is 60%. The participant might compare their split to the opponent's. If the opponent is taking more, that could be seen as unfair. Alternatively, the participant might compare their split to their contribution-based fair share.\n\nWait, the fair share based on contribution is (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), since each token is \u00a33. So combined_earning = 3*(token_self + token_opp). Therefore, the fair share in \u00a3 would be 3*token_self. Because each token is \u00a33, so participant's contribution is worth 3*token_self, opponent's is 3*token_opp. The total is 3*(token_self + token_opp). So, the fair split for the participant would be 3*token_self, and for the opponent 3*token_opp. But the proposed split_self is the amount offered to the participant. So, the difference between split_self and 3*token_self could be a measure of fairness. If split_self is less than 3*token_self, that's unfair (participant is getting less than their contribution), which might lead to rejection. Alternatively, the ratio or percentage difference.\n\nAlternatively, the fair percentage would be (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these two percentages could be a key factor. Let's call this delta_fair = splitperc_self - fair_percent. If delta_fair is positive, the offer is more than fair; if negative, less. The utility could be influenced by this delta, scaled by a parameter.\n\nAnother angle is the concept of inequity aversion, where people dislike unequal outcomes. The model could include terms for advantageous and disadvantageous inequality. For example, if the participant's share is less than the opponent's, that's disadvantageous inequality, which reduces utility. But in this setup, when the participant is the responder, the split is proposed by the opponent, so the opponent's share is split_opp. But since combined_earning is fixed, split_self + split_opp = combined_earning. So, if split_self is low, split_opp is high. The participant might compare their split to the opponent's. For example, if split_self < split_opp, that's disadvantageous, leading to lower utility. But maybe the participant's own contribution (token_self) vs the opponent's (token_opp) also matters. For instance, if the participant contributed more, they might expect a higher split, so even if split_self is higher than split_opp, if it's not proportional to their contribution, they might reject.\n\nThis suggests that the model should account for both the difference between the proposed split and the fair split based on contribution, and possibly the absolute amount.\n\nLet me formalize this. The fair share based on contribution is 3*token_self. The proposed share is split_self. The difference is split_self - 3*token_self. If this is positive, it's a surplus; if negative, a deficit. The utility could be a function of this difference. But maybe participants have different sensitivities to surplus and deficit. For example, a parameter \u03b1 that scales the deficit (if split_self < 3*token_self) and another parameter \u03b2 that scales the surplus. But that might complicate the model with more parameters. Alternatively, a single parameter that scales the difference.\n\nAlternatively, the ratio of split_self to fair share: split_self / (3*token_self). If this is 1, it's fair. If less than 1, unfair. Taking the natural log of this ratio could be a term, but that might not be intuitive. Alternatively, the difference as a proportion: (split_self - 3*token_self) / (3*token_self). This represents the relative gain or loss compared to the fair share.\n\nBut perhaps a simpler approach is to model the utility as a linear combination of the difference between the proposed split and the fair share, plus the absolute amount offered. For example:\n\nUtility = \u03b1*(split_self - 3*token_self) + \u03b2*split_self\n\nHere, \u03b1 represents the sensitivity to fairness (difference from fair share), and \u03b2 represents the sensitivity to the absolute amount. If \u03b1 is positive, participants prefer offers that are more than their fair share; if negative, they might reject even if above fair share (unlikely). Wait, but if split_self is less than fair share, the term (split_self - fair) is negative, so \u03b1 being positive would penalize that. So \u03b1 should be positive, so that higher (more fair) offers increase utility. The \u03b2 term would capture the idea that even if the offer is slightly unfair, a higher absolute amount might still be acceptable. For example, if split_self is \u00a35 when fair is \u00a36, but \u00a35 is still a decent amount, the \u03b2 term could make the utility positive.\n\nBut maybe the fair share isn't exactly 3*token_self. Perhaps participants have a different notion of fairness, such as equal split (50/50) regardless of contribution. But the trial_type variable indicates whether contributions are equal, opponent more, or participant more. So maybe the model should account for different fairness norms based on trial_type. However, trial_type is a categorical variable (1,2,3), which might complicate the model. Alternatively, the model could use the actual contributions (token_self and token_opp) to compute a fair share, which would be more continuous.\n\nAnother consideration is the total combined_earning. For example, in a high combined_earning scenario, a small percentage difference could mean a larger absolute difference. But the model already includes split_self, which is in \u00a3, so the absolute amount is considered.\n\nWait, combined_earning is 3*(token_self + token_opp), so split_self is part of that. So split_self is the actual amount offered, and splitperc_self is the percentage. But since combined_earning varies, the same percentage can correspond to different absolute amounts. However, the model should use variables that are present in the dataset. The variables available include split_self (amount), splitperc_self (percentage), token_self, token_opp, etc.\n\nBut according to the problem statement, the model should utilize the given variables appropriately but not necessarily all. So perhaps using split_self and the fair share based on tokens.\n\nLet me think again. The fair share in \u00a3 is 3*token_self. The offered amount is split_self. The difference is split_self - 3*token_self. Let's call this 'fair_diff' = split_self - 3*token_self. If fair_diff is positive, the offer is more than fair; if negative, less. The utility could be a function of fair_diff. Additionally, the absolute amount split_self might matter. For example, even if the offer is unfair (fair_diff negative), a higher split_self might still be acceptable. So the utility could be:\n\nUtility = \u03b1 * fair_diff + \u03b2 * split_self\n\nBut this could lead to multicollinearity if fair_diff and split_self are correlated. Let's see: split_self = fair_diff + 3*token_self. So split_self is directly related to fair_diff plus 3*token_self. So including both might introduce collinearity. Alternatively, maybe just using fair_diff and 3*token_self. Wait, but 3*token_self is the fair share. So split_self = fair_share + fair_diff. So including both fair_diff and split_self would be redundant. Therefore, perhaps the model should include either fair_diff or split_self, but not both. Alternatively, combining them in a different way.\n\nAlternatively, the utility could be modeled as:\n\nUtility = \u03b1 * (split_self / (3*token_self)) + \u03b2 * split_self\n\nHere, the first term is the ratio of the offered amount to the fair share. If this ratio is 1, it's fair. If it's greater than 1, it's generous. The second term is the absolute amount. But this might still have issues. For example, if token_self is zero (though in the dataset, token_self is the participant's contribution, which I assume is at least 1, but maybe not). Also, if token_self is zero, this term becomes undefined, but the dataset probably doesn't have that.\n\nAlternatively, the utility could be:\n\nUtility = \u03b1 * (split_self - \u03b8 * 3*token_self) + \u03b2 * split_self\n\nHere, \u03b8 is a parameter that represents the proportion of the fair share that the participant expects. For example, if \u03b8=1, they expect exactly their contribution. If \u03b8=0.5, they might accept half of their contribution. But \u03b8 could be a learnable parameter. However, this introduces another parameter. Let's see: \u03b8 would determine the reference point. So the difference is split_self - \u03b8*(3*token_self). Then \u03b1 scales this difference, and \u03b2 scales the absolute split_self. But this adds \u03b8 as a parameter. However, the problem states that the model should have a small number of learnable parameters. Let's count: \u03b1, \u03b2, \u03b8. That's three parameters. Maybe that's manageable, but perhaps we can simplify.\n\nAlternatively, maybe \u03b8 is fixed at 1, assuming participants expect their fair share. Then the model would have \u03b1 and \u03b2 as parameters. But perhaps participants have different fairness thresholds, so \u03b8 should be learnable. Let's consider that.\n\nAnother angle: Inequity aversion as in Fehr & Schmidt (1999), where utility depends on both advantageous and disadvantageous inequality. In their model, utility is:\n\nU = x - \u03b1 * max{y - x, 0} - \u03b2 * max{x - y, 0}\n\nWhere x is the participant's payoff, y is the other's payoff. But in this case, when the participant is the responder, x is split_self, and y is split_opp. But split_opp = combined_earning - split_self. So y = combined_earning - split_self. So the inequality terms would be:\n\ndisadvantageous = max( (combined_earning - split_self) - split_self, 0 ) = max( combined_earning - 2*split_self, 0 )\n\nadvantageous = max( split_self - (combined_earning - split_self), 0 ) = max( 2*split_self - combined_earning, 0 )\n\nBut Fehr & Schmidt's model uses \u03b1 and \u03b2 as parameters for disadvantageous and advantageous inequality, respectively. However, in this task, the participant is only deciding to accept or reject, so their utility would be based on their own payoff and the inequality. If they reject, they both get zero. So the utility of accepting is U_accept = split_self - \u03b1 * max( (split_opp - split_self), 0 ) - \u03b2 * max( (split_self - split_opp), 0 )\n\nBut split_opp = combined_earning - split_self. So:\n\nU_accept = split_self - \u03b1 * max( (combined_earning - 2*split_self), 0 ) - \u03b2 * max( (2*split_self - combined_earning), 0 )\n\nBut this model has two parameters, \u03b1 and \u03b2. However, in the responder's decision, rejecting gives both players zero. So the utility of rejecting is U_reject = 0. The participant accepts if U_accept > U_reject. But in the Fehr & Schmidt model, the utility function is linear in own payoff and the inequality terms. However, this model might not account for the contribution-based fairness, only the outcome inequality.\n\nIn the current task, the contributions (tokens) determine the total combined_earning, but the split is proposed by the opponent. The participant's sense of fairness might depend on both the outcome inequality (split_self vs split_opp) and the contribution-based fairness (split_self vs 3*token_self). So combining both aspects might be necessary.\n\nPerhaps the utility function should include both terms: one for outcome inequality and one for contribution-based fairness. For example:\n\nUtility = \u03b3*(split_self - 3*token_self) + \u03b1*max( (split_opp - split_self), 0 ) + \u03b2*max( (split_self - split_opp), 0 )\n\nHere, \u03b3 captures sensitivity to contribution-based fairness, \u03b1 to disadvantageous inequality (opponent gets more), and \u03b2 to advantageous inequality (participant gets more). But this introduces three parameters. However, the problem states a slight bias towards fewer parameters. Maybe we can simplify. For example, if we assume that participants are only sensitive to disadvantageous inequality (\u03b1) and contribution-based fairness (\u03b3), and not to advantageous inequality (\u03b2=0). That reduces to two parameters.\n\nAlternatively, perhaps the contribution-based fairness is the main driver, and outcome inequality is less important. Or vice versa. It's a modeling choice.\n\nAnother approach is to consider that the participant's utility is a function of how much they get compared to their fair share, and how much the opponent gets compared to their fair share. For example:\n\nfair_self = 3*token_self\n\nfair_opp = 3*token_opp\n\nThen, the utility could be:\n\nUtility = split_self + \u03b7*(split_opp - fair_opp)\n\nHere, \u03b7 is a parameter indicating concern for the opponent's fairness. If \u03b7 is positive, the participant cares if the opponent is getting more than their fair share (which might be negative for the participant's utility). But this might not capture the rejection motive well.\n\nAlternatively, the participant might compare their split to both their own fair share and the opponent's split. For example:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*(split_opp - fair_opp)\n\nBut split_opp = combined_earning - split_self, and fair_opp = 3*token_opp. So this can be rewritten in terms of split_self and the tokens. However, this introduces two parameters, \u03b1 and \u03b2.\n\nBut given that combined_earning = 3*(token_self + token_opp), fair_self + fair_opp = 3*(token_self + token_opp) = combined_earning. So split_self + split_opp = combined_earning. Therefore, split_opp - fair_opp = (combined_earning - split_self) - (3*token_opp) = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self = fair_self - split_self. So split_opp - fair_opp = -(split_self - fair_self). Therefore, the utility becomes:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*(- (split_self - fair_self)) = (\u03b1 - \u03b2)*(split_self - fair_self)\n\nThis simplifies to a single parameter (\u03b1 - \u03b2), which would not allow distinguishing between the two. So this approach might not be useful.\n\nBack to the earlier idea: combining contribution-based fairness and outcome inequality. Let's try a model with two parameters: one for the sensitivity to the difference between the offered amount and the fair share (\u03b1), and another for the sensitivity to the absolute offer (\u03b2). So:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*split_self\n\nThis can be rewritten as:\n\nUtility = (\u03b1 + \u03b2)*split_self - \u03b1*fair_self\n\nBut fair_self is 3*token_self, which is a data variable. So the model would have two learnable parameters: \u03b1 and \u03b2. However, this might lead to identifiability issues if \u03b1 and \u03b2 are highly correlated. For example, increasing \u03b1 and decreasing \u03b2 could result in similar utilities. To avoid this, perhaps one parameter is sufficient. For instance, if we fix \u03b2=0, then the model only has \u03b1, but that might not capture the effect of absolute offers. Alternatively, combine the two into a single parameter that balances the two effects.\n\nAlternatively, use a single parameter that represents the trade-off between fairness and absolute gain. For example:\n\nUtility = \u03b1*(split_self - fair_self) + (1 - \u03b1)*split_self\n\nBut this constrains the parameters, which might not be ideal. Alternatively, use a non-linear combination, but that complicates the model.\n\nAnother angle: the probability of accepting is influenced by how much the offer exceeds the participant's minimum acceptable threshold. The threshold could be a function of their fair share. For example, the threshold could be \u03b8*fair_self, where \u03b8 is a parameter between 0 and 1. If the offer is above \u03b8*fair_self, the participant is more likely to accept. The utility could be the difference between the offer and the threshold:\n\nUtility = split_self - \u03b8*fair_self\n\nThen, the probability of accepting is logistic(Utility). If Utility > 0, the probability is greater than 0.5. This model has one learnable parameter, \u03b8. However, this assumes that the threshold is a fixed proportion of the fair share. But participants might have varying thresholds based on other factors, like the absolute amount or the opponent's share.\n\nAlternatively, the threshold could be a combination of a proportion of the fair share and a fixed amount. For example:\n\nUtility = split_self - (\u03b8*fair_self + \u03b3)\n\nHere, \u03b8 and \u03b3 are learnable parameters. This allows the threshold to be a linear function of the fair share plus a constant. But this adds two parameters. However, with 36 instances per participant, having two parameters might be manageable for parameter recovery.\n\nBut the problem states that parameter recovery is a priority, so we need to balance model complexity. Let's consider starting with a simple model and see if it can be expanded.\n\nAnother consideration is the role of trial_type. For example, in trials where the participant contributed more (trial_type=3), they might have a higher threshold. But including trial_type would require interaction terms or separate parameters for each trial type, which complicates the model. Since trial_type is derived from token_self and token_opp, perhaps the model can capture the same information through the token variables, avoiding the need for trial_type as a separate variable.\n\nPutting this all together, a plausible model is one where the utility is a function of the offer's deviation from the fair share and the absolute offer. To keep the number of parameters low, perhaps use two parameters: one for the fairness deviation and one for the absolute offer. However, to avoid collinearity, maybe combine them in a way that each parameter captures a distinct aspect.\n\nWait, let's think about the variables:\n\n- split_self: the amount offered to the participant.\n- fair_self: 3*token_self (the fair share based on contribution).\n- split_opp: combined_earning - split_self.\n\nThe utility could be modeled as:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*split_self + \u03b3*(split_opp - fair_opp)\n\nBut split_opp - fair_opp = (combined_earning - split_self) - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self = fair_self - split_self. So this term is the negative of (split_self - fair_self). Therefore, the third term becomes \u03b3*(fair_self - split_self). So the utility is:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*split_self + \u03b3*(fair_self - split_self)\n= (\u03b1 - \u03b3)*(split_self - fair_self) + \u03b2*split_self\n\nThis again combines parameters, leading to potential identifiability issues. So perhaps this approach isn't helpful.\n\nAlternative approach: Use the ratio of the offer to the fair share. For example:\n\nUtility = \u03b1*(split_self / fair_self) + \u03b2*split_self\n\nIf split_self equals fair_self, the ratio is 1. If it's higher, the ratio is greater than 1. This captures proportional fairness. The \u03b2 term captures the absolute amount. But again, this has two parameters.\n\nAlternatively, use a single parameter that combines both effects. For example:\n\nUtility = split_self + \u03b1*(split_self - fair_self)\n\nThis can be rewritten as:\n\nUtility = (1 + \u03b1)*split_self - \u03b1*fair_self\n\nHere, \u03b1 represents the weight given to the fairness deviation relative to the absolute offer. If \u03b1 is positive, the participant cares about getting more than their fair share; if negative, they penalize deviations below the fair share more. This model has one learnable parameter, \u03b1. However, this might not capture cases where the absolute amount matters independently of fairness. For example, even if the offer is below fair_self, a higher split_self might still be acceptable. But in this model, the utility increases with split_self regardless of \u03b1. So if \u03b1 is negative, the term (split_self - fair_self) could reduce utility when split_self < fair_self. Wait, let's see:\n\nIf split_self < fair_self, then (split_self - fair_self) is negative. If \u03b1 is positive, then \u03b1*(split_self - fair_self) is negative, reducing utility. If \u03b1 is negative, then multiplying by a negative makes it positive, increasing utility. That doesn't make sense. So perhaps \u03b1 should be constrained to be positive. Then, when the offer is below fair_self, the fairness term reduces utility, and when above, increases it. The absolute term split_self always adds to utility. So this model combines both the absolute offer and the fairness deviation with a single parameter. This might be a good balance between simplicity and capturing both effects.\n\nBut does this model allow for cases where a higher absolute offer can offset a lower fairness? For example, if split_self is slightly below fair_self but is a large amount, the utility could still be positive. Let's see:\n\nSuppose fair_self = 6 (token_self=2), split_self=5. Then split_self - fair_self = -1. Utility = 5 + \u03b1*(-1). If \u03b1=2, utility is 5 -2 =3. If \u03b1=0.5, utility is 5 -0.5=4.5. Wait, but in this case, even though the offer is below fair_self, the utility is still positive because split_self is 5. But if \u03b1 is large enough, say \u03b1=6, then utility=5 -6= -1, which would predict rejection. So the model allows for both possibilities based on the value of \u03b1. A participant with high \u03b1 is very sensitive to fairness deviations, while a low \u03b1 is more influenced by the absolute amount.\n\nThis seems plausible. The model has one learnable parameter, \u03b1, which is the sensitivity to the fairness deviation. The utility is split_self plus \u03b1 times the deviation from fair_self. The probability of accepting is then logistic(Utility). Since the logistic function converts utility to a probability between 0 and 1, the model's prediction for 'accept' is 1 if the probability is above 0.5, which corresponds to Utility > 0.\n\nBut wait, the problem states that the utility should be such that less than 0 corresponds to less than 50% probability. That's exactly what the logistic function does: P(accept) = 1 / (1 + exp(-Utility)). So when Utility < 0, P(accept) < 0.5.\n\nThis model seems to meet the requirements. It uses split_self and token_self (to compute fair_self) from the dataset. The only learnable parameter is \u03b1, which can be bounded between, say, -10 and 10 to allow for sufficient flexibility but avoiding infinity. However, since \u03b1 represents the sensitivity to fairness, we might expect it to be positive (penalizing offers below fair_self). But to allow for the possibility that some participants might have a negative \u03b1 (though it's unclear what that would mean), we can set a generous bound like -20 to 20.\n\nWait, but if \u03b1 is negative, then the term \u03b1*(split_self - fair_self) would increase utility when split_self < fair_self. That would mean that participants with negative \u03b1 are more likely to accept offers that are below their fair share, which seems counterintuitive. So perhaps \u03b1 should be constrained to be non-negative. In that case, the bounds for \u03b1 would be [0, upper_bound]. Let's say 0 to 20. This would make sense: \u03b1 >= 0, so offers below fair_self reduce utility, and offers above increase it.\n\nBut the problem states that bounds should be generous and not necessarily restricted unless there's a theoretical reason. If we allow \u03b1 to be negative, the model can capture cases where participants are more accepting of offers below their fair share as \u03b1 becomes more negative, which might not make sense. So perhaps it's better to set \u03b1's lower bound to 0. This way, \u03b1 can only penalize or reward based on the fairness deviation.\n\nSo, the model would be:\n\nUtility = split_self + \u03b1*(split_self - fair_self)\n\nBut fair_self is 3*token_self. So:\n\nUtility = split_self + \u03b1*(split_self - 3*token_self)\n\nThis can be rewritten as:\n\nUtility = (1 + \u03b1)*split_self - 3*\u03b1*token_self\n\nBut this is equivalent to the original expression. The parameter \u03b1 is learnable, with \u03b1 >= 0.\n\nThis model has one learnable parameter, \u03b1, which is the weight given to the deviation from the fair share. The utility increases with the split_self and with the extent to which split_self exceeds the fair share. If split_self is below the fair share, the second term reduces utility. The higher \u03b1 is, the more the participant cares about fairness relative to the absolute amount.\n\nThis model is simple, has one parameter, and should be identifiable. It uses variables available in the dataset (split_self and token_self). It also accounts for both the absolute offer and the fairness based on contribution.\n\nNow, checking if this model can be extended or if there are other variables to consider. For example, the opponent's token (token_opp) might influence the participant's perception of fairness. If the opponent contributed more, maybe the participant is more lenient. But in the current model, the fair_self is based on the participant's own contribution. Including token_opp would require another parameter. For example:\n\nUtility = split_self + \u03b1*(split_self - 3*token_self) + \u03b3*(token_opp - token_self)\n\nHere, \u03b3 captures the effect of the opponent's contribution relative to the participant's. If \u03b3 is positive, when the opponent contributed more (token_opp > token_self), the utility increases, making acceptance more likely. But this adds another parameter, \u03b3. However, with 36 trials per participant, adding another parameter might be feasible, but the problem suggests a slight bias towards fewer parameters. So perhaps stick with one parameter.\n\nAlternatively, the trial_type variable indicates whether contributions are equal, opponent more, or participant more. This could be used to modulate \u03b1. For example, different alphas for each trial type. But that would require three parameters, which complicates the model. Given the need for parameter recovery, it's better to avoid that.\n\nAnother consideration: the utility might include a term for the total combined_earning. For example, participants might be more generous when the total is higher. But combined_earning is 3*(token_self + token_opp), which is already determined by token_self and token_opp. Including it might not add new information.\n\nIn conclusion, the proposed model is:\n\nUtility = split_self + \u03b1*(split_self - 3*token_self)\n\nThis model uses split_self and token_self from the dataset. The learnable parameter \u03b1 has a lower bound of 0 (since negative values would imply that participants prefer offers below their fair share) and an upper bound that's generous, say 20. The utility is then passed through a logistic function to determine the probability of acceptance.\n\nNow, let's formalize this in the required format.\n\nThe mathematical model would be:\n\nU = split_self + \u03b1 * (split_self - 3 * token_self)\n\nWhich can be simplified as:\n\nU = split_self * (1 + \u03b1) - 3 * \u03b1 * token_self\n\nBut to keep it in terms of the original variables, we'll present it as:\n\nU = split_self + \u03b1 * (split_self - 3 * token_self)\n\nVariables:\n\n- split_self: from the data, the proposed share for the participant.\n- token_self: from the data, the participant's contribution.\n- \u03b1: learnable parameter, sensitivity to fairness deviation.\n\nThe target variable is 'accept'.\n\nFor the variable descriptions:\n\n- \u03b1: learnable, range from 0 to 20 (inclusive).\n- split_self: data variable.\n- token_self: data variable.\n\nThe utility U is calculated, and the probability of acceptance is 1 / (1 + exp(-U)).\n\nThis model should be able to predict responder behavior with one learnable parameter, which is good for parameter recovery. It incorporates both the absolute offer and the deviation from the fair share based on contribution.\n</think>\n\n<MODEL>\nU = split_self + alpha * (split_self - 3 * token_self)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model combining absolute offer value and deviation from contribution-based fairness. The utility increases with the proposed amount (split_self) and is adjusted by a learnable parameter (\u03b1) scaling the difference between the offer and the participant's fair share (3 * token_self). The model predicts acceptance probability via logistic transformation of the utility, capturing sensitivity to both absolute gains and fairness relative to contribution.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = split_self + alpha * (split_self - 3 * token_self)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = split_self + alpha * (split_self - 3 * token_self)\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n",
      "timestamp": 1743517563.7807722
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair share (3 * token_self)",
      "range": {
        "min": 0,
        "max": 20,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = split_self + alpha * (split_self - 3 * token_self)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair share (3 * token_self)",
      "range": {
        "min": 0,
        "max": 20,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model combining absolute offer value and deviation from contribution-based fairness. The utility increases with the proposed amount (split_self) and is adjusted by a learnable parameter (\u03b1) scaling the difference between the offer and the participant's fair share (3 * token_self). The model predicts acceptance probability via logistic transformation of the utility, capturing sensitivity to both absolute gains and fairness relative to contribution.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, so we'll only consider trials where trial_role is 1 (responder). The target variable is 'accept', which is binary (1 or 0). The model should predict utility, which will then be converted to a probability via a logistic function. Also, parameter recovery is important, so we need to keep the number of learnable parameters manageable to avoid overfitting and high covariance.\n\nFirst, let's think about the key factors influencing a responder's decision. The main variables provided are token_self (participant's contribution), token_opp (opponent's contribution), split_self (proposed share in \u00a3), split_opp (opponent's proposed share), and combined_earning. The trial_type indicates whether contributions are equal, opponent more, or participant more. \n\nThe responder's decision likely depends on fairness considerations. But fairness here isn't just about the split percentages (splitperc_self and splitperc_opp) because the combined_earning varies. For example, a 50% split when both contributed equally might be acceptable, but if the combined amount is low, maybe not. Alternatively, if the participant contributed more tokens, they might expect a larger share.\n\nSo, the utility function should compare the proposed split to some notion of a fair share. The fair share could be based on the participant's contribution relative to the total. For instance, if the participant contributed token_self tokens out of (token_self + token_opp), their fair share proportion would be token_self / (token_self + token_opp). The proposed split (splitperc_self) is given, so the difference between the proposed and fair share could be a key factor.\n\nAnother aspect is the absolute amount offered. Even if the split is unfair, a higher absolute amount might still be accepted. So, the model might need parameters to weigh both the proportional fairness and the absolute offer.\n\nLet's outline possible components:\n\n1. Fairness Component: The difference between the proposed percentage (splitperc_self) and the fair percentage based on contribution. Let's call this \u0394fair = splitperc_self - (token_self / (token_self + token_opp)) * 100. If \u0394fair is positive, the offer is more than fair; if negative, less. This could be multiplied by a parameter \u03b1 to represent sensitivity to fairness.\n\n2. Absolute Offer Component: The actual amount offered (split_self) could be weighted by a parameter \u03b2. People might accept higher amounts even if unfair.\n\n3. Disadvantageous Inequality Aversion: If the participant's split is less than the opponent's, they might have negative utility. But splitperc_opp is the opponent's proposed percentage. Wait, in the responder role, the split_opp is the opponent's proposed share (since the proposer is the opponent). So, when the participant is responder, the proposer is the opponent, so split_opp is what the opponent is taking, and split_self is what's offered to the participant. So, the split percentages sum to 100. So, if splitperc_self is 40%, splitperc_opp is 60%. The participant might compare their split to the opponent's. If the opponent is taking more, that could be seen as unfair. Alternatively, the participant might compare their split to their contribution-based fair share.\n\nWait, the fair share based on contribution is (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), since each token is \u00a33. So combined_earning = 3*(token_self + token_opp). Therefore, the fair share in \u00a3 would be 3*token_self. Because each token is \u00a33, so participant's contribution is worth 3*token_self, opponent's is 3*token_opp. The total is 3*(token_self + token_opp). So, the fair split for the participant would be 3*token_self, and for the opponent 3*token_opp. But the proposed split_self is the amount offered to the participant. So, the difference between split_self and 3*token_self could be a measure of fairness. If split_self is less than 3*token_self, that's unfair (participant is getting less than their contribution), which might lead to rejection. Alternatively, the ratio or percentage difference.\n\nAlternatively, the fair percentage would be (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these two percentages could be a key factor. Let's call this delta_fair = splitperc_self - fair_percent. If delta_fair is positive, the offer is more than fair; if negative, less. The utility could be influenced by this delta, scaled by a parameter.\n\nAnother angle is the concept of inequity aversion, where people dislike unequal outcomes. The model could include terms for advantageous and disadvantageous inequality. For example, if the participant's share is less than the opponent's, that's disadvantageous inequality, which reduces utility. But in this setup, when the participant is the responder, the split is proposed by the opponent, so the opponent's share is split_opp. But since combined_earning is fixed, split_self + split_opp = combined_earning. So, if split_self is low, split_opp is high. The participant might compare their split to the opponent's. For example, if split_self < split_opp, that's disadvantageous, leading to lower utility. But maybe the participant's own contribution (token_self) vs the opponent's (token_opp) also matters. For instance, if the participant contributed more, they might expect a higher split, so even if split_self is higher than split_opp, if it's not proportional to their contribution, they might reject.\n\nThis suggests that the model should account for both the difference between the proposed split and the fair split based on contribution, and possibly the absolute amount.\n\nLet me formalize this. The fair share based on contribution is 3*token_self. The proposed share is split_self. The difference is split_self - 3*token_self. If this is positive, it's a surplus; if negative, a deficit. The utility could be a function of this difference. But maybe participants have different sensitivities to surplus and deficit. For example, a parameter \u03b1 that scales the deficit (if split_self < 3*token_self) and another parameter \u03b2 that scales the surplus. But that might complicate the model with more parameters. Alternatively, a single parameter that scales the difference.\n\nAlternatively, the ratio of split_self to fair share: split_self / (3*token_self). If this is 1, it's fair. If less than 1, unfair. Taking the natural log of this ratio could be a term, but that might not be intuitive. Alternatively, the difference as a proportion: (split_self - 3*token_self) / (3*token_self). This represents the relative gain or loss compared to the fair share.\n\nBut perhaps a simpler approach is to model the utility as a linear combination of the difference between the proposed split and the fair share, plus the absolute amount offered. For example:\n\nUtility = \u03b1*(split_self - 3*token_self) + \u03b2*split_self\n\nHere, \u03b1 represents the sensitivity to fairness (difference from fair share), and \u03b2 represents the sensitivity to the absolute amount. If \u03b1 is positive, participants prefer offers that are more than their fair share; if negative, they might reject even if above fair share (unlikely). Wait, but if split_self is less than fair share, the term (split_self - fair) is negative, so \u03b1 being positive would penalize that. So \u03b1 should be positive, so that higher (more fair) offers increase utility. The \u03b2 term would capture the idea that even if the offer is slightly unfair, a higher absolute amount might still be acceptable. For example, if split_self is \u00a35 when fair is \u00a36, but \u00a35 is still a decent amount, the \u03b2 term could make the utility positive.\n\nBut maybe the fair share isn't exactly 3*token_self. Perhaps participants have a different notion of fairness, such as equal split (50/50) regardless of contribution. But the trial_type variable indicates whether contributions are equal, opponent more, or participant more. So maybe the model should account for different fairness norms based on trial_type. However, trial_type is a categorical variable (1,2,3), which might complicate the model. Alternatively, the model could use the actual contributions (token_self and token_opp) to compute a fair share, which would be more continuous.\n\nAnother consideration is the total combined_earning. For example, in a high combined_earning scenario, a small percentage difference could mean a larger absolute difference. But the model already includes split_self, which is in \u00a3, so the absolute amount is considered.\n\nWait, combined_earning is 3*(token_self + token_opp), so split_self is part of that. So split_self is the actual amount offered, and splitperc_self is the percentage. But since combined_earning varies, the same percentage can correspond to different absolute amounts. However, the model should use variables that are present in the dataset. The variables available include split_self (amount), splitperc_self (percentage), token_self, token_opp, etc.\n\nBut according to the problem statement, the model should utilize the given variables appropriately but not necessarily all. So perhaps using split_self and the fair share based on tokens.\n\nLet me think again. The fair share in \u00a3 is 3*token_self. The offered amount is split_self. The difference is split_self - 3*token_self. Let's call this 'fair_diff' = split_self - 3*token_self. If fair_diff is positive, the offer is more than fair; if negative, less. The utility could be a function of fair_diff. Additionally, the absolute amount split_self might matter. For example, even if the offer is unfair (fair_diff negative), a higher split_self might still be acceptable. So the utility could be:\n\nUtility = \u03b1 * fair_diff + \u03b2 * split_self\n\nBut this could lead to multicollinearity if fair_diff and split_self are correlated. Let's see: split_self = fair_diff + 3*token_self. So split_self is directly related to fair_diff plus 3*token_self. So including both might introduce collinearity. Alternatively, maybe just using fair_diff and 3*token_self. Wait, but 3*token_self is the fair share. So split_self = fair_share + fair_diff. So including both fair_diff and split_self would be redundant. Therefore, perhaps the model should include either fair_diff or split_self, but not both. Alternatively, combining them in a different way.\n\nAlternatively, the utility could be modeled as:\n\nUtility = \u03b1 * (split_self / (3*token_self)) + \u03b2 * split_self\n\nHere, the first term is the ratio of the offered amount to the fair share. If this ratio is 1, it's fair. If it's greater than 1, it's generous. The second term is the absolute amount. But this might still have issues. For example, if token_self is zero (though in the dataset, token_self is the participant's contribution, which I assume is at least 1, but maybe not). Also, if token_self is zero, this term becomes undefined, but the dataset probably doesn't have that.\n\nAlternatively, the utility could be:\n\nUtility = \u03b1 * (split_self - \u03b8 * 3*token_self) + \u03b2 * split_self\n\nHere, \u03b8 is a parameter that represents the proportion of the fair share that the participant expects. For example, if \u03b8=1, they expect exactly their contribution. If \u03b8=0.5, they might accept half of their contribution. But \u03b8 could be a learnable parameter. However, this introduces another parameter. Let's see: \u03b8 would determine the reference point. So the difference is split_self - \u03b8*(3*token_self). Then \u03b1 scales this difference, and \u03b2 scales the absolute split_self. But this adds \u03b8 as a parameter. However, the problem states that the model should have a small number of learnable parameters. Let's count: \u03b1, \u03b2, \u03b8. That's three parameters. Maybe that's manageable, but perhaps we can simplify.\n\nAlternatively, maybe \u03b8 is fixed at 1, assuming participants expect their fair share. Then the model would have \u03b1 and \u03b2 as parameters. But perhaps participants have different fairness thresholds, so \u03b8 should be learnable. Let's consider that.\n\nAnother angle: Inequity aversion as in Fehr & Schmidt (1999), where utility depends on both advantageous and disadvantageous inequality. In their model, utility is:\n\nU = x - \u03b1 * max{y - x, 0} - \u03b2 * max{x - y, 0}\n\nWhere x is the participant's payoff, y is the other's payoff. But in this case, when the participant is the responder, x is split_self, and y is split_opp. But split_opp = combined_earning - split_self. So y = combined_earning - split_self. So the inequality terms would be:\n\ndisadvantageous = max( (combined_earning - split_self) - split_self, 0 ) = max( combined_earning - 2*split_self, 0 )\n\nadvantageous = max( split_self - (combined_earning - split_self), 0 ) = max( 2*split_self - combined_earning, 0 )\n\nBut Fehr & Schmidt's model uses \u03b1 and \u03b2 as parameters for disadvantageous and advantageous inequality, respectively. However, in this task, the participant is only deciding to accept or reject, so their utility would be based on their own payoff and the inequality. If they reject, they both get zero. So the utility of accepting is U_accept = split_self - \u03b1 * max( (split_opp - split_self), 0 ) - \u03b2 * max( (split_self - split_opp), 0 )\n\nBut split_opp = combined_earning - split_self. So:\n\nU_accept = split_self - \u03b1 * max( (combined_earning - 2*split_self), 0 ) - \u03b2 * max( (2*split_self - combined_earning), 0 )\n\nBut this model has two parameters, \u03b1 and \u03b2. However, in the responder's decision, rejecting gives both players zero. So the utility of rejecting is U_reject = 0. The participant accepts if U_accept > U_reject. But in the Fehr & Schmidt model, the utility function is linear in own payoff and the inequality terms. However, this model might not account for the contribution-based fairness, only the outcome inequality.\n\nIn the current task, the contributions (tokens) determine the total combined_earning, but the split is proposed by the opponent. The participant's sense of fairness might depend on both the outcome inequality (split_self vs split_opp) and the contribution-based fairness (split_self vs 3*token_self). So combining both aspects might be necessary.\n\nPerhaps the utility function should include both terms: one for outcome inequality and one for contribution-based fairness. For example:\n\nUtility = \u03b3*(split_self - 3*token_self) + \u03b1*max( (split_opp - split_self), 0 ) + \u03b2*max( (split_self - split_opp), 0 )\n\nHere, \u03b3 captures sensitivity to contribution-based fairness, \u03b1 to disadvantageous inequality (opponent gets more), and \u03b2 to advantageous inequality (participant gets more). But this introduces three parameters. However, the problem states a slight bias towards fewer parameters. Maybe we can simplify. For example, if we assume that participants are only sensitive to disadvantageous inequality (\u03b1) and contribution-based fairness (\u03b3), and not to advantageous inequality (\u03b2=0). That reduces to two parameters.\n\nAlternatively, perhaps the contribution-based fairness is the main driver, and outcome inequality is less important. Or vice versa. It's a modeling choice.\n\nAnother approach is to consider that the participant's utility is a function of how much they get compared to their fair share, and how much the opponent gets compared to their fair share. For example:\n\nfair_self = 3*token_self\n\nfair_opp = 3*token_opp\n\nThen, the utility could be:\n\nUtility = split_self + \u03b7*(split_opp - fair_opp)\n\nHere, \u03b7 is a parameter indicating concern for the opponent's fairness. If \u03b7 is positive, the participant cares if the opponent is getting more than their fair share (which might be negative for the participant's utility). But this might not capture the rejection motive well.\n\nAlternatively, the participant might compare their split to both their own fair share and the opponent's split. For example:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*(split_opp - fair_opp)\n\nBut split_opp = combined_earning - split_self, and fair_opp = 3*token_opp. So this can be rewritten in terms of split_self and the tokens. However, this introduces two parameters, \u03b1 and \u03b2.\n\nBut given that combined_earning = 3*(token_self + token_opp), fair_self + fair_opp = 3*(token_self + token_opp) = combined_earning. So split_self + split_opp = combined_earning. Therefore, split_opp - fair_opp = (combined_earning - split_self) - (3*token_opp) = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self = fair_self - split_self. So split_opp - fair_opp = -(split_self - fair_self). Therefore, the utility becomes:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*(- (split_self - fair_self)) = (\u03b1 - \u03b2)*(split_self - fair_self)\n\nThis simplifies to a single parameter (\u03b1 - \u03b2), which would not allow distinguishing between the two. So this approach might not be useful.\n\nBack to the earlier idea: combining contribution-based fairness and outcome inequality. Let's try a model with two parameters: one for the sensitivity to the difference between the offered amount and the fair share (\u03b1), and another for the sensitivity to the absolute offer (\u03b2). So:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*split_self\n\nThis can be rewritten as:\n\nUtility = (\u03b1 + \u03b2)*split_self - \u03b1*fair_self\n\nBut fair_self is 3*token_self, which is a data variable. So the model would have two learnable parameters: \u03b1 and \u03b2. However, this might lead to identifiability issues if \u03b1 and \u03b2 are highly correlated. For example, increasing \u03b1 and decreasing \u03b2 could result in similar utilities. To avoid this, perhaps one parameter is sufficient. For instance, if we fix \u03b2=0, then the model only has \u03b1, but that might not capture the effect of absolute offers. Alternatively, combine the two into a single parameter that balances the two effects.\n\nAlternatively, use a single parameter that represents the trade-off between fairness and absolute gain. For example:\n\nUtility = \u03b1*(split_self - fair_self) + (1 - \u03b1)*split_self\n\nBut this constrains the parameters, which might not be ideal. Alternatively, use a non-linear combination, but that complicates the model.\n\nAnother angle: the probability of accepting is influenced by how much the offer exceeds the participant's minimum acceptable threshold. The threshold could be a function of their fair share. For example, the threshold could be \u03b8*fair_self, where \u03b8 is a parameter between 0 and 1. If the offer is above \u03b8*fair_self, the participant is more likely to accept. The utility could be the difference between the offer and the threshold:\n\nUtility = split_self - \u03b8*fair_self\n\nThen, the probability of accepting is logistic(Utility). If Utility > 0, the probability is greater than 0.5. This model has one learnable parameter, \u03b8. However, this assumes that the threshold is a fixed proportion of the fair share. But participants might have varying thresholds based on other factors, like the absolute amount or the opponent's share.\n\nAlternatively, the threshold could be a combination of a proportion of the fair share and a fixed amount. For example:\n\nUtility = split_self - (\u03b8*fair_self + \u03b3)\n\nHere, \u03b8 and \u03b3 are learnable parameters. This allows the threshold to be a linear function of the fair share plus a constant. But this adds two parameters. However, with 36 instances per participant, having two parameters might be manageable for parameter recovery.\n\nBut the problem states that parameter recovery is a priority, so we need to balance model complexity. Let's consider starting with a simple model and see if it can be expanded.\n\nAnother consideration is the role of trial_type. For example, in trials where the participant contributed more (trial_type=3), they might have a higher threshold. But including trial_type would require interaction terms or separate parameters for each trial type, which complicates the model. Since trial_type is derived from token_self and token_opp, perhaps the model can capture the same information through the token variables, avoiding the need for trial_type as a separate variable.\n\nPutting this all together, a plausible model is one where the utility is a function of the offer's deviation from the fair share and the absolute offer. To keep the number of parameters low, perhaps use two parameters: one for the fairness deviation and one for the absolute offer. However, to avoid collinearity, maybe combine them in a way that each parameter captures a distinct aspect.\n\nWait, let's think about the variables:\n\n- split_self: the amount offered to the participant.\n- fair_self: 3*token_self (the fair share based on contribution).\n- split_opp: combined_earning - split_self.\n\nThe utility could be modeled as:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*split_self + \u03b3*(split_opp - fair_opp)\n\nBut split_opp - fair_opp = (combined_earning - split_self) - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self = fair_self - split_self. So this term is the negative of (split_self - fair_self). Therefore, the third term becomes \u03b3*(fair_self - split_self). So the utility is:\n\nUtility = \u03b1*(split_self - fair_self) + \u03b2*split_self + \u03b3*(fair_self - split_self)\n= (\u03b1 - \u03b3)*(split_self - fair_self) + \u03b2*split_self\n\nThis again combines parameters, leading to potential identifiability issues. So perhaps this approach isn't helpful.\n\nAlternative approach: Use the ratio of the offer to the fair share. For example:\n\nUtility = \u03b1*(split_self / fair_self) + \u03b2*split_self\n\nIf split_self equals fair_self, the ratio is 1. If it's higher, the ratio is greater than 1. This captures proportional fairness. The \u03b2 term captures the absolute amount. But again, this has two parameters.\n\nAlternatively, use a single parameter that combines both effects. For example:\n\nUtility = split_self + \u03b1*(split_self - fair_self)\n\nThis can be rewritten as:\n\nUtility = (1 + \u03b1)*split_self - \u03b1*fair_self\n\nHere, \u03b1 represents the weight given to the fairness deviation relative to the absolute offer. If \u03b1 is positive, the participant cares about getting more than their fair share; if negative, they penalize deviations below the fair share more. This model has one learnable parameter, \u03b1. However, this might not capture cases where the absolute amount matters independently of fairness. For example, even if the offer is below fair_self, a higher split_self might still be acceptable. But in this model, the utility increases with split_self regardless of \u03b1. So if \u03b1 is negative, the term (split_self - fair_self) could reduce utility when split_self < fair_self. Wait, let's see:\n\nIf split_self < fair_self, then (split_self - fair_self) is negative. If \u03b1 is positive, then \u03b1*(split_self - fair_self) is negative, reducing utility. If \u03b1 is negative, then multiplying by a negative makes it positive, increasing utility. That doesn't make sense. So perhaps \u03b1 should be constrained to be positive. Then, when the offer is below fair_self, the fairness term reduces utility, and when above, increases it. The absolute term split_self always adds to utility. So this model combines both the absolute offer and the fairness deviation with a single parameter. This might be a good balance between simplicity and capturing both effects.\n\nBut does this model allow for cases where a higher absolute offer can offset a lower fairness? For example, if split_self is slightly below fair_self but is a large amount, the utility could still be positive. Let's see:\n\nSuppose fair_self = 6 (token_self=2), split_self=5. Then split_self - fair_self = -1. Utility = 5 + \u03b1*(-1). If \u03b1=2, utility is 5 -2 =3. If \u03b1=0.5, utility is 5 -0.5=4.5. Wait, but in this case, even though the offer is below fair_self, the utility is still positive because split_self is 5. But if \u03b1 is large enough, say \u03b1=6, then utility=5 -6= -1, which would predict rejection. So the model allows for both possibilities based on the value of \u03b1. A participant with high \u03b1 is very sensitive to fairness deviations, while a low \u03b1 is more influenced by the absolute amount.\n\nThis seems plausible. The model has one learnable parameter, \u03b1, which is the sensitivity to the fairness deviation. The utility is split_self plus \u03b1 times the deviation from fair_self. The probability of accepting is then logistic(Utility). Since the logistic function converts utility to a probability between 0 and 1, the model's prediction for 'accept' is 1 if the probability is above 0.5, which corresponds to Utility > 0.\n\nBut wait, the problem states that the utility should be such that less than 0 corresponds to less than 50% probability. That's exactly what the logistic function does: P(accept) = 1 / (1 + exp(-Utility)). So when Utility < 0, P(accept) < 0.5.\n\nThis model seems to meet the requirements. It uses split_self and token_self (to compute fair_self) from the dataset. The only learnable parameter is \u03b1, which can be bounded between, say, -10 and 10 to allow for sufficient flexibility but avoiding infinity. However, since \u03b1 represents the sensitivity to fairness, we might expect it to be positive (penalizing offers below fair_self). But to allow for the possibility that some participants might have a negative \u03b1 (though it's unclear what that would mean), we can set a generous bound like -20 to 20.\n\nWait, but if \u03b1 is negative, then the term \u03b1*(split_self - fair_self) would increase utility when split_self < fair_self. That would mean that participants with negative \u03b1 are more likely to accept offers that are below their fair share, which seems counterintuitive. So perhaps \u03b1 should be constrained to be non-negative. In that case, the bounds for \u03b1 would be [0, upper_bound]. Let's say 0 to 20. This would make sense: \u03b1 >= 0, so offers below fair_self reduce utility, and offers above increase it.\n\nBut the problem states that bounds should be generous and not necessarily restricted unless there's a theoretical reason. If we allow \u03b1 to be negative, the model can capture cases where participants are more accepting of offers below their fair share as \u03b1 becomes more negative, which might not make sense. So perhaps it's better to set \u03b1's lower bound to 0. This way, \u03b1 can only penalize or reward based on the fairness deviation.\n\nSo, the model would be:\n\nUtility = split_self + \u03b1*(split_self - fair_self)\n\nBut fair_self is 3*token_self. So:\n\nUtility = split_self + \u03b1*(split_self - 3*token_self)\n\nThis can be rewritten as:\n\nUtility = (1 + \u03b1)*split_self - 3*\u03b1*token_self\n\nBut this is equivalent to the original expression. The parameter \u03b1 is learnable, with \u03b1 >= 0.\n\nThis model has one learnable parameter, \u03b1, which is the weight given to the deviation from the fair share. The utility increases with the split_self and with the extent to which split_self exceeds the fair share. If split_self is below the fair share, the second term reduces utility. The higher \u03b1 is, the more the participant cares about fairness relative to the absolute amount.\n\nThis model is simple, has one parameter, and should be identifiable. It uses variables available in the dataset (split_self and token_self). It also accounts for both the absolute offer and the fairness based on contribution.\n\nNow, checking if this model can be extended or if there are other variables to consider. For example, the opponent's token (token_opp) might influence the participant's perception of fairness. If the opponent contributed more, maybe the participant is more lenient. But in the current model, the fair_self is based on the participant's own contribution. Including token_opp would require another parameter. For example:\n\nUtility = split_self + \u03b1*(split_self - 3*token_self) + \u03b3*(token_opp - token_self)\n\nHere, \u03b3 captures the effect of the opponent's contribution relative to the participant's. If \u03b3 is positive, when the opponent contributed more (token_opp > token_self), the utility increases, making acceptance more likely. But this adds another parameter, \u03b3. However, with 36 trials per participant, adding another parameter might be feasible, but the problem suggests a slight bias towards fewer parameters. So perhaps stick with one parameter.\n\nAlternatively, the trial_type variable indicates whether contributions are equal, opponent more, or participant more. This could be used to modulate \u03b1. For example, different alphas for each trial type. But that would require three parameters, which complicates the model. Given the need for parameter recovery, it's better to avoid that.\n\nAnother consideration: the utility might include a term for the total combined_earning. For example, participants might be more generous when the total is higher. But combined_earning is 3*(token_self + token_opp), which is already determined by token_self and token_opp. Including it might not add new information.\n\nIn conclusion, the proposed model is:\n\nUtility = split_self + \u03b1*(split_self - 3*token_self)\n\nThis model uses split_self and token_self from the dataset. The learnable parameter \u03b1 has a lower bound of 0 (since negative values would imply that participants prefer offers below their fair share) and an upper bound that's generous, say 20. The utility is then passed through a logistic function to determine the probability of acceptance.\n\nNow, let's formalize this in the required format.\n\nThe mathematical model would be:\n\nU = split_self + \u03b1 * (split_self - 3 * token_self)\n\nWhich can be simplified as:\n\nU = split_self * (1 + \u03b1) - 3 * \u03b1 * token_self\n\nBut to keep it in terms of the original variables, we'll present it as:\n\nU = split_self + \u03b1 * (split_self - 3 * token_self)\n\nVariables:\n\n- split_self: from the data, the proposed share for the participant.\n- token_self: from the data, the participant's contribution.\n- \u03b1: learnable parameter, sensitivity to fairness deviation.\n\nThe target variable is 'accept'.\n\nFor the variable descriptions:\n\n- \u03b1: learnable, range from 0 to 20 (inclusive).\n- split_self: data variable.\n- token_self: data variable.\n\nThe utility U is calculated, and the probability of acceptance is 1 / (1 + exp(-U)).\n\nThis model should be able to predict responder behavior with one learnable parameter, which is good for parameter recovery. It incorporates both the absolute offer and the deviation from the fair share based on contribution.\n</think>\n\n<MODEL>\nU = split_self + alpha * (split_self - 3 * token_self)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 20,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model combining absolute offer value and deviation from contribution-based fairness. The utility increases with the proposed amount (split_self) and is adjusted by a learnable parameter (\u03b1) scaling the difference between the offer and the participant's fair share (3 * token_self). The model predicts acceptance probability via logistic transformation of the utility, capturing sensitivity to both absolute gains and fairness relative to contribution.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = split_self + alpha * (split_self - 3 * token_self)\nSummary: A utility model combining absolute offer value and deviation from contribution-based fairness. The utility increases with the proposed amount (split_self) and is adjusted by a learnable parameter (\u03b1) scaling the difference between the offer and the participant's fair share (3 * token_self). The model predicts acceptance probability via logistic transformation of the utility, capturing sensitivity to both absolute gains and fairness relative to contribution.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = split_self + alpha * (split_self - 3 * token_self)\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8400488400488401,
  "fitting_results": [
    {
      "alpha": 20.0,
      "success": true,
      "log_likelihood": -69.07755278652137,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5631766882043492,
      "success": true,
      "log_likelihood": -81.81670191636715,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4340855770196574,
      "success": true,
      "log_likelihood": -35.169380795515956,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5001891177900908,
      "success": true,
      "log_likelihood": -77.37695695839032,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9931400291576544,
      "success": true,
      "log_likelihood": -43.22378014786537,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0017190432864698,
      "success": true,
      "log_likelihood": -44.08975995136462,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9858386072949958,
      "success": true,
      "log_likelihood": -43.987387136754386,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.2183579537802003,
      "success": true,
      "log_likelihood": -63.78137298064189,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6228329506193326,
      "success": true,
      "log_likelihood": -0.7929674299183119,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 15.802198714948403,
      "success": true,
      "log_likelihood": -92.1034037193069,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.121260145812335,
      "success": true,
      "log_likelihood": -23.097897803807363,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1532289253703436e-15,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4883031743680111,
      "success": true,
      "log_likelihood": -3.7485821421031056,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.47621609074981525,
      "success": true,
      "log_likelihood": -19.31452717992967,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4706712643794346,
      "success": true,
      "log_likelihood": -59.349035574466804,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5140756797916909,
      "success": true,
      "log_likelihood": -25.47273571325755,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 16.630498723818743,
      "success": true,
      "log_likelihood": -207.23265836699304,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.41472030932636,
      "success": true,
      "log_likelihood": -90.33428397343842,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5933848639879389,
      "success": true,
      "log_likelihood": -69.45760307018801,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.383339820668755,
      "success": true,
      "log_likelihood": -65.73587090592873,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3564100779867438,
      "success": true,
      "log_likelihood": -51.831788612532385,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.157998334879355,
      "success": true,
      "log_likelihood": -12.836555988681283,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.45544470592595165,
      "success": true,
      "log_likelihood": -39.11081025838146,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6130347271418158,
      "success": true,
      "log_likelihood": -91.22230590928613,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 20.0,
      "success": true,
      "log_likelihood": -69.07755278652135,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.442623856468015,
      "success": true,
      "log_likelihood": -152.66482398302426,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5044649141880768,
      "success": true,
      "log_likelihood": -90.13646066556619,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 9.25234647219316e-16,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8169073699400753,
      "success": true,
      "log_likelihood": -64.17401682995957,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8708108225379804,
      "success": true,
      "log_likelihood": -17.232774792981964,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5229394175489357,
      "success": true,
      "log_likelihood": -70.71977377545764,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9727605253500395,
      "success": true,
      "log_likelihood": -110.82996075490223,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.23590791632272848,
      "success": true,
      "log_likelihood": -38.68462982815923,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0215346896612938,
      "success": true,
      "log_likelihood": -127.88140117538171,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.659814859353064,
      "success": true,
      "log_likelihood": -46.10140612359101,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -23.025863872743603,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0213356314605813,
      "success": true,
      "log_likelihood": -90.90804320466563,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9922297013797889,
      "success": true,
      "log_likelihood": -63.77032216384145,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6580012688056293,
      "success": true,
      "log_likelihood": -96.03054229468867,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.439727991562905,
      "success": true,
      "log_likelihood": -84.46905631232241,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.535937817450449,
      "success": true,
      "log_likelihood": -134.34151578043466,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7570533786687115,
      "success": true,
      "log_likelihood": -65.59493280169099,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0493611988188987,
      "success": true,
      "log_likelihood": -209.10274885946086,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7791773822303298,
      "success": true,
      "log_likelihood": -109.86330773074515,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 20.0,
      "success": true,
      "log_likelihood": -115.12925464660228,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.39901522391361,
      "success": true,
      "log_likelihood": -113.09880593944852,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6967046048586205,
      "success": true,
      "log_likelihood": -177.71928593844837,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.9992229088212365,
      "success": true,
      "log_likelihood": -162.57125844043753,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.371377382626511,
      "success": true,
      "log_likelihood": -11.282174529492886,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.348577710805585,
      "success": true,
      "log_likelihood": -69.2851243663487,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.17910947938590205,
      "success": true,
      "log_likelihood": -84.40625806652831,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 20.0,
      "success": true,
      "log_likelihood": -92.10340371656187,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4398808668899185,
      "success": true,
      "log_likelihood": -75.15376107161282,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4421850184493785,
      "success": true,
      "log_likelihood": -145.68400642499853,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.72881885591018,
      "success": true,
      "log_likelihood": -62.774514614165035,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 20.0,
      "success": true,
      "log_likelihood": -69.07755278652138,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 19.057136948641148,
      "success": true,
      "log_likelihood": -322.3619130169667,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.0008701683581265,
      "success": true,
      "log_likelihood": -77.43639241612678,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4050944835544763,
      "success": true,
      "log_likelihood": -56.22288379806905,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.32565468762437066,
      "success": true,
      "log_likelihood": -36.715037648974516,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4389330415574457,
      "success": true,
      "log_likelihood": -45.161997332096675,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9678080590774271,
      "success": true,
      "log_likelihood": -95.1186785458284,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.22639456530403285,
      "success": true,
      "log_likelihood": -49.763130513527905,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 20.0,
      "success": true,
      "log_likelihood": 3.599990971811932e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 19.645140472486943,
      "success": true,
      "log_likelihood": -115.12925464660229,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4245289833631034,
      "success": true,
      "log_likelihood": -72.22478305408792,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6421082437032212,
      "success": true,
      "log_likelihood": -82.09250607431022,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4879143456889248,
      "success": true,
      "log_likelihood": -158.18607415489862,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9034592706936228,
      "success": true,
      "log_likelihood": -65.76996345386614,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.20085777451524175,
      "success": true,
      "log_likelihood": -33.38938098578202,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 20.0,
      "success": true,
      "log_likelihood": -92.10340371656179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.408709558500235,
      "success": true,
      "log_likelihood": -201.10249468810147,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 20.0,
      "success": true,
      "log_likelihood": -69.07755278652137,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8510030393147593,
      "success": true,
      "log_likelihood": -210.1988423452951,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -68.73866400294936,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.0008698428755896,
      "success": true,
      "log_likelihood": -123.48809427620762,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.383627765687629,
      "success": true,
      "log_likelihood": -38.539729442791064,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7099310571549073,
      "success": true,
      "log_likelihood": -216.74015089130012,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 20.0,
      "success": true,
      "log_likelihood": -46.05170185648092,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7096461123291443,
      "success": true,
      "log_likelihood": -116.7499616406003,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.033792324915633,
      "success": true,
      "log_likelihood": -77.93913360335065,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2634367322437353,
      "success": true,
      "log_likelihood": -19.416835219026144,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3430916598970715,
      "success": true,
      "log_likelihood": -84.05530594083314,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9844519220994311,
      "success": true,
      "log_likelihood": -168.75713809299918,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.899152539010826,
        "p": 6.0169841183931445e-37
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 155.13493815216663,
  "bic_results": {
    "average_bic": 155.13493815216663,
    "individual_bics": [
      141.73862451149884,
      167.2169227711904,
      73.92228052948802,
      158.33743285523676,
      90.03107923418685,
      91.76303884118535,
      3.5835448238623537,
      91.55829321196488,
      3.5835448238623537,
      131.1462648997399,
      5.169453798292734,
      187.7903263770699,
      49.779314546070836,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      11.080683222662321,
      42.21257329831545,
      3.5835448238623537,
      122.28159008738972,
      54.52899036497121,
      418.0488356724422,
      184.25208688533294,
      142.49872507883214,
      135.05526075031358,
      107.24709616352088,
      29.256630915818675,
      81.80513945521903,
      186.02813075702838,
      3.5835448238623537,
      141.7386245114988,
      308.91316690450464,
      183.8564402695885,
      3.5835448238623537,
      131.93155259837525,
      38.049068524420036,
      145.0230664893714,
      225.24344044826057,
      80.95277859477457,
      259.34632128921953,
      95.78633118563813,
      49.635246683943315,
      185.39960534778737,
      131.12416326613902,
      195.64460352783345,
      172.52163156310093,
      272.26655049932543,
      134.77338454183808,
      421.78901665737783,
      223.3101343999464,
      233.84202823166066,
      229.78113081735316,
      359.02209081535284,
      328.7260358193312,
      26.14786799744188,
      142.1537676711535,
      172.39603507151273,
      187.79032637157985,
      153.89104108168175,
      294.95153178845317,
      129.1325481667862,
      141.73862451149887,
      648.3073449723895,
      158.45630377070967,
      116.0292865345942,
      77.01359423640514,
      93.90751360264946,
      193.8208760301129,
      103.10977996551192,
      3.583518931256128,
      233.8420282316607,
      148.03308504663195,
      167.76853108707655,
      319.95566724825335,
      135.1234458461884,
      70.36228091002015,
      187.79032637157968,
      405.78850831465905,
      141.73862451149884,
      423.9812036290463,
      141.06084694435484,
      250.55970749087135,
      80.66297782403824,
      437.06382072105635,
      95.68692265141794,
      237.08344221965672,
      159.46178614515742,
      42.4171893765084,
      171.69413082012238,
      341.0977951244545
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.899",
  "previous_models": [
    "Specification: U = split_self + alpha * (split_self - 3 * token_self)\nSummary: A utility model combining absolute offer value and deviation from contribution-based fairness. The utility increases with the proposed amount (split_self) and is adjusted by a learnable parameter (\u03b1) scaling the difference between the offer and the participant's fair share (3 * token_self). The model predicts acceptance probability via logistic transformation of the utility, capturing sensitivity to both absolute gains and fairness relative to contribution.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 155.13493815216663\nOverall Accuracy: 0.8400\n\nParameter Recovery:\n- alpha: r = 0.899"
  ]
}