{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility should be designed such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on the utility of accepting the proposal. The utility function should incorporate factors that influence the responder's decision.\n\nLooking at the dataset variables, important ones for the responder's decision might include:\n\n- split_self: The proposed share for the participant in \u00a3.\n- split_opp: The opponent's proposed share.\n- token_self and token_opp: The number of tokens each contributed, which translate to their contributions to the combined_earning (each token is \u00a33). So combined_earning is 3*(token_self + token_opp).\n- trial_type: Whether contributions are equal, opponent more, or participant more. This might affect fairness perceptions.\n\nSince the model should predict responder behavior, we focus on trials where trial_role is 1 (responder). The proposer's role (trial_role=2) might not be relevant here, but the model should only use variables available in the dataset.\n\nThe key psychological factors here are likely fairness and self-interest. The responder's utility might depend on how fair the proposed split is relative to their contribution. For example, if the participant contributed more tokens, they might expect a larger share. If the split_self is less than what they consider fair based on their contribution, they might be more likely to reject.\n\nSo, the utility could be a function of the difference between the proposed share and what's considered a fair share. The fair share might be proportional to their contribution. Let's define the fair share as (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and this fair share could be a factor.\n\nBut maybe participants have different sensitivities to this difference. Also, there might be a baseline acceptance tendency. So, parameters could include:\n\n- \u03b1 (alpha): Sensitivity to the difference between proposed and fair share.\n- \u03b2 (beta): Baseline acceptance tendency.\n\nAnother consideration is the absolute amount of the split_self. Even if the split is fair, if the absolute amount is too low, the responder might reject. But since combined_earning varies, perhaps the percentage split (splitperc_self) is more relevant. However, splitperc_self is already provided. Alternatively, using the actual monetary amounts might be better because the same percentage could correspond to different absolute amounts depending on combined_earning.\n\nWait, the note says that two equal split_self variables don't represent the same fairness because combined_earning varies. So, split_self alone isn't enough to judge fairness. The fair share should be based on the contributions (tokens). So, the fair share would be (token_self / total_tokens) * combined_earning. Since each token is \u00a33, combined_earning is 3*(token_self + token_opp). Therefore, the fair share in \u00a3 is (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) = 3*token_self. Wait, that's interesting. So the fair share based on contribution is exactly 3*token_self. Because each token is worth \u00a33, so the participant's contribution is token_self * \u00a33, and the opponent's is token_opp * \u00a33. The total combined_earning is 3*(token_self + token_opp). So, if the split were exactly proportional to contributions, the participant should get 3*token_self, and the opponent 3*token_opp. But the proposed split_self might differ from this.\n\nSo, the difference between split_self and 3*token_self could be a key factor. If split_self is less than 3*token_self, that's unfair to the participant, making them more likely to reject. Conversely, if it's more, they might accept. But participants might have different thresholds for what's acceptable. So, the utility could be:\n\nUtility = \u03b2 + \u03b1*(split_self - 3*token_self)\n\nHere, \u03b2 is a baseline tendency to accept, and \u03b1 scales the impact of the fairness difference. If the utility is positive, the probability of acceptance is higher than 50%. If negative, lower.\n\nBut maybe participants also care about the opponent's share relative to their contribution. For example, if the opponent is getting more than their fair share (split_opp > 3*token_opp), that could also affect the utility. However, since split_opp + split_self = combined_earning, which is 3*(token_self + token_opp), then split_opp = combined_earning - split_self. So, split_opp = 3*(token_self + token_opp) - split_self. Therefore, split_opp - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self. So, the difference between split_opp and the opponent's fair share is the same as the negative of the participant's difference. Therefore, including both might be redundant. So perhaps focusing on the participant's difference is sufficient.\n\nAlternatively, participants might have different sensitivities to advantageous versus disadvantageous inequality. For example, if split_self is more than fair, they might still be happy, but if it's less, they are unhappy. This could be modeled with a parameter that differentiates between positive and negative differences. But that would complicate the model with more parameters. Since the instruction suggests a slight bias towards fewer parameters, maybe a linear term is sufficient.\n\nAnother consideration is the trial_type (1=equal, 2=opponent more, 3=participant more). This might influence the responder's perception. For example, if the trial_type is 'opponent more' (2), the participant might be more sensitive to unfair splits. But including trial_type as a moderator would require interaction terms, adding more parameters. To keep the model simple, perhaps trial_type is not directly included, but the model relies on the token contributions and proposed splits, which already capture the trial_type information (since trial_type is determined by token_self vs token_opp).\n\nSo, the core model could be:\n\nUtility = \u03b2 + \u03b1*(split_self - fair_self)\n\nWhere fair_self = 3*token_self.\n\nThen, the probability of accepting is logistic(Utility). So, P(accept=1) = 1 / (1 + exp(-Utility)).\n\nThis model has two learnable parameters: \u03b2 (baseline) and \u03b1 (sensitivity to fairness difference). The utility can be unbounded, and when Utility < 0, the probability is less than 0.5.\n\nBut wait, the note says that in each trial, the combined_earning varies, so two equal split_self values don't represent the same fairness. However, in the model above, split_self is compared to 3*token_self, which is the fair share based on contribution. So even if split_self is the same in two trials, if 3*token_self is different, the difference (split_self - fair_self) would vary, thus affecting utility. That seems appropriate.\n\nBut maybe participants don't just care about the absolute difference in \u00a3, but the proportional difference. For example, a \u00a31 difference when fair_self is \u00a310 is 10%, but when fair_self is \u00a320, it's 5%. So, perhaps the model should use a proportional difference. Let's consider:\n\nsplit_self / fair_self - 1 = (split_self - fair_self)/fair_self.\n\nSo, the utility could be:\n\nUtility = \u03b2 + \u03b1*( (split_self - fair_self) / fair_self )\n\nThis would make the difference a percentage of the fair share. But this introduces division by fair_self, which could be zero if token_self is zero. However, in the dataset, token_self is the participant's contribution, which might not be zero in any trial. The problem states that in the pretask, participants find a number of tokens, so token_self is at least some positive integer. So division is safe.\n\nAlternatively, using the absolute difference in \u00a3 might be more straightforward. Let's stick with the absolute difference for simplicity unless there's a reason to think otherwise.\n\nAnother angle: participants might have a notion of equitable split, which could be influenced by both their own and the opponent's contributions. For example, if the participant contributed more, they expect a higher split. The model above captures this by comparing split_self to their fair share. But maybe there's a non-linear effect. For example, participants might be more sensitive to underpayment when their contribution is higher. However, adding non-linear terms would require more parameters. To keep the model simple, a linear term is better.\n\nNow, considering parameter bounds. The parameters \u03b1 and \u03b2 need to have finite bounds. For \u03b2, the baseline tendency, it's reasonable to set a range like [-5, 5], as logistic functions saturate beyond that. For \u03b1, which scales the fairness difference, if the difference is in \u00a3, and the maximum possible split_self could be up to combined_earning (if the proposer offers all to the responder), then the difference (split_self - fair_self) could range from (0 - 3*token_self) to (3*(token_self + token_opp) - 3*token_self) = 3*token_opp. So the maximum possible difference is 3*token_opp. But token_opp varies per trial. However, \u03b1 is a fixed parameter across trials. So, to ensure that the product \u03b1*(split_self - fair_self) doesn't lead to utility extremes, \u03b1 should be bounded. For example, if \u03b1 is in [-1, 1], then even with a large difference, the term is manageable. But perhaps a wider range like [-10, 10] is better to allow the model to fit different sensitivities. However, the exact bounds can be set based on typical values in similar models.\n\nAlternatively, if using the proportional difference (split_self - fair_self)/fair_self, then the difference is a ratio. For example, if split_self is half of fair_self, the term is -0.5. If split_self is double, it's +1.0. Then \u03b1 could be in a range like [-5,5], which when multiplied by the ratio gives a reasonable utility range.\n\nBut let's proceed with the absolute difference first. So:\n\nUtility = \u03b2 + \u03b1*(split_self - 3*token_self)\n\nParameters:\n\n- \u03b2: baseline utility, range [-10, 10] (inclusive)\n- \u03b1: sensitivity to fairness difference, range [-10, 10] (inclusive)\n\nThese ranges are generous but finite. The model's utility is then passed through a logistic function to get the probability of acceptance.\n\nNow, check if this model meets the requirements:\n\n1. Predicts responder behavior (accept) using the specified variables. The variables used are split_self and token_self, which are in the dataset. Also, combined_earning is 3*(token_self + token_opp), but we don't need to include it directly since split_self is given.\n\n2. The model uses only the provided variables. Yes, split_self and token_self are available.\n\n3. The utility is unbounded, and when utility < 0, the probability is less than 50%. Correct.\n\nParameter recovery is feasible with two parameters, which is manageable. The model is simple, avoiding high covariance between parameters. The parameters \u03b1 and \u03b2 are likely to be identifiable as they capture different aspects: baseline acceptance and sensitivity to fairness.\n\nAnother possible factor is the absolute amount of split_self. For example, even if the split is fair, if the absolute amount is too low, the participant might reject. But in this model, the absolute amount is already considered through split_self. However, the fair_self (3*token_self) is the participant's contribution in \u00a3. So, if split_self is equal to fair_self, the utility is \u03b2. So \u03b2 represents the baseline utility when the split is exactly fair. If \u03b2 is positive, even a fair split has a higher chance of acceptance. If \u03b2 is negative, even a fair split might be rejected. But perhaps participants have a baseline tendency to accept or reject regardless of fairness. This is captured by \u03b2.\n\nAlternatively, maybe participants have a different utility function that includes both the absolute split and the fairness. For example:\n\nUtility = \u03b2 + \u03b1*(split_self - fair_self) + \u03b3*split_self\n\nHere, \u03b3 captures the sensitivity to the absolute amount. But this adds another parameter (\u03b3), increasing model complexity. The instructions suggest a slight bias towards fewer parameters, so maybe stick with two parameters.\n\nAlternatively, the model could use the ratio of split_self to fair_self. For example:\n\nUtility = \u03b2 + \u03b1*(split_self / fair_self - 1)\n\nThis makes the fairness term a percentage difference. For example, if split_self is 1.2 times fair_self, the term is 0.2*\u03b1. This might normalize the effect across different contribution levels. However, this introduces division by fair_self, which is 3*token_self. Since token_self is at least 1 (assuming participants contribute at least one token), this is safe. This could be a better approach as it accounts for proportional differences. Let's consider this:\n\nUtility = \u03b2 + \u03b1*( (split_self / (3*token_self)) - 1 )\n\nThis simplifies to \u03b2 + \u03b1*(split_self - 3*token_self)/(3*token_self). So it's scaling the difference by the fair share. This might make \u03b1 more interpretable as a sensitivity to proportional differences. For example, \u03b1=2 would mean that a 10% increase over fair_self adds 0.2 to the utility.\n\nBut whether to use absolute or proportional difference depends on the assumed psychological mechanism. If participants are more sensitive to proportional unfairness, the proportional model is better. If they care about absolute \u00a3 differences, the absolute model is better. Given that the note mentions that equal split_self doesn't imply the same fairness because combined_earning varies, the proportional model might be more appropriate because it accounts for the participant's contribution.\n\nSo, revising the model:\n\nUtility = \u03b2 + \u03b1*( (split_self / fair_self) - 1 )\n\nWhere fair_self = 3*token_self.\n\nThis way, even if split_self is the same in two trials, if fair_self is different (due to different token_self), the utility will differ. For example, if split_self is \u00a36, and fair_self is \u00a36 (token_self=2), the term is 0. If fair_self is \u00a33 (token_self=1), split_self \u00a36 gives a term of (6/3 -1) = 1, so utility is \u03b2 + \u03b1*1.\n\nThis captures the proportional aspect, which aligns with the note about combined_earning varying.\n\nAlternatively, using the ratio directly:\n\nUtility = \u03b2 + \u03b1*(split_self / fair_self)\n\nBut then \u03b2 would represent the baseline when the ratio is 0, which isn't possible. The previous formulation centers the term around 1, so when split_self equals fair_self, the term is 0, and utility is \u03b2. That seems better.\n\nSo, the model becomes:\n\nUtility = \u03b2 + \u03b1*( (split_self / (3*token_self)) - 1 )\n\nThis can be rewritten as:\n\nUtility = \u03b2 + \u03b1*(split_self - 3*token_self) / (3*token_self)\n\nBut mathematically, it's the same as the previous expression.\n\nNow, parameters \u03b1 and \u03b2. \u03b1 represents how much the participant cares about proportional deviations from their fair share. A positive \u03b1 means that getting more than fair increases utility, leading to higher acceptance. A negative \u03b1 would imply that getting more than fair decreases utility, which doesn't make sense, so we might expect \u03b1 to be positive. However, the model should allow for the parameter to be learned, so the range for \u03b1 should be from, say, -10 to 10, but in practice, we might expect it to be positive. But leaving the bounds symmetric allows the model to capture unexpected patterns.\n\n\u03b2 is the baseline utility when the split is exactly fair. If \u03b2 is positive, even a fair split has a high chance of acceptance. If \u03b2 is negative, even a fair split might be rejected.\n\nThis model has two parameters, which is manageable for parameter recovery. The variables used are split_self and token_self, both available in the dataset.\n\nAnother consideration: the opponent's contribution. Maybe participants also compare their split to the opponent's split relative to the opponent's contribution. For example, if the opponent is getting more than their fair share, the participant might feel it's unfair. However, since split_opp = combined_earning - split_self, and combined_earning is 3*(token_self + token_opp), split_opp = 3*(token_self + token_opp) - split_self. The opponent's fair share is 3*token_opp. So, the difference for the opponent is split_opp - 3*token_opp = 3*token_self - split_self. Which is the negative of the participant's difference (split_self - 3*token_self). So, including both would introduce collinearity. Therefore, focusing on the participant's difference is sufficient.\n\nAlternatively, participants might care about the difference between their split and the opponent's split relative to their contributions. For example, (split_self / token_self) vs (split_opp / token_opp). If these ratios are unequal, it's unfair. But this would require more parameters. For example:\n\nUtility = \u03b2 + \u03b1*( (split_self / token_self) - (split_opp / token_opp) )\n\nBut split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self. So substituting:\n\nUtility = \u03b2 + \u03b1*( (split_self / token_self) - ( (3*(token_self + token_opp) - split_self) / token_opp ) )\n\nThis complicates the model and introduces more variables (token_opp), but token_opp is available in the dataset. However, this adds complexity with more parameters. Given the instruction to prefer fewer parameters, this might not be ideal. But perhaps this captures a different fairness notion. However, this model would have only \u03b1 and \u03b2 as parameters, same as before. Let's see:\n\nLet me expand the term:\n\n(split_self / token_self) - (split_opp / token_opp) = (split_self / token_self) - ( (3*(token_self + token_opp) - split_self) / token_opp )\n\n= (split_self / token_self) - [3*(token_self + token_opp)/token_opp - split_self/token_opp ]\n\n= split_self*(1/token_self + 1/token_opp) - 3*(token_self + token_opp)/token_opp\n\nThis seems complicated. Maybe this is a better way to model inequity aversion, where participants dislike unequal outcomes. But inequity aversion models often use the difference between own and other's payoffs. For example, Fehr & Schmidt's model:\n\nUtility = split_self - \u03b1*max(split_opp - split_self, 0) - \u03b2*max(split_self - split_opp, 0)\n\nBut this requires two parameters (\u03b1 and \u03b2) for disadvantageous and advantageous inequality. However, this model is more complex and has more parameters. Given the instruction to have a slight bias towards fewer parameters, maybe the initial model is better.\n\nAlternatively, a simplified inequity aversion model could be:\n\nUtility = split_self - \u03b3*(split_opp - split_self)\n\nBut this also introduces another parameter. However, in this case, \u03b3 is the sensitivity to inequality. But this model would have parameters \u03b2 (baseline) and \u03b3. But maybe:\n\nUtility = \u03b2 + \u03b1*split_self + \u03b3*(split_opp - split_self)\n\nBut this complicates things further. The original model with two parameters seems more parsimonious.\n\nConsidering all this, the initial model focusing on the participant's split relative to their fair share (3*token_self) with parameters \u03b1 and \u03b2 seems appropriate.\n\nNow, variable descriptions:\n\nVariables in the model:\n\n- \u03b2 (beta): Baseline utility when the split is fair. Learnable, range [-10, 10].\n- \u03b1 (alpha): Sensitivity to the proportional deviation from the fair share. Learnable, range [-10, 10].\n- split_self: From data, the proposed share for the participant.\n- token_self: From data, the participant's tokens.\n- fair_self: Calculated as 3*token_self.\n- The term (split_self / fair_self - 1) is calculated.\n\nThe utility is:\n\nUtility = beta + alpha * (split_self / (3 * token_self) - 1)\n\nThis can be written as:\n\nUtility = beta + alpha * ((split_self - 3 * token_self) / (3 * token_self))\n\nBut mathematically equivalent.\n\nThe variables in the JSON would include beta and alpha as learnable parameters, and split_self and token_self as data variables. The calculated variables (fair_self, the ratio term) are not listed as variables in the JSON since they are derived from data variables.\n\nThe target variable is 'accept', which is binary. The model's utility is passed through a logistic function to predict the probability of acceptance.\n\nSo, the final model specification is:\n\n<MODEL>\nUtility = beta + alpha * (split_self / (3 * token_self) - 1)\n</MODEL>\n\nVariables:\n\n- beta: learnable, range [-10, 10]\n- alpha: learnable, range [-10, 10]\n- split_self: from data\n- token_self: from data\n\nThe JSON for variables would list beta and alpha with their descriptions and ranges. Other variables like split_self and token_self are data variables, so they are marked as source:data and not learnable.\n\nNow, checking if all variables in the model are accounted for in the JSON. The model uses beta, alpha, split_self, and token_self. The JSON should include these.\n\nThe summary should explain that the model captures the responder's utility based on the deviation of the proposed split from their contribution-based fair share, with parameters for baseline acceptance and sensitivity to proportional fairness.\n</think>\n\n<MODEL>\nUtility = beta + alpha * (split_self / (3 * token_self) - 1)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"beta\": {\n      \"description\": \"Baseline utility when proposed split equals fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to proportional deviation from fair share\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on proportional deviation of the proposed split from the participant's contribution-based fair share. The model uses two parameters: a baseline utility (beta) and a sensitivity parameter (alpha) to fairness deviations. Utility is converted to acceptance probability via logistic function, with negative utility corresponding to <50% acceptance likelihood.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['beta', 'alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                Utility = beta + alpha * (split_self / (3 * token_self) - 1)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"beta\": {\n      \"description\": \"Baseline utility when proposed split equals fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to proportional deviation from fair share\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, beta=0, alpha=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = beta + alpha * (split_self / (3 * token_self) - 1)\n\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = \"\"\"[\n  {\n    \"ID\": 1,\n    \"group\": \"Control\",\n    \"trial_number\": 1,\n    \"trial_type\": 1,\n    \"trial_role\": 1,\n    \"token_opp\": 1,\n    \"token_self\": 1,\n    \"combined_earning\": 6,\n    \"split_opp\": 3.0,\n    \"split_self\": 3.0,\n    \"splitperc_opp\": 50,\n    \"splitperc_self\": 50,\n    \"accept\": 1.0,\n    \"accepted_amount\": 3.0,\n    \"accepted_perc\": 50.0,\n    \"proposed_perc\": 50.0,\n    \"proposed_amount\": 3.0\n  },\n  {\n    \"ID\": 2,\n    \"group\": \"Control\",\n    \"trial_number\": 2,\n    \"trial_type\": 1,\n    \"trial_role\": 1,\n    \"token_opp\": 1,\n    \"token_self\": 1,\n    \"combined_earning\": 6,\n    \"split_opp\": 3.0,\n    \"split_self\": 3.0,\n    \"splitperc_opp\": 50,\n    \"splitperc_self\": 50,\n    \"accept\": 1.0,\n    \"accepted_amount\": 3.0,\n    \"accepted_perc\": 50.0,\n    \"proposed_perc\": 50.0,\n    \"proposed_amount\": 3.0\n  },\n  {\n    \"ID\": 3,\n    \"group\": \"Control\",\n    \"trial_number\": 3,\n    \"trial_type\": 1,\n    \"trial_role\": 1,\n    \"token_opp\": 1,\n    \"token_self\": 1,\n    \"combined_earning\": 6,\n    \"split_opp\": 3.0,\n    \"split_self\": 3.0,\n    \"splitperc_opp\": 50,\n    \"splitperc_self\": 50,\n    \"accept\": 1.0,\n    \"accepted_amount\": 3.0,\n    \"accepted_perc\": 50.0,\n    \"proposed_perc\": 50.0,\n    \"proposed_amount\": 3.0\n  },\n  {\n    \"ID\": 4,\n    \"group\": \"Control\",\n    \"trial_number\": 4,\n    \"trial_type\": 1,\n    \"trial_role\": 1,\n    \"token_opp\": 1,\n    \"token_self\": 1,\n    \"combined_earning\": 6,\n    \"split_opp\": 3.0,\n    \"split_self\": 3.0,\n    \"splitperc_opp\": 50,\n    \"splitperc_self\": 50,\n    \"accept\": 1.0,\n    \"accepted_amount\": 3.0,\n    \"accepted_perc\": 50.0,\n    \"proposed_perc\": 50.0,\n    \"proposed_amount\": 3.0\n  }\n]\"\"\"\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1743518778.98961
    }
  ],
  "learnable_parameters": {
    "beta": {
      "description": "Baseline utility when proposed split equals fair share (3 * token_self)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to proportional deviation from fair share",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "Utility = beta + alpha * (split_self / (3 * token_self) - 1)",
  "variable_descriptions": {
    "beta": {
      "description": "Baseline utility when proposed split equals fair share (3 * token_self)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to proportional deviation from fair share",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model where acceptance probability depends on proportional deviation of the proposed split from the participant's contribution-based fair share. The model uses two parameters: a baseline utility (beta) and a sensitivity parameter (alpha) to fairness deviations. Utility is converted to acceptance probability via logistic function, with negative utility corresponding to <50% acceptance likelihood.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility should be designed such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The target variable is 'accept', which is binary (1 or 0). The model should predict this based on the utility of accepting the proposal. The utility function should incorporate factors that influence the responder's decision.\n\nLooking at the dataset variables, important ones for the responder's decision might include:\n\n- split_self: The proposed share for the participant in \u00a3.\n- split_opp: The opponent's proposed share.\n- token_self and token_opp: The number of tokens each contributed, which translate to their contributions to the combined_earning (each token is \u00a33). So combined_earning is 3*(token_self + token_opp).\n- trial_type: Whether contributions are equal, opponent more, or participant more. This might affect fairness perceptions.\n\nSince the model should predict responder behavior, we focus on trials where trial_role is 1 (responder). The proposer's role (trial_role=2) might not be relevant here, but the model should only use variables available in the dataset.\n\nThe key psychological factors here are likely fairness and self-interest. The responder's utility might depend on how fair the proposed split is relative to their contribution. For example, if the participant contributed more tokens, they might expect a larger share. If the split_self is less than what they consider fair based on their contribution, they might be more likely to reject.\n\nSo, the utility could be a function of the difference between the proposed share and what's considered a fair share. The fair share might be proportional to their contribution. Let's define the fair share as (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and this fair share could be a factor.\n\nBut maybe participants have different sensitivities to this difference. Also, there might be a baseline acceptance tendency. So, parameters could include:\n\n- \u03b1 (alpha): Sensitivity to the difference between proposed and fair share.\n- \u03b2 (beta): Baseline acceptance tendency.\n\nAnother consideration is the absolute amount of the split_self. Even if the split is fair, if the absolute amount is too low, the responder might reject. But since combined_earning varies, perhaps the percentage split (splitperc_self) is more relevant. However, splitperc_self is already provided. Alternatively, using the actual monetary amounts might be better because the same percentage could correspond to different absolute amounts depending on combined_earning.\n\nWait, the note says that two equal split_self variables don't represent the same fairness because combined_earning varies. So, split_self alone isn't enough to judge fairness. The fair share should be based on the contributions (tokens). So, the fair share would be (token_self / total_tokens) * combined_earning. Since each token is \u00a33, combined_earning is 3*(token_self + token_opp). Therefore, the fair share in \u00a3 is (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) = 3*token_self. Wait, that's interesting. So the fair share based on contribution is exactly 3*token_self. Because each token is worth \u00a33, so the participant's contribution is token_self * \u00a33, and the opponent's is token_opp * \u00a33. The total combined_earning is 3*(token_self + token_opp). So, if the split were exactly proportional to contributions, the participant should get 3*token_self, and the opponent 3*token_opp. But the proposed split_self might differ from this.\n\nSo, the difference between split_self and 3*token_self could be a key factor. If split_self is less than 3*token_self, that's unfair to the participant, making them more likely to reject. Conversely, if it's more, they might accept. But participants might have different thresholds for what's acceptable. So, the utility could be:\n\nUtility = \u03b2 + \u03b1*(split_self - 3*token_self)\n\nHere, \u03b2 is a baseline tendency to accept, and \u03b1 scales the impact of the fairness difference. If the utility is positive, the probability of acceptance is higher than 50%. If negative, lower.\n\nBut maybe participants also care about the opponent's share relative to their contribution. For example, if the opponent is getting more than their fair share (split_opp > 3*token_opp), that could also affect the utility. However, since split_opp + split_self = combined_earning, which is 3*(token_self + token_opp), then split_opp = combined_earning - split_self. So, split_opp = 3*(token_self + token_opp) - split_self. Therefore, split_opp - 3*token_opp = 3*(token_self + token_opp) - split_self - 3*token_opp = 3*token_self - split_self. So, the difference between split_opp and the opponent's fair share is the same as the negative of the participant's difference. Therefore, including both might be redundant. So perhaps focusing on the participant's difference is sufficient.\n\nAlternatively, participants might have different sensitivities to advantageous versus disadvantageous inequality. For example, if split_self is more than fair, they might still be happy, but if it's less, they are unhappy. This could be modeled with a parameter that differentiates between positive and negative differences. But that would complicate the model with more parameters. Since the instruction suggests a slight bias towards fewer parameters, maybe a linear term is sufficient.\n\nAnother consideration is the trial_type (1=equal, 2=opponent more, 3=participant more). This might influence the responder's perception. For example, if the trial_type is 'opponent more' (2), the participant might be more sensitive to unfair splits. But including trial_type as a moderator would require interaction terms, adding more parameters. To keep the model simple, perhaps trial_type is not directly included, but the model relies on the token contributions and proposed splits, which already capture the trial_type information (since trial_type is determined by token_self vs token_opp).\n\nSo, the core model could be:\n\nUtility = \u03b2 + \u03b1*(split_self - fair_self)\n\nWhere fair_self = 3*token_self.\n\nThen, the probability of accepting is logistic(Utility). So, P(accept=1) = 1 / (1 + exp(-Utility)).\n\nThis model has two learnable parameters: \u03b2 (baseline) and \u03b1 (sensitivity to fairness difference). The utility can be unbounded, and when Utility < 0, the probability is less than 0.5.\n\nBut wait, the note says that in each trial, the combined_earning varies, so two equal split_self values don't represent the same fairness. However, in the model above, split_self is compared to 3*token_self, which is the fair share based on contribution. So even if split_self is the same in two trials, if 3*token_self is different, the difference (split_self - fair_self) would vary, thus affecting utility. That seems appropriate.\n\nBut maybe participants don't just care about the absolute difference in \u00a3, but the proportional difference. For example, a \u00a31 difference when fair_self is \u00a310 is 10%, but when fair_self is \u00a320, it's 5%. So, perhaps the model should use a proportional difference. Let's consider:\n\nsplit_self / fair_self - 1 = (split_self - fair_self)/fair_self.\n\nSo, the utility could be:\n\nUtility = \u03b2 + \u03b1*( (split_self - fair_self) / fair_self )\n\nThis would make the difference a percentage of the fair share. But this introduces division by fair_self, which could be zero if token_self is zero. However, in the dataset, token_self is the participant's contribution, which might not be zero in any trial. The problem states that in the pretask, participants find a number of tokens, so token_self is at least some positive integer. So division is safe.\n\nAlternatively, using the absolute difference in \u00a3 might be more straightforward. Let's stick with the absolute difference for simplicity unless there's a reason to think otherwise.\n\nAnother angle: participants might have a notion of equitable split, which could be influenced by both their own and the opponent's contributions. For example, if the participant contributed more, they expect a higher split. The model above captures this by comparing split_self to their fair share. But maybe there's a non-linear effect. For example, participants might be more sensitive to underpayment when their contribution is higher. However, adding non-linear terms would require more parameters. To keep the model simple, a linear term is better.\n\nNow, considering parameter bounds. The parameters \u03b1 and \u03b2 need to have finite bounds. For \u03b2, the baseline tendency, it's reasonable to set a range like [-5, 5], as logistic functions saturate beyond that. For \u03b1, which scales the fairness difference, if the difference is in \u00a3, and the maximum possible split_self could be up to combined_earning (if the proposer offers all to the responder), then the difference (split_self - fair_self) could range from (0 - 3*token_self) to (3*(token_self + token_opp) - 3*token_self) = 3*token_opp. So the maximum possible difference is 3*token_opp. But token_opp varies per trial. However, \u03b1 is a fixed parameter across trials. So, to ensure that the product \u03b1*(split_self - fair_self) doesn't lead to utility extremes, \u03b1 should be bounded. For example, if \u03b1 is in [-1, 1], then even with a large difference, the term is manageable. But perhaps a wider range like [-10, 10] is better to allow the model to fit different sensitivities. However, the exact bounds can be set based on typical values in similar models.\n\nAlternatively, if using the proportional difference (split_self - fair_self)/fair_self, then the difference is a ratio. For example, if split_self is half of fair_self, the term is -0.5. If split_self is double, it's +1.0. Then \u03b1 could be in a range like [-5,5], which when multiplied by the ratio gives a reasonable utility range.\n\nBut let's proceed with the absolute difference first. So:\n\nUtility = \u03b2 + \u03b1*(split_self - 3*token_self)\n\nParameters:\n\n- \u03b2: baseline utility, range [-10, 10] (inclusive)\n- \u03b1: sensitivity to fairness difference, range [-10, 10] (inclusive)\n\nThese ranges are generous but finite. The model's utility is then passed through a logistic function to get the probability of acceptance.\n\nNow, check if this model meets the requirements:\n\n1. Predicts responder behavior (accept) using the specified variables. The variables used are split_self and token_self, which are in the dataset. Also, combined_earning is 3*(token_self + token_opp), but we don't need to include it directly since split_self is given.\n\n2. The model uses only the provided variables. Yes, split_self and token_self are available.\n\n3. The utility is unbounded, and when utility < 0, the probability is less than 50%. Correct.\n\nParameter recovery is feasible with two parameters, which is manageable. The model is simple, avoiding high covariance between parameters. The parameters \u03b1 and \u03b2 are likely to be identifiable as they capture different aspects: baseline acceptance and sensitivity to fairness.\n\nAnother possible factor is the absolute amount of split_self. For example, even if the split is fair, if the absolute amount is too low, the participant might reject. But in this model, the absolute amount is already considered through split_self. However, the fair_self (3*token_self) is the participant's contribution in \u00a3. So, if split_self is equal to fair_self, the utility is \u03b2. So \u03b2 represents the baseline utility when the split is exactly fair. If \u03b2 is positive, even a fair split has a higher chance of acceptance. If \u03b2 is negative, even a fair split might be rejected. But perhaps participants have a baseline tendency to accept or reject regardless of fairness. This is captured by \u03b2.\n\nAlternatively, maybe participants have a different utility function that includes both the absolute split and the fairness. For example:\n\nUtility = \u03b2 + \u03b1*(split_self - fair_self) + \u03b3*split_self\n\nHere, \u03b3 captures the sensitivity to the absolute amount. But this adds another parameter (\u03b3), increasing model complexity. The instructions suggest a slight bias towards fewer parameters, so maybe stick with two parameters.\n\nAlternatively, the model could use the ratio of split_self to fair_self. For example:\n\nUtility = \u03b2 + \u03b1*(split_self / fair_self - 1)\n\nThis makes the fairness term a percentage difference. For example, if split_self is 1.2 times fair_self, the term is 0.2*\u03b1. This might normalize the effect across different contribution levels. However, this introduces division by fair_self, which is 3*token_self. Since token_self is at least 1 (assuming participants contribute at least one token), this is safe. This could be a better approach as it accounts for proportional differences. Let's consider this:\n\nUtility = \u03b2 + \u03b1*( (split_self / (3*token_self)) - 1 )\n\nThis simplifies to \u03b2 + \u03b1*(split_self - 3*token_self)/(3*token_self). So it's scaling the difference by the fair share. This might make \u03b1 more interpretable as a sensitivity to proportional differences. For example, \u03b1=2 would mean that a 10% increase over fair_self adds 0.2 to the utility.\n\nBut whether to use absolute or proportional difference depends on the assumed psychological mechanism. If participants are more sensitive to proportional unfairness, the proportional model is better. If they care about absolute \u00a3 differences, the absolute model is better. Given that the note mentions that equal split_self doesn't imply the same fairness because combined_earning varies, the proportional model might be more appropriate because it accounts for the participant's contribution.\n\nSo, revising the model:\n\nUtility = \u03b2 + \u03b1*( (split_self / fair_self) - 1 )\n\nWhere fair_self = 3*token_self.\n\nThis way, even if split_self is the same in two trials, if fair_self is different (due to different token_self), the utility will differ. For example, if split_self is \u00a36, and fair_self is \u00a36 (token_self=2), the term is 0. If fair_self is \u00a33 (token_self=1), split_self \u00a36 gives a term of (6/3 -1) = 1, so utility is \u03b2 + \u03b1*1.\n\nThis captures the proportional aspect, which aligns with the note about combined_earning varying.\n\nAlternatively, using the ratio directly:\n\nUtility = \u03b2 + \u03b1*(split_self / fair_self)\n\nBut then \u03b2 would represent the baseline when the ratio is 0, which isn't possible. The previous formulation centers the term around 1, so when split_self equals fair_self, the term is 0, and utility is \u03b2. That seems better.\n\nSo, the model becomes:\n\nUtility = \u03b2 + \u03b1*( (split_self / (3*token_self)) - 1 )\n\nThis can be rewritten as:\n\nUtility = \u03b2 + \u03b1*(split_self - 3*token_self) / (3*token_self)\n\nBut mathematically, it's the same as the previous expression.\n\nNow, parameters \u03b1 and \u03b2. \u03b1 represents how much the participant cares about proportional deviations from their fair share. A positive \u03b1 means that getting more than fair increases utility, leading to higher acceptance. A negative \u03b1 would imply that getting more than fair decreases utility, which doesn't make sense, so we might expect \u03b1 to be positive. However, the model should allow for the parameter to be learned, so the range for \u03b1 should be from, say, -10 to 10, but in practice, we might expect it to be positive. But leaving the bounds symmetric allows the model to capture unexpected patterns.\n\n\u03b2 is the baseline utility when the split is exactly fair. If \u03b2 is positive, even a fair split has a high chance of acceptance. If \u03b2 is negative, even a fair split might be rejected.\n\nThis model has two parameters, which is manageable for parameter recovery. The variables used are split_self and token_self, both available in the dataset.\n\nAnother consideration: the opponent's contribution. Maybe participants also compare their split to the opponent's split relative to the opponent's contribution. For example, if the opponent is getting more than their fair share, the participant might feel it's unfair. However, since split_opp = combined_earning - split_self, and combined_earning is 3*(token_self + token_opp), split_opp = 3*(token_self + token_opp) - split_self. The opponent's fair share is 3*token_opp. So, the difference for the opponent is split_opp - 3*token_opp = 3*token_self - split_self. Which is the negative of the participant's difference (split_self - 3*token_self). So, including both would introduce collinearity. Therefore, focusing on the participant's difference is sufficient.\n\nAlternatively, participants might care about the difference between their split and the opponent's split relative to their contributions. For example, (split_self / token_self) vs (split_opp / token_opp). If these ratios are unequal, it's unfair. But this would require more parameters. For example:\n\nUtility = \u03b2 + \u03b1*( (split_self / token_self) - (split_opp / token_opp) )\n\nBut split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self. So substituting:\n\nUtility = \u03b2 + \u03b1*( (split_self / token_self) - ( (3*(token_self + token_opp) - split_self) / token_opp ) )\n\nThis complicates the model and introduces more variables (token_opp), but token_opp is available in the dataset. However, this adds complexity with more parameters. Given the instruction to prefer fewer parameters, this might not be ideal. But perhaps this captures a different fairness notion. However, this model would have only \u03b1 and \u03b2 as parameters, same as before. Let's see:\n\nLet me expand the term:\n\n(split_self / token_self) - (split_opp / token_opp) = (split_self / token_self) - ( (3*(token_self + token_opp) - split_self) / token_opp )\n\n= (split_self / token_self) - [3*(token_self + token_opp)/token_opp - split_self/token_opp ]\n\n= split_self*(1/token_self + 1/token_opp) - 3*(token_self + token_opp)/token_opp\n\nThis seems complicated. Maybe this is a better way to model inequity aversion, where participants dislike unequal outcomes. But inequity aversion models often use the difference between own and other's payoffs. For example, Fehr & Schmidt's model:\n\nUtility = split_self - \u03b1*max(split_opp - split_self, 0) - \u03b2*max(split_self - split_opp, 0)\n\nBut this requires two parameters (\u03b1 and \u03b2) for disadvantageous and advantageous inequality. However, this model is more complex and has more parameters. Given the instruction to have a slight bias towards fewer parameters, maybe the initial model is better.\n\nAlternatively, a simplified inequity aversion model could be:\n\nUtility = split_self - \u03b3*(split_opp - split_self)\n\nBut this also introduces another parameter. However, in this case, \u03b3 is the sensitivity to inequality. But this model would have parameters \u03b2 (baseline) and \u03b3. But maybe:\n\nUtility = \u03b2 + \u03b1*split_self + \u03b3*(split_opp - split_self)\n\nBut this complicates things further. The original model with two parameters seems more parsimonious.\n\nConsidering all this, the initial model focusing on the participant's split relative to their fair share (3*token_self) with parameters \u03b1 and \u03b2 seems appropriate.\n\nNow, variable descriptions:\n\nVariables in the model:\n\n- \u03b2 (beta): Baseline utility when the split is fair. Learnable, range [-10, 10].\n- \u03b1 (alpha): Sensitivity to the proportional deviation from the fair share. Learnable, range [-10, 10].\n- split_self: From data, the proposed share for the participant.\n- token_self: From data, the participant's tokens.\n- fair_self: Calculated as 3*token_self.\n- The term (split_self / fair_self - 1) is calculated.\n\nThe utility is:\n\nUtility = beta + alpha * (split_self / (3 * token_self) - 1)\n\nThis can be written as:\n\nUtility = beta + alpha * ((split_self - 3 * token_self) / (3 * token_self))\n\nBut mathematically equivalent.\n\nThe variables in the JSON would include beta and alpha as learnable parameters, and split_self and token_self as data variables. The calculated variables (fair_self, the ratio term) are not listed as variables in the JSON since they are derived from data variables.\n\nThe target variable is 'accept', which is binary. The model's utility is passed through a logistic function to predict the probability of acceptance.\n\nSo, the final model specification is:\n\n<MODEL>\nUtility = beta + alpha * (split_self / (3 * token_self) - 1)\n</MODEL>\n\nVariables:\n\n- beta: learnable, range [-10, 10]\n- alpha: learnable, range [-10, 10]\n- split_self: from data\n- token_self: from data\n\nThe JSON for variables would list beta and alpha with their descriptions and ranges. Other variables like split_self and token_self are data variables, so they are marked as source:data and not learnable.\n\nNow, checking if all variables in the model are accounted for in the JSON. The model uses beta, alpha, split_self, and token_self. The JSON should include these.\n\nThe summary should explain that the model captures the responder's utility based on the deviation of the proposed split from their contribution-based fair share, with parameters for baseline acceptance and sensitivity to proportional fairness.\n</think>\n\n<MODEL>\nUtility = beta + alpha * (split_self / (3 * token_self) - 1)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"beta\": {\n      \"description\": \"Baseline utility when proposed split equals fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to proportional deviation from fair share\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on proportional deviation of the proposed split from the participant's contribution-based fair share. The model uses two parameters: a baseline utility (beta) and a sensitivity parameter (alpha) to fairness deviations. Utility is converted to acceptance probability via logistic function, with negative utility corresponding to <50% acceptance likelihood.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: Utility = beta + alpha * (split_self / (3 * token_self) - 1)\nSummary: A utility model where acceptance probability depends on proportional deviation of the proposed split from the participant's contribution-based fair share. The model uses two parameters: a baseline utility (beta) and a sensitivity parameter (alpha) to fairness deviations. Utility is converted to acceptance probability via logistic function, with negative utility corresponding to <50% acceptance likelihood.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, beta=0, alpha=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = beta + alpha * (split_self / (3 * token_self) - 1)\n\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.7793040293040293,
  "fitting_results": [
    {
      "beta": -0.12291586123346215,
      "alpha": 0.6168525258530239,
      "success": true,
      "log_likelihood": -6.587131168080202,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.36016465336991593,
      "alpha": 0.9306005041572212,
      "success": true,
      "log_likelihood": -14.829415373831713,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.7641647592537121,
      "alpha": 1.8591113878630001,
      "success": true,
      "log_likelihood": -9.102956056170548,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.28664706682056657,
      "alpha": 0.9031449192141537,
      "success": true,
      "log_likelihood": -14.520280061770595,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.5773809239914817,
      "alpha": 1.6396676098756406,
      "success": true,
      "log_likelihood": -9.62757413354674,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6057339625631815,
      "alpha": 1.2278938001088098,
      "success": true,
      "log_likelihood": -12.066171857144635,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -6.259187482794399,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 8.715653697475231,
      "success": true,
      "log_likelihood": -276.29658820875017,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -10.0,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -322.36191301696465,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.4006852207305402,
      "alpha": 9.02631359142191,
      "success": true,
      "log_likelihood": -1.1554142112147743,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -4.277639149745234,
      "alpha": -1.5141642434254194,
      "success": true,
      "log_likelihood": -92.10340357983036,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6426258580582342,
      "alpha": 1.922914094951997,
      "success": true,
      "log_likelihood": -8.176637788232998,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.287534319997043,
      "alpha": -4.635241125405036,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -3.9049144001165406,
      "success": true,
      "log_likelihood": -115.12925464660226,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": -92.10340371656177,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.5066372051998748,
      "alpha": 1.1758360582223644,
      "success": true,
      "log_likelihood": -12.801881301228743,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.0529057766604564,
      "alpha": -1.8026028354736587,
      "success": true,
      "log_likelihood": -115.12830788615751,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.13158722110693113,
      "alpha": 0.5569487551541482,
      "success": true,
      "log_likelihood": -17.69162976448604,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.3202164540128065,
      "alpha": 0.7828937983251831,
      "success": true,
      "log_likelihood": -16.41306021296818,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.4235627468307325,
      "alpha": 1.0902000803577558,
      "success": true,
      "log_likelihood": -13.404049061453986,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.5516065969465087,
      "alpha": 1.2218754869968216,
      "success": true,
      "log_likelihood": -12.40799555611285,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 8.59363435644422,
      "alpha": -0.05856381415434875,
      "success": true,
      "log_likelihood": -368.4136148770475,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.8085318880969379,
      "alpha": 3.6288493640147355,
      "success": true,
      "log_likelihood": -3.4019072288759213,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6596067116515952,
      "alpha": 0.8563591701471157,
      "success": true,
      "log_likelihood": -8.738231705767047,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 8.250392370319666,
      "alpha": -8.245135980366616,
      "success": true,
      "log_likelihood": -184.20680743672378,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -8.313638280488957,
      "alpha": -1.0141059632222778,
      "success": true,
      "log_likelihood": -69.07755278652138,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.1239053487503668,
      "alpha": 0.4415850947128018,
      "success": true,
      "log_likelihood": -20.05956342917932,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.2867800211479045,
      "alpha": 0.968892471255807,
      "success": true,
      "log_likelihood": -13.569222019142325,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 9.111751201493263,
      "alpha": 2.9311181333185026,
      "success": true,
      "log_likelihood": -299.3360620869261,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.921575522754893,
      "alpha": 1.7390638128219795,
      "success": true,
      "log_likelihood": -8.832710933990818,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 9.609510639319591,
      "alpha": -6.352958646258903,
      "success": true,
      "log_likelihood": -414.46531673712843,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 3.14675631841373,
      "success": true,
      "log_likelihood": -276.3102111568857,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -6.737381223588154,
      "success": true,
      "log_likelihood": -69.07755278652135,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.33434848355325725,
      "alpha": 0.6899330409869452,
      "success": true,
      "log_likelihood": -17.12268351471618,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -8.171667309413502,
      "alpha": -4.538577348237189,
      "success": true,
      "log_likelihood": -184.20680743672372,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 6.045661395754969,
      "alpha": -1.822233444652987,
      "success": true,
      "log_likelihood": -23.02585092644049,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": -368.4136148770475,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.5139667247587152,
      "alpha": 1.0845618538421278,
      "success": true,
      "log_likelihood": -13.319330489838341,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -7.137115584680444,
      "alpha": -3.6803529130310864,
      "success": true,
      "log_likelihood": -598.6721241773857,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.2777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -1.303321745748935,
      "success": true,
      "log_likelihood": -391.43946580708797,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.14845581247394876,
      "alpha": 0.5569514332175065,
      "success": true,
      "log_likelihood": -18.14747292479306,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -6.413722845286909,
      "alpha": 3.043791219476603,
      "success": true,
      "log_likelihood": -345.3877639470014,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.07315208000058165,
      "alpha": 0.16674271529413004,
      "success": true,
      "log_likelihood": -24.026373229637517,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.28185894101729947,
      "alpha": 0.46746097844809814,
      "success": true,
      "log_likelihood": -18.42276058969287,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -10.0,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -115.12925464660033,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.2485593541794642,
      "alpha": 0.6077355110108267,
      "success": true,
      "log_likelihood": -18.48199417848363,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -0.27733548809822767,
      "success": true,
      "log_likelihood": -345.3877639470071,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.36633406814282476,
      "alpha": 1.3671006943286934,
      "success": true,
      "log_likelihood": -24.509032945666803,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.7328939503979665,
      "alpha": 2.6965544568827555,
      "success": true,
      "log_likelihood": -5.081552911955783,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -10.0,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -230.25850929680269,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 1.2167961041297244,
      "success": true,
      "log_likelihood": -92.1034037165618,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.08325473237606998,
      "alpha": 0.5548626073706411,
      "success": true,
      "log_likelihood": -8.770101537579693,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.43499886756620515,
      "alpha": 0.6194986123491701,
      "success": true,
      "log_likelihood": -13.728722484279318,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.23873948954631138,
      "alpha": 0.4748941593845315,
      "success": true,
      "log_likelihood": -19.70547223459688,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -5.671689739175923,
      "alpha": -0.277084625544358,
      "success": true,
      "log_likelihood": -414.46531673712843,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -6.3673051055709635,
      "alpha": 5.387353604793566,
      "success": true,
      "log_likelihood": -69.07755167572428,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -7.256388904783568,
      "alpha": -5.325302888053347,
      "success": true,
      "log_likelihood": -368.41354703309673,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.13869371995154417,
      "alpha": 1.018083268771319,
      "success": true,
      "log_likelihood": -9.940384469159296,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.5497040117683127,
      "alpha": -0.8150437757238578,
      "success": true,
      "log_likelihood": -368.39148062415666,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6959990059948129,
      "alpha": 0.8480330594027644,
      "success": true,
      "log_likelihood": -7.364083961823166,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.0985666561514247,
      "alpha": -1.3106585361844658,
      "success": true,
      "log_likelihood": -115.1292524261838,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.3219965702466433,
      "alpha": 0.7432442803839698,
      "success": true,
      "log_likelihood": -16.825007598594013,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 8.799766053836496,
      "alpha": -6.687453850589944,
      "success": true,
      "log_likelihood": -69.07755278652137,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -10.0,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -92.1034037165599,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -10.0,
      "alpha": 10.0,
      "success": true,
      "log_likelihood": -115.12925464660033,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.2951785031792263,
      "alpha": 0.7218196381925971,
      "success": true,
      "log_likelihood": -17.10203109918386,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.3591486678614097,
      "alpha": 0.5159553505400678,
      "success": true,
      "log_likelihood": -15.606514038441725,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.0850258458442034,
      "alpha": 0.2291665067944731,
      "success": true,
      "log_likelihood": -23.40108888964006,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.49236934695294066,
      "alpha": 0.8980401315425225,
      "success": true,
      "log_likelihood": -14.029003095297696,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 8.615990932930462,
      "alpha": 2.132413574253466,
      "success": true,
      "log_likelihood": -46.051701856480875,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -10.0,
      "alpha": -9.632842172355852,
      "success": true,
      "log_likelihood": -46.05170185460763,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.137531489089336,
      "alpha": 0.23757998166217853,
      "success": true,
      "log_likelihood": -22.5572033130205,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.12291298749120429,
      "alpha": 0.6168600399779798,
      "success": true,
      "log_likelihood": -6.587131167339261,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.11357320813417981,
      "alpha": 0.2713711673094885,
      "success": true,
      "log_likelihood": -22.86887437481424,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 5.0422688127184045,
      "alpha": 2.8209638620688526,
      "success": true,
      "log_likelihood": -69.07752392114239,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -8.95054346895294,
      "alpha": 0.41075077045450215,
      "success": true,
      "log_likelihood": -92.10340371656179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.092133489027101,
      "alpha": 2.879510224347762,
      "success": true,
      "log_likelihood": -18.690624904595026,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.13228171692337212,
      "alpha": 0.22478036848331012,
      "success": true,
      "log_likelihood": -22.692040207033454,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.023170667206107942,
      "alpha": 1.0572455365094837,
      "success": true,
      "log_likelihood": -5.778449589969694,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.8090910283166064,
      "alpha": 0.6273307347198305,
      "success": true,
      "log_likelihood": -253.28419147571626,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -5.833318236113554,
      "alpha": 4.323416133098606,
      "success": true,
      "log_likelihood": -184.20680743672108,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.7834405539918381,
      "alpha": 0.8990883340789158,
      "success": true,
      "log_likelihood": -5.517065731733926,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.2616351002385879,
      "alpha": 0.727321641422987,
      "success": true,
      "log_likelihood": -16.875157343978785,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": -10.0,
      "success": true,
      "log_likelihood": -253.28436022684528,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "beta": {
        "r": 0.889903403302902,
        "p": 3.5253643497642056e-35
      },
      "alpha": {
        "r": 0.3554773708128231,
        "p": 0.00028410729257075047
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 195.17621560201087,
  "bic_results": {
    "average_bic": 195.17621560201087,
    "individual_bics": [
      20.34130021307262,
      36.82586862457565,
      25.372949989253314,
      36.20759800045341,
      26.4221861440057,
      31.299381591201488,
      7.167037869712219,
      559.7602142944125,
      7.167037869712219,
      651.8908639108415,
      9.477866299341768,
      191.37384503657293,
      23.520313453378215,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      237.42554717011674,
      191.37384531003576,
      7.167037869712219,
      32.770800479369704,
      237.42365364922725,
      42.5502974058843,
      39.99315830284858,
      33.97513599982019,
      31.98302898913792,
      743.9942676310072,
      13.970852334664063,
      24.643501288446316,
      375.5806527503598,
      7.167037869712219,
      145.32214344995498,
      47.28616473527086,
      34.30548191519687,
      7.167037869712219,
      605.8391620507645,
      24.832459744893853,
      836.0976713511691,
      559.7874601906836,
      145.32214344995492,
      41.41240490634458,
      375.58065275035966,
      53.2187397297932,
      743.9942676310072,
      33.805698856588904,
      1204.5112862316837,
      790.0459694910882,
      43.46198372649834,
      697.942565770915,
      55.21978433618725,
      44.01255905629796,
      237.42554717011288,
      44.13102623387948,
      697.9425657709264,
      56.185103768245824,
      17.330143700823786,
      467.6840564705176,
      191.37384531003582,
      24.707240952071608,
      34.62448284547086,
      46.577982346105976,
      836.0976713511691,
      145.32214122836078,
      743.9941319431057,
      27.047806815230814,
      743.9499991252255,
      21.89520580055855,
      237.4255427292798,
      40.817053074100244,
      145.32214344995495,
      191.373845310032,
      237.42554717011288,
      41.37110007527994,
      38.38006595379567,
      53.96921565619234,
      35.22504406750761,
      99.27044158987397,
      99.27044158612748,
      52.28144450295322,
      20.34130021159074,
      52.9047866265407,
      145.322085719197,
      191.3738453100358,
      44.54828768610227,
      52.551118290979126,
      18.723937056851607,
      513.7354208283448,
      375.5806527503544,
      18.20116934038007,
      40.91735256486979,
      513.7357583306028
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- beta: r = 0.890\n- alpha: r = 0.355",
  "previous_models": [
    "Specification: Utility = beta + alpha * (split_self / (3 * token_self) - 1)\nSummary: A utility model where acceptance probability depends on proportional deviation of the proposed split from the participant's contribution-based fair share. The model uses two parameters: a baseline utility (beta) and a sensitivity parameter (alpha) to fairness deviations. Utility is converted to acceptance probability via logistic function, with negative utility corresponding to <50% acceptance likelihood.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 195.17621560201087\nOverall Accuracy: 0.7793\n\nParameter Recovery:\n- beta: r = 0.890\n- alpha: r = 0.355"
  ]
}