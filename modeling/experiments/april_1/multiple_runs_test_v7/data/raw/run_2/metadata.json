{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility function should be such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The main target is \"accept\", which is binary (1 or 0). The model should predict this based on the utility of the proposal from the responder's perspective.\n\nThe dataset includes variables like token_self and token_opp, which represent the contributions of the participant and opponent, respectively. The combined_earning is the total money, calculated as 3*(token_self + token_opp). The split_self and split_opp are the proposed shares in pounds, and splitperc_self and splitperc_opp are the percentages. Since the trial_type indicates whether contributions are equal, opponent more, or participant more, this might influence fairness perceptions.\n\nThe responder's decision likely depends on how fair the proposed split is relative to their contribution. A common approach in such economic games is to model fairness using inequity aversion, where individuals prefer equitable outcomes and experience disutility from unequal splits. \n\nSo, the utility function could compare the proposed split to what would be considered a fair split based on contributions. The fair split might be proportional to each participant's contribution. For example, if the participant contributed token_self tokens and the opponent contributed token_opp, the fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. \n\nThe difference between the proposed split_self and this fair share could be a key factor. Additionally, people might have different sensitivities to being undercompensated versus overcompensated. For instance, they might be more sensitive to receiving less than their fair share (negative inequity) than receiving more (positive inequity). \n\nAnother consideration is the absolute amount of the proposed split. Even if the split is fair, a very low absolute amount might be rejected. So, the model should include both the fairness component and the absolute offer.\n\nPutting this together, the utility (U) could be a function of the proposed split (split_self), the fair split based on contributions, and parameters that capture sensitivity to inequity and the weight of the absolute offer.\n\nLet me outline possible components:\n\n1. Fairness Component: The difference between the proposed split and the fair share. Let's call this \u0394 = split_self - fair_self. Here, fair_self = (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so fair_self simplifies to 3*token_self. Wait, because combined_earning is 3*(token_self + token_opp), so the fair share based on contribution would be (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) = 3*token_self. So the fair split for the participant is exactly 3*token_self. Therefore, the proposed split_self minus 3*token_self gives the deviation from the fair share. \n\nBut wait, split_self is the proposed amount for the participant. So if split_self is less than 3*token_self, that's unfair to the participant. If it's more, it's advantageous. So the difference (split_self - 3*token_self) captures this. Let's call this \u0394. \n\nBut participants might react differently to positive vs. negative \u0394. For example, they might be more sensitive to negative \u0394 (being shortchanged) than positive. So we can model this with two parameters: \u03b1 (sensitivity to disadvantageous inequity, when \u0394 < 0) and \u03b2 (sensitivity to advantageous inequity, when \u0394 > 0). However, since the responder is deciding to accept or reject, perhaps only the negative \u0394 matters, because if the split is advantageous, they might still accept. Alternatively, maybe both matter but with different weights.\n\nAlternatively, the model could use a single parameter for inequity aversion, combining both cases. But splitting into two parameters might allow more flexibility. However, given the instruction to have a slight bias towards fewer parameters, maybe a single parameter.\n\nWait, but in the ultimatum game literature, responders often reject offers that are too low even if it's costly, indicating sensitivity to unfairness. So perhaps the key term is the difference between the proposed split and the fair split, but with a parameter that scales how much this affects utility.\n\nAnother angle: the utility could be the proposed split_self minus some measure of inequity. For example, if the responder feels the split is unfair, that subtracts from their utility. The inequity could be calculated as the difference between the split and the fair share, multiplied by a parameter.\n\nBut maybe the responder's utility is a combination of their own monetary gain and their aversion to inequity. So U = split_self - \u03b8 * |split_self - fair_self|. Here, \u03b8 represents the degree of inequity aversion. If \u03b8 is positive, deviations from the fair split (both positive and negative) reduce utility. But this might not capture the asymmetry between advantageous and disadvantageous inequity.\n\nAlternatively, separate parameters for negative and positive deviations. For example:\n\nU = split_self + \u03b1 * max(fair_self - split_self, 0) + \u03b2 * max(split_self - fair_self, 0)\n\nWait, no. If split_self is less than fair_self, then the participant is getting less than fair, which would decrease utility. So maybe:\n\nU = split_self - \u03b1 * (fair_self - split_self) if split_self < fair_self\n\nU = split_self - \u03b2 * (split_self - fair_self) if split_self > fair_self\n\nBut this complicates the model. Alternatively, using a linear combination with separate terms for positive and negative differences.\n\nAlternatively, model the utility as:\n\nU = \u03b3 * split_self + \u03b8 * (split_self - fair_self)\n\nBut this might not capture the non-linear aspects. Alternatively, use a reference point (fair_self) and have a linear loss or gain around that.\n\nAnother approach is to use a Fehr-Schmidt model of inequity aversion. In their model, utility depends on one's own payoff and the difference between one's own and others' payoffs. But in this case, the responder's own payoff is split_self, and the opponent's is split_opp. The combined_earning is split_self + split_opp. So the Fehr-Schmidt utility could be:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequality (when the opponent gets more), and \u03b2 is aversion to advantageous inequality. But in the context of this task, the split is proposed by the proposer (opponent), so the responder might be more sensitive to being offered less than their fair share based on contribution, not just the split amounts.\n\nAlternatively, the fair_self is 3*token_self, as calculated earlier. So the difference between split_self and fair_self is a key term. Let's call this \u0394 = split_self - 3*token_self. Then, the utility could be:\n\nU = split_self + \u03b1 * \u0394 if \u0394 < 0 (i.e., penalty for being below fair share)\nU = split_self + \u03b2 * \u0394 if \u0394 >= 0 (i.e., reward for being above fair share)\n\nBut this introduces two parameters, \u03b1 and \u03b2. However, the problem states that parameter recovery is important, so having too many parameters could be an issue. Maybe combine them into a single parameter, but that might not capture the asymmetry.\n\nAlternatively, model the utility as:\n\nU = split_self + \u03b8 * (split_self - fair_self)\n\nBut this would treat over and under the same. If \u03b8 is positive, then getting more than fair increases utility, less decreases. But maybe \u03b8 is negative, meaning that any deviation from fair is bad. Wait, that might not make sense. Alternatively, \u03b8 could be a parameter that scales how much deviations from fair affect utility. For example, if \u03b8 is positive, then getting more than fair is good, less is bad. But responders might reject offers that are too low even if U is positive. Hmm.\n\nAlternatively, the utility could be the monetary amount minus a term that represents the disutility from unfairness. For example:\n\nU = split_self - \u03b8 * (fair_self - split_self)^2\n\nThis would penalize deviations from the fair split quadratically. But this introduces a non-linearity. However, the problem states that the model should predict utility where less than 0 corresponds to less than 50% chance of acceptance. So the logistic function would convert U to a probability via P(accept) = 1 / (1 + exp(-U)). So if U is negative, P < 0.5.\n\nBut perhaps a linear model would suffice. Let's think of the key factors:\n\n1. The monetary gain: split_self. Higher offers are more likely to be accepted.\n\n2. The fairness of the offer relative to contribution: (split_self - fair_self). If this is positive, the offer is generous; if negative, it's unfair.\n\nBut how to combine these? Maybe the utility is a weighted sum:\n\nU = \u03b2 * split_self + \u03b8 * (split_self - fair_self)\n\nBut this might not capture the right dynamics. Alternatively, the fairness term could be (split_self / fair_self - 1), which represents the percentage deviation from the fair share. For example, if split_self is 90% of fair_self, that's a 10% under. So:\n\nU = \u03b2 * split_self + \u03b8 * (split_self / fair_self - 1)\n\nBut this introduces a ratio, which could complicate things. Alternatively, use the ratio directly as a measure of fairness. For instance, the split_self divided by fair_self gives how much of their fair share they're getting. If this is 1, it's fair. Less than 1 is unfair, more than 1 is generous.\n\nSo, U = \u03b2 * split_self + \u03b8 * (split_self / fair_self)\n\nBut then \u03b8 would scale the effect of the proportional fairness. Alternatively, maybe:\n\nU = split_self + \u03b8 * (split_self / fair_self - 1)\n\nThis way, \u03b8 represents the sensitivity to proportional fairness. If \u03b8 is positive, getting more than fair increases utility, less decreases. But if \u03b8 is negative, deviations in either direction reduce utility. Wait, no. If \u03b8 is positive, then (split_self/fair_self -1) is positive when split_self > fair_self, adding to utility. If split_self < fair_self, the term is negative, so \u03b8 * negative term would subtract from utility. So \u03b8 being positive would mean that the responder values both higher absolute offers and higher proportional fairness. But maybe the responder is willing to accept lower absolute offers if they are fair, but this model might not capture that. Hmm.\n\nAlternatively, the utility could be a combination of the absolute offer and the difference from the fair offer. For example:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - fair_self)\n\nHere, \u03b1 is the weight on the absolute offer, and \u03b2 is the weight on the deviation from fair. If \u03b2 is positive, then offers above fair increase utility, below decrease. But if \u03b2 is negative, that would mean that any deviation (positive or negative) reduces utility. But that might not make sense. Alternatively, maybe \u03b2 is a parameter that is positive, so higher deviations above fair increase utility, but lower (below) decrease. But how does that interact with \u03b1?\n\nAlternatively, the model should capture that participants are more likely to accept when the offer is higher and when it's closer to their fair share. So perhaps:\n\nU = split_self + \u03b8 * (split_self - fair_self)\n\nThis combines the absolute offer with the deviation from fair. If \u03b8 is positive, then offers above fair are more desirable, below less. But if \u03b8 is negative, then being above fair reduces utility, which doesn't make sense. So \u03b8 should be positive. But this might not capture the rejection of unfair offers. For example, if the offer is very low (split_self is much less than fair_self), then U could be negative, leading to rejection. If split_self is high enough, U is positive, leading to acceptance.\n\nWait, let's test with numbers. Suppose fair_self is 10 (3*token_self where token_self is 10/3, but maybe token_self is 3, so fair_self is 9). Suppose split_self is 5. Then U = 5 + \u03b8*(5-9) = 5 -4\u03b8. If \u03b8 is 0.5, then U=5-2=3, which is positive, so P(accept) is high. But if \u03b8 is 2, U=5-8=-3, leading to rejection. So \u03b8 controls how much the deviation from fair affects the utility. If \u03b8 is high, even small negative deviations can make U negative, leading to rejection. If \u03b8 is low, the absolute split_self dominates.\n\nBut maybe participants care more about the ratio than the absolute difference. For example, being offered 5 when fair is 10 (50%) is different from being offered 15 when fair is 20 (75%). So perhaps using a proportional term:\n\nU = split_self + \u03b8 * (split_self / fair_self - 1)\n\nIn this case, if split_self is equal to fair_self, the second term is zero. If split_self is half, then (0.5 -1) = -0.5, multiplied by \u03b8. So U = split_self - 0.5\u03b8. If \u03b8 is large enough, this could make U negative. For example, split_self=5, fair_self=10, \u03b8=10: U=5 -5=0. So P(accept)=0.5. If split_self=5 and \u03b8=12, U=5-6= -1, P= ~27%.\n\nBut this model combines the absolute split_self with a proportional fairness term. However, the problem states that the utility should be such that less than 0 corresponds to less than 50% chance. So the logistic function would map U to probabilities.\n\nAnother consideration is the role of the proposer's contribution (token_opp). Wait, the fair_self is based on the participant's and opponent's tokens. So fair_self = 3 * token_self, and fair_opp = 3 * token_opp. The combined_earning is 3*(token_self + token_opp). So the split_self + split_opp must equal combined_earning. Therefore, split_opp = combined_earning - split_self. But in the responder's perspective, they are evaluating the split_self offered to them. The proposer's contribution (token_opp) might influence the responder's perception of fairness. For example, if the proposer contributed more, maybe the responder expects a more favorable split.\n\nBut according to the problem statement, trial_type is 1 (equal), 2 (opponent more), 3 (participant more). This might affect the responder's expectations. However, the model should use variables available in the dataset. Since trial_type is available, perhaps it's a categorical variable that can be included. But trial_type is determined by token_self and token_opp. For example, trial_type=1 when token_self = token_opp, 2 when token_opp > token_self, etc. So maybe trial_type can be used to modulate the fairness calculation. However, including trial_type as a factor might complicate the model. Alternatively, since token_self and token_opp are available, we can compute the ratio or difference directly.\n\nWait, the fair_self is 3*token_self, which is the amount the participant contributed in monetary terms. The split_self is what's offered. So the difference between split_self and fair_self is a measure of how much the proposer is offering relative to the participant's contribution. If the proposer is offering less than fair_self, that's unfair. The responder's utility could depend on this difference, scaled by a parameter.\n\nSo, the model could be:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b8 represents the sensitivity to being offered less than the fair share. If the offer is equal to or more than fair_self, the second term is zero. If it's less, the term \u03b8*(fair_self - split_self) is subtracted, reducing utility. This way, the model captures the disutility from being shortchanged. The higher \u03b8, the more the responder penalizes unfair offers.\n\nAdditionally, the absolute value of split_self might matter. For example, even if the offer is fair (split_self = fair_self), if the absolute amount is too low, the responder might reject it. But in this model, if split_self is equal to fair_self, U = split_self. So if split_self is positive, U is positive, leading to acceptance. However, if the fair_self is very low (e.g., token_self is 1, so fair_self=3), and split_self is 3, then U=3. But maybe in reality, even a fair offer of 3 might be rejected if it's too low. So perhaps the model should include an intercept term, representing the minimal acceptable amount irrespective of fairness. For example:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0) - \u03b1\n\nHere, \u03b1 is a baseline parameter. If split_self is exactly fair_self, then U = fair_self - \u03b1. If \u03b1 is positive, then even fair offers could have U < 0 if fair_self < \u03b1. But this might not be desired. Alternatively, \u03b1 could represent a fixed cost of accepting, but that's not mentioned in the task.\n\nAlternatively, the model could have a parameter that scales the absolute offer. For example:\n\nU = \u03b2 * split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b2 represents the marginal utility of money. If \u03b2 is high, the responder values each pound more. \u03b8 is the penalty for unfairness. This allows for a trade-off between the absolute offer and fairness. For example, a higher \u03b2 would mean the responder is more willing to accept unfair offers if the absolute amount is high enough.\n\nThis seems plausible. The utility is a linear combination of the offer's monetary value and the unfairness penalty. The parameters \u03b2 and \u03b8 are learnable. The probability of acceptance is logistic(U).\n\nBut the problem states that the model should have a slight bias towards fewer parameters. So maybe combining \u03b2 and \u03b8 into a single parameter. For example, if we fix \u03b2=1, then U = split_self - \u03b8 * max(fair_self - split_self, 0). This reduces the number of parameters to one (\u03b8). However, this assumes that the marginal utility of money is 1, which might not be flexible enough. Alternatively, keeping both \u03b2 and \u03b8 as parameters.\n\nBut parameter recovery is a priority. If two parameters are highly correlated, it might be hard to recover them. For example, if \u03b2 and \u03b8 are both scaling factors, they might covary. To avoid this, perhaps fix \u03b2 to 1, as it's the coefficient for split_self, and have \u03b8 as the only learnable parameter. This would make the model have one parameter, which is good for recovery.\n\nAlternatively, include both \u03b2 and \u03b8 but with appropriate bounds. Let's think: \u03b2 should be positive, as more money is better. \u03b8 should also be positive, as being offered less than fair is bad. So:\n\nU = \u03b2 * split_self - \u03b8 * max(fair_self - split_self, 0)\n\nBut with \u03b2 and \u03b8 as learnable parameters. However, this introduces two parameters. If they are not identifiable, recovery is hard. For example, if \u03b2 and \u03b8 are scaled similarly, the model might not distinguish them. To test, suppose an offer of split_self=5, fair_self=10. Then U=5\u03b2 -5\u03b8. If \u03b2=1 and \u03b8=2, U=5-10=-5. If \u03b2=2 and \u03b8=3, U=10-15=-5. So different parameters can lead to the same utility, making them non-identifiable. Hence, including both might be problematic.\n\nTherefore, perhaps fixing \u03b2=1 to avoid identifiability issues. Then the model becomes:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nThis way, \u03b8 is the only learnable parameter, representing the penalty for each pound below the fair share. This model has one parameter, which is good for recovery.\n\nAnother consideration: the fair_self is 3*token_self. So the term max(fair_self - split_self, 0) is the amount by which the offer is less than the fair share. Multiplying by \u03b8 gives the disutility from this unfairness.\n\nSo, for example, if \u03b8=0, the responder only cares about the absolute offer. If \u03b8>0, they penalize offers below their fair share. The higher \u03b8, the more they are willing to reject unfair offers even if the absolute amount is decent.\n\nThis model seems simple and has one learnable parameter, which is good for parameter recovery. Let's test with some values:\n\nCase 1: Offer is fair (split_self = fair_self). Then U = split_self. Since split_self is positive, U is positive, so P(accept) > 0.5.\n\nCase 2: Offer is below fair. Suppose split_self = 5, fair_self=10. Then U=5 - \u03b8*(5). If \u03b8=1, U=0 \u2192 P=0.5. If \u03b8=2, U=5-10= -5 \u2192 P\u22480.007.\n\nCase 3: Offer is above fair. U = split_self, since max term is zero. So even if the offer is very generous, the utility is just split_self, which is positive, leading to acceptance.\n\nThis makes sense. Responders accept all offers that are at or above their fair share, and reject those below based on how much below they are and their \u03b8.\n\nBut wait, in reality, people might accept offers above fair_self, which this model does. However, in some cases, even if the offer is above fair_self but the absolute amount is very low, they might still reject. For example, if fair_self is 3 (token_self=1), and the offer is 4 (split_self=4), which is above fair, but 4 is still a small amount. But according to the model, U=4, which is positive, so P(accept) is high. However, in reality, someone might reject a very low absolute offer even if it's above fair. But the model doesn't capture that. However, given the task's variables, maybe this is acceptable. Alternatively, include an intercept term.\n\nWait, the problem states that the utility variable may be unbounded, but the model must predict accept based on utility. The logistic function converts U to a probability. So even if the offer is above fair_self, if split_self is very low, U could be low but positive. For example, split_self=1, fair_self=0.5 (token_self=0.5/3? Wait, token_self is an integer, as per the dataset variables (int64). So token_self is an integer, so fair_self=3*token_self. So if token_self=1, fair_self=3. If split_self=2, which is below fair, U=2 - \u03b8*(1). If \u03b8=3, U=2-3= -1 \u2192 P= ~27%. If split_self=4 (above fair), U=4 \u2192 P\u224898%, which is high. But if split_self=1 and fair_self=3, U=1 - 2\u03b8. If \u03b8=1, U= -1 \u2192 P=27%. So the model allows for rejecting below-fair offers and accepting above-fair, but in reality, even above-fair but low absolute offers might be rejected. However, the model as is doesn't account for that. To capture that, we might need another parameter, like a intercept term \u03b1:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0) - \u03b1\n\nHere, \u03b1 represents a fixed cost or threshold. If \u03b1 is positive, even offers above fair_self could have U negative if split_self < \u03b1. For example, \u03b1=5: split_self=4, fair_self=3 \u2192 U=4 -0 -5= -1 \u2192 P=27%. But this complicates the model with another parameter. However, the problem allows for a few parameters as long as they are recoverable. But given the instruction to have a slight bias towards fewer parameters, maybe stick with the original model with \u03b8 only.\n\nAlternatively, include both \u03b8 and \u03b1. But that's two parameters. Let's see if that's manageable. The utility would be:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0) - \u03b1\n\nHere, \u03b1 is a baseline parameter. If \u03b1 is positive, the responder requires a higher split_self to accept. For example, even if the offer is fair (split_self = fair_self), U = fair_self - \u03b1. If fair_self > \u03b1, U is positive. If fair_self < \u03b1, U is negative. But this might not align with the task's requirements. For instance, if the participant's fair_self is low (e.g., 3), and \u03b1 is 5, then even a fair offer would have U=3-5=-2 \u2192 rejected. But in reality, participants might accept fair offers regardless of the absolute amount. So this might not be appropriate.\n\nAlternatively, \u03b1 could represent a minimal acceptable amount irrespective of fairness. For example:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0) + \u03b3 * (split_self - \u03b1)\n\nBut this complicates further. Maybe it's better to start with the simpler model and see if it fits.\n\nAnother angle: the problem mentions that two equal split_self variables do not represent the same fairness because combined_earning varies. For example, a split_self of 5 when combined_earning is 10 (so 50%) is different from split_self=5 when combined_earning is 20 (25%). But in the current model, the fairness is based on the participant's contribution (fair_self=3*token_self), which is independent of combined_earning. Wait, combined_earning is 3*(token_self + token_opp). So fair_self is 3*token_self, and the fair split would be split_self=3*token_self, split_opp=3*token_opp. So the fairness is based on contribution, not on the total. Therefore, the model accounts for varying combined_earning because fair_self is based on token_self, which varies per trial.\n\nThus, the model U = split_self - \u03b8 * max(fair_self - split_self, 0) should handle varying combined_earning because fair_self is 3*token_self, which depends on the trial's token_self.\n\nNow, considering the variables in the dataset: token_self and token_opp are available, so fair_self can be computed as 3*token_self. split_self is available. So the model can be implemented using these variables.\n\nThe model's parameters would be \u03b8, which has to be bounded. Since \u03b8 represents the penalty per unit of unfairness (pounds below fair_self), it should be positive. We can set a reasonable bound, like 0 to 10, inclusive. This allows \u03b8 to be up to 10, meaning that each pound below fair_self reduces utility by 10, which would heavily penalize unfair offers.\n\nSo the variable description for \u03b8 would be:\n\n\"theta\": {\n  \"description\": \"Sensitivity to disadvantageous inequity (penalty per \u00a31 below fair share)\",\n  \"range\": {\n    \"min\": 0,\n    \"max\": 10,\n    \"inclusive_min\": true,\n    \"inclusive_max\": true\n  },\n  \"learnable\": true,\n  \"source\": \"learnable\"\n}\n\nOther variables:\n\n- split_self: from data, used in the model.\n- fair_self: calculated as 3 * token_self.\n- max_term: calculated as max(fair_self - split_self, 0).\n\nBut in the model formula, these can be incorporated directly.\n\nThe mathematical model would be:\n\nU = split_self - theta * max(3*token_self - split_self, 0)\n\nThis utility is then passed through a logistic function to get P(accept) = 1 / (1 + exp(-U)).\n\nThis model uses only one learnable parameter (theta), which is good for parameter recovery. It captures the idea that responders penalize offers below their fair share (based on their contribution) and the penalty is proportional to the deficit. The absolute offer also matters, as higher offers increase utility.\n\nTesting edge cases:\n\n- If theta is 0: U = split_self. So any positive split_self leads to P>0.5. But if split_self is 0, U=0 \u2192 P=0.5. However, in the dataset, split_self is a float, but likely positive.\n\n- If theta is very high (e.g., 10), then even a small deficit leads to a large penalty. For example, split_self=8, fair_self=10: U=8 -10*2=8-20=-12 \u2192 P\u22480.000006%, so rejection.\n\nThis seems to align with typical ultimatum game behavior where low offers are rejected.\n\nNow, the model needs to predict 'accept' based on U. The target variable is 'accept', which is binary. The model's utility function is as above, and the probability of acceptance is logistic(U). The parameters can be estimated via maximum likelihood estimation.\n\nAnother consideration: the participant's role is responder (trial_role=1). The model should only consider trials where trial_role=1. So during model fitting, we filter the dataset to responder trials.\n\nIn terms of variable usage, the model uses split_self and token_self from the dataset. combined_earning is not directly used because fair_self is computed as 3*token_self, and combined_earning is 3*(token_self + token_opp), but the model doesn't need combined_earning since the fair_self is based on the participant's contribution.\n\nThus, the model is:\n\n<MODEL>\nU = split_self - theta * max(3 * token_self - split_self, 0)\n</MODEL>\n\nVariables:\n\n- theta: learnable parameter, range [0, 10]\n- split_self: from data\n- token_self: from data\n- max term is calculated\n\nThe variable descriptions in JSON would be:\n\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity to disadvantageous inequity (penalty per \u00a31 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe target variable is 'accept'.\n\nThis model should meet the requirements: predicts responder behavior, uses available variables, has one learnable parameter with bounds, and the utility determines the probability via logistic function.\n</think>\n\n<MODEL>\nU = split_self - theta * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity to disadvantageous inequity (penalty per \u00a31 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the absolute monetary offer and a penalty for offers below the participant's contribution-based fair share. The penalty scales linearly with the deficit relative to 3\u00d7token_self (fair_self). Uses one learnable parameter (theta) controlling sensitivity to disadvantageous inequity, ensuring good parameter recovery. Captures trade-offs between monetary gain and fairness considerations.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['theta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = split_self - theta * max(3 * token_self - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity to disadvantageous inequity (penalty per \\u00a31 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, theta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        fair_self = 3 * token_self\n        \n        U = split_self - theta * max(fair_self - split_self, 0)\n        \n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '''\n[\n  {\n    \"ID\": 1,\n    \"group\": \"control\",\n    \"trial_number\": 1,\n    \"trial_type\": 1,\n    \"trial_role\": 1,\n    \"token_opp\": 5,\n    \"token_self\": 5,\n    \"combined_earning\": 30,\n    \"split_opp\": 15.0,\n    \"split_self\": 15.0,\n    \"splitperc_opp\": 50,\n    \"splitperc_self\": 50,\n    \"accept\": 1.0,\n    \"accepted_amount\": 15.0,\n    \"accepted_perc\": 50.0,\n    \"proposed_perc\": 50.0,\n    \"proposed_amount\": 15.0\n  },\n  {\n    \"ID\": 2,\n    \"group\": \"control\",\n    \"trial_number\": 2,\n    \"trial_type\": 2,\n    \"trial_role\": 1,\n    \"token_opp\": 7,\n    \"token_self\": 3,\n    \"combined_earning\": 30,\n    \"split_opp\": 20.0,\n    \"split_self\": 10.0,\n    \"splitperc_opp\": 67,\n    \"splitperc_self\": 33,\n    \"accept\": 0.0,\n    \"accepted_amount\": 0.0,\n    \"accepted_perc\": 0.0,\n    \"proposed_perc\": 33.0,\n    \"proposed_amount\": 10.0\n  },\n  {\n    \"ID\": 3,\n    \"group\": \"control\",\n    \"trial_number\": 3,\n    \"trial_type\": 3,\n    \"trial_role\": 1,\n    \"token_opp\": 3,\n    \"token_self\": 7,\n    \"combined_earning\": 30,\n    \"split_opp\": 10.0,\n    \"split_self\": 20.0,\n    \"splitperc_opp\": 33,\n    \"splitperc_self\": 67,\n    \"accept\": 1.0,\n    \"accepted_amount\": 20.0,\n    \"accepted_perc\": 67.0,\n    \"proposed_perc\": 67.0,\n    \"proposed_amount\": 20.0\n  }\n]\n'''\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1743518874.883052
    }
  ],
  "learnable_parameters": {
    "theta": {
      "description": "Sensitivity to disadvantageous inequity (penalty per \u00a31 below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = split_self - theta * max(3 * token_self - split_self, 0)",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "theta": {
      "description": "Sensitivity to disadvantageous inequity (penalty per \u00a31 below fair share)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model where acceptance probability depends on the absolute monetary offer and a penalty for offers below the participant's contribution-based fair share. The penalty scales linearly with the deficit relative to 3\u00d7token_self (fair_self). Uses one learnable parameter (theta) controlling sensitivity to disadvantageous inequity, ensuring good parameter recovery. Captures trade-offs between monetary gain and fairness considerations.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (acting as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Also, the utility function should be such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The main target is \"accept\", which is binary (1 or 0). The model should predict this based on the utility of the proposal from the responder's perspective.\n\nThe dataset includes variables like token_self and token_opp, which represent the contributions of the participant and opponent, respectively. The combined_earning is the total money, calculated as 3*(token_self + token_opp). The split_self and split_opp are the proposed shares in pounds, and splitperc_self and splitperc_opp are the percentages. Since the trial_type indicates whether contributions are equal, opponent more, or participant more, this might influence fairness perceptions.\n\nThe responder's decision likely depends on how fair the proposed split is relative to their contribution. A common approach in such economic games is to model fairness using inequity aversion, where individuals prefer equitable outcomes and experience disutility from unequal splits. \n\nSo, the utility function could compare the proposed split to what would be considered a fair split based on contributions. The fair split might be proportional to each participant's contribution. For example, if the participant contributed token_self tokens and the opponent contributed token_opp, the fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. \n\nThe difference between the proposed split_self and this fair share could be a key factor. Additionally, people might have different sensitivities to being undercompensated versus overcompensated. For instance, they might be more sensitive to receiving less than their fair share (negative inequity) than receiving more (positive inequity). \n\nAnother consideration is the absolute amount of the proposed split. Even if the split is fair, a very low absolute amount might be rejected. So, the model should include both the fairness component and the absolute offer.\n\nPutting this together, the utility (U) could be a function of the proposed split (split_self), the fair split based on contributions, and parameters that capture sensitivity to inequity and the weight of the absolute offer.\n\nLet me outline possible components:\n\n1. Fairness Component: The difference between the proposed split and the fair share. Let's call this \u0394 = split_self - fair_self. Here, fair_self = (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so fair_self simplifies to 3*token_self. Wait, because combined_earning is 3*(token_self + token_opp), so the fair share based on contribution would be (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) = 3*token_self. So the fair split for the participant is exactly 3*token_self. Therefore, the proposed split_self minus 3*token_self gives the deviation from the fair share. \n\nBut wait, split_self is the proposed amount for the participant. So if split_self is less than 3*token_self, that's unfair to the participant. If it's more, it's advantageous. So the difference (split_self - 3*token_self) captures this. Let's call this \u0394. \n\nBut participants might react differently to positive vs. negative \u0394. For example, they might be more sensitive to negative \u0394 (being shortchanged) than positive. So we can model this with two parameters: \u03b1 (sensitivity to disadvantageous inequity, when \u0394 < 0) and \u03b2 (sensitivity to advantageous inequity, when \u0394 > 0). However, since the responder is deciding to accept or reject, perhaps only the negative \u0394 matters, because if the split is advantageous, they might still accept. Alternatively, maybe both matter but with different weights.\n\nAlternatively, the model could use a single parameter for inequity aversion, combining both cases. But splitting into two parameters might allow more flexibility. However, given the instruction to have a slight bias towards fewer parameters, maybe a single parameter.\n\nWait, but in the ultimatum game literature, responders often reject offers that are too low even if it's costly, indicating sensitivity to unfairness. So perhaps the key term is the difference between the proposed split and the fair split, but with a parameter that scales how much this affects utility.\n\nAnother angle: the utility could be the proposed split_self minus some measure of inequity. For example, if the responder feels the split is unfair, that subtracts from their utility. The inequity could be calculated as the difference between the split and the fair share, multiplied by a parameter.\n\nBut maybe the responder's utility is a combination of their own monetary gain and their aversion to inequity. So U = split_self - \u03b8 * |split_self - fair_self|. Here, \u03b8 represents the degree of inequity aversion. If \u03b8 is positive, deviations from the fair split (both positive and negative) reduce utility. But this might not capture the asymmetry between advantageous and disadvantageous inequity.\n\nAlternatively, separate parameters for negative and positive deviations. For example:\n\nU = split_self + \u03b1 * max(fair_self - split_self, 0) + \u03b2 * max(split_self - fair_self, 0)\n\nWait, no. If split_self is less than fair_self, then the participant is getting less than fair, which would decrease utility. So maybe:\n\nU = split_self - \u03b1 * (fair_self - split_self) if split_self < fair_self\n\nU = split_self - \u03b2 * (split_self - fair_self) if split_self > fair_self\n\nBut this complicates the model. Alternatively, using a linear combination with separate terms for positive and negative differences.\n\nAlternatively, model the utility as:\n\nU = \u03b3 * split_self + \u03b8 * (split_self - fair_self)\n\nBut this might not capture the non-linear aspects. Alternatively, use a reference point (fair_self) and have a linear loss or gain around that.\n\nAnother approach is to use a Fehr-Schmidt model of inequity aversion. In their model, utility depends on one's own payoff and the difference between one's own and others' payoffs. But in this case, the responder's own payoff is split_self, and the opponent's is split_opp. The combined_earning is split_self + split_opp. So the Fehr-Schmidt utility could be:\n\nU = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequality (when the opponent gets more), and \u03b2 is aversion to advantageous inequality. But in the context of this task, the split is proposed by the proposer (opponent), so the responder might be more sensitive to being offered less than their fair share based on contribution, not just the split amounts.\n\nAlternatively, the fair_self is 3*token_self, as calculated earlier. So the difference between split_self and fair_self is a key term. Let's call this \u0394 = split_self - 3*token_self. Then, the utility could be:\n\nU = split_self + \u03b1 * \u0394 if \u0394 < 0 (i.e., penalty for being below fair share)\nU = split_self + \u03b2 * \u0394 if \u0394 >= 0 (i.e., reward for being above fair share)\n\nBut this introduces two parameters, \u03b1 and \u03b2. However, the problem states that parameter recovery is important, so having too many parameters could be an issue. Maybe combine them into a single parameter, but that might not capture the asymmetry.\n\nAlternatively, model the utility as:\n\nU = split_self + \u03b8 * (split_self - fair_self)\n\nBut this would treat over and under the same. If \u03b8 is positive, then getting more than fair increases utility, less decreases. But maybe \u03b8 is negative, meaning that any deviation from fair is bad. Wait, that might not make sense. Alternatively, \u03b8 could be a parameter that scales how much deviations from fair affect utility. For example, if \u03b8 is positive, then getting more than fair is good, less is bad. But responders might reject offers that are too low even if U is positive. Hmm.\n\nAlternatively, the utility could be the monetary amount minus a term that represents the disutility from unfairness. For example:\n\nU = split_self - \u03b8 * (fair_self - split_self)^2\n\nThis would penalize deviations from the fair split quadratically. But this introduces a non-linearity. However, the problem states that the model should predict utility where less than 0 corresponds to less than 50% chance of acceptance. So the logistic function would convert U to a probability via P(accept) = 1 / (1 + exp(-U)). So if U is negative, P < 0.5.\n\nBut perhaps a linear model would suffice. Let's think of the key factors:\n\n1. The monetary gain: split_self. Higher offers are more likely to be accepted.\n\n2. The fairness of the offer relative to contribution: (split_self - fair_self). If this is positive, the offer is generous; if negative, it's unfair.\n\nBut how to combine these? Maybe the utility is a weighted sum:\n\nU = \u03b2 * split_self + \u03b8 * (split_self - fair_self)\n\nBut this might not capture the right dynamics. Alternatively, the fairness term could be (split_self / fair_self - 1), which represents the percentage deviation from the fair share. For example, if split_self is 90% of fair_self, that's a 10% under. So:\n\nU = \u03b2 * split_self + \u03b8 * (split_self / fair_self - 1)\n\nBut this introduces a ratio, which could complicate things. Alternatively, use the ratio directly as a measure of fairness. For instance, the split_self divided by fair_self gives how much of their fair share they're getting. If this is 1, it's fair. Less than 1 is unfair, more than 1 is generous.\n\nSo, U = \u03b2 * split_self + \u03b8 * (split_self / fair_self)\n\nBut then \u03b8 would scale the effect of the proportional fairness. Alternatively, maybe:\n\nU = split_self + \u03b8 * (split_self / fair_self - 1)\n\nThis way, \u03b8 represents the sensitivity to proportional fairness. If \u03b8 is positive, getting more than fair increases utility, less decreases. But if \u03b8 is negative, deviations in either direction reduce utility. Wait, no. If \u03b8 is positive, then (split_self/fair_self -1) is positive when split_self > fair_self, adding to utility. If split_self < fair_self, the term is negative, so \u03b8 * negative term would subtract from utility. So \u03b8 being positive would mean that the responder values both higher absolute offers and higher proportional fairness. But maybe the responder is willing to accept lower absolute offers if they are fair, but this model might not capture that. Hmm.\n\nAlternatively, the utility could be a combination of the absolute offer and the difference from the fair offer. For example:\n\nU = \u03b1 * split_self + \u03b2 * (split_self - fair_self)\n\nHere, \u03b1 is the weight on the absolute offer, and \u03b2 is the weight on the deviation from fair. If \u03b2 is positive, then offers above fair increase utility, below decrease. But if \u03b2 is negative, that would mean that any deviation (positive or negative) reduces utility. But that might not make sense. Alternatively, maybe \u03b2 is a parameter that is positive, so higher deviations above fair increase utility, but lower (below) decrease. But how does that interact with \u03b1?\n\nAlternatively, the model should capture that participants are more likely to accept when the offer is higher and when it's closer to their fair share. So perhaps:\n\nU = split_self + \u03b8 * (split_self - fair_self)\n\nThis combines the absolute offer with the deviation from fair. If \u03b8 is positive, then offers above fair are more desirable, below less. But if \u03b8 is negative, then being above fair reduces utility, which doesn't make sense. So \u03b8 should be positive. But this might not capture the rejection of unfair offers. For example, if the offer is very low (split_self is much less than fair_self), then U could be negative, leading to rejection. If split_self is high enough, U is positive, leading to acceptance.\n\nWait, let's test with numbers. Suppose fair_self is 10 (3*token_self where token_self is 10/3, but maybe token_self is 3, so fair_self is 9). Suppose split_self is 5. Then U = 5 + \u03b8*(5-9) = 5 -4\u03b8. If \u03b8 is 0.5, then U=5-2=3, which is positive, so P(accept) is high. But if \u03b8 is 2, U=5-8=-3, leading to rejection. So \u03b8 controls how much the deviation from fair affects the utility. If \u03b8 is high, even small negative deviations can make U negative, leading to rejection. If \u03b8 is low, the absolute split_self dominates.\n\nBut maybe participants care more about the ratio than the absolute difference. For example, being offered 5 when fair is 10 (50%) is different from being offered 15 when fair is 20 (75%). So perhaps using a proportional term:\n\nU = split_self + \u03b8 * (split_self / fair_self - 1)\n\nIn this case, if split_self is equal to fair_self, the second term is zero. If split_self is half, then (0.5 -1) = -0.5, multiplied by \u03b8. So U = split_self - 0.5\u03b8. If \u03b8 is large enough, this could make U negative. For example, split_self=5, fair_self=10, \u03b8=10: U=5 -5=0. So P(accept)=0.5. If split_self=5 and \u03b8=12, U=5-6= -1, P= ~27%.\n\nBut this model combines the absolute split_self with a proportional fairness term. However, the problem states that the utility should be such that less than 0 corresponds to less than 50% chance. So the logistic function would map U to probabilities.\n\nAnother consideration is the role of the proposer's contribution (token_opp). Wait, the fair_self is based on the participant's and opponent's tokens. So fair_self = 3 * token_self, and fair_opp = 3 * token_opp. The combined_earning is 3*(token_self + token_opp). So the split_self + split_opp must equal combined_earning. Therefore, split_opp = combined_earning - split_self. But in the responder's perspective, they are evaluating the split_self offered to them. The proposer's contribution (token_opp) might influence the responder's perception of fairness. For example, if the proposer contributed more, maybe the responder expects a more favorable split.\n\nBut according to the problem statement, trial_type is 1 (equal), 2 (opponent more), 3 (participant more). This might affect the responder's expectations. However, the model should use variables available in the dataset. Since trial_type is available, perhaps it's a categorical variable that can be included. But trial_type is determined by token_self and token_opp. For example, trial_type=1 when token_self = token_opp, 2 when token_opp > token_self, etc. So maybe trial_type can be used to modulate the fairness calculation. However, including trial_type as a factor might complicate the model. Alternatively, since token_self and token_opp are available, we can compute the ratio or difference directly.\n\nWait, the fair_self is 3*token_self, which is the amount the participant contributed in monetary terms. The split_self is what's offered. So the difference between split_self and fair_self is a measure of how much the proposer is offering relative to the participant's contribution. If the proposer is offering less than fair_self, that's unfair. The responder's utility could depend on this difference, scaled by a parameter.\n\nSo, the model could be:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b8 represents the sensitivity to being offered less than the fair share. If the offer is equal to or more than fair_self, the second term is zero. If it's less, the term \u03b8*(fair_self - split_self) is subtracted, reducing utility. This way, the model captures the disutility from being shortchanged. The higher \u03b8, the more the responder penalizes unfair offers.\n\nAdditionally, the absolute value of split_self might matter. For example, even if the offer is fair (split_self = fair_self), if the absolute amount is too low, the responder might reject it. But in this model, if split_self is equal to fair_self, U = split_self. So if split_self is positive, U is positive, leading to acceptance. However, if the fair_self is very low (e.g., token_self is 1, so fair_self=3), and split_self is 3, then U=3. But maybe in reality, even a fair offer of 3 might be rejected if it's too low. So perhaps the model should include an intercept term, representing the minimal acceptable amount irrespective of fairness. For example:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0) - \u03b1\n\nHere, \u03b1 is a baseline parameter. If split_self is exactly fair_self, then U = fair_self - \u03b1. If \u03b1 is positive, then even fair offers could have U < 0 if fair_self < \u03b1. But this might not be desired. Alternatively, \u03b1 could represent a fixed cost of accepting, but that's not mentioned in the task.\n\nAlternatively, the model could have a parameter that scales the absolute offer. For example:\n\nU = \u03b2 * split_self - \u03b8 * max(fair_self - split_self, 0)\n\nHere, \u03b2 represents the marginal utility of money. If \u03b2 is high, the responder values each pound more. \u03b8 is the penalty for unfairness. This allows for a trade-off between the absolute offer and fairness. For example, a higher \u03b2 would mean the responder is more willing to accept unfair offers if the absolute amount is high enough.\n\nThis seems plausible. The utility is a linear combination of the offer's monetary value and the unfairness penalty. The parameters \u03b2 and \u03b8 are learnable. The probability of acceptance is logistic(U).\n\nBut the problem states that the model should have a slight bias towards fewer parameters. So maybe combining \u03b2 and \u03b8 into a single parameter. For example, if we fix \u03b2=1, then U = split_self - \u03b8 * max(fair_self - split_self, 0). This reduces the number of parameters to one (\u03b8). However, this assumes that the marginal utility of money is 1, which might not be flexible enough. Alternatively, keeping both \u03b2 and \u03b8 as parameters.\n\nBut parameter recovery is a priority. If two parameters are highly correlated, it might be hard to recover them. For example, if \u03b2 and \u03b8 are both scaling factors, they might covary. To avoid this, perhaps fix \u03b2 to 1, as it's the coefficient for split_self, and have \u03b8 as the only learnable parameter. This would make the model have one parameter, which is good for recovery.\n\nAlternatively, include both \u03b2 and \u03b8 but with appropriate bounds. Let's think: \u03b2 should be positive, as more money is better. \u03b8 should also be positive, as being offered less than fair is bad. So:\n\nU = \u03b2 * split_self - \u03b8 * max(fair_self - split_self, 0)\n\nBut with \u03b2 and \u03b8 as learnable parameters. However, this introduces two parameters. If they are not identifiable, recovery is hard. For example, if \u03b2 and \u03b8 are scaled similarly, the model might not distinguish them. To test, suppose an offer of split_self=5, fair_self=10. Then U=5\u03b2 -5\u03b8. If \u03b2=1 and \u03b8=2, U=5-10=-5. If \u03b2=2 and \u03b8=3, U=10-15=-5. So different parameters can lead to the same utility, making them non-identifiable. Hence, including both might be problematic.\n\nTherefore, perhaps fixing \u03b2=1 to avoid identifiability issues. Then the model becomes:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0)\n\nThis way, \u03b8 is the only learnable parameter, representing the penalty for each pound below the fair share. This model has one parameter, which is good for recovery.\n\nAnother consideration: the fair_self is 3*token_self. So the term max(fair_self - split_self, 0) is the amount by which the offer is less than the fair share. Multiplying by \u03b8 gives the disutility from this unfairness.\n\nSo, for example, if \u03b8=0, the responder only cares about the absolute offer. If \u03b8>0, they penalize offers below their fair share. The higher \u03b8, the more they are willing to reject unfair offers even if the absolute amount is decent.\n\nThis model seems simple and has one learnable parameter, which is good for parameter recovery. Let's test with some values:\n\nCase 1: Offer is fair (split_self = fair_self). Then U = split_self. Since split_self is positive, U is positive, so P(accept) > 0.5.\n\nCase 2: Offer is below fair. Suppose split_self = 5, fair_self=10. Then U=5 - \u03b8*(5). If \u03b8=1, U=0 \u2192 P=0.5. If \u03b8=2, U=5-10= -5 \u2192 P\u22480.007.\n\nCase 3: Offer is above fair. U = split_self, since max term is zero. So even if the offer is very generous, the utility is just split_self, which is positive, leading to acceptance.\n\nThis makes sense. Responders accept all offers that are at or above their fair share, and reject those below based on how much below they are and their \u03b8.\n\nBut wait, in reality, people might accept offers above fair_self, which this model does. However, in some cases, even if the offer is above fair_self but the absolute amount is very low, they might still reject. For example, if fair_self is 3 (token_self=1), and the offer is 4 (split_self=4), which is above fair, but 4 is still a small amount. But according to the model, U=4, which is positive, so P(accept) is high. However, in reality, someone might reject a very low absolute offer even if it's above fair. But the model doesn't capture that. However, given the task's variables, maybe this is acceptable. Alternatively, include an intercept term.\n\nWait, the problem states that the utility variable may be unbounded, but the model must predict accept based on utility. The logistic function converts U to a probability. So even if the offer is above fair_self, if split_self is very low, U could be low but positive. For example, split_self=1, fair_self=0.5 (token_self=0.5/3? Wait, token_self is an integer, as per the dataset variables (int64). So token_self is an integer, so fair_self=3*token_self. So if token_self=1, fair_self=3. If split_self=2, which is below fair, U=2 - \u03b8*(1). If \u03b8=3, U=2-3= -1 \u2192 P= ~27%. If split_self=4 (above fair), U=4 \u2192 P\u224898%, which is high. But if split_self=1 and fair_self=3, U=1 - 2\u03b8. If \u03b8=1, U= -1 \u2192 P=27%. So the model allows for rejecting below-fair offers and accepting above-fair, but in reality, even above-fair but low absolute offers might be rejected. However, the model as is doesn't account for that. To capture that, we might need another parameter, like a intercept term \u03b1:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0) - \u03b1\n\nHere, \u03b1 represents a fixed cost or threshold. If \u03b1 is positive, even offers above fair_self could have U negative if split_self < \u03b1. For example, \u03b1=5: split_self=4, fair_self=3 \u2192 U=4 -0 -5= -1 \u2192 P=27%. But this complicates the model with another parameter. However, the problem allows for a few parameters as long as they are recoverable. But given the instruction to have a slight bias towards fewer parameters, maybe stick with the original model with \u03b8 only.\n\nAlternatively, include both \u03b8 and \u03b1. But that's two parameters. Let's see if that's manageable. The utility would be:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0) - \u03b1\n\nHere, \u03b1 is a baseline parameter. If \u03b1 is positive, the responder requires a higher split_self to accept. For example, even if the offer is fair (split_self = fair_self), U = fair_self - \u03b1. If fair_self > \u03b1, U is positive. If fair_self < \u03b1, U is negative. But this might not align with the task's requirements. For instance, if the participant's fair_self is low (e.g., 3), and \u03b1 is 5, then even a fair offer would have U=3-5=-2 \u2192 rejected. But in reality, participants might accept fair offers regardless of the absolute amount. So this might not be appropriate.\n\nAlternatively, \u03b1 could represent a minimal acceptable amount irrespective of fairness. For example:\n\nU = split_self - \u03b8 * max(fair_self - split_self, 0) + \u03b3 * (split_self - \u03b1)\n\nBut this complicates further. Maybe it's better to start with the simpler model and see if it fits.\n\nAnother angle: the problem mentions that two equal split_self variables do not represent the same fairness because combined_earning varies. For example, a split_self of 5 when combined_earning is 10 (so 50%) is different from split_self=5 when combined_earning is 20 (25%). But in the current model, the fairness is based on the participant's contribution (fair_self=3*token_self), which is independent of combined_earning. Wait, combined_earning is 3*(token_self + token_opp). So fair_self is 3*token_self, and the fair split would be split_self=3*token_self, split_opp=3*token_opp. So the fairness is based on contribution, not on the total. Therefore, the model accounts for varying combined_earning because fair_self is based on token_self, which varies per trial.\n\nThus, the model U = split_self - \u03b8 * max(fair_self - split_self, 0) should handle varying combined_earning because fair_self is 3*token_self, which depends on the trial's token_self.\n\nNow, considering the variables in the dataset: token_self and token_opp are available, so fair_self can be computed as 3*token_self. split_self is available. So the model can be implemented using these variables.\n\nThe model's parameters would be \u03b8, which has to be bounded. Since \u03b8 represents the penalty per unit of unfairness (pounds below fair_self), it should be positive. We can set a reasonable bound, like 0 to 10, inclusive. This allows \u03b8 to be up to 10, meaning that each pound below fair_self reduces utility by 10, which would heavily penalize unfair offers.\n\nSo the variable description for \u03b8 would be:\n\n\"theta\": {\n  \"description\": \"Sensitivity to disadvantageous inequity (penalty per \u00a31 below fair share)\",\n  \"range\": {\n    \"min\": 0,\n    \"max\": 10,\n    \"inclusive_min\": true,\n    \"inclusive_max\": true\n  },\n  \"learnable\": true,\n  \"source\": \"learnable\"\n}\n\nOther variables:\n\n- split_self: from data, used in the model.\n- fair_self: calculated as 3 * token_self.\n- max_term: calculated as max(fair_self - split_self, 0).\n\nBut in the model formula, these can be incorporated directly.\n\nThe mathematical model would be:\n\nU = split_self - theta * max(3*token_self - split_self, 0)\n\nThis utility is then passed through a logistic function to get P(accept) = 1 / (1 + exp(-U)).\n\nThis model uses only one learnable parameter (theta), which is good for parameter recovery. It captures the idea that responders penalize offers below their fair share (based on their contribution) and the penalty is proportional to the deficit. The absolute offer also matters, as higher offers increase utility.\n\nTesting edge cases:\n\n- If theta is 0: U = split_self. So any positive split_self leads to P>0.5. But if split_self is 0, U=0 \u2192 P=0.5. However, in the dataset, split_self is a float, but likely positive.\n\n- If theta is very high (e.g., 10), then even a small deficit leads to a large penalty. For example, split_self=8, fair_self=10: U=8 -10*2=8-20=-12 \u2192 P\u22480.000006%, so rejection.\n\nThis seems to align with typical ultimatum game behavior where low offers are rejected.\n\nNow, the model needs to predict 'accept' based on U. The target variable is 'accept', which is binary. The model's utility function is as above, and the probability of acceptance is logistic(U). The parameters can be estimated via maximum likelihood estimation.\n\nAnother consideration: the participant's role is responder (trial_role=1). The model should only consider trials where trial_role=1. So during model fitting, we filter the dataset to responder trials.\n\nIn terms of variable usage, the model uses split_self and token_self from the dataset. combined_earning is not directly used because fair_self is computed as 3*token_self, and combined_earning is 3*(token_self + token_opp), but the model doesn't need combined_earning since the fair_self is based on the participant's contribution.\n\nThus, the model is:\n\n<MODEL>\nU = split_self - theta * max(3 * token_self - split_self, 0)\n</MODEL>\n\nVariables:\n\n- theta: learnable parameter, range [0, 10]\n- split_self: from data\n- token_self: from data\n- max term is calculated\n\nThe variable descriptions in JSON would be:\n\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity to disadvantageous inequity (penalty per \u00a31 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe target variable is 'accept'.\n\nThis model should meet the requirements: predicts responder behavior, uses available variables, has one learnable parameter with bounds, and the utility determines the probability via logistic function.\n</think>\n\n<MODEL>\nU = split_self - theta * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"theta\": {\n      \"description\": \"Sensitivity to disadvantageous inequity (penalty per \u00a31 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model where acceptance probability depends on the absolute monetary offer and a penalty for offers below the participant's contribution-based fair share. The penalty scales linearly with the deficit relative to 3\u00d7token_self (fair_self). Uses one learnable parameter (theta) controlling sensitivity to disadvantageous inequity, ensuring good parameter recovery. Captures trade-offs between monetary gain and fairness considerations.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = split_self - theta * max(3 * token_self - split_self, 0)\nSummary: A utility model where acceptance probability depends on the absolute monetary offer and a penalty for offers below the participant's contribution-based fair share. The penalty scales linearly with the deficit relative to 3\u00d7token_self (fair_self). Uses one learnable parameter (theta) controlling sensitivity to disadvantageous inequity, ensuring good parameter recovery. Captures trade-offs between monetary gain and fairness considerations.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, theta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        fair_self = 3 * token_self\n        \n        U = split_self - theta * max(fair_self - split_self, 0)\n        \n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8385225885225885,
  "fitting_results": [
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955368,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.563155685030141,
      "success": true,
      "log_likelihood": -81.81670194547485,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4340476921780345,
      "success": true,
      "log_likelihood": -35.169381373975675,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4999690954765519,
      "success": true,
      "log_likelihood": -77.37696770094625,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.9931402909021754,
      "success": true,
      "log_likelihood": -43.22378014785825,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.001717514105437,
      "success": true,
      "log_likelihood": -44.0897599518571,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.9851410210108735,
      "success": true,
      "log_likelihood": -43.987317847812704,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.6735601695235607,
      "success": true,
      "log_likelihood": -59.98235971417545,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.6228329616145627,
      "success": true,
      "log_likelihood": -0.7929674299182906,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 7.289529074356935,
      "success": true,
      "log_likelihood": -81.73955824340914,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 2.1212281962786115,
      "success": true,
      "log_likelihood": -23.09789778661155,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.48830317365006726,
      "success": true,
      "log_likelihood": -3.748582142103103,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.47621340813013446,
      "success": true,
      "log_likelihood": -19.314527175655684,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4707017019263113,
      "success": true,
      "log_likelihood": -59.34903543647616,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.514075615007084,
      "success": true,
      "log_likelihood": -25.47273571324445,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -207.32983075072002,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.415023684093979,
      "success": true,
      "log_likelihood": -90.33391205712786,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.8927956945509707,
      "success": true,
      "log_likelihood": -71.75384199943379,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.383525321542014,
      "success": true,
      "log_likelihood": -65.73588849488759,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.3564099922973754,
      "success": true,
      "log_likelihood": -51.83178861253826,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 3.157996998749472,
      "success": true,
      "log_likelihood": -12.83655598856815,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.45544797875591725,
      "success": true,
      "log_likelihood": -39.110810260015796,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.6129932115274479,
      "success": true,
      "log_likelihood": -91.2223062774409,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847535e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955366,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4424228713099037,
      "success": true,
      "log_likelihood": -152.66483706967514,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.5044587928971838,
      "success": true,
      "log_likelihood": -90.13646052090729,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -1.2942703121847536e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.8167409120905721,
      "success": true,
      "log_likelihood": -64.17402744103951,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.8708110097830489,
      "success": true,
      "log_likelihood": -17.232774792979853,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.5229386811234042,
      "success": true,
      "log_likelihood": -70.71977377289987,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.9728274911137468,
      "success": true,
      "log_likelihood": -110.82995971003007,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.2359075816964784,
      "success": true,
      "log_likelihood": -38.68462982810702,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.392939013158221,
      "success": true,
      "log_likelihood": -130.7786555165868,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 7.659733306478988,
      "success": true,
      "log_likelihood": -46.10140611819656,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -23.025863872743603,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.021446998558736,
      "success": true,
      "log_likelihood": -90.90803812075883,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.992005388272277,
      "success": true,
      "log_likelihood": -63.77035007888005,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.6583974704212128,
      "success": true,
      "log_likelihood": -96.03032159797752,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.43967246919705,
      "success": true,
      "log_likelihood": -84.46905501789396,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.5361749848629918,
      "success": true,
      "log_likelihood": -134.34149659491132,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.7569286167520317,
      "success": true,
      "log_likelihood": -65.5949324066253,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.0474269738850344,
      "success": true,
      "log_likelihood": -209.10327284526284,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.7788082522392539,
      "success": true,
      "log_likelihood": -109.86331594948635,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -115.22642481011445,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.3990159576216235,
      "success": true,
      "log_likelihood": -113.09880594029391,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.7055726152667018,
      "success": true,
      "log_likelihood": -177.7517821946354,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.5533743429492064,
      "success": true,
      "log_likelihood": -156.95876345761138,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 2.3713759341808838,
      "success": true,
      "log_likelihood": -11.28217452946199,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 6.887380004101612,
      "success": true,
      "log_likelihood": -94.29948073240942,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.17925031391207072,
      "success": true,
      "log_likelihood": -84.4061825485897,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -92.20057619959418,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.4397725450564521,
      "success": true,
      "log_likelihood": -75.15375506495353,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4396832158051658,
      "success": true,
      "log_likelihood": -145.68589712615912,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.7288186594777155,
      "success": true,
      "log_likelihood": -62.77451461416674,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955369,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4035608721535047,
      "success": true,
      "log_likelihood": -123.92536537730507,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 7.000870150372758,
      "success": true,
      "log_likelihood": -77.43639241612668,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.405094502925778,
      "success": true,
      "log_likelihood": -56.22288379806897,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.3256566356634357,
      "success": true,
      "log_likelihood": -36.71503764891717,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.4389310073767099,
      "success": true,
      "log_likelihood": -45.16199733088192,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.9679644045240954,
      "success": true,
      "log_likelihood": -95.11863704934112,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.22639715172186334,
      "success": true,
      "log_likelihood": -49.763130506121705,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -0.09717469987590818,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -115.2264271296346,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4224442083231343,
      "success": true,
      "log_likelihood": -72.22645908055839,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.6573237830945351,
      "success": true,
      "log_likelihood": -82.00668038152759,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.4877647867308676,
      "success": true,
      "log_likelihood": -158.18609097797116,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.8994790340327551,
      "success": true,
      "log_likelihood": -65.73429321290344,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.2002452940199443,
      "success": true,
      "log_likelihood": -33.38969483030706,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -92.20057619959414,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.408708238830669,
      "success": true,
      "log_likelihood": -201.10249470402738,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -69.17472526955369,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.391976860644706,
      "success": true,
      "log_likelihood": -222.09707833262385,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.0,
      "success": true,
      "log_likelihood": -68.73866400294936,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 7.0008702653172525,
      "success": true,
      "log_likelihood": -123.48809205576497,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.3834011457245186,
      "success": true,
      "log_likelihood": -38.53976481886529,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.7092435348963156,
      "success": true,
      "log_likelihood": -216.74023887579705,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 10.0,
      "success": true,
      "log_likelihood": -46.148876559956825,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.709658851282856,
      "success": true,
      "log_likelihood": -116.74996155999922,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 4.033790454643754,
      "success": true,
      "log_likelihood": -77.93913360343423,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.26325939234039336,
      "success": true,
      "log_likelihood": -19.41683704092972,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 1.3437850826141313,
      "success": true,
      "log_likelihood": -84.05516216656108,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "theta": 0.9827303903604233,
      "success": true,
      "log_likelihood": -168.75843603586196,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "theta": {
        "r": 0.9800726627337899,
        "p": 1.3309820697903081e-70
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 151.28612626772318,
  "bic_results": {
    "average_bic": 151.28612626772318,
    "individual_bics": [
      141.93296947756346,
      167.2169228294058,
      73.92228168640746,
      158.33745434034861,
      90.03107923417261,
      91.7630388421703,
      3.5835448238623537,
      91.55815463408152,
      3.5835448238623537,
      123.54823836680701,
      5.169453798292691,
      167.06263542527438,
      49.77931451167921,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      3.5835448238623537,
      11.080683222662316,
      42.21257328976748,
      3.5835448238623537,
      122.28158981140842,
      54.52899036494501,
      418.24318043989615,
      184.25134305271183,
      147.0912029373237,
      135.0552959282313,
      107.24709616353263,
      29.25663091559241,
      81.8051394584877,
      186.0281314933379,
      3.5835448238623537,
      141.93296947756343,
      308.9131930778064,
      183.8564399802707,
      3.5835448238623537,
      131.93157382053514,
      38.049068524415816,
      145.02306648425585,
      225.24343835851624,
      80.95277859467015,
      265.1408299716297,
      95.78633117484922,
      49.635246683943315,
      185.39959517997377,
      131.12421909621622,
      195.64416213441115,
      172.52162897424404,
      272.26651212827875,
      134.77338375170672,
      421.7900646289818,
      223.3101508374288,
      234.036368558685,
      229.78113081904394,
      359.08708332772693,
      317.5010458536789,
      26.14786799738009,
      192.18248040327495,
      172.3958840356355,
      187.98467133764447,
      153.89102906836317,
      294.95531319077435,
      129.1325481667896,
      141.9329694775635,
      251.43424969306625,
      158.45630377070947,
      116.02928653459405,
      77.01359423629044,
      93.90751360021994,
      193.82079303713834,
      103.10977995069952,
      3.7778683382079263,
      234.0363731977253,
      148.03643709957288,
      167.59687970151128,
      319.95570089439843,
      135.05210536426299,
      70.36290859907022,
      187.9846713376444,
      405.78850834651087,
      141.9329694775635,
      447.7776756037038,
      141.06084694435484,
      250.55970304998604,
      80.66304857618668,
      437.0639966900502,
      95.88127205836976,
      237.08344205845455,
      159.46178614532457,
      42.41719302031555,
      171.69384327157826,
      341.10039101018003
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 1,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- theta: r = 0.980",
  "previous_models": [
    "Specification: U = split_self - theta * max(3 * token_self - split_self, 0)\nSummary: A utility model where acceptance probability depends on the absolute monetary offer and a penalty for offers below the participant's contribution-based fair share. The penalty scales linearly with the deficit relative to 3\u00d7token_self (fair_self). Uses one learnable parameter (theta) controlling sensitivity to disadvantageous inequity, ensuring good parameter recovery. Captures trade-offs between monetary gain and fairness considerations.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 151.28612626772318\nOverall Accuracy: 0.8385\n\nParameter Recovery:\n- theta: r = 0.980"
  ]
}