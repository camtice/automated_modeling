{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and have parameters that can be recovered with good accuracy. Also, the utility function should be such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The main target is 'accept', which is binary (1 or 0). The model should predict the probability of accepting based on the utility of the offer. The utility should be a function of the proposed split and other relevant factors.\n\nLooking at the dataset variables, important ones for the responder's decision might include:\n\n- split_self: the amount offered to the participant.\n- split_opp: the amount the opponent would take.\n- combined_earning: total money available, which is derived from tokens (each token is \u00a33). So combined_earning = 3*(token_self + token_opp). But since combined_earning is already a variable, maybe we can use that directly.\n- token_self and token_opp: these represent contributions to the pot. The participant's own contribution (token_self) might influence their perception of fairness. For example, if they contributed more, they might expect a larger share.\n\nThe trial_type indicates whether contributions are equal, opponent more, or participant more. This could affect fairness perceptions. However, since trial_type is a categorical variable, maybe we can model the effect of contribution differences directly using token_self and token_opp.\n\nThe key psychological factors here are likely fairness considerations and self-interest. Participants might compare the proposed split to what they consider a fair split based on their contribution. If the offer is below this fair reference point, they might be more likely to reject it, even at a cost to themselves (as in the ultimatum game).\n\nSo, the utility function could compare the proposed split (split_self) to a \"fair\" split based on contributions. The fair split could be proportional to their contribution. For example, the fair share might be (token_self / (token_self + token_opp)) * combined_earning. Let's call this fair_self. Then, the difference between split_self and fair_self would influence utility.\n\nBut participants might also have different sensitivities to unfairness. Some might be more tolerant of advantageous inequality (getting more than fair) versus disadvantageous inequality (getting less than fair). This is similar to the Fehr-Schmidt inequality aversion model.\n\nAlternatively, the model could use a reference-dependent approach where the utility is the offered amount minus some reference point (like the fair share), multiplied by a parameter. If the offer is below the reference, the utility decreases, and if above, it might increase or have a different sensitivity.\n\nAnother consideration is the total combined earning. If the total is higher, maybe the same absolute difference in split is perceived differently. For example, a \u00a31 difference in a \u00a310 total vs. \u00a3100 total. So perhaps the model should consider relative differences, like (split_self - fair_self)/combined_earning, to normalize it.\n\nLet's outline possible components:\n\n1. Self-interest: The utility from the offered amount split_self. Higher offers are better.\n\n2. Fairness: The difference between split_self and the fair share. If split_self is less than fair, this would decrease utility. If more, maybe increase or have a different effect.\n\n3. Parameters to weigh these components.\n\nPossible model:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self - fair_self)\n\nBut \u03b2 could be different for cases where split_self is less than or greater than fair_self. For example, \u03b2_neg for disadvantageous inequality and \u03b2_pos for advantageous. However, adding more parameters might complicate the model. Since parameter recovery is a priority, maybe keep it simple.\n\nAlternatively, use a linear function of split_self and the difference from fair_self. But how to compute fair_self?\n\nFair_self = (token_self / (token_self + token_opp)) * combined_earning\n\nBut combined_earning is 3*(token_self + token_opp), so fair_self simplifies to 3*token_self. Because:\n\ncombined_earning = 3*(token_self + token_opp)\nfair_self = (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3*token_self\n\nWait, that's interesting. So the fair_self is actually 3 times the participant's own tokens. Because each token is \u00a33. So if the participant contributed token_self tokens, their fair share would be 3* token_self. Similarly for the opponent, fair_opp = 3* token_opp.\n\nBut the split_self is the amount offered to the participant. So the difference between split_self and 3*token_self could be a key factor. If split_self is less than 3*token_self, that's disadvantageous, and if more, advantageous.\n\nSo the model could be:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self - 3*token_self)\n\nBut maybe participants are only sensitive to disadvantageous inequality. So if split_self < 3*token_self, then the term is (split_self - 3*token_self), multiplied by a negative \u03b2 (since it's a loss). Alternatively, use a max function.\n\nAlternatively, model the fairness term as (split_self - reference), where reference is a function of contributions. The reference could be 3*token_self (their own contribution's value) or maybe a proportion of the combined earnings based on their contribution.\n\nWait, but combined_earning is 3*(token_self + token_opp). So the fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning = 3*token_self. So that's the same as before.\n\nSo the fair_self is 3*token_self. So the difference between split_self and fair_self is split_self - 3*token_self. If this is positive, the participant is getting more than their fair share; if negative, less.\n\nBut participants might have different sensitivities to these two cases. For example, they might dislike getting less (negative difference) more than they like getting more. So perhaps we can have two parameters: one for when the difference is negative (disadvantageous inequality) and one for when it's positive (advantageous). But that adds another parameter. Let's see.\n\nAlternatively, use a single parameter that scales the difference. But if the effect is asymmetric, a single parameter might not capture it. However, given the need to limit parameters for better recovery, maybe a single parameter is better.\n\nAnother angle: the participant's utility could be a function of their own monetary gain (split_self) and the difference between their gain and the opponent's gain (split_opp). Because split_opp is the opponent's share. But split_opp is part of the data. Wait, split_opp + split_self = combined_earning? Because combined_earning is the total. So split_opp = combined_earning - split_self. So maybe the model can include the difference between split_self and split_opp. But that's equivalent to split_self - (combined_earning - split_self) = 2*split_self - combined_earning. But perhaps participants compare their share to the opponent's. If they get less than the opponent, that's unfair.\n\nBut in the context of contributions, maybe the fair comparison is split_self vs 3*token_self and split_opp vs 3*token_opp. So if split_self is less than 3*token_self, the participant feels they are being shortchanged relative to their contribution. Similarly, if split_opp is more than 3*token_opp, the opponent is getting more than their contribution, which might be seen as unfair.\n\nBut perhaps the key factor is the participant's own share relative to their contribution. So the model could focus on split_self compared to 3*token_self. Let's call this difference D = split_self - 3*token_self. If D is negative, the offer is unfair (participant gets less than their contribution's worth). If positive, they get more.\n\nThe utility could then be modeled as:\n\nUtility = \u03b1 * split_self + \u03b2 * D\n\nBut if D is negative, \u03b2 would be a parameter that determines how much the participant dislikes getting less than their fair share. If \u03b2 is positive, then a negative D would reduce utility. Alternatively, maybe \u03b2 is a weight that's applied only when D is negative. For example:\n\nUtility = \u03b1 * split_self + \u03b2 * min(D, 0)\n\nThis way, only disadvantageous inequality (D < 0) affects utility. This is similar to the Fehr-Schmidt model where disadvantageous inequality has a stronger effect.\n\nAlternatively, use a piecewise function. But to keep the model simple and with few parameters, perhaps use a linear combination of split_self and the difference from fair_self, allowing the parameter \u03b2 to be negative (so that when D is negative, the utility decreases).\n\nAnother consideration: the utility should be converted to a probability via logistic function. So the probability of accepting is 1 / (1 + exp(-Utility)). The requirement is that when Utility < 0, the probability is less than 0.5. That's naturally handled by the logistic function.\n\nNow, parameters. The model needs learnable parameters. Let's say \u03b1 (sensitivity to own monetary gain) and \u03b2 (sensitivity to deviation from fair share). Both need to be learnable. But what bounds?\n\n\u03b1 should be positive because higher split_self should increase utility. So \u03b1's lower bound could be 0. But maybe allow some negative values? Unlikely, but perhaps set a reasonable range like -5 to 5. But given that split_self is in \u00a3, and higher split_self should lead to higher utility, \u03b1 should be positive. So maybe set a lower bound of 0. However, if the data shows that participants sometimes reject higher offers (maybe due to other factors), but in general, higher offers are better. So \u03b1 should be positive. So perhaps set \u03b1's min to 0, max to some upper limit, say 10.\n\nFor \u03b2, if the term is (split_self - 3*token_self), then when split_self is less than 3*token_self, the term is negative. If \u03b2 is positive, then a negative D would reduce utility, making acceptance less likely. So \u03b2 should be positive. But maybe participants have different sensitivities. So \u03b2's range could be from 0 to some upper bound, say 5. Alternatively, allow negative values, but that would invert the effect. So better to set \u03b2's min to 0.\n\nWait, but if \u03b2 is positive, then when D is negative (split_self < fair_self), the term \u03b2*D would be negative, reducing utility. That's correct. So \u03b2 should be positive. So \u03b1 and \u03b2 are both positive parameters.\n\nAlternatively, maybe the model should have:\n\nUtility = \u03b1 * split_self + \u03b2 * (fair_self - split_self)\n\nIn this case, if split_self < fair_self, (fair_self - split_self) is positive, and \u03b2 positive would mean that this difference reduces utility. Wait, no. If split_self is less than fair_self, then (fair_self - split_self) is positive. If \u03b2 is positive, then that term would add to the utility, which is not desired. So perhaps the model should be:\n\nUtility = \u03b1 * split_self - \u03b2 * (fair_self - split_self)\n\nWhich simplifies to:\n\nUtility = (\u03b1 + \u03b2) * split_self - \u03b2 * fair_self\n\nBut this complicates the interpretation. Alternatively, stick with the original D = split_self - fair_self, and have:\n\nUtility = \u03b1 * split_self + \u03b2 * D\n\nBut when D is negative (split_self < fair_self), \u03b2 * D would be negative, reducing utility. So \u03b2 should be positive to penalize disadvantageous inequality.\n\nSo the model would be:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self - fair_self)\n\nBut fair_self is 3 * token_self. So substituting:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self - 3 * token_self)\n\nCombine terms:\n\nUtility = (\u03b1 + \u03b2) * split_self - 3 * \u03b2 * token_self\n\nBut this might lead to identifiability issues between \u03b1 and \u03b2. For example, if \u03b1 and \u03b2 are both learnable, their sum could be confounded. To avoid this, perhaps the model should have separate terms for split_self and the difference. Alternatively, re-parameterize.\n\nAlternatively, model the utility as:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self / fair_self - 1)\n\nThis would represent the proportional deviation from the fair share. For example, if split_self is 10% below fair_self, this term is -0.1 * \u03b2. But this introduces division, which might complicate things, especially if fair_self is zero (though token_self is at least 1, since it's the number of tokens found). Wait, token_self is an integer, but in the dataset, it's possible that token_self could be zero? The problem statement says \"participant finds a number of tokens\", so probably token_self is at least 1. So fair_self would be 3 * token_self, which is at least 3.\n\nBut using proportional terms might normalize the effect across different contribution levels. For example, a \u00a33 difference when fair_self is \u00a330 (10% less) vs when fair_self is \u00a36 (50% less). The proportional term would capture that.\n\nSo the model could be:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self / (3 * token_self) - 1)\n\nThis way, when split_self equals fair_self (3*token_self), the second term is zero. If split_self is less, the term is negative, multiplied by \u03b2 (which should be positive to decrease utility). If split_self is more, the term is positive, which could increase utility. But maybe participants don't care about advantageous inequality. So perhaps only consider the case when split_self is less than fair_self. But how to model that.\n\nAlternatively, use a max function: max( (fair_self - split_self) / fair_self, 0 ). This would only consider disadvantageous inequality. Then the utility would be:\n\nUtility = \u03b1 * split_self - \u03b2 * max( (fair_self - split_self) / fair_self, 0 )\n\nBut this introduces a non-linearity. However, since the model is for utility which is then passed through a logistic function, non-linear terms are acceptable.\n\nBut this adds complexity. Let's think about the parameters. If we use the proportional term (split_self / fair_self -1), then when split_self is less than fair_self, the term is negative. If \u03b2 is positive, then this would reduce utility, which is desired. If split_self is more than fair_self, the term is positive, which could increase utility. But maybe participants don't get utility from advantageous inequality. So perhaps the model should only penalize disadvantageous inequality. So the term would be max( (fair_self - split_self) / fair_self, 0 ), multiplied by \u03b2. So:\n\nUtility = \u03b1 * split_self - \u03b2 * max( (fair_self - split_self) / fair_self, 0 )\n\nThis way, only when split_self is less than fair_self does the penalty apply. The penalty is proportional to the percentage shortfall. For example, if split_self is 20% below fair_self, the penalty is \u03b2 * 0.2.\n\nThis model has two parameters: \u03b1 (sensitivity to own monetary gain) and \u03b2 (sensitivity to disadvantageous inequality proportional to fair share). Both parameters should be positive. The bounds for \u03b1 could be [0, 10], and \u03b2 [0, 10], with inclusive bounds. These ranges are generous but prevent unrealistic values.\n\nAnother consideration: the combined_earning might affect the utility. For example, participants might be more willing to accept a lower percentage if the total is larger. But in the current model, split_self is in absolute terms. If the model uses split_self, which is in \u00a3, then higher combined_earning would naturally lead to higher split_self. But maybe participants evaluate the split in percentage terms. However, the dataset includes splitperc_self, which is the percentage offered. But the problem states that two equal split_self variables do not represent the same fairness because combined_earning can vary. So the model should account for the context of combined_earning.\n\nWait, the note says: \"two equal split_self variables do not represent the same 'fairness'\". For example, if split_self is \u00a35 in a \u00a310 pot (50%) vs \u00a35 in a \u00a320 pot (25%). So the same split_self can be fair or unfair depending on the total. Therefore, the model should consider the split relative to the total or relative to the fair share based on contributions.\n\nBut in the previous model, we considered split_self relative to fair_self (3*token_self). However, the combined_earning is 3*(token_self + token_opp). So the fair_self is 3*token_self, which is part of the total. So the split_self is compared to the fair_self, which is a fixed value based on their own contribution. But the total could vary based on both contributions. For example, if the participant contributed 5 tokens and the opponent 5, total is 30, fair_self is 15. If the participant contributed 10 and opponent 0, total is 30, fair_self is 30. So in that case, split_self would need to be 30 to be fair, but the total is 30, so split_opp would be 0. But that's an extreme case.\n\nBut the key is that the fair_self is based on their own contribution, not the total. So comparing split_self to fair_self (3*token_self) captures the idea of fairness based on their contribution, regardless of the opponent's contribution. However, the opponent's contribution affects the total, which in turn affects the possible split_self. For example, if the opponent contributed more, the total is higher, but the fair_self is still 3*token_self. So the participant's fair share is fixed based on their own contribution, and any split_self below that is considered unfair.\n\nBut maybe participants also consider the opponent's contribution. For example, if the opponent contributed more, the participant might feel entitled to a smaller share. Wait, no. If the opponent contributed more, the total is higher, but the participant's fair share is still their own contribution's worth. So if the opponent contributed more, their own fair share is the same, but the total is higher. So the split_self could be higher than fair_self if the proposer is offering more than the participant's contribution. But that's up to the proposer.\n\nAlternatively, participants might have a different notion of fairness, such as equal split (50/50) regardless of contributions. But the trial_type variable indicates whether the contributions are equal, etc. However, the model should use the variables provided. Since token_self and token_opp are available, the model can compute the fair_self as 3*token_self.\n\nSo, the model should include both the absolute offer (split_self) and the deviation from the fair_self, possibly in a proportional manner.\n\nPutting this together, the utility function could be:\n\nUtility = \u03b1 * split_self - \u03b2 * max( (fair_self - split_self) / fair_self, 0 )\n\nHere, \u03b1 is the sensitivity to the absolute offer, and \u03b2 is the sensitivity to the proportional disadvantageous inequality. Both parameters are positive. The max function ensures that only disadvantageous inequality (split_self < fair_self) is penalized.\n\nThis model has two learnable parameters, \u03b1 and \u03b2, with ranges [0, 10] each. These bounds are generous but prevent extreme values that could hinder parameter recovery.\n\nAnother possible variable is the trial_role, but since the model is only for responders (trial_role=1), this variable can be ignored as the model is conditioned on the responder role.\n\nNow, checking the dataset variables: split_self is available, token_self is available, so fair_self can be computed as 3 * token_self. The model uses these variables.\n\nThe target variable is 'accept', which is binary. The utility is passed through a logistic function to get the probability of acceptance. So the model's utility function is as above, and the probability is 1 / (1 + exp(-Utility)).\n\nThis model should meet the requirements: it uses the provided variables (split_self, token_self), has two learnable parameters with finite bounds, and the utility can be negative (leading to <50% probability) when the penalty term outweighs the split_self term.\n\nTesting edge cases: if split_self is equal to fair_self, the penalty term is zero, so Utility = \u03b1 * split_self. If split_self is above fair_self, penalty is zero, so Utility is \u03b1 * split_self. If split_self is below, then Utility is \u03b1 * split_self - \u03b2 * (fair_self - split_self)/fair_self.\n\nFor example, if split_self is 10, fair_self is 15 (so token_self=5), then (15-10)/15 = 1/3. So Utility = 10\u03b1 - \u03b2*(1/3). If \u03b1 is 0.5 and \u03b2 is 2, then Utility = 5 - 0.666 = 4.333, which is positive, leading to high probability of acceptance. But if split_self is 12, fair_self=15: (15-12)/15=0.2. Utility=12\u03b1 - 0.2\u03b2. If \u03b1=0.5, \u03b2=5: 6 -1=5. Still positive. Wait, but maybe in this case, the participant would accept. But if split_self is 5, fair_self=15: (15-5)/15=0.666. Utility=5\u03b1 -0.666\u03b2. If \u03b1=0.5, \u03b2=5: 2.5 -3.333= -0.833. Utility is negative, so probability <0.5, leading to likely rejection.\n\nThis seems to align with the desired behavior.\n\nAnother example: if split_self is very high, say 30, fair_self=15. Then penalty term is zero. Utility=30\u03b1. If \u03b1 is positive, this is high, leading to acceptance.\n\nThus, the model captures the trade-off between self-interest (split_self) and fairness (deviation from fair_self in disadvantageous cases).\n\nNow, the variables:\n\n- \u03b1: learnable, range [0,10], inclusive.\n- \u03b2: learnable, range [0,10], inclusive.\n- split_self: from data.\n- token_self: from data.\n- fair_self: calculated as 3 * token_self.\n\nThe utility is:\n\nUtility = \u03b1 * split_self - \u03b2 * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n\nBut to write this concisely in mathematical notation:\n\nLet\u2019s denote D = (3 * token_self - split_self) / (3 * token_self)\n\nThen, Utility = \u03b1 * split_self - \u03b2 * max(D, 0)\n\nBut in LaTeX:\n\nU = \\alpha \\cdot \\text{split\\_self} - \\beta \\cdot \\max\\left( \\frac{3 \\cdot \\text{token\\_self} - \\text{split\\_self}}{3 \\cdot \\text{token\\_self}}, 0 \\right)\n\nAlternatively, since 3*token_self is fair_self:\n\nU = \\alpha \\cdot \\text{split\\_self} - \\beta \\cdot \\max\\left( \\frac{\\text{fair\\_self} - \\text{split\\_self}}{\\text{fair\\_self}}, 0 \\right)\n\nBut fair_self is 3*token_self, which is a calculated variable. However, in the variables section, we can describe fair_self as calculated from token_self.\n\nBut in the model formula, we can directly use the variables from the dataset. So the model would be:\n\nU = \u03b1 * split_self - \u03b2 * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n\nThis is the utility function. The probability of acceptance is logistic(U).\n\nNow, the variables section:\n\nVariables:\n\n- \u03b1: learnable, description: \"Sensitivity to the absolute proposed share for the participant (split_self)\", range [0,10], inclusive.\n- \u03b2: learnable, description: \"Sensitivity to disadvantageous inequality relative to fair share\", range [0,10], inclusive.\n- split_self: from data, no range needed.\n- token_self: from data.\n- The term (3 * token_self - split_self) / (3 * token_self) is calculated.\n\nBut in the variables JSON, we need to list all variables used in the model. The model uses split_self, token_self, and the calculated terms. However, the variables section should list each variable in the model formula. Let's break it down:\n\nThe model formula has:\n\nU = \u03b1 * split_self - \u03b2 * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n\nSo the variables involved are:\n\n- \u03b1 (learnable)\n- \u03b2 (learnable)\n- split_self (data)\n- token_self (data)\n- The term (3 * token_self - split_self) is part of the calculation, but it's not a separate variable; it's derived from token_self and split_self.\n\nThus, in the variables section, we need to list \u03b1, \u03b2, split_self, and token_self. The other terms are calculated within the model.\n\nSo the variables JSON would be:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to the absolute proposed share for the participant (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequality relative to fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe max function and the division are part of the model's calculation but don't introduce new variables. The key variables are alpha, beta, split_self, and token_self.\n\nThis model has two learnable parameters with clear roles, bounded between 0 and 10. The utility function combines self-interest and fairness considerations, specifically penalizing offers below the fair share based on the participant's contribution. The use of proportional disadvantageous inequality helps normalize the effect across different contribution levels, addressing the note that equal split_self values can represent different fairness levels depending on combined_earning.\n\nThe summary should concisely describe the model:\n\n\"A utility model combining self-interest and proportional disadvantageous inequality. The model uses two parameters: sensitivity to the proposed share (\u03b1) and sensitivity to receiving less than one's fair share based on contribution (\u03b2). The fair share is calculated as three times the participant's tokens. Utility decreases linearly with the proportional shortfall below the fair share, predicting lower acceptance likelihood for unfair offers.\"\n\nThis captures the essence of the model without unnecessary details.\n</think>\n\n<MODEL>\nU = alpha * split_self - beta * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to the absolute proposed share for the participant (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequality relative to fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model combining self-interest and proportional disadvantageous inequality. The model uses two parameters: sensitivity to the proposed share (\u03b1) and sensitivity to receiving less than one's fair share based on contribution (\u03b2). The fair share is calculated as three times the participant's tokens. Utility decreases linearly with the proportional shortfall below the fair share, predicting lower acceptance likelihood for unfair offers.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = alpha * split_self - beta * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to the absolute proposed share for the participant (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequality relative to fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = alpha * split_self - beta * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"group_3\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 4, \"token_self\": 4, \"combined_earning\": 24, \"split_opp\": 12.0, \"split_self\": 12.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 12.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 12.0}, {\"ID\": 1, \"group\": \"group_3\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 4, \"token_self\": 4, \"combined_earning\": 24, \"split_opp\": 16.8, \"split_self\": 7.2, \"splitperc_opp\": 70, \"splitperc_self\": 30, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 30.0, \"proposed_amount\": 7.2}, {\"ID\": 2, \"group\": \"group_3\", \"trial_number\": 3, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 4, \"token_self\": 4, \"combined_earning\": 24, \"split_opp\": 14.4, \"split_self\": 9.6, \"splitperc_opp\": 60, \"splitperc_self\": 40, \"accept\": 1.0, \"accepted_amount\": 9.6, \"accepted_perc\": 40.0, \"proposed_perc\": 40.0, \"proposed_amount\": 9.6}, {\"ID\": 3, \"group\": \"group_3\", \"trial_number\": 4, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 4, \"token_self\": 4, \"combined_earning\": 24, \"split_opp\": 12.0, \"split_self\": 12.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 12.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 12.0}, {\"ID\": 4, \"group\": \"group_3\", \"trial_number\": 5, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 4, \"token_self\": 4, \"combined_earning\": 24, \"split_opp\": 16.8, \"split_self\": 7.2, \"splitperc_opp\": 70, \"splitperc_self\": 30, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 30.0, \"proposed_amount\": 7.2}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1743519346.052611
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to the absolute proposed share for the participant (split_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to disadvantageous inequality relative to fair share",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = alpha * split_self - beta * max( (3 * token_self - split_self) / (3 * token_self), 0 )",
  "variable_descriptions": {
    "alpha": {
      "description": "Sensitivity to the absolute proposed share for the participant (split_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to disadvantageous inequality relative to fair share",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model combining self-interest and proportional disadvantageous inequality. The model uses two parameters: sensitivity to the proposed share (\u03b1) and sensitivity to receiving less than one's fair share based on contribution (\u03b2). The fair share is calculated as three times the participant's tokens. Utility decreases linearly with the proportional shortfall below the fair share, predicting lower acceptance likelihood for unfair offers.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and have parameters that can be recovered with good accuracy. Also, the utility function should be such that a negative utility corresponds to less than 50% probability of acceptance.\n\nFirst, I need to understand the key variables involved. The main target is 'accept', which is binary (1 or 0). The model should predict the probability of accepting based on the utility of the offer. The utility should be a function of the proposed split and other relevant factors.\n\nLooking at the dataset variables, important ones for the responder's decision might include:\n\n- split_self: the amount offered to the participant.\n- split_opp: the amount the opponent would take.\n- combined_earning: total money available, which is derived from tokens (each token is \u00a33). So combined_earning = 3*(token_self + token_opp). But since combined_earning is already a variable, maybe we can use that directly.\n- token_self and token_opp: these represent contributions to the pot. The participant's own contribution (token_self) might influence their perception of fairness. For example, if they contributed more, they might expect a larger share.\n\nThe trial_type indicates whether contributions are equal, opponent more, or participant more. This could affect fairness perceptions. However, since trial_type is a categorical variable, maybe we can model the effect of contribution differences directly using token_self and token_opp.\n\nThe key psychological factors here are likely fairness considerations and self-interest. Participants might compare the proposed split to what they consider a fair split based on their contribution. If the offer is below this fair reference point, they might be more likely to reject it, even at a cost to themselves (as in the ultimatum game).\n\nSo, the utility function could compare the proposed split (split_self) to a \"fair\" split based on contributions. The fair split could be proportional to their contribution. For example, the fair share might be (token_self / (token_self + token_opp)) * combined_earning. Let's call this fair_self. Then, the difference between split_self and fair_self would influence utility.\n\nBut participants might also have different sensitivities to unfairness. Some might be more tolerant of advantageous inequality (getting more than fair) versus disadvantageous inequality (getting less than fair). This is similar to the Fehr-Schmidt inequality aversion model.\n\nAlternatively, the model could use a reference-dependent approach where the utility is the offered amount minus some reference point (like the fair share), multiplied by a parameter. If the offer is below the reference, the utility decreases, and if above, it might increase or have a different sensitivity.\n\nAnother consideration is the total combined earning. If the total is higher, maybe the same absolute difference in split is perceived differently. For example, a \u00a31 difference in a \u00a310 total vs. \u00a3100 total. So perhaps the model should consider relative differences, like (split_self - fair_self)/combined_earning, to normalize it.\n\nLet's outline possible components:\n\n1. Self-interest: The utility from the offered amount split_self. Higher offers are better.\n\n2. Fairness: The difference between split_self and the fair share. If split_self is less than fair, this would decrease utility. If more, maybe increase or have a different effect.\n\n3. Parameters to weigh these components.\n\nPossible model:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self - fair_self)\n\nBut \u03b2 could be different for cases where split_self is less than or greater than fair_self. For example, \u03b2_neg for disadvantageous inequality and \u03b2_pos for advantageous. However, adding more parameters might complicate the model. Since parameter recovery is a priority, maybe keep it simple.\n\nAlternatively, use a linear function of split_self and the difference from fair_self. But how to compute fair_self?\n\nFair_self = (token_self / (token_self + token_opp)) * combined_earning\n\nBut combined_earning is 3*(token_self + token_opp), so fair_self simplifies to 3*token_self. Because:\n\ncombined_earning = 3*(token_self + token_opp)\nfair_self = (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3*token_self\n\nWait, that's interesting. So the fair_self is actually 3 times the participant's own tokens. Because each token is \u00a33. So if the participant contributed token_self tokens, their fair share would be 3* token_self. Similarly for the opponent, fair_opp = 3* token_opp.\n\nBut the split_self is the amount offered to the participant. So the difference between split_self and 3*token_self could be a key factor. If split_self is less than 3*token_self, that's disadvantageous, and if more, advantageous.\n\nSo the model could be:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self - 3*token_self)\n\nBut maybe participants are only sensitive to disadvantageous inequality. So if split_self < 3*token_self, then the term is (split_self - 3*token_self), multiplied by a negative \u03b2 (since it's a loss). Alternatively, use a max function.\n\nAlternatively, model the fairness term as (split_self - reference), where reference is a function of contributions. The reference could be 3*token_self (their own contribution's value) or maybe a proportion of the combined earnings based on their contribution.\n\nWait, but combined_earning is 3*(token_self + token_opp). So the fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning = 3*token_self. So that's the same as before.\n\nSo the fair_self is 3*token_self. So the difference between split_self and fair_self is split_self - 3*token_self. If this is positive, the participant is getting more than their fair share; if negative, less.\n\nBut participants might have different sensitivities to these two cases. For example, they might dislike getting less (negative difference) more than they like getting more. So perhaps we can have two parameters: one for when the difference is negative (disadvantageous inequality) and one for when it's positive (advantageous). But that adds another parameter. Let's see.\n\nAlternatively, use a single parameter that scales the difference. But if the effect is asymmetric, a single parameter might not capture it. However, given the need to limit parameters for better recovery, maybe a single parameter is better.\n\nAnother angle: the participant's utility could be a function of their own monetary gain (split_self) and the difference between their gain and the opponent's gain (split_opp). Because split_opp is the opponent's share. But split_opp is part of the data. Wait, split_opp + split_self = combined_earning? Because combined_earning is the total. So split_opp = combined_earning - split_self. So maybe the model can include the difference between split_self and split_opp. But that's equivalent to split_self - (combined_earning - split_self) = 2*split_self - combined_earning. But perhaps participants compare their share to the opponent's. If they get less than the opponent, that's unfair.\n\nBut in the context of contributions, maybe the fair comparison is split_self vs 3*token_self and split_opp vs 3*token_opp. So if split_self is less than 3*token_self, the participant feels they are being shortchanged relative to their contribution. Similarly, if split_opp is more than 3*token_opp, the opponent is getting more than their contribution, which might be seen as unfair.\n\nBut perhaps the key factor is the participant's own share relative to their contribution. So the model could focus on split_self compared to 3*token_self. Let's call this difference D = split_self - 3*token_self. If D is negative, the offer is unfair (participant gets less than their contribution's worth). If positive, they get more.\n\nThe utility could then be modeled as:\n\nUtility = \u03b1 * split_self + \u03b2 * D\n\nBut if D is negative, \u03b2 would be a parameter that determines how much the participant dislikes getting less than their fair share. If \u03b2 is positive, then a negative D would reduce utility. Alternatively, maybe \u03b2 is a weight that's applied only when D is negative. For example:\n\nUtility = \u03b1 * split_self + \u03b2 * min(D, 0)\n\nThis way, only disadvantageous inequality (D < 0) affects utility. This is similar to the Fehr-Schmidt model where disadvantageous inequality has a stronger effect.\n\nAlternatively, use a piecewise function. But to keep the model simple and with few parameters, perhaps use a linear combination of split_self and the difference from fair_self, allowing the parameter \u03b2 to be negative (so that when D is negative, the utility decreases).\n\nAnother consideration: the utility should be converted to a probability via logistic function. So the probability of accepting is 1 / (1 + exp(-Utility)). The requirement is that when Utility < 0, the probability is less than 0.5. That's naturally handled by the logistic function.\n\nNow, parameters. The model needs learnable parameters. Let's say \u03b1 (sensitivity to own monetary gain) and \u03b2 (sensitivity to deviation from fair share). Both need to be learnable. But what bounds?\n\n\u03b1 should be positive because higher split_self should increase utility. So \u03b1's lower bound could be 0. But maybe allow some negative values? Unlikely, but perhaps set a reasonable range like -5 to 5. But given that split_self is in \u00a3, and higher split_self should lead to higher utility, \u03b1 should be positive. So maybe set a lower bound of 0. However, if the data shows that participants sometimes reject higher offers (maybe due to other factors), but in general, higher offers are better. So \u03b1 should be positive. So perhaps set \u03b1's min to 0, max to some upper limit, say 10.\n\nFor \u03b2, if the term is (split_self - 3*token_self), then when split_self is less than 3*token_self, the term is negative. If \u03b2 is positive, then a negative D would reduce utility, making acceptance less likely. So \u03b2 should be positive. But maybe participants have different sensitivities. So \u03b2's range could be from 0 to some upper bound, say 5. Alternatively, allow negative values, but that would invert the effect. So better to set \u03b2's min to 0.\n\nWait, but if \u03b2 is positive, then when D is negative (split_self < fair_self), the term \u03b2*D would be negative, reducing utility. That's correct. So \u03b2 should be positive. So \u03b1 and \u03b2 are both positive parameters.\n\nAlternatively, maybe the model should have:\n\nUtility = \u03b1 * split_self + \u03b2 * (fair_self - split_self)\n\nIn this case, if split_self < fair_self, (fair_self - split_self) is positive, and \u03b2 positive would mean that this difference reduces utility. Wait, no. If split_self is less than fair_self, then (fair_self - split_self) is positive. If \u03b2 is positive, then that term would add to the utility, which is not desired. So perhaps the model should be:\n\nUtility = \u03b1 * split_self - \u03b2 * (fair_self - split_self)\n\nWhich simplifies to:\n\nUtility = (\u03b1 + \u03b2) * split_self - \u03b2 * fair_self\n\nBut this complicates the interpretation. Alternatively, stick with the original D = split_self - fair_self, and have:\n\nUtility = \u03b1 * split_self + \u03b2 * D\n\nBut when D is negative (split_self < fair_self), \u03b2 * D would be negative, reducing utility. So \u03b2 should be positive to penalize disadvantageous inequality.\n\nSo the model would be:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self - fair_self)\n\nBut fair_self is 3 * token_self. So substituting:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self - 3 * token_self)\n\nCombine terms:\n\nUtility = (\u03b1 + \u03b2) * split_self - 3 * \u03b2 * token_self\n\nBut this might lead to identifiability issues between \u03b1 and \u03b2. For example, if \u03b1 and \u03b2 are both learnable, their sum could be confounded. To avoid this, perhaps the model should have separate terms for split_self and the difference. Alternatively, re-parameterize.\n\nAlternatively, model the utility as:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self / fair_self - 1)\n\nThis would represent the proportional deviation from the fair share. For example, if split_self is 10% below fair_self, this term is -0.1 * \u03b2. But this introduces division, which might complicate things, especially if fair_self is zero (though token_self is at least 1, since it's the number of tokens found). Wait, token_self is an integer, but in the dataset, it's possible that token_self could be zero? The problem statement says \"participant finds a number of tokens\", so probably token_self is at least 1. So fair_self would be 3 * token_self, which is at least 3.\n\nBut using proportional terms might normalize the effect across different contribution levels. For example, a \u00a33 difference when fair_self is \u00a330 (10% less) vs when fair_self is \u00a36 (50% less). The proportional term would capture that.\n\nSo the model could be:\n\nUtility = \u03b1 * split_self + \u03b2 * (split_self / (3 * token_self) - 1)\n\nThis way, when split_self equals fair_self (3*token_self), the second term is zero. If split_self is less, the term is negative, multiplied by \u03b2 (which should be positive to decrease utility). If split_self is more, the term is positive, which could increase utility. But maybe participants don't care about advantageous inequality. So perhaps only consider the case when split_self is less than fair_self. But how to model that.\n\nAlternatively, use a max function: max( (fair_self - split_self) / fair_self, 0 ). This would only consider disadvantageous inequality. Then the utility would be:\n\nUtility = \u03b1 * split_self - \u03b2 * max( (fair_self - split_self) / fair_self, 0 )\n\nBut this introduces a non-linearity. However, since the model is for utility which is then passed through a logistic function, non-linear terms are acceptable.\n\nBut this adds complexity. Let's think about the parameters. If we use the proportional term (split_self / fair_self -1), then when split_self is less than fair_self, the term is negative. If \u03b2 is positive, then this would reduce utility, which is desired. If split_self is more than fair_self, the term is positive, which could increase utility. But maybe participants don't get utility from advantageous inequality. So perhaps the model should only penalize disadvantageous inequality. So the term would be max( (fair_self - split_self) / fair_self, 0 ), multiplied by \u03b2. So:\n\nUtility = \u03b1 * split_self - \u03b2 * max( (fair_self - split_self) / fair_self, 0 )\n\nThis way, only when split_self is less than fair_self does the penalty apply. The penalty is proportional to the percentage shortfall. For example, if split_self is 20% below fair_self, the penalty is \u03b2 * 0.2.\n\nThis model has two parameters: \u03b1 (sensitivity to own monetary gain) and \u03b2 (sensitivity to disadvantageous inequality proportional to fair share). Both parameters should be positive. The bounds for \u03b1 could be [0, 10], and \u03b2 [0, 10], with inclusive bounds. These ranges are generous but prevent unrealistic values.\n\nAnother consideration: the combined_earning might affect the utility. For example, participants might be more willing to accept a lower percentage if the total is larger. But in the current model, split_self is in absolute terms. If the model uses split_self, which is in \u00a3, then higher combined_earning would naturally lead to higher split_self. But maybe participants evaluate the split in percentage terms. However, the dataset includes splitperc_self, which is the percentage offered. But the problem states that two equal split_self variables do not represent the same fairness because combined_earning can vary. So the model should account for the context of combined_earning.\n\nWait, the note says: \"two equal split_self variables do not represent the same 'fairness'\". For example, if split_self is \u00a35 in a \u00a310 pot (50%) vs \u00a35 in a \u00a320 pot (25%). So the same split_self can be fair or unfair depending on the total. Therefore, the model should consider the split relative to the total or relative to the fair share based on contributions.\n\nBut in the previous model, we considered split_self relative to fair_self (3*token_self). However, the combined_earning is 3*(token_self + token_opp). So the fair_self is 3*token_self, which is part of the total. So the split_self is compared to the fair_self, which is a fixed value based on their own contribution. But the total could vary based on both contributions. For example, if the participant contributed 5 tokens and the opponent 5, total is 30, fair_self is 15. If the participant contributed 10 and opponent 0, total is 30, fair_self is 30. So in that case, split_self would need to be 30 to be fair, but the total is 30, so split_opp would be 0. But that's an extreme case.\n\nBut the key is that the fair_self is based on their own contribution, not the total. So comparing split_self to fair_self (3*token_self) captures the idea of fairness based on their contribution, regardless of the opponent's contribution. However, the opponent's contribution affects the total, which in turn affects the possible split_self. For example, if the opponent contributed more, the total is higher, but the fair_self is still 3*token_self. So the participant's fair share is fixed based on their own contribution, and any split_self below that is considered unfair.\n\nBut maybe participants also consider the opponent's contribution. For example, if the opponent contributed more, the participant might feel entitled to a smaller share. Wait, no. If the opponent contributed more, the total is higher, but the participant's fair share is still their own contribution's worth. So if the opponent contributed more, their own fair share is the same, but the total is higher. So the split_self could be higher than fair_self if the proposer is offering more than the participant's contribution. But that's up to the proposer.\n\nAlternatively, participants might have a different notion of fairness, such as equal split (50/50) regardless of contributions. But the trial_type variable indicates whether the contributions are equal, etc. However, the model should use the variables provided. Since token_self and token_opp are available, the model can compute the fair_self as 3*token_self.\n\nSo, the model should include both the absolute offer (split_self) and the deviation from the fair_self, possibly in a proportional manner.\n\nPutting this together, the utility function could be:\n\nUtility = \u03b1 * split_self - \u03b2 * max( (fair_self - split_self) / fair_self, 0 )\n\nHere, \u03b1 is the sensitivity to the absolute offer, and \u03b2 is the sensitivity to the proportional disadvantageous inequality. Both parameters are positive. The max function ensures that only disadvantageous inequality (split_self < fair_self) is penalized.\n\nThis model has two learnable parameters, \u03b1 and \u03b2, with ranges [0, 10] each. These bounds are generous but prevent extreme values that could hinder parameter recovery.\n\nAnother possible variable is the trial_role, but since the model is only for responders (trial_role=1), this variable can be ignored as the model is conditioned on the responder role.\n\nNow, checking the dataset variables: split_self is available, token_self is available, so fair_self can be computed as 3 * token_self. The model uses these variables.\n\nThe target variable is 'accept', which is binary. The utility is passed through a logistic function to get the probability of acceptance. So the model's utility function is as above, and the probability is 1 / (1 + exp(-Utility)).\n\nThis model should meet the requirements: it uses the provided variables (split_self, token_self), has two learnable parameters with finite bounds, and the utility can be negative (leading to <50% probability) when the penalty term outweighs the split_self term.\n\nTesting edge cases: if split_self is equal to fair_self, the penalty term is zero, so Utility = \u03b1 * split_self. If split_self is above fair_self, penalty is zero, so Utility is \u03b1 * split_self. If split_self is below, then Utility is \u03b1 * split_self - \u03b2 * (fair_self - split_self)/fair_self.\n\nFor example, if split_self is 10, fair_self is 15 (so token_self=5), then (15-10)/15 = 1/3. So Utility = 10\u03b1 - \u03b2*(1/3). If \u03b1 is 0.5 and \u03b2 is 2, then Utility = 5 - 0.666 = 4.333, which is positive, leading to high probability of acceptance. But if split_self is 12, fair_self=15: (15-12)/15=0.2. Utility=12\u03b1 - 0.2\u03b2. If \u03b1=0.5, \u03b2=5: 6 -1=5. Still positive. Wait, but maybe in this case, the participant would accept. But if split_self is 5, fair_self=15: (15-5)/15=0.666. Utility=5\u03b1 -0.666\u03b2. If \u03b1=0.5, \u03b2=5: 2.5 -3.333= -0.833. Utility is negative, so probability <0.5, leading to likely rejection.\n\nThis seems to align with the desired behavior.\n\nAnother example: if split_self is very high, say 30, fair_self=15. Then penalty term is zero. Utility=30\u03b1. If \u03b1 is positive, this is high, leading to acceptance.\n\nThus, the model captures the trade-off between self-interest (split_self) and fairness (deviation from fair_self in disadvantageous cases).\n\nNow, the variables:\n\n- \u03b1: learnable, range [0,10], inclusive.\n- \u03b2: learnable, range [0,10], inclusive.\n- split_self: from data.\n- token_self: from data.\n- fair_self: calculated as 3 * token_self.\n\nThe utility is:\n\nUtility = \u03b1 * split_self - \u03b2 * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n\nBut to write this concisely in mathematical notation:\n\nLet\u2019s denote D = (3 * token_self - split_self) / (3 * token_self)\n\nThen, Utility = \u03b1 * split_self - \u03b2 * max(D, 0)\n\nBut in LaTeX:\n\nU = \\alpha \\cdot \\text{split\\_self} - \\beta \\cdot \\max\\left( \\frac{3 \\cdot \\text{token\\_self} - \\text{split\\_self}}{3 \\cdot \\text{token\\_self}}, 0 \\right)\n\nAlternatively, since 3*token_self is fair_self:\n\nU = \\alpha \\cdot \\text{split\\_self} - \\beta \\cdot \\max\\left( \\frac{\\text{fair\\_self} - \\text{split\\_self}}{\\text{fair\\_self}}, 0 \\right)\n\nBut fair_self is 3*token_self, which is a calculated variable. However, in the variables section, we can describe fair_self as calculated from token_self.\n\nBut in the model formula, we can directly use the variables from the dataset. So the model would be:\n\nU = \u03b1 * split_self - \u03b2 * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n\nThis is the utility function. The probability of acceptance is logistic(U).\n\nNow, the variables section:\n\nVariables:\n\n- \u03b1: learnable, description: \"Sensitivity to the absolute proposed share for the participant (split_self)\", range [0,10], inclusive.\n- \u03b2: learnable, description: \"Sensitivity to disadvantageous inequality relative to fair share\", range [0,10], inclusive.\n- split_self: from data, no range needed.\n- token_self: from data.\n- The term (3 * token_self - split_self) / (3 * token_self) is calculated.\n\nBut in the variables JSON, we need to list all variables used in the model. The model uses split_self, token_self, and the calculated terms. However, the variables section should list each variable in the model formula. Let's break it down:\n\nThe model formula has:\n\nU = \u03b1 * split_self - \u03b2 * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n\nSo the variables involved are:\n\n- \u03b1 (learnable)\n- \u03b2 (learnable)\n- split_self (data)\n- token_self (data)\n- The term (3 * token_self - split_self) is part of the calculation, but it's not a separate variable; it's derived from token_self and split_self.\n\nThus, in the variables section, we need to list \u03b1, \u03b2, split_self, and token_self. The other terms are calculated within the model.\n\nSo the variables JSON would be:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to the absolute proposed share for the participant (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequality relative to fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe max function and the division are part of the model's calculation but don't introduce new variables. The key variables are alpha, beta, split_self, and token_self.\n\nThis model has two learnable parameters with clear roles, bounded between 0 and 10. The utility function combines self-interest and fairness considerations, specifically penalizing offers below the fair share based on the participant's contribution. The use of proportional disadvantageous inequality helps normalize the effect across different contribution levels, addressing the note that equal split_self values can represent different fairness levels depending on combined_earning.\n\nThe summary should concisely describe the model:\n\n\"A utility model combining self-interest and proportional disadvantageous inequality. The model uses two parameters: sensitivity to the proposed share (\u03b1) and sensitivity to receiving less than one's fair share based on contribution (\u03b2). The fair share is calculated as three times the participant's tokens. Utility decreases linearly with the proportional shortfall below the fair share, predicting lower acceptance likelihood for unfair offers.\"\n\nThis captures the essence of the model without unnecessary details.\n</think>\n\n<MODEL>\nU = alpha * split_self - beta * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to the absolute proposed share for the participant (split_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequality relative to fair share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA utility model combining self-interest and proportional disadvantageous inequality. The model uses two parameters: sensitivity to the proposed share (\u03b1) and sensitivity to receiving less than one's fair share based on contribution (\u03b2). The fair share is calculated as three times the participant's tokens. Utility decreases linearly with the proportional shortfall below the fair share, predicting lower acceptance likelihood for unfair offers.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = alpha * split_self - beta * max( (3 * token_self - split_self) / (3 * token_self), 0 )\nSummary: A utility model combining self-interest and proportional disadvantageous inequality. The model uses two parameters: sensitivity to the proposed share (\u03b1) and sensitivity to receiving less than one's fair share based on contribution (\u03b2). The fair share is calculated as three times the participant's tokens. Utility decreases linearly with the proportional shortfall below the fair share, predicting lower acceptance likelihood for unfair offers.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        utility = alpha * split_self - beta * max( (3 * token_self - split_self) / (3 * token_self), 0 )\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.7316849816849816,
  "fitting_results": [
    {
      "alpha": 7.725568674461055,
      "beta": 5.905901596472703,
      "success": true,
      "log_likelihood": -759.8530806877354,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.08333333333333333,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05249417014832049,
      "beta": 0.5350392005363458,
      "success": true,
      "log_likelihood": -16.713797158183727,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07816917147423189,
      "beta": 0.7472174367147841,
      "success": true,
      "log_likelihood": -13.32576987738697,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07024140788495448,
      "beta": 0.8496018398798721,
      "success": true,
      "log_likelihood": -12.922080390336344,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05214611603334367,
      "beta": 0.6870760062571349,
      "success": true,
      "log_likelihood": -14.893825746586602,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08230165089766431,
      "beta": 0.5321086140562585,
      "success": true,
      "log_likelihood": -14.314405096645878,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 8.386550425800728,
      "beta": 1.8395115334955714,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.429762440605526,
      "beta": 0.9731645199364369,
      "success": true,
      "log_likelihood": -276.31021115688566,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.807065411055068,
      "beta": 2.2550578055017576,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06163912369215785,
      "beta": 0.9024872093064799,
      "success": true,
      "log_likelihood": -12.529835763501815,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1367460877727991,
      "beta": 1.6780757260456165,
      "success": true,
      "log_likelihood": -7.8824058816176015,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.009909772140011285,
      "beta": 1.060649205398875,
      "success": true,
      "log_likelihood": -8.122330953738155,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 9.173719397093693,
      "beta": 6.081672022051329,
      "success": true,
      "log_likelihood": -483.5428695272499,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 9.474653116192863,
      "beta": 6.679927006311005,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.2769291131467013,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -2.9723151605775453e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.13660332998334207,
      "beta": 0.3043920226905593,
      "success": true,
      "log_likelihood": -8.539772506411316,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05104876691819359,
      "beta": 0.4595107174272777,
      "success": true,
      "log_likelihood": -17.640932063947282,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12393816818871124,
      "beta": 0.32250988825615956,
      "success": true,
      "log_likelihood": -9.777891942371214,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04244776917897908,
      "beta": 0.6035201680655266,
      "success": true,
      "log_likelihood": -16.038883082084016,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.058178968243792,
      "beta": 4.234015373168342,
      "success": true,
      "log_likelihood": -391.41734127624864,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 9.967246624532144,
      "beta": 6.148237130476264,
      "success": true,
      "log_likelihood": -414.46531673712843,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0771666428471072,
      "beta": 0.5978994425737439,
      "success": true,
      "log_likelihood": -14.399964349005423,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09177204867179632,
      "beta": 0.7684346343060613,
      "success": true,
      "log_likelihood": -12.344788889682887,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10544305062293075,
      "beta": 2.115841187755456,
      "success": true,
      "log_likelihood": -5.880349837543702,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.603836157058705,
      "beta": 0.8533022949035063,
      "success": true,
      "log_likelihood": -92.10340371656179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7198930903059622,
      "beta": 4.273041420655345,
      "success": true,
      "log_likelihood": -133.5511679628308,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.002230563229144887,
      "beta": 1.1300473722681876,
      "success": true,
      "log_likelihood": -7.078106647182278,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03328336429876026,
      "beta": 0.41118841408826445,
      "success": true,
      "log_likelihood": -19.1801612007132,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.054663404442990206,
      "beta": 0.7688855565565903,
      "success": true,
      "log_likelihood": -13.91275680882254,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.997558202588795,
      "beta": 5.86889468905356,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.13492532031399598,
      "beta": 0.7979358943310944,
      "success": true,
      "log_likelihood": -9.91509413760015,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 9.391362254481054,
      "beta": 9.691169990887493,
      "success": true,
      "log_likelihood": -253.28435578596296,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04708467284980545,
      "beta": 0.5051346341976475,
      "success": true,
      "log_likelihood": -17.34440316956668,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0331974120401406,
      "beta": 9.047121033570665,
      "success": true,
      "log_likelihood": -155.44623894482885,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08751806802520258,
      "beta": 0.05654820326448072,
      "success": true,
      "log_likelihood": -9.729954675779716,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03960881194871272,
      "beta": 0.27382179446468913,
      "success": true,
      "log_likelihood": -20.41456347057459,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.030435415030556672,
      "beta": 0.9523000072839085,
      "success": true,
      "log_likelihood": -10.529498249089379,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 8.904375830560783,
      "beta": 4.611678351664958,
      "success": true,
      "log_likelihood": -23.02585092644049,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0661491722598457,
      "beta": 0.5291849410335245,
      "success": true,
      "log_likelihood": -15.786279268878586,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08049409413425548,
      "beta": 0.5599076424870969,
      "success": true,
      "log_likelihood": -14.342725234972978,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07434754358354476,
      "beta": 0.29544101680919094,
      "success": true,
      "log_likelihood": -15.33102463580575,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.056542578420263945,
      "beta": 0.5199878323165859,
      "success": true,
      "log_likelihood": -16.622537493553335,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 8.14237515993455,
      "beta": 7.197265387527214,
      "success": true,
      "log_likelihood": -506.5687204572903,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03343071076339607,
      "beta": 0.5312478762375644,
      "success": true,
      "log_likelihood": -17.06870771829761,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.022728789833382182,
      "beta": 0.16002477922040564,
      "success": true,
      "log_likelihood": -23.057816682186104,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2129245991698259,
      "beta": 7.073448127540723,
      "success": true,
      "log_likelihood": -144.5419763889388,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.672955953327628,
      "beta": 0.5762016932721303,
      "success": true,
      "log_likelihood": -713.8013788276545,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.1388888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05831225976665341,
      "beta": 0.48486211826017006,
      "success": true,
      "log_likelihood": -16.812836164490204,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0374991225156156,
      "beta": 0.2064755786850833,
      "success": true,
      "log_likelihood": -20.927351367300478,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.825071583886527,
      "beta": 8.339056518202199,
      "success": true,
      "log_likelihood": -506.5532728412147,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3888888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1185537789382921,
      "beta": 1.9367140580385054,
      "success": true,
      "log_likelihood": -7.045720335295759,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04672104318778256,
      "beta": 1.0147238102540215,
      "success": true,
      "log_likelihood": -10.824370656367893,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 9.073784729709109,
      "beta": 6.23378337757676,
      "success": true,
      "log_likelihood": -92.1034037165618,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.23883518590255,
      "beta": 1.9871540952398847,
      "success": true,
      "log_likelihood": -736.8272297576949,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.1111111111111111,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.558345193660419,
      "beta": 7.4140164669041475,
      "success": true,
      "log_likelihood": -161.1807233870226,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03987748716736085,
      "beta": 0.24495986299047487,
      "success": true,
      "log_likelihood": -20.50833290807182,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.143697936145428,
      "beta": 4.136031329957332,
      "success": true,
      "log_likelihood": -414.461664120971,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0022305957574419626,
      "beta": 1.130041369930898,
      "success": true,
      "log_likelihood": -7.078106647240354,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03517363897275401,
      "beta": 0.3819344122008491,
      "success": true,
      "log_likelihood": -19.591924644580875,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.046402746521818056,
      "beta": 1.1804337262662543,
      "success": true,
      "log_likelihood": -9.196042306742067,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0870767020589532,
      "beta": 0.7170103489039578,
      "success": true,
      "log_likelihood": -12.942508139519676,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1051531867460081,
      "beta": 0.11970455478227825,
      "success": true,
      "log_likelihood": -8.809891155546236,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08057268080597858,
      "beta": 0.14942429149244746,
      "success": true,
      "log_likelihood": -12.652452861410312,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 7.562516345227315,
      "beta": 7.4922446586118845,
      "success": true,
      "log_likelihood": -368.41319973981774,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652137,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.13110634363492785,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.02239071594709035,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.602052780784865,
      "beta": 3.402391512351388,
      "success": true,
      "log_likelihood": -713.8013788276544,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.1388888888888889,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05517674063732425,
      "beta": 0.4836520522285565,
      "success": true,
      "log_likelihood": -17.080421958600013,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 8.72706694851635,
      "beta": 8.648262062394524,
      "success": true,
      "log_likelihood": -184.20680299584163,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.589020782780569,
      "beta": 1.9024655683157443,
      "success": true,
      "log_likelihood": -414.4653167371285,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06300711050954706,
      "beta": 0.32033830630869276,
      "success": true,
      "log_likelihood": -17.049350721395978,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -46.051701856480875,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -25.798537903521417,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.031955940504749064,
      "beta": 0.15002055479040913,
      "success": true,
      "log_likelihood": -21.749336432318692,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0022306601395316312,
      "beta": 1.130047662637247,
      "success": true,
      "log_likelihood": -7.078106647155375,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.021700471013069905,
      "beta": 0.17425312554498693,
      "success": true,
      "log_likelihood": -23.067199324266063,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.403081147586026,
      "beta": 1.4285152089528441,
      "success": true,
      "log_likelihood": -69.07755278652131,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.7537932643267038,
      "success": true,
      "log_likelihood": -9.991308782614933,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.490858888426338,
      "beta": 6.433425202333982,
      "success": true,
      "log_likelihood": -414.43239369374777,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8631991738074372,
      "beta": 8.70533485644339,
      "success": true,
      "log_likelihood": -235.87125453583084,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.011016894574482456,
      "beta": 1.2821587481224257,
      "success": true,
      "log_likelihood": -6.843298401431886,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.046477732442921506,
      "beta": 0.20186827941106347,
      "success": true,
      "log_likelihood": -19.394283666537127,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.017740599988179326,
      "beta": 0.7166157306820359,
      "success": true,
      "log_likelihood": -12.633145969931448,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1574986039857753,
      "beta": 0.29489282434403713,
      "success": true,
      "log_likelihood": -6.979247599874676,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1156784387233641,
      "beta": 1.0810954048996682,
      "success": true,
      "log_likelihood": -9.683522101227256,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03224106969773671,
      "beta": 0.08088215135107997,
      "success": true,
      "log_likelihood": -21.10313575815837,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.67034683677974,
        "p": 2.37727802613092e-14
      },
      "beta": {
        "r": 0.2178229133429862,
        "p": 0.029476408417387724
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 216.28657361254145,
  "bic_results": {
    "average_bic": 216.28657361254145,
    "individual_bics": [
      1526.873199252383,
      40.59463219327967,
      33.81857763168616,
      33.011198657584906,
      36.954689370085426,
      35.795848070203974,
      7.167037869712219,
      559.7874601906835,
      7.167037869712219,
      32.22670940391585,
      22.931849640147423,
      23.41169978438853,
      974.252776931412,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      7.167097323215431,
      24.246582889734853,
      7.167037869712219,
      42.44890200480678,
      26.72282176165465,
      39.24480404108025,
      790.0017204294095,
      836.0976713511691,
      35.96696657492306,
      31.856615656277995,
      18.927737551999623,
      191.3738453100358,
      274.2693738025738,
      7.167037869712219,
      21.323251171276777,
      45.52736027833862,
      34.9925514945573,
      7.167037869712219,
      26.997226152112518,
      513.7357494488382,
      41.855844216045575,
      318.0595157665699,
      26.62694722847165,
      47.996164818061395,
      28.226034375090975,
      53.2187397297932,
      38.73959641466939,
      35.85248834685817,
      37.82908714852372,
      40.41211286401889,
      1020.3044787914928,
      41.30445331350744,
      53.282671241284426,
      296.2509906547898,
      1434.7697955322212,
      40.792710205892625,
      49.021740611513174,
      1020.2735835593417,
      21.25847854750374,
      28.815779189648005,
      191.37384531003582,
      1480.821497392302,
      329.5284846509574,
      48.18370369305586,
      836.0903661188543,
      21.323251171392926,
      46.35088716607397,
      25.559122490396355,
      33.05205415595157,
      24.78682018800469,
      32.471943599732846,
      743.9934373565477,
      145.32214344995495,
      7.2118193088064,
      1434.769795532221,
      41.327881794112244,
      375.5806438685955,
      836.0976713511692,
      41.265739319704174,
      99.27044158987397,
      58.76411368395505,
      50.6657107415496,
      21.32325117122297,
      53.301436525444345,
      145.32214344995484,
      27.149655442142084,
      836.0318252644078,
      478.9095469485739,
      20.853634679775993,
      45.95560520998647,
      32.433329816775114,
      21.125533076661572,
      26.534082079366733,
      49.37330939322896
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.670\n- beta: r = 0.218",
  "previous_models": [
    "Specification: U = alpha * split_self - beta * max( (3 * token_self - split_self) / (3 * token_self), 0 )\nSummary: A utility model combining self-interest and proportional disadvantageous inequality. The model uses two parameters: sensitivity to the proposed share (\u03b1) and sensitivity to receiving less than one's fair share based on contribution (\u03b2). The fair share is calculated as three times the participant's tokens. Utility decreases linearly with the proportional shortfall below the fair share, predicting lower acceptance likelihood for unfair offers.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 216.28657361254145\nOverall Accuracy: 0.7317\n\nParameter Recovery:\n- alpha: r = 0.670\n- beta: r = 0.218"
  ]
}