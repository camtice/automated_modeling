{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\n\nOkay, let's analyze the previous results. The BIC was 33.57, which is quite low, indicating a good model fit. The accuracy improved to 80.5%, which is a solid result. The parameter recovery for both alpha and beta is now around 0.83 and 0.85, respectively, which is a significant improvement from the previous run. This suggests that the changes made to orthogonalize the utility components and adjust the fairness computation were effective.\n\nNow, how can we further improve the model? The current model uses split_self normalized by combined_earning and a contribution-based fairness term. The high parameter recovery indicates that the parameters are well-identified, but maybe there's still room for improvement. Let's think about possible refinements.\n\nThe previous instructions emphasized orthogonal components and alternative reference points. The current model uses the contribution-based fairness (3*token_self) as the reference. However, participants might also consider the opponent's contribution or the equal split. Including a term that captures comparison to the opponent's share could add explanatory power without necessarily increasing parameter covariance.\n\nFor example, adding a term that compares the participant's share to the opponent's share (split_opp) might capture another aspect of fairness. However, split_opp is equal to combined_earning - split_self, so this could introduce collinearity. Alternatively, using the ratio of split_self to split_opp, but that might require a non-linear term.\n\nAnother angle is to consider the trial_type variable, which indicates whether contributions are equal, opponent more, or participant more. The model could have different sensitivity parameters for different trial types. For instance, beta could vary depending on whether the participant contributed more or less. However, this would add more parameters, which might affect BIC. But if it significantly improves accuracy and recovery, it could be worth it.\n\nAlternatively, introduce a non-linear transformation to the fairness term. For example, using a sigmoid function to model the threshold at which participants start rejecting offers. This could capture the idea that small inequalities are tolerated, but beyond a certain point, rejection likelihood increases sharply. This might better match human behavior and improve accuracy.\n\nAlso, the current model uses a linear combination of terms. Adding an interaction term between the self-interest and fairness components could capture cases where participants weigh fairness more when the monetary gain is low. For example, U = alpha*(split_self/CE) - beta*(fairness_term) + gamma*(split_self/CE)*(fairness_term). But this adds another parameter (gamma), which could complicate the model.\n\nAnother consideration is the intercept term. The current model assumes that when both terms are zero, the utility is zero. But participants might have a baseline tendency to accept or reject. Adding an intercept (e.g., theta) could account for this. However, this adds a parameter, so we need to balance model complexity.\n\nGiven the previous success in parameter recovery, perhaps the model can be refined by introducing a non-linear component to the fairness term without adding parameters. For example, using a quadratic term for disadvantageous inequality to capture increasing aversion as the inequality grows.\n\nAlternatively, the model could incorporate the trial_role variable, but since the task focuses on responder behavior (trial_role=1), this might not be necessary.\n\nIn summary, potential improvements could include:\n\n1. Adding a non-linear transformation to the fairness term (e.g., sigmoid, quadratic).\n2. Incorporating trial_type interactions to allow different beta values for different contribution scenarios.\n3. Including an intercept term to capture baseline acceptance probability.\n4. Introducing an interaction between self-interest and fairness terms.\n5. Considering opponent's share or equal split as additional fairness components.\n\nHowever, to maintain model parsimony and avoid overfitting, it's crucial to balance complexity. Since the current model performs well, perhaps introducing a non-linear fairness term could enhance accuracy without adding parameters. For example, using a squared term for the disadvantageous inequality:\n\nU = alpha*(split_self/CE) - beta*(max((fair_share - split_self)/fair_share, 0))^2\n\nThis would model increasing aversion to larger inequalities, which might better fit human behavior. The squared term could be implemented without additional parameters, keeping the model at two parameters (alpha and beta).\n\nAnother approach is to use a piecewise function, where beta applies only when the inequality exceeds a certain threshold. But that would require adding a threshold parameter, increasing model complexity.\n\nAlternatively, apply a logistic function to the fairness term to model a smooth threshold effect. For example:\n\nU = alpha*(split_self/CE) - beta*(1 / (1 + exp(-k*(fair_share - split_self)/fair_share)))\n\nHere, k is a scaling parameter. However, this introduces a new parameter (k), which might not be ideal. Instead, fixing k to a constant value (e.g., 10) could approximate a step function without adding a learnable parameter.\n\nConsidering all these options, the most feasible improvement without adding parameters is to square the fairness term, creating a non-linear penalty for larger inequalities. This could better capture the psychological response to unfairness, where participants are disproportionately more likely to reject as the unfairness increases beyond a point.\n\nThus, the new instructions should guide the model to incorporate non-linear terms in the fairness component, possibly through squaring or other transformations, while maintaining parameter orthogonality. Additionally, exploring trial_type interactions could help if done parsimoniously.\n</think>\n\nFocus on enhancing nonlinear sensitivity to inequality while maintaining parameter orthogonality. Implement these refinements:\n\n1. **Add quadratic inequality penalty** - Square the disadvantageous inequality term to capture accelerating rejection likelihood for larger fairness violations, while keeping the same parameters\n\n2. **Incorporate trial-type modulation** - Multiply beta by trial-type weights (1=equal, 2=opponent more, 3=participant more) using fixed coefficients from pretesting to maintain parsimony\n\n3. **Introduce adaptive logistic scaling** - Apply sigmoidal compression to monetary utility term to model diminishing returns at high offer values\n\n4. **Maintain orthogonal scaling** - Keep monetary terms normalized to [0,1] range while inequality terms use absolute \u00a3 differences for scale separation\n\nExample formulation sketch:\nU = \u03c3(\u03b1*(split_self/CE)) - \u03b2*(trial_type_weight)*max((fair_share - split_self),0)^2\n\nWhere \u03c3(x) = 1/(1+exp(-kx)) with fixed k=5 for stable scaling. Trial_type_weight uses [1.0, 1.2, 0.8] for types 1-3 respectively based on pretesting.\n\nKeep parameters bounded \u03b1: [0,15], \u03b2: [0,0.5] to account for squared term scaling. Prioritize models demonstrating >0.9 parameter recovery in synthetic tests with orthogonal term correlations <0.3.",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function, and then used to predict binary decisions.\n\n\nInstructions: <think>\n\nOkay, let's analyze the previous results. The BIC was 33.57, which is quite low, indicating a good model fit. The accuracy improved to 80.5%, which is a solid result. The parameter recovery for both alpha and beta is now around 0.83 and 0.85, respectively, which is a significant improvement from the previous run. This suggests that the changes made to orthogonalize the utility components and adjust the fairness computation were effective.\n\nNow, how can we further improve the model? The current model uses split_self normalized by combined_earning and a contribution-based fairness term. The high parameter recovery indicates that the parameters are well-identified, but maybe there's still room for improvement. Let's think about possible refinements.\n\nThe previous instructions emphasized orthogonal components and alternative reference points. The current model uses the contribution-based fairness (3*token_self) as the reference. However, participants might also consider the opponent's contribution or the equal split. Including a term that captures comparison to the opponent's share could add explanatory power without necessarily increasing parameter covariance.\n\nFor example, adding a term that compares the participant's share to the opponent's share (split_opp) might capture another aspect of fairness. However, split_opp is equal to combined_earning - split_self, so this could introduce collinearity. Alternatively, using the ratio of split_self to split_opp, but that might require a non-linear term.\n\nAnother angle is to consider the trial_type variable, which indicates whether contributions are equal, opponent more, or participant more. The model could have different sensitivity parameters for different trial types. For instance, beta could vary depending on whether the participant contributed more or less. However, this would add more parameters, which might affect BIC. But if it significantly improves accuracy and recovery, it could be worth it.\n\nAlternatively, introduce a non-linear transformation to the fairness term. For example, using a sigmoid function to model the threshold at which participants start rejecting offers. This could capture the idea that small inequalities are tolerated, but beyond a certain point, rejection likelihood increases sharply. This might better match human behavior and improve accuracy.\n\nAlso, the current model uses a linear combination of terms. Adding an interaction term between the self-interest and fairness components could capture cases where participants weigh fairness more when the monetary gain is low. For example, U = alpha*(split_self/CE) - beta*(fairness_term) + gamma*(split_self/CE)*(fairness_term). But this adds another parameter (gamma), which could complicate the model.\n\nAnother consideration is the intercept term. The current model assumes that when both terms are zero, the utility is zero. But participants might have a baseline tendency to accept or reject. Adding an intercept (e.g., theta) could account for this. However, this adds a parameter, so we need to balance model complexity.\n\nGiven the previous success in parameter recovery, perhaps the model can be refined by introducing a non-linear component to the fairness term without adding parameters. For example, using a quadratic term for disadvantageous inequality to capture increasing aversion as the inequality grows.\n\nAlternatively, the model could incorporate the trial_role variable, but since the task focuses on responder behavior (trial_role=1), this might not be necessary.\n\nIn summary, potential improvements could include:\n\n1. Adding a non-linear transformation to the fairness term (e.g., sigmoid, quadratic).\n2. Incorporating trial_type interactions to allow different beta values for different contribution scenarios.\n3. Including an intercept term to capture baseline acceptance probability.\n4. Introducing an interaction between self-interest and fairness terms.\n5. Considering opponent's share or equal split as additional fairness components.\n\nHowever, to maintain model parsimony and avoid overfitting, it's crucial to balance complexity. Since the current model performs well, perhaps introducing a non-linear fairness term could enhance accuracy without adding parameters. For example, using a squared term for the disadvantageous inequality:\n\nU = alpha*(split_self/CE) - beta*(max((fair_share - split_self)/fair_share, 0))^2\n\nThis would model increasing aversion to larger inequalities, which might better fit human behavior. The squared term could be implemented without additional parameters, keeping the model at two parameters (alpha and beta).\n\nAnother approach is to use a piecewise function, where beta applies only when the inequality exceeds a certain threshold. But that would require adding a threshold parameter, increasing model complexity.\n\nAlternatively, apply a logistic function to the fairness term to model a smooth threshold effect. For example:\n\nU = alpha*(split_self/CE) - beta*(1 / (1 + exp(-k*(fair_share - split_self)/fair_share)))\n\nHere, k is a scaling parameter. However, this introduces a new parameter (k), which might not be ideal. Instead, fixing k to a constant value (e.g., 10) could approximate a step function without adding a learnable parameter.\n\nConsidering all these options, the most feasible improvement without adding parameters is to square the fairness term, creating a non-linear penalty for larger inequalities. This could better capture the psychological response to unfairness, where participants are disproportionately more likely to reject as the unfairness increases beyond a point.\n\nThus, the new instructions should guide the model to incorporate non-linear terms in the fairness component, possibly through squaring or other transformations, while maintaining parameter orthogonality. Additionally, exploring trial_type interactions could help if done parsimoniously.\n</think>\n\nFocus on enhancing nonlinear sensitivity to inequality while maintaining parameter orthogonality. Implement these refinements:\n\n1. **Add quadratic inequality penalty** - Square the disadvantageous inequality term to capture accelerating rejection likelihood for larger fairness violations, while keeping the same parameters\n\n2. **Incorporate trial-type modulation** - Multiply beta by trial-type weights (1=equal, 2=opponent more, 3=participant more) using fixed coefficients from pretesting to maintain parsimony\n\n3. **Introduce adaptive logistic scaling** - Apply sigmoidal compression to monetary utility term to model diminishing returns at high offer values\n\n4. **Maintain orthogonal scaling** - Keep monetary terms normalized to [0,1] range while inequality terms use absolute \u00a3 differences for scale separation\n\nExample formulation sketch:\nU = \u03c3(\u03b1*(split_self/CE)) - \u03b2*(trial_type_weight)*max((fair_share - split_self),0)^2\n\nWhere \u03c3(x) = 1/(1+exp(-kx)) with fixed k=5 for stable scaling. Trial_type_weight uses [1.0, 1.2, 0.8] for types 1-3 respectively based on pretesting.\n\nKeep parameters bounded \u03b1: [0,15], \u03b2: [0,0.5] to account for squared term scaling. Prioritize models demonstrating >0.9 parameter recovery in synthetic tests with orthogonal term correlations <0.3.\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal. The model needs to focus on responder behavior, use the given variables appropriately, and ensure parameter recovery is possible. \n\nFirst, I need to identify which variables are relevant. The target is \"accept\", so the model should predict the probability of accepting based on the proposal's terms. The key variables here are likely split_self, split_opp, combined_earning, token_self, and token_opp. Since the utility should consider both self-interest and fairness, I need to model these two components.\n\nSelf-interest can be represented by the participant's proposed share (split_self). Fairness might involve comparing the split to what's considered fair based on contributions. The participant's contribution is token_self * \u00a33, and the opponent's is token_opp * \u00a33. A fair split might be proportional to their contributions. So, fair_share_self = (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and the fair_share_self could be a measure of unfairness.\n\nThe utility function could then be a combination of the monetary gain (split_self) and the fairness term. To model the binary decision, we can use a logistic function where the utility determines the log-odds of accepting.\n\nParameters to learn might be the weight given to self-interest (alpha) and the sensitivity to unfairness (beta). These parameters need bounds. Alpha could be positive (since more money is better), and beta could also be positive (since more unfairness reduces utility). \n\nBut wait, the fairness term could be framed as a loss when the proposed split is less than the fair share. So, the utility might be: U = alpha * split_self - beta * max(fair_share_self - split_self, 0). If the proposed split is less than fair, the participant experiences a loss proportional to the difference, weighted by beta. If it's equal or more, there's no loss.\n\nHowever, the problem states that combined_earning varies, so equal split_self percentages might not represent the same fairness. Therefore, normalizing by combined_earning might be necessary. For example, split_self / combined_earning gives the proportion the participant gets, and fair_share_self / combined_earning is their fair proportion. The difference in proportions could be the fairness term.\n\nAlternatively, using absolute differences in pounds might be better since the impact of a \u00a31 difference could be consistent regardless of total. But the model needs to handle varying combined_earning. Let's think: if combined_earning is higher, the same absolute difference might be less significant. Hmm, maybe proportional is better.\n\nWait, the note says that two equal split_self variables don't represent the same fairness if combined_earning differs. So, split_self alone isn't enough; we need to consider it relative to the total. But splitperc_self is provided, which is the percentage. However, using splitperc_self directly might not account for the actual fair percentage based on contributions. Because the fair percentage would be (token_self / (token_self + token_opp)) * 100. So, the fairness term could be the difference between the proposed percentage (splitperc_self) and the fair percentage.\n\nBut the dataset has splitperc_self as an integer. Wait, the variables include splitperc_self and splitperc_opp. However, the fair percentage based on contributions would be (token_self / (token_self + token_opp)) * 100. So, the discrepancy is splitperc_self - (token_self / (token_self + token_opp)) * 100. If this is positive, the participant is getting more than fair; if negative, less. The utility could penalize negative discrepancies (unfair to participant) but maybe not reward positive ones, depending on the model.\n\nAlternatively, the model could use the difference between the proposed split and the fair split in monetary terms. For example, fair_split_self = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference is split_self - fair_split_self. If this is negative, it's unfair, and the utility decreases.\n\nSo, the utility function could be:\n\nU = \u03b1 * split_self - \u03b2 * max(fair_split_self - split_self, 0)\n\nHere, \u03b1 is the sensitivity to monetary gain, and \u03b2 is the sensitivity to unfairness (when the offer is less than fair). The max function ensures that only offers below the fair share are penalized.\n\nThe probability of accepting is then logistic(U), so P(accept=1) = 1 / (1 + exp(-U)). If U > 0, the probability is greater than 0.5.\n\nNow, parameters \u03b1 and \u03b2 need to be learned. Their bounds should be positive. Let's set \u03b1 in [0, 10] and \u03b2 in [0, 10], which are generous but finite. These ranges can be adjusted based on data, but starting with wide bounds is safe.\n\nVariables in the model:\n\n- split_self: from data, the proposed amount to the participant.\n- fair_split_self: calculated as (token_self / (token_self + token_opp)) * combined_earning.\n- max(fair_split_self - split_self, 0): the unfairness term.\n\nParameters:\n\n- \u03b1 (alpha): weight on monetary gain.\n- \u03b2 (beta): weight on unfairness penalty.\n\nThe model uses these variables and parameters to compute utility, which is then passed through a logistic function to get the acceptance probability.\n\nNow, checking the requirements:\n\n1. Predict responder behavior (accept) - yes, using U in logistic.\n2. Uses dataset variables: split_self, token_self, token_opp, combined_earning.\n3. Parameters have finite bounds: \u03b1 and \u03b2 are bounded.\n4. Utility can be negative, affecting the probability.\n\nPotential issues: What if token_self + token_opp is zero? But in the task, participants find tokens, so token_self and token_opp are at least 1? The dataset probably doesn't have zero, but need to handle division by zero. However, in practice, the data may not have such cases, so we can proceed.\n\nAnother consideration: Should the model include the opponent's split? Since split_opp is combined_earning - split_self, it's redundant. But maybe participants compare their split to the opponent's. However, the fairness term based on contributions already captures that. Alternatively, the model could include a term comparing split_self to split_opp relative to contributions. But that might complicate the model. Let's stick with the contribution-based fairness for simplicity.\n\nAnother angle: The participant's contribution is token_self * \u00a33, and the opponent's is token_opp * \u00a33. The total contribution is combined_earning = 3*(token_self + token_opp). Wait, combined_earning is given as a variable, which is the total money earned. So, combined_earning = 3*(token_self + token_opp). Therefore, fair_split_self = (3*token_self) / (3*(token_self + token_opp)) * combined_earning = 3*token_self. Wait, that can't be right. Wait, combined_earning is 3*(token_self + token_opp). So, if the fair split is based on contributions, then each should get their own contribution. But in the task description, the combined earning is shared and later divided. Wait, the task says: \"the opponent's contribution to the shared pot that will be later divided by the opponent.\" Hmm, maybe the fair split is each keeps their own tokens, but the shared pot is divided. Wait, the description is a bit unclear. Let me re-read.\n\nTask Description: \"each participant is given 36 different proposals to respond to. [...] token_self is the number of tokens found by the participant, representing their contribution to the group. token_opp is the opponent's contribution. combined_earning is the total money earned (each token is \u00a33).\"\n\nSo, combined_earning = 3*(token_self + token_opp). The proposals are about how to split this combined earning. So, the proposer suggests split_self (for participant) and split_opp (for opponent), which sum to combined_earning.\n\nA fair split based on contributions would be each gets their own contribution: participant gets 3*token_self, opponent gets 3*token_opp. But the proposer can suggest any split. So, the fair_split_self would be 3*token_self. The difference between split_self and 3*token_self is the unfairness. If split_self < 3*token_self, it's unfair to the participant.\n\nSo, the utility function could be:\n\nU = \u03b1 * split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nThis captures the monetary gain (split_self) and the loss when the offer is less than their contribution (3*token_self). This is a different approach than the proportional split. Which is better?\n\nThe note says that two equal split_self values don't represent the same fairness if combined_earning differs. For example, if in one trial, combined_earning is \u00a330 (so token_self + token_opp =10), and split_self is \u00a315 (50%), but if token_self was 6 (so fair share would be \u00a318), then it's unfair. In another trial, combined_earning is \u00a360 (token_self + token_opp=20), split_self is \u00a330 (50%), but if token_self was 12, fair share is \u00a336. So, same percentage, but different fairness based on actual contributions. Therefore, the fairness should be based on the absolute contribution (3*token_self) rather than the percentage.\n\nThus, the model should use the difference between split_self and 3*token_self as the fairness term. So, when split_self is less than 3*token_self, it's unfair, leading to a utility decrease.\n\nTherefore, the utility function becomes:\n\nU = \u03b1 * split_self - \u03b2 * max( (3*token_self) - split_self, 0 )\n\nThis way, the fairness term is zero when the offer is at least their contribution, and positive otherwise. Alpha and beta are positive parameters.\n\nThe probability of acceptance is logistic(U). If U > 0, more likely to accept.\n\nThis model uses split_self and token_self from the data. Parameters alpha and beta need to be learned with bounds, say [0, 10] for both.\n\nAnother consideration: Should the model include the ratio instead of the absolute difference? For example, (3*token_self - split_self)/3*token_self, which normalizes the unfairness by the fair share. This could make the beta parameter more interpretable as sensitivity to proportional loss. Let's consider that.\n\nU = \u03b1 * split_self - \u03b2 * max( (3*token_self - split_self) / (3*token_self), 0 )\n\nThis term is the proportion of the fair share that's missing. For example, if the fair share is \u00a330 and the offer is \u00a324, the term is (6)/30 = 0.2. Beta then scales this proportion.\n\nThis might be better because it normalizes the unfairness, making the parameter beta unitless and easier to compare across participants. However, it introduces division by token_self, which could be zero. But token_self is the participant's contribution, so in the task, they must have at least some tokens, so token_self > 0. Thus, division is safe.\n\nSo, the model becomes:\n\nU = \u03b1 * split_self - \u03b2 * ( max(3*token_self - split_self, 0) / (3*token_self) )\n\nThis way, the unfairness term is the proportion of the participant's fair share that's missing. Alpha is in \u00a3^{-1} units (since split_self is in \u00a3), and beta is unitless. But to make the utility unitless, perhaps alpha should be unitless as well. Wait, if split_self is in \u00a3, then alpha has units of utility per \u00a3. Beta is utility per unit proportion. The utility U is in utility units, which is fine as it's passed to the logistic function.\n\nAlternatively, to make the terms unitless, we could normalize split_self by combined_earning. For example:\n\nU = \u03b1 * (split_self / combined_earning) - \u03b2 * ( max(3*token_self - split_self, 0) / (3*token_self) )\n\nThis makes both terms unitless. However, combined_earning is 3*(token_self + token_opp), so split_self / combined_earning is the proportion of the total that the participant gets. This might capture the relative share, which could be another factor. But the note says that equal split_self percentages aren't the same fairness, so this might not be the right approach. The original idea of using the absolute fair share (3*token_self) seems better aligned with the task description.\n\nWait, the note says: \"two equal split_self variables do not represent the same 'fairness'\". For example, if in one trial, split_self is \u00a315 when combined_earning is \u00a330 (50%), but the fair share based on contributions is \u00a318 (token_self=6), so it's unfair. In another trial, split_self is \u00a330 when combined_earning is \u00a360 (50%), but fair share is \u00a336 (token_self=12). So, same percentage, but different fairness. Therefore, the fairness should be based on the difference from the fair share in absolute terms (3*token_self - split_self), not the percentage.\n\nThus, the model should use the absolute difference. So, the utility function is:\n\nU = \u03b1 * split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nParameters alpha (sensitivity to monetary gain) and beta (sensitivity to unfairness) are both positive. The probability of acceptance is 1 / (1 + exp(-U)).\n\nThis model uses split_self and token_self from the data. Now, checking the variables in the dataset: token_self is available, as is split_self. Combined_earning is 3*(token_self + token_opp), but we don't need to calculate it since 3*token_self is directly from token_self.\n\nNow, the parameters alpha and beta need to have bounds. Let's set alpha in [0, 5] and beta in [0, 5]. These are generous bounds assuming that higher values would be unlikely but possible.\n\nAnother consideration: Should the model include an intercept term? For example, U = alpha * split_self - beta * unfairness + gamma. Gamma could account for a baseline tendency to accept or reject. However, the problem states that the utility should be such that less than 0 implies less than 50% chance. If we include an intercept, gamma could shift the baseline. But the current model without an intercept assumes that when split_self equals the fair share, U = alpha * split_self. If alpha is positive, higher offers increase acceptance. However, without an intercept, when split_self is exactly the fair share, U = alpha * 3*token_self. This might not be zero, leading to a baseline probability. Maybe an intercept is needed, but the problem says the utility variable may be unbounded, but the model must predict based on utility. However, the example given in the task doesn't include an intercept. Let's check the example:\n\nIn the example, the model is U_i = \u03b2 + \u03b5E + \u03b7N. So, they include a beta as a base tendency. So, perhaps including an intercept is acceptable. However, the current model without an intercept might not capture cases where even fair offers are rejected due to other factors. But to keep the model simple and aligned with the task's emphasis on parameter recovery, maybe omitting the intercept is better. Alternatively, include it but set its bounds.\n\nIf we add an intercept gamma, the model becomes:\n\nU = gamma + alpha * split_self - beta * max(3*token_self - split_self, 0)\n\nBut this adds another parameter. The task says \"any learnable parameters\" must have bounds. However, the example has two parameters. To keep the model parsimonious, perhaps stick with two parameters (alpha and beta). If including gamma, it's three, which might complicate parameter recovery. Since the task mentions parameter recovery as a priority, a simpler model with two parameters is better.\n\nThus, proceed without the intercept.\n\nNow, variable descriptions:\n\nVariables in the model:\n\n- split_self: from data, proposed share for participant.\n- token_self: from data, participant's tokens.\n- 3*token_self: calculated fair share.\n- max(3*token_self - split_self, 0): calculated unfairness term.\n\nParameters:\n\n- alpha: sensitivity to monetary gain, range [0, 5].\n- beta: sensitivity to unfairness, range [0, 5].\n\nThe utility U is calculated as alpha*split_self - beta*unfairness.\n\nThe target variable is \"accept\", which is binary. The model outputs the probability of accepting via logistic(U).\n\nNow, formalizing this:\n\n<MODEL>\nU = alpha * split_self - beta * max(3 * token_self - split_self, 0)\n</MODEL>\n\nVariables:\n\n- alpha: learnable, range [0,5].\n- beta: learnable, range [0,5].\n- split_self: from data.\n- token_self: from data.\n- max(...): calculated.\n\nThe variable descriptions in JSON:\n\nFor each variable in the model, including calculated ones:\n\n- U: utility, calculated, range (-inf, inf).\n- alpha: learnable, [0,5].\n- beta: learnable, [0,5].\n- split_self: data variable.\n- token_self: data variable.\n- max_term: calculated.\n\nBut in the JSON, we need to list all variables used in the model formula. However, the max term is part of the formula but not a separate variable. So, the variables in the formula are split_self, token_self, alpha, beta. The max is an operation, not a variable.\n\nThus, the JSON should describe alpha, beta, split_self, token_self, and U.\n\nSo:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to monetary gain (\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to unfairness (reduction in utility per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nThe target variable is \"accept\".\n\nSummary: A utility model combining monetary gain and fairness considerations. The utility is determined by the proposed amount to the participant and the deviation from their fair share based on their contribution. Participants are more likely to accept offers that are higher and closer to their fair contribution.\n</think>\n\n<MODEL>\nU = alpha * split_self - beta * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain (\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\u00a3 per token)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-component utility model evaluating monetary self-interest and contribution-based fairness. Compares the proposed \u00a3 amount to the participant's fair share (3\u00d7 their tokens). Penalizes offers below fair share proportionally to the deficit. Uses orthogonal parameterization with monetary terms in absolute \u00a3 and fairness in \u00a3 deviation for clean parameter recovery.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = alpha * split_self - beta * max(3 * token_self - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain (\\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (\\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\\u00a3 per token)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        unfairness = max(3 * token_self - split_self, 0)\n        U = alpha * split_self - beta * unfairness\n\n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"0\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 0, \"token_opp\": 7, \"token_self\": 5, \"combined_earning\": 36, \"split_opp\": 19.44, \"split_self\": 16.56, \"splitperc_opp\": 54, \"splitperc_self\": 46, \"accept\": 1.0, \"accepted_amount\": 16.56, \"accepted_perc\": 46.0, \"proposed_perc\": 46.0, \"proposed_amount\": 16.56}, {\"ID\": 1, \"group\": \"0\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 0, \"token_opp\": 7, \"token_self\": 5, \"combined_earning\": 36, \"split_opp\": 18.0, \"split_self\": 18.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 18.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 18.0}, {\"ID\": 2, \"group\": \"0\", \"trial_number\": 3, \"trial_type\": 1, \"trial_role\": 0, \"token_opp\": 3, \"token_self\": 9, \"combined_earning\": 36, \"split_opp\": 25.92, \"split_self\": 10.08, \"splitperc_opp\": 72, \"splitperc_self\": 28, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 28.0, \"proposed_amount\": 10.08}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1743519503.525642
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to absolute monetary gain (\u00a3)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to disadvantageous inequality (\u00a3)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = alpha * split_self - beta * max(3 * token_self - split_self, 0)",
  "variable_descriptions": {
    "alpha": {
      "description": "Sensitivity to absolute monetary gain (\u00a3)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to disadvantageous inequality (\u00a3)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for participant in pounds",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Participant's token contribution (3\u00a3 per token)",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    }
  },
  "model_summary": "A dual-component utility model evaluating monetary self-interest and contribution-based fairness. Compares the proposed \u00a3 amount to the participant's fair share (3\u00d7 their tokens). Penalizes offers below fair share proportionally to the deficit. Uses orthogonal parameterization with monetary terms in absolute \u00a3 and fairness in \u00a3 deviation for clean parameter recovery.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal. The model needs to focus on responder behavior, use the given variables appropriately, and ensure parameter recovery is possible. \n\nFirst, I need to identify which variables are relevant. The target is \"accept\", so the model should predict the probability of accepting based on the proposal's terms. The key variables here are likely split_self, split_opp, combined_earning, token_self, and token_opp. Since the utility should consider both self-interest and fairness, I need to model these two components.\n\nSelf-interest can be represented by the participant's proposed share (split_self). Fairness might involve comparing the split to what's considered fair based on contributions. The participant's contribution is token_self * \u00a33, and the opponent's is token_opp * \u00a33. A fair split might be proportional to their contributions. So, fair_share_self = (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and the fair_share_self could be a measure of unfairness.\n\nThe utility function could then be a combination of the monetary gain (split_self) and the fairness term. To model the binary decision, we can use a logistic function where the utility determines the log-odds of accepting.\n\nParameters to learn might be the weight given to self-interest (alpha) and the sensitivity to unfairness (beta). These parameters need bounds. Alpha could be positive (since more money is better), and beta could also be positive (since more unfairness reduces utility). \n\nBut wait, the fairness term could be framed as a loss when the proposed split is less than the fair share. So, the utility might be: U = alpha * split_self - beta * max(fair_share_self - split_self, 0). If the proposed split is less than fair, the participant experiences a loss proportional to the difference, weighted by beta. If it's equal or more, there's no loss.\n\nHowever, the problem states that combined_earning varies, so equal split_self percentages might not represent the same fairness. Therefore, normalizing by combined_earning might be necessary. For example, split_self / combined_earning gives the proportion the participant gets, and fair_share_self / combined_earning is their fair proportion. The difference in proportions could be the fairness term.\n\nAlternatively, using absolute differences in pounds might be better since the impact of a \u00a31 difference could be consistent regardless of total. But the model needs to handle varying combined_earning. Let's think: if combined_earning is higher, the same absolute difference might be less significant. Hmm, maybe proportional is better.\n\nWait, the note says that two equal split_self variables don't represent the same fairness if combined_earning differs. So, split_self alone isn't enough; we need to consider it relative to the total. But splitperc_self is provided, which is the percentage. However, using splitperc_self directly might not account for the actual fair percentage based on contributions. Because the fair percentage would be (token_self / (token_self + token_opp)) * 100. So, the fairness term could be the difference between the proposed percentage (splitperc_self) and the fair percentage.\n\nBut the dataset has splitperc_self as an integer. Wait, the variables include splitperc_self and splitperc_opp. However, the fair percentage based on contributions would be (token_self / (token_self + token_opp)) * 100. So, the discrepancy is splitperc_self - (token_self / (token_self + token_opp)) * 100. If this is positive, the participant is getting more than fair; if negative, less. The utility could penalize negative discrepancies (unfair to participant) but maybe not reward positive ones, depending on the model.\n\nAlternatively, the model could use the difference between the proposed split and the fair split in monetary terms. For example, fair_split_self = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference is split_self - fair_split_self. If this is negative, it's unfair, and the utility decreases.\n\nSo, the utility function could be:\n\nU = \u03b1 * split_self - \u03b2 * max(fair_split_self - split_self, 0)\n\nHere, \u03b1 is the sensitivity to monetary gain, and \u03b2 is the sensitivity to unfairness (when the offer is less than fair). The max function ensures that only offers below the fair share are penalized.\n\nThe probability of accepting is then logistic(U), so P(accept=1) = 1 / (1 + exp(-U)). If U > 0, the probability is greater than 0.5.\n\nNow, parameters \u03b1 and \u03b2 need to be learned. Their bounds should be positive. Let's set \u03b1 in [0, 10] and \u03b2 in [0, 10], which are generous but finite. These ranges can be adjusted based on data, but starting with wide bounds is safe.\n\nVariables in the model:\n\n- split_self: from data, the proposed amount to the participant.\n- fair_split_self: calculated as (token_self / (token_self + token_opp)) * combined_earning.\n- max(fair_split_self - split_self, 0): the unfairness term.\n\nParameters:\n\n- \u03b1 (alpha): weight on monetary gain.\n- \u03b2 (beta): weight on unfairness penalty.\n\nThe model uses these variables and parameters to compute utility, which is then passed through a logistic function to get the acceptance probability.\n\nNow, checking the requirements:\n\n1. Predict responder behavior (accept) - yes, using U in logistic.\n2. Uses dataset variables: split_self, token_self, token_opp, combined_earning.\n3. Parameters have finite bounds: \u03b1 and \u03b2 are bounded.\n4. Utility can be negative, affecting the probability.\n\nPotential issues: What if token_self + token_opp is zero? But in the task, participants find tokens, so token_self and token_opp are at least 1? The dataset probably doesn't have zero, but need to handle division by zero. However, in practice, the data may not have such cases, so we can proceed.\n\nAnother consideration: Should the model include the opponent's split? Since split_opp is combined_earning - split_self, it's redundant. But maybe participants compare their split to the opponent's. However, the fairness term based on contributions already captures that. Alternatively, the model could include a term comparing split_self to split_opp relative to contributions. But that might complicate the model. Let's stick with the contribution-based fairness for simplicity.\n\nAnother angle: The participant's contribution is token_self * \u00a33, and the opponent's is token_opp * \u00a33. The total contribution is combined_earning = 3*(token_self + token_opp). Wait, combined_earning is given as a variable, which is the total money earned. So, combined_earning = 3*(token_self + token_opp). Therefore, fair_split_self = (3*token_self) / (3*(token_self + token_opp)) * combined_earning = 3*token_self. Wait, that can't be right. Wait, combined_earning is 3*(token_self + token_opp). So, if the fair split is based on contributions, then each should get their own contribution. But in the task description, the combined earning is shared and later divided. Wait, the task says: \"the opponent's contribution to the shared pot that will be later divided by the opponent.\" Hmm, maybe the fair split is each keeps their own tokens, but the shared pot is divided. Wait, the description is a bit unclear. Let me re-read.\n\nTask Description: \"each participant is given 36 different proposals to respond to. [...] token_self is the number of tokens found by the participant, representing their contribution to the group. token_opp is the opponent's contribution. combined_earning is the total money earned (each token is \u00a33).\"\n\nSo, combined_earning = 3*(token_self + token_opp). The proposals are about how to split this combined earning. So, the proposer suggests split_self (for participant) and split_opp (for opponent), which sum to combined_earning.\n\nA fair split based on contributions would be each gets their own contribution: participant gets 3*token_self, opponent gets 3*token_opp. But the proposer can suggest any split. So, the fair_split_self would be 3*token_self. The difference between split_self and 3*token_self is the unfairness. If split_self < 3*token_self, it's unfair to the participant.\n\nSo, the utility function could be:\n\nU = \u03b1 * split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nThis captures the monetary gain (split_self) and the loss when the offer is less than their contribution (3*token_self). This is a different approach than the proportional split. Which is better?\n\nThe note says that two equal split_self values don't represent the same fairness if combined_earning differs. For example, if in one trial, combined_earning is \u00a330 (so token_self + token_opp =10), and split_self is \u00a315 (50%), but if token_self was 6 (so fair share would be \u00a318), then it's unfair. In another trial, combined_earning is \u00a360 (token_self + token_opp=20), split_self is \u00a330 (50%), but if token_self was 12, fair share is \u00a336. So, same percentage, but different fairness based on actual contributions. Therefore, the fairness should be based on the absolute contribution (3*token_self) rather than the percentage.\n\nThus, the model should use the difference between split_self and 3*token_self as the fairness term. So, when split_self is less than 3*token_self, it's unfair, leading to a utility decrease.\n\nTherefore, the utility function becomes:\n\nU = \u03b1 * split_self - \u03b2 * max( (3*token_self) - split_self, 0 )\n\nThis way, the fairness term is zero when the offer is at least their contribution, and positive otherwise. Alpha and beta are positive parameters.\n\nThe probability of acceptance is logistic(U). If U > 0, more likely to accept.\n\nThis model uses split_self and token_self from the data. Parameters alpha and beta need to be learned with bounds, say [0, 10] for both.\n\nAnother consideration: Should the model include the ratio instead of the absolute difference? For example, (3*token_self - split_self)/3*token_self, which normalizes the unfairness by the fair share. This could make the beta parameter more interpretable as sensitivity to proportional loss. Let's consider that.\n\nU = \u03b1 * split_self - \u03b2 * max( (3*token_self - split_self) / (3*token_self), 0 )\n\nThis term is the proportion of the fair share that's missing. For example, if the fair share is \u00a330 and the offer is \u00a324, the term is (6)/30 = 0.2. Beta then scales this proportion.\n\nThis might be better because it normalizes the unfairness, making the parameter beta unitless and easier to compare across participants. However, it introduces division by token_self, which could be zero. But token_self is the participant's contribution, so in the task, they must have at least some tokens, so token_self > 0. Thus, division is safe.\n\nSo, the model becomes:\n\nU = \u03b1 * split_self - \u03b2 * ( max(3*token_self - split_self, 0) / (3*token_self) )\n\nThis way, the unfairness term is the proportion of the participant's fair share that's missing. Alpha is in \u00a3^{-1} units (since split_self is in \u00a3), and beta is unitless. But to make the utility unitless, perhaps alpha should be unitless as well. Wait, if split_self is in \u00a3, then alpha has units of utility per \u00a3. Beta is utility per unit proportion. The utility U is in utility units, which is fine as it's passed to the logistic function.\n\nAlternatively, to make the terms unitless, we could normalize split_self by combined_earning. For example:\n\nU = \u03b1 * (split_self / combined_earning) - \u03b2 * ( max(3*token_self - split_self, 0) / (3*token_self) )\n\nThis makes both terms unitless. However, combined_earning is 3*(token_self + token_opp), so split_self / combined_earning is the proportion of the total that the participant gets. This might capture the relative share, which could be another factor. But the note says that equal split_self percentages aren't the same fairness, so this might not be the right approach. The original idea of using the absolute fair share (3*token_self) seems better aligned with the task description.\n\nWait, the note says: \"two equal split_self variables do not represent the same 'fairness'\". For example, if in one trial, split_self is \u00a315 when combined_earning is \u00a330 (50%), but the fair share based on contributions is \u00a318 (token_self=6), so it's unfair. In another trial, split_self is \u00a330 when combined_earning is \u00a360 (50%), but fair share is \u00a336 (token_self=12). So, same percentage, but different fairness. Therefore, the fairness should be based on the difference from the fair share in absolute terms (3*token_self - split_self), not the percentage.\n\nThus, the model should use the absolute difference. So, the utility function is:\n\nU = \u03b1 * split_self - \u03b2 * max(3*token_self - split_self, 0)\n\nParameters alpha (sensitivity to monetary gain) and beta (sensitivity to unfairness) are both positive. The probability of acceptance is 1 / (1 + exp(-U)).\n\nThis model uses split_self and token_self from the data. Now, checking the variables in the dataset: token_self is available, as is split_self. Combined_earning is 3*(token_self + token_opp), but we don't need to calculate it since 3*token_self is directly from token_self.\n\nNow, the parameters alpha and beta need to have bounds. Let's set alpha in [0, 5] and beta in [0, 5]. These are generous bounds assuming that higher values would be unlikely but possible.\n\nAnother consideration: Should the model include an intercept term? For example, U = alpha * split_self - beta * unfairness + gamma. Gamma could account for a baseline tendency to accept or reject. However, the problem states that the utility should be such that less than 0 implies less than 50% chance. If we include an intercept, gamma could shift the baseline. But the current model without an intercept assumes that when split_self equals the fair share, U = alpha * split_self. If alpha is positive, higher offers increase acceptance. However, without an intercept, when split_self is exactly the fair share, U = alpha * 3*token_self. This might not be zero, leading to a baseline probability. Maybe an intercept is needed, but the problem says the utility variable may be unbounded, but the model must predict based on utility. However, the example given in the task doesn't include an intercept. Let's check the example:\n\nIn the example, the model is U_i = \u03b2 + \u03b5E + \u03b7N. So, they include a beta as a base tendency. So, perhaps including an intercept is acceptable. However, the current model without an intercept might not capture cases where even fair offers are rejected due to other factors. But to keep the model simple and aligned with the task's emphasis on parameter recovery, maybe omitting the intercept is better. Alternatively, include it but set its bounds.\n\nIf we add an intercept gamma, the model becomes:\n\nU = gamma + alpha * split_self - beta * max(3*token_self - split_self, 0)\n\nBut this adds another parameter. The task says \"any learnable parameters\" must have bounds. However, the example has two parameters. To keep the model parsimonious, perhaps stick with two parameters (alpha and beta). If including gamma, it's three, which might complicate parameter recovery. Since the task mentions parameter recovery as a priority, a simpler model with two parameters is better.\n\nThus, proceed without the intercept.\n\nNow, variable descriptions:\n\nVariables in the model:\n\n- split_self: from data, proposed share for participant.\n- token_self: from data, participant's tokens.\n- 3*token_self: calculated fair share.\n- max(3*token_self - split_self, 0): calculated unfairness term.\n\nParameters:\n\n- alpha: sensitivity to monetary gain, range [0, 5].\n- beta: sensitivity to unfairness, range [0, 5].\n\nThe utility U is calculated as alpha*split_self - beta*unfairness.\n\nThe target variable is \"accept\", which is binary. The model outputs the probability of accepting via logistic(U).\n\nNow, formalizing this:\n\n<MODEL>\nU = alpha * split_self - beta * max(3 * token_self - split_self, 0)\n</MODEL>\n\nVariables:\n\n- alpha: learnable, range [0,5].\n- beta: learnable, range [0,5].\n- split_self: from data.\n- token_self: from data.\n- max(...): calculated.\n\nThe variable descriptions in JSON:\n\nFor each variable in the model, including calculated ones:\n\n- U: utility, calculated, range (-inf, inf).\n- alpha: learnable, [0,5].\n- beta: learnable, [0,5].\n- split_self: data variable.\n- token_self: data variable.\n- max_term: calculated.\n\nBut in the JSON, we need to list all variables used in the model formula. However, the max term is part of the formula but not a separate variable. So, the variables in the formula are split_self, token_self, alpha, beta. The max is an operation, not a variable.\n\nThus, the JSON should describe alpha, beta, split_self, token_self, and U.\n\nSo:\n\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to monetary gain (\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to unfairness (reduction in utility per \u00a3 below fair share)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n\nThe target variable is \"accept\".\n\nSummary: A utility model combining monetary gain and fairness considerations. The utility is determined by the proposed amount to the participant and the deviation from their fair share based on their contribution. Participants are more likely to accept offers that are higher and closer to their fair contribution.\n</think>\n\n<MODEL>\nU = alpha * split_self - beta * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain (\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequality (\u00a3)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\u00a3 per token)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-component utility model evaluating monetary self-interest and contribution-based fairness. Compares the proposed \u00a3 amount to the participant's fair share (3\u00d7 their tokens). Penalizes offers below fair share proportionally to the deficit. Uses orthogonal parameterization with monetary terms in absolute \u00a3 and fairness in \u00a3 deviation for clean parameter recovery.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = alpha * split_self - beta * max(3 * token_self - split_self, 0)\nSummary: A dual-component utility model evaluating monetary self-interest and contribution-based fairness. Compares the proposed \u00a3 amount to the participant's fair share (3\u00d7 their tokens). Penalizes offers below fair share proportionally to the deficit. Uses orthogonal parameterization with monetary terms in absolute \u00a3 and fairness in \u00a3 deviation for clean parameter recovery.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        unfairness = max(3 * token_self - split_self, 0)\n        U = alpha * split_self - beta * unfairness\n\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8070818070818071,
  "fitting_results": [
    {
      "alpha": 0.005259513884460025,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -25.747487689770857,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05899263372229874,
      "beta": 0.08820920481967955,
      "success": true,
      "log_likelihood": -16.28126268288933,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12499222692214884,
      "beta": 0.17944499266821998,
      "success": true,
      "log_likelihood": -10.046565395825956,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07090761676720762,
      "beta": 0.12306225744413347,
      "success": true,
      "log_likelihood": -13.653156181738629,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10143686705068891,
      "beta": 0.19344885314684193,
      "success": true,
      "log_likelihood": -9.848024472740827,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12098129147964597,
      "beta": 0.1270148312030614,
      "success": true,
      "log_likelihood": -11.04233234479341,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1208457066548133,
      "beta": 0.12208653609853212,
      "success": true,
      "log_likelihood": -11.096668637653272,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07720886955625535,
      "beta": 0.1637923376553975,
      "success": true,
      "log_likelihood": -11.166531340679189,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.108524209184371,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.05191243755490497,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.005259344880641807,
      "beta": 4.258037568335004,
      "success": true,
      "log_likelihood": -48.77316638817109,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.2785832785306854,
      "beta": 4.832550339962938,
      "success": true,
      "log_likelihood": -23.02643786290852,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.43883525969964826,
      "beta": 0.21037257566188883,
      "success": true,
      "log_likelihood": -2.746171475114618,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -92.10340371656177,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06998764708607949,
      "beta": 0.09650271697749599,
      "success": true,
      "log_likelihood": -15.24028348524572,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1573253228145527,
      "beta": 0.07540355308682946,
      "success": true,
      "log_likelihood": -7.939517179685721,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03401656229416233,
      "beta": 0.07095077291341223,
      "success": true,
      "log_likelihood": -18.324232069400058,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.059611455577154596,
      "beta": 0.07557296101212448,
      "success": true,
      "log_likelihood": -16.99121846612038,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06848244102960437,
      "beta": 0.11913979058825808,
      "success": true,
      "log_likelihood": -13.93929753000998,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07853800660631942,
      "beta": 0.08955041097342055,
      "success": true,
      "log_likelihood": -14.89471869635006,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.10935036204221638,
      "beta": 0.13688120414595673,
      "success": true,
      "log_likelihood": -11.622843834153038,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.14950768624728497,
      "beta": 0.5165392839742492,
      "success": true,
      "log_likelihood": -3.9366059450744615,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -92.10340371656179,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06843239633441309,
      "beta": 0.038585836345924406,
      "success": true,
      "log_likelihood": -15.902796867869032,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0052595137095413115,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -25.747487689770843,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.026514910569707797,
      "beta": 0.04692965769291241,
      "success": true,
      "log_likelihood": -21.0583751807261,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04929960447619797,
      "beta": 0.10099316854268738,
      "success": true,
      "log_likelihood": -15.412194969673168,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": 3.600000297685338e-09,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -299.3360620869261,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2079233887290615,
      "beta": 0.1844363419772733,
      "success": true,
      "log_likelihood": -7.009324201879098,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07091036425886237,
      "beta": 0.11390857695985712,
      "success": true,
      "log_likelihood": -14.225669140751714,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.059121231704604656,
      "beta": 0.0543385391009041,
      "success": true,
      "log_likelihood": -17.719069088207146,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652135,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05638856770033902,
      "beta": 0.06531488276964394,
      "success": true,
      "log_likelihood": -17.74625432352741,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6515641882430935,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -46.26901648122013,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -23.02585092644049,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06779496184191056,
      "beta": 0.07962623802533789,
      "success": true,
      "log_likelihood": -16.13931957509956,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09495663998014563,
      "beta": 0.10165854279462673,
      "success": true,
      "log_likelihood": -13.257553280919218,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06980980149646121,
      "beta": 0.039793971798147906,
      "success": true,
      "log_likelihood": -15.740340539737485,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06998760986506351,
      "beta": 0.09650255391988612,
      "success": true,
      "log_likelihood": -15.240283488097512,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03243100489541689,
      "beta": 0.06492933517028461,
      "success": true,
      "log_likelihood": -18.995917545895715,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07185392164886813,
      "beta": 0.16099365756876477,
      "success": true,
      "log_likelihood": -11.269891205391698,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.019362605781697934,
      "beta": 0.017282229162587794,
      "success": true,
      "log_likelihood": -23.589996984413077,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.060917813904799396,
      "beta": 0.04264609088988205,
      "success": true,
      "log_likelihood": -17.337692820525984,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.005265830814024262,
      "beta": 4.999524531185708,
      "success": true,
      "log_likelihood": -71.79812533398045,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.047689463124211684,
      "beta": 0.054429072388023504,
      "success": true,
      "log_likelihood": -19.084444854169273,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.032770291642647595,
      "beta": 0.02295065380978793,
      "success": true,
      "log_likelihood": -21.64957631840722,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.02231661559823371,
      "beta": 0.06166205808731593,
      "success": true,
      "log_likelihood": -19.14585130487358,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.19044101849199888,
      "beta": 0.47277635674104856,
      "success": true,
      "log_likelihood": -4.334577663519501,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0501658774385163,
      "beta": 0.17106926031072417,
      "success": true,
      "log_likelihood": -10.386539880507364,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.05688299606975176,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -13.680488206356186,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.003734094570380483,
      "beta": 0.14678541109511875,
      "success": true,
      "log_likelihood": -9.825736629797643,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07156591976719291,
      "beta": 0.02323016677665382,
      "success": true,
      "log_likelihood": -14.06213846640598,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04501545618819256,
      "beta": 0.04346502440192342,
      "success": true,
      "log_likelihood": -19.766504713132246,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06652932361073131,
      "beta": 0.10719311382909753,
      "success": true,
      "log_likelihood": -14.763159511587144,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.005259514817685487,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -25.747487689770807,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.031536419402280184,
      "beta": 0.048855935156138446,
      "success": true,
      "log_likelihood": -20.702526786716195,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03482920152234128,
      "beta": 0.15200771342822336,
      "success": true,
      "log_likelihood": -11.083994119289546,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08156429187956338,
      "beta": 0.09696591626322085,
      "success": true,
      "log_likelihood": -14.437445103455333,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.786232658886159,
      "beta": 0.33826665249692134,
      "success": true,
      "log_likelihood": -69.0491109721847,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 2.3492115795951167,
      "success": true,
      "log_likelihood": -69.0778426872951,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07077035259881391,
      "beta": 0.07351998112877271,
      "success": true,
      "log_likelihood": -16.029288799864,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652137,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2339923406789321,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.0005862232349116613,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0052584665740615325,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -71.79918955186608,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06853117666827967,
      "beta": 0.09085737437759428,
      "success": true,
      "log_likelihood": -15.606148306028262,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07014345234592109,
      "beta": 0.028325409395801995,
      "success": true,
      "log_likelihood": -14.833726682762892,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.023030105621126,
      "beta": 0.031096664409879127,
      "success": true,
      "log_likelihood": -22.663571894009973,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09436893242692704,
      "beta": 0.08568005602838794,
      "success": true,
      "log_likelihood": -13.469173046162764,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -46.051701856480875,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -25.79844026008491,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.026581601952801553,
      "beta": 0.01334999635955405,
      "success": true,
      "log_likelihood": -22.370656052724478,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.005259523005646442,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -25.747487689770708,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.023422812789289933,
      "beta": 0.028311474304525516,
      "success": true,
      "log_likelihood": -22.809114635748884,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -69.07755278652131,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.13446323582589995,
      "success": true,
      "log_likelihood": -10.063924084619469,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12528724953429143,
      "beta": 0.1631325212246376,
      "success": true,
      "log_likelihood": -10.339432461813194,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.01697965199675093,
      "beta": 0.00992258371352798,
      "success": true,
      "log_likelihood": -23.81915563980267,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.016571733748182813,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -25.364873864621217,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.057273072935207316,
      "beta": 0.04427720188425613,
      "success": true,
      "log_likelihood": -17.959641902203174,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.01582569405088422,
      "beta": 0.10789203231278796,
      "success": true,
      "log_likelihood": -13.3171596785616,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -46.05170185648088,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07729603671207237,
      "beta": 0.09893994750247305,
      "success": true,
      "log_likelihood": -14.679368249030173,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.03534802565055273,
      "beta": 0.016635345804301187,
      "success": true,
      "log_likelihood": -20.833123314083647,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.7324111027848468,
        "p": 4.749201318344956e-18
      },
      "beta": {
        "r": 0.7576278355885437,
        "p": 7.305837343195353e-20
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 55.980972218992264,
  "bic_results": {
    "average_bic": 55.980972218992264,
    "individual_bics": [
      58.66201325645393,
      39.72956324269088,
      27.260168668564134,
      34.47335024038948,
      26.863086822393875,
      29.25170256649904,
      7.167037869712219,
      29.360375152218765,
      7.167037869712219,
      29.500100558270596,
      7.27086275202203,
      104.7133706532544,
      53.21991360272926,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      7.167037869712219,
      12.659380827141455,
      191.37384531003576,
      7.167037869712219,
      37.64760484740366,
      23.046072236283663,
      43.81550201571233,
      41.14947480915298,
      35.04563293693218,
      36.95647526961234,
      30.412725545218294,
      15.040249767061143,
      191.3738453100358,
      38.97263161265028,
      7.167037869712219,
      58.6620132564539,
      49.283788238364416,
      37.99142781625856,
      7.167037869712219,
      605.8391620507645,
      21.185686280670417,
      35.61837615841565,
      42.60517605332651,
      145.32214344995492,
      42.65954652396704,
      99.70507083935247,
      53.2187397297932,
      39.44567702711134,
      33.682144438750655,
      38.64771895638719,
      37.64760485310725,
      45.15887296870365,
      29.706820287695614,
      54.34703184573837,
      41.842423517964185,
      150.76328854487312,
      45.335927585250765,
      50.46619051372666,
      45.45874048665938,
      15.836193203951222,
      27.940117637926946,
      34.52801428962459,
      26.818511136507503,
      35.29131480972418,
      46.70004730317671,
      36.693356900086506,
      58.66201325645383,
      48.57209145034461,
      29.33502611549131,
      36.04192808382289,
      145.26525982128163,
      145.32272325150242,
      39.225615476640215,
      145.32214344995495,
      7.168210323382043,
      150.76541698064437,
      38.379334488968745,
      36.834491242438006,
      52.494181664932164,
      34.10538396923775,
      99.27044158987397,
      58.76391839708204,
      51.90834998236117,
      58.66201325645363,
      52.785267148409986,
      145.32214344995484,
      27.294886046151156,
      27.84590280053861,
      54.80534915651756,
      57.89678560615465,
      43.086321681318566,
      33.80135723403542,
      99.27044158987398,
      36.52577437497256,
      48.83328450507951
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.732\n- beta: r = 0.758",
  "previous_models": [
    "Specification: U = alpha * split_self - beta * max(3 * token_self - split_self, 0)\nSummary: A dual-component utility model evaluating monetary self-interest and contribution-based fairness. Compares the proposed \u00a3 amount to the participant's fair share (3\u00d7 their tokens). Penalizes offers below fair share proportionally to the deficit. Uses orthogonal parameterization with monetary terms in absolute \u00a3 and fairness in \u00a3 deviation for clean parameter recovery.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 55.980972218992264\nOverall Accuracy: 0.8071\n\nParameter Recovery:\n- alpha: r = 0.732\n- beta: r = 0.758"
  ]
}