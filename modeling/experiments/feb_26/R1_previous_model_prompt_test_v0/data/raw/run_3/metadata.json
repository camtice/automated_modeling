{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. Start with variables with no learnable parameters, and then shift to comprehensively explore the place with one learnable paramter, and then two, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = beta * split_self - alpha * max(3 * token_self - split_self, 0)\nSummary: A utility model capturing trade-offs between absolute monetary gain and aversion to receiving less than one's contribution-based fair share. The utility combines a weighted monetary value (beta * split_self) with a penalty for deficits relative to the fair share (alpha * max(3*token_self - split_self, 0)). Two learnable parameters (alpha and beta) govern sensitivity to fairness and monetary value, respectively. The model reflects inequity aversion principles while maintaining identifiability through distinct parameter roles.\nBIC: None\nModel 2:\nSpecification: U_accept = beta * split_self + alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: A dual-component utility model combining absolute monetary value (split_self) and proportional fairness relative to contributions. The fairness component compares the offered percentage (splitperc_self) to the contribution-based fair percentage [(token_self/(token_self+token_opp))*100]. Two learnable parameters (beta, alpha) capture sensitivity to monetary gains and equity considerations respectively, enabling simultaneous modeling of self-interest and contribution-based inequity aversion.\nBIC: None",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. Start with variables with no learnable parameters, and then shift to comprehensively explore the place with one learnable paramter, and then two, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = beta * split_self - alpha * max(3 * token_self - split_self, 0)\nSummary: A utility model capturing trade-offs between absolute monetary gain and aversion to receiving less than one's contribution-based fair share. The utility combines a weighted monetary value (beta * split_self) with a penalty for deficits relative to the fair share (alpha * max(3*token_self - split_self, 0)). Two learnable parameters (alpha and beta) govern sensitivity to fairness and monetary value, respectively. The model reflects inequity aversion principles while maintaining identifiability through distinct parameter roles.\nBIC: None\nModel 2:\nSpecification: U_accept = beta * split_self + alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: A dual-component utility model combining absolute monetary value (split_self) and proportional fairness relative to contributions. The fairness component compares the offered percentage (splitperc_self) to the contribution-based fair percentage [(token_self/(token_self+token_opp))*100]. Two learnable parameters (beta, alpha) capture sensitivity to monetary gains and equity considerations respectively, enabling simultaneous modeling of self-interest and contribution-based inequity aversion.\nBIC: None\n\nPrevious Models:\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer for the responder in the given experiment. The model needs to be based on psychological principles, have no more than two learnable parameters, and use variables from the dataset provided.\n\nFirst, looking at the previous models, Model 1 focuses on absolute monetary gain and aversion to receiving less than one's contribution-based fair share. Model 2 combines absolute monetary value with proportional fairness relative to contributions. Both models use two parameters (alpha and beta) to capture different aspects of fairness and self-interest.\n\nTo build a new model that explores a different mechanism, I need to consider other psychological principles. One possibility is the concept of reciprocity or intention-based fairness. However, the dataset might not have variables that directly capture the opponent's intentions. Another angle is to consider the relative difference between the proposed split and the fair split based on contributions, perhaps using a nonlinear function to model aversion to unfairness.\n\nWait, the dataset includes variables like token_self and token_opp, which represent the contributions. The combined_earning is 3*(token_self + token_opp). The split_self is the amount offered to the participant. A fair split based on contributions would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the actual offer (split_self) and this fair amount could be a key factor. If the offer is less than the fair amount, the responder might feel treated unfairly, leading to a negative utility. Conversely, if it's more, they might feel positively, but perhaps with diminishing returns.\n\nAlternatively, using the percentage split (splitperc_self) compared to the fair percentage (token_self / total tokens * 100). The difference here could be squared to model aversion to any deviation from fairness, whether advantageous or disadvantageous. But the problem states that the model should not include parameters for general stochasticity, so focusing on deterministic components.\n\nAnother thought: the utility could be a combination of the monetary gain (split_self) and a term that penalizes deviations from the fair split. For example, the fair split is 3*token_self (since each token is \u00a33), but the combined_earning is 3*(token_self + token_opp). Wait, the fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so the fair share is 3*token_self. Wait, that's interesting. Because combined_earning is 3*(token_self + token_opp), so if each person's contribution is their tokens, then the fair split for self would be 3*token_self. So split_self - 3*token_self would be the difference between the offer and the fair amount. If the offer is less than fair, that's a negative difference, which could be a penalty. If it's more, that's a positive difference. But maybe people are more sensitive to being undercut than over-rewarded. So perhaps a linear term for the monetary gain (split_self) and a separate term for the difference from the fair amount, with a parameter that scales the importance of fairness.\n\nWait, in Model 1, they used max(3*token_self - split_self, 0), which is the deficit relative to the fair share. So that's a one-sided penalty. But maybe a two-sided penalty, where both under and over offers affect utility, but asymmetrically. However, with two parameters, perhaps we can model the monetary term and the fairness term. Let's think: U_accept = beta * split_self + alpha * (split_self - 3*token_self). Here, beta is the weight on the monetary gain, and alpha is the weight on the difference from the fair share. But this might lead to covariance between beta and alpha. Alternatively, combining them into a single term. Wait, but if alpha is positive, then getting more than fair share increases utility, which might not align with some theories. Alternatively, if the fair share is considered a reference point, then deviations from it could be modeled with a loss aversion parameter. For example, using a piecewise function where negative deviations (split_self < 3*token_self) are penalized more than positive deviations are rewarded. But that would require more parameters. However, the problem allows two parameters. Let's see.\n\nAlternatively, using a quadratic term for the difference from the fair share. U_accept = beta * split_self - alpha * (split_self - 3*token_self)^2. This would penalize both under and over offers relative to the fair share, with the idea that people prefer offers close to their fair share. But this might not capture the directionality\u2014people might be okay with over-offers but not under. However, the quadratic term would treat both as negative if alpha is positive. Wait, if alpha is positive, then the term -alpha*(difference)^2 would always reduce utility, which might not make sense. Alternatively, if the model is U_accept = beta * split_self + alpha * (split_self - 3*token_self). But this would be a linear term. If alpha is positive, then getting more than fair share increases utility, which is plausible. But maybe people have a reference point and are sensitive to both gains and losses around it. But with two parameters, perhaps a better approach is to model the utility as a combination of the monetary value and the ratio of the offer to the fair share.\n\nWait, another approach: the ratio of the offer to the fair share. For example, (split_self / (3*token_self)). If this ratio is 1, it's fair. If it's less than 1, it's unfair. The utility could be beta * split_self + alpha * (split_self / (3*token_self)). But this might not capture nonlinear effects. Alternatively, using a logarithmic function of the ratio, which can model diminishing sensitivity. For example, U = beta * split_self + alpha * ln(split_self / (3*token_self)). But if split_self is less than 3*token_self, the ln would be negative, which could decrease utility. However, this introduces a non-linearity. But with two parameters, beta and alpha, this could work. However, the problem states that the model should not include parameters for general stochasticity, which is handled by a temperature parameter later. So the model needs to be deterministic in its utility calculation.\n\nAlternatively, using the difference between the offered percentage (splitperc_self) and the fair percentage (token_self / (token_self + token_opp) * 100). The fair percentage is (token_self / total_tokens) * 100. The difference between splitperc_self and this fair percentage could be a term. So U_accept = beta * split_self + alpha * (splitperc_self - fair_percent). Here, fair_percent is (token_self / (token_self + token_opp)) * 100. This is similar to Model 2, but Model 2 uses splitperc_self minus fair_percent. Wait, Model 2's specification is: splitperc_self - (token_self/(token_self + token_opp)) * 100. So that's exactly this term. So if we use that, but perhaps square it to model aversion to any deviation, but that would require a different parameterization. However, since Model 2 already exists, we need a different approach.\n\nAnother angle: the participant's contribution (token_self) and the opponent's contribution (token_opp) could influence the utility. For example, the utility could depend on the ratio of the split to the opponent's split. If the responder is getting a smaller share than the proposer, that might be unfair. But in the responder role, the split_self is what's offered to them, and split_opp is what the proposer is keeping. So perhaps the ratio split_self / split_opp. If that's less than 1, it's unfair. But this might not account for the contributions. Alternatively, comparing the split percentages. For example, if the responder's percentage is less than their contribution percentage, that's unfair.\n\nWait, the fair percentage is (token_self / (token_self + token_opp)) * 100. If splitperc_self is less than this, it's unfair. The difference could be a term. So U_accept = beta * split_self + alpha * (splitperc_self - fair_percent). But this is similar to Model 2, which uses splitperc_self - fair_percent. So perhaps to differentiate, we need a different functional form. Maybe instead of a linear term, a threshold or nonlinear function. For example, if the split is below the fair percentage, the term is negative, but if above, it's positive. But with two parameters, perhaps a linear term is the way to go.\n\nAlternatively, consider that the utility is influenced by both the absolute amount (split_self) and the relative fairness compared to the opponent's contribution. For example, the opponent's contribution is token_opp, so their fair share is 3*token_opp. The proposer's split_opp is the amount they keep. The responder might compare their own split_self to the proposer's split_opp. If the proposer is keeping more than their fair share, that could be a negative term. So U_accept = beta * split_self + alpha * (split_opp - 3*token_opp). But split_opp is the amount the proposer keeps, which should be 3*(token_self + token_opp) - split_self. Wait, combined_earning is 3*(token_self + token_opp), so split_opp = combined_earning - split_self. So split_opp = 3*(token_self + token_opp) - split_self. So the proposer's share is split_opp. The fair share for the proposer would be 3*token_opp. So the difference between split_opp and 3*token_opp is (split_opp - 3*token_opp) = [3*(token_self + token_opp) - split_self] - 3*token_opp = 3*token_self - split_self. So this term is the same as the one in Model 1. So if we model the utility as beta * split_self + alpha * (3*token_self - split_self), that's similar to Model 1 but without the max function. However, Model 1 uses max(3*token_self - split_self, 0), which only penalizes when the offer is less than the fair share. If we remove the max, then the term becomes (3*token_self - split_self), which is positive when the offer is less than fair, negative otherwise. So including this term would mean that when the offer is less than fair, the term is positive, multiplied by alpha (which could be negative, leading to a penalty). Wait, if alpha is a learnable parameter, say positive, then (3*token_self - split_self) would be positive when split_self < 3*token_self, leading to alpha * positive, which would add to the utility. But that's the opposite of what we want. So perhaps alpha should be negative. But if alpha is a parameter to be learned, its sign would be determined during fitting. Alternatively, the model could be U_accept = beta * split_self - alpha * (3*token_self - split_self). Then, if split_self < 3*token_self, the term (3*token_self - split_self) is positive, so the utility decreases by alpha * that amount. If split_self is more than fair, the term becomes negative, so subtracting a negative would add to utility. This way, alpha represents the sensitivity to deviations below the fair share. But this is similar to Model 1, which uses max(3*token_self - split_self, 0), so only penalizing below-fair offers. The difference here is that this model would also reward offers above fair, which might not be intended. But perhaps participants do derive positive utility from receiving more than their fair share. However, in Model 1, the penalty is only applied when the offer is below fair. So the current model would have a different dynamic.\n\nAlternatively, using a term that is the difference between split_self and the fair share: (split_self - 3*token_self). If this is positive, the offer is generous; if negative, it's unfair. Then, the utility could be beta * split_self + alpha * (split_self - 3*token_self). This combines the absolute monetary gain with the deviation from the fair share. If alpha is positive, then offers above fair share increase utility, below decrease. But this might conflate the two terms, as split_self is part of both. For example, beta * split_self + alpha * split_self - alpha * 3*token_self = (beta + alpha) * split_self - alpha * 3*token_self. This could lead to identifiability issues between beta and alpha. If beta and alpha are parameters, their sum would be the effective weight on split_self. This might make parameter recovery difficult because of high covariance.\n\nHmm. To avoid this, perhaps the model should separate the terms more distinctly. For example, one term is the absolute offer (split_self), and the other is the fair share (3*token_self). So U_accept = beta * split_self + alpha * 3*token_self. But this would model the utility as a combination of the offer and the fair share. If alpha is positive, then higher fair shares (i.e., higher contributions) increase the utility, which might not make sense. Alternatively, if the participant feels entitled to their fair share, then the utility could be the difference between the offer and the fair share: beta * (split_self - 3*token_self). But this would be a single parameter model. However, the task allows up to two parameters. So perhaps combining both the absolute offer and the difference from fair share.\n\nWait, another angle: the participant's utility could depend on both their own monetary gain and the ratio of their gain to their contribution. For example, U = beta * split_self + alpha * (split_self / (3*token_self)). Here, the second term represents how much they're getting per unit of their contribution. If their contribution is higher, they might expect a higher split_self. This ratio could capture that. If split_self is less than 3*token_self, the ratio is less than 1, which could decrease utility. But this introduces a non-linear term (ratio) which might interact with the absolute term. However, with two parameters, this could be manageable. The parameters beta and alpha would capture the weight on absolute vs. relative gains.\n\nAlternatively, using the ratio of the offer to the fair share: split_self / (3*token_self). If this is 1, it's fair. The utility could be beta * split_self + alpha * (split_self / (3*token_self)). This combines absolute and relative measures. But again, this might lead to parameter covariance, as both terms involve split_self. For example, if split_self is high, both terms increase. However, the parameters could adjust to balance this.\n\nBut perhaps a better approach is to model the utility as a function that combines the monetary gain and a fairness term that is the difference between the offer and the fair share, but with a nonlinear function. For example, using a hyperbolic tangent or sigmoid function to model diminishing sensitivity. However, introducing nonlinearities might complicate parameter recovery, but the problem allows it as long as there are no more than two parameters.\n\nWait, the problem states that the model should have no more than two learnable parameters and should avoid high covariance between them. So perhaps a linear combination of two terms that are not directly proportional. For example, one term is the split_self, and the other is the difference between split_self and the fair share. But as mentioned earlier, this could lead to covariance. Alternatively, using split_self and the fair share as separate terms. So U = beta * split_self + alpha * (3*token_self). This would model the utility as a function of both the offer and what the participant contributed. If alpha is positive, higher contributions increase the utility for the same offer, which might reflect a sense of entitlement. But this might not capture the comparison between offer and contribution. Alternatively, the model could be U = beta * split_self + alpha * (split_self - 3*token_self). This is similar to earlier thoughts. But again, this combines split_self in both terms.\n\nAlternatively, think about the ratio of the offer to the fair share. Let\u2019s define fair_self = 3 * token_self. Then, the ratio is split_self / fair_self. If this ratio is 1, it's fair. The utility could be beta * split_self + alpha * (split_self / fair_self - 1). This way, when the ratio is 1, the second term is zero. If the ratio is greater than 1, the term is positive; if less, negative. This captures proportional deviation from fairness. But this still has split_self in both terms. However, beta and alpha could adjust the weights. For example, beta captures the linear effect of money, and alpha captures the proportional fairness.\n\nBut perhaps a better way is to separate the terms. For example, the utility could be a weighted sum of the absolute offer and the proportional offer relative to the fair share. So U = beta * split_self + alpha * (split_self / fair_self). Here, fair_self is 3*token_self. This way, the first term is the absolute money, and the second term is how much they're getting relative to their fair share. If alpha is positive, getting more relative to fair increases utility. But this might not capture aversion to unfairness when the offer is below fair. Alternatively, if the model is U = beta * split_self + alpha * (split_self >= fair_self ? 1 : 0). But that's a step function, which might be too simplistic and not differentiable, making parameter estimation hard.\n\nAlternatively, using a piecewise linear function with one parameter. For example, U = beta * split_self + alpha * max(fair_self - split_self, 0). This is similar to Model 1, which uses max(3*token_self - split_self, 0). But Model 1 already exists. So to differentiate, perhaps use a different fairness component.\n\nWait, the previous models are:\n\nModel 1: U_accept = beta * split_self - alpha * max(3*token_self - split_self, 0)\n\nModel 2: U_accept = beta * split_self + alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nSo Model 1 uses a one-sided penalty (only when offer is below fair share), and Model 2 uses a proportional fairness term.\n\nTo propose a new model, perhaps consider the difference between the offer and the fair share, but squared, to capture aversion to any deviation from fairness. For example, U = beta * split_self - alpha * (split_self - fair_self)^2. Here, fair_self is 3*token_self. This would penalize both offers below and above the fair share, with the penalty increasing quadratically with the deviation. This assumes that participants prefer offers close to their fair share, whether lower or higher. However, in reality, people might not mind getting more than their fair share. But this model would capture a preference for equity, where any deviation is seen as negative. However, this might not be the case, but it's a different mechanism from the previous models.\n\nThe quadratic term would create a parabolic utility function with a peak at the fair share. So the utility increases with split_self up to the fair share, then decreases beyond that. But this might not align with typical findings where people prefer more money, but the model could still be considered as a possible explanation. However, with two parameters (beta and alpha), this could be feasible. The beta term would capture the linear monetary gain, and alpha the sensitivity to deviations from fairness. If alpha is positive, then the quadratic term reduces utility as the offer deviates from fair_self. If the offer is higher than fair_self, the squared term is positive, so -alpha*(...)^2 would decrease utility. But if participants actually prefer higher offers, this might not fit. Alternatively, if the model is U = beta * split_self + alpha * (split_self - fair_self)^2, then a positive alpha would mean that offers above fair_self increase utility quadratically, which might not be realistic. So the sign of alpha would determine the direction. But in this case, to penalize deviations, alpha should be negative. However, since alpha is a learnable parameter, the model can learn the appropriate sign.\n\nBut the problem states that the model should not include parameters for general stochasticity, which is handled later. So the model must be deterministic. The quadratic model could be a way to capture aversion to any deviation from the fair share. This would be a different psychological mechanism than the previous models, which only consider one-sided penalties or proportional differences.\n\nAnother consideration is the parameter recovery. If the model has parameters that are not orthogonal, like beta and alpha in the quadratic model, they might be harder to recover. But the problem allows two parameters as long as they are not highly covariant. The quadratic model's parameters might be recoverable if the data contains sufficient variation in split_self and fair_self.\n\nSo, putting this together, the model would be:\n\nU_accept = beta * split_self - alpha * (split_self - 3*token_self)^2\n\nHere, beta represents the sensitivity to the absolute monetary gain, and alpha represents the sensitivity to deviations from the fair share (3*token_self). A positive alpha would mean that deviations (both above and below) reduce utility, implying a preference for exact fairness. A negative alpha would imply that deviations increase utility, which is unlikely, but the parameter can be bounded to be positive.\n\nThe variables involved are split_self (from the data) and token_self (from the data). The parameters are beta and alpha, both learnable with appropriate bounds. Beta can be any real number, but likely positive since higher offers should increase utility. Alpha should be non-negative to ensure that deviations are penalized.\n\nFor the variable descriptions:\n\n- split_self: data variable, the proposed share for the participant.\n- token_self: data variable, the participant's contribution.\n- beta: learnable parameter, sensitivity to monetary gain. Range could be from 0 to a large positive number, but to allow for negative values (though unlikely), maybe set min to -10 and max to 10, but with inclusive bounds.\n- alpha: learnable parameter, sensitivity to fairness deviations. Should be non-negative. Range from 0 to a large number, say 10.\n\nBut the problem states that bounds should be generous. So for beta, perhaps min -10 to max 10, inclusive. For alpha, min 0 to max 10, inclusive.\n\nThis model introduces a new mechanism where participants are averse to any deviation from their fair share, whether advantageous or disadvantageous. This is different from Models 1 and 2, which focus on one-sided penalties or proportional fairness. The quadratic term captures a nonlinear aversion to inequity, which could be a testable hypothesis.\n\nAnother check: does this model use variables available in the dataset? Yes, split_self and token_self are present. The combined_earning is 3*(token_self + token_opp), but the model uses 3*token_self, which is derived from token_self. Since token_self is available, this is acceptable.\n\nThe utility function is U_accept = beta * split_self - alpha * (split_self - 3*token_self)^2. This meets the requirements: only responder behavior (since trial_role is 1), uses relevant variables, two learnable parameters, no stochasticity parameters, and maps to observable behavior (acceptance based on monetary gain and fairness).\n\nNow, considering parameter recovery, if the data has cases where offers are both above and below the fair share, the parameters beta and alpha can be identified. For example, when offers are above the fair share, the quadratic term will reduce utility if alpha is positive, countering the monetary gain. The balance between beta and alpha determines whether higher offers are still accepted despite being above fair.\n\nIn summary, this model explores a different psychological mechanism\u2014nonlinear aversion to deviations from a fair share\u2014using two learnable parameters. It's distinct from previous models and uses available variables appropriately.\n</think>\n\n<MODEL>\nU_accept = beta * split_self - alpha * (split_self - 3 * token_self)^2\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute monetary gain\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviations from contribution-based fairness\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's contribution in tokens (each worth \u00a33)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA quadratic utility model combining absolute monetary value with aversion to deviations from contribution-based fairness. The utility function penalizes both advantageous and disadvantageous inequity relative to the participant's fair share (3 * token_self). Two parameters control sensitivity to money (beta) and fairness deviations (alpha), capturing tension between self-interest and nonlinear equity preferences. The quadratic term reflects stronger aversion to large inequities than linear models.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return the utility values for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['beta', 'alpha']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of utility values for each trial\n\n                Your code will be implemented within the following code block:\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    utility_values = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        utility_values.append(beta + (epsilon * E) + (eta * N))\n    return utility_values\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE UTLITY VALUES.\n                Model Specification:\n                U_accept = beta * split_self - alpha * (split_self - 3 * token_self)^2\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute monetary gain\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviations from contribution-based fairness\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's contribution in tokens (each worth \\u00a33)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "```python\nimport json\nimport math\n\ndef simulate_model(trial_data, beta=1, alpha=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        utility = beta * split_self - alpha * math.pow(split_self - 3 * token_self, 2)\n        utility_values.append(utility)\n    return utility_values\n\n# Data as list of dictionaries\ndata_json = \"\"\"\n[\n  {\"split_self\": 10, \"token_self\": 2},\n  {\"split_self\": 5, \"token_self\": 1},\n  {\"split_self\": 15, \"token_self\": 3},\n  {\"split_self\": 2, \"token_self\": 2}\n\n]\n\"\"\"\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data, beta=0.5, alpha=0.2)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n\n```"
    }
  ],
  "learnable_parameters": {
    "beta": {
      "description": "Sensitivity to absolute monetary gain",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to deviations from contribution-based fairness",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U_accept = beta * split_self - alpha * (split_self - 3 * token_self)^2",
  "variable_descriptions": {
    "U_accept": {
      "description": "Utility of accepting the offer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "beta": {
      "description": "Sensitivity to absolute monetary gain",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "alpha": {
      "description": "Sensitivity to deviations from contribution-based fairness",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed monetary amount for the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Participant's contribution in tokens (each worth \u00a33)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A quadratic utility model combining absolute monetary value with aversion to deviations from contribution-based fairness. The utility function penalizes both advantageous and disadvantageous inequity relative to the participant's fair share (3 * token_self). Two parameters control sensitivity to money (beta) and fairness deviations (alpha), capturing tension between self-interest and nonlinear equity preferences. The quadratic term reflects stronger aversion to large inequities than linear models.",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer for the responder in the given experiment. The model needs to be based on psychological principles, have no more than two learnable parameters, and use variables from the dataset provided.\n\nFirst, looking at the previous models, Model 1 focuses on absolute monetary gain and aversion to receiving less than one's contribution-based fair share. Model 2 combines absolute monetary value with proportional fairness relative to contributions. Both models use two parameters (alpha and beta) to capture different aspects of fairness and self-interest.\n\nTo build a new model that explores a different mechanism, I need to consider other psychological principles. One possibility is the concept of reciprocity or intention-based fairness. However, the dataset might not have variables that directly capture the opponent's intentions. Another angle is to consider the relative difference between the proposed split and the fair split based on contributions, perhaps using a nonlinear function to model aversion to unfairness.\n\nWait, the dataset includes variables like token_self and token_opp, which represent the contributions. The combined_earning is 3*(token_self + token_opp). The split_self is the amount offered to the participant. A fair split based on contributions would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the actual offer (split_self) and this fair amount could be a key factor. If the offer is less than the fair amount, the responder might feel treated unfairly, leading to a negative utility. Conversely, if it's more, they might feel positively, but perhaps with diminishing returns.\n\nAlternatively, using the percentage split (splitperc_self) compared to the fair percentage (token_self / total tokens * 100). The difference here could be squared to model aversion to any deviation from fairness, whether advantageous or disadvantageous. But the problem states that the model should not include parameters for general stochasticity, so focusing on deterministic components.\n\nAnother thought: the utility could be a combination of the monetary gain (split_self) and a term that penalizes deviations from the fair split. For example, the fair split is 3*token_self (since each token is \u00a33), but the combined_earning is 3*(token_self + token_opp). Wait, the fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. But combined_earning is 3*(token_self + token_opp), so the fair share is 3*token_self. Wait, that's interesting. Because combined_earning is 3*(token_self + token_opp), so if each person's contribution is their tokens, then the fair split for self would be 3*token_self. So split_self - 3*token_self would be the difference between the offer and the fair amount. If the offer is less than fair, that's a negative difference, which could be a penalty. If it's more, that's a positive difference. But maybe people are more sensitive to being undercut than over-rewarded. So perhaps a linear term for the monetary gain (split_self) and a separate term for the difference from the fair amount, with a parameter that scales the importance of fairness.\n\nWait, in Model 1, they used max(3*token_self - split_self, 0), which is the deficit relative to the fair share. So that's a one-sided penalty. But maybe a two-sided penalty, where both under and over offers affect utility, but asymmetrically. However, with two parameters, perhaps we can model the monetary term and the fairness term. Let's think: U_accept = beta * split_self + alpha * (split_self - 3*token_self). Here, beta is the weight on the monetary gain, and alpha is the weight on the difference from the fair share. But this might lead to covariance between beta and alpha. Alternatively, combining them into a single term. Wait, but if alpha is positive, then getting more than fair share increases utility, which might not align with some theories. Alternatively, if the fair share is considered a reference point, then deviations from it could be modeled with a loss aversion parameter. For example, using a piecewise function where negative deviations (split_self < 3*token_self) are penalized more than positive deviations are rewarded. But that would require more parameters. However, the problem allows two parameters. Let's see.\n\nAlternatively, using a quadratic term for the difference from the fair share. U_accept = beta * split_self - alpha * (split_self - 3*token_self)^2. This would penalize both under and over offers relative to the fair share, with the idea that people prefer offers close to their fair share. But this might not capture the directionality\u2014people might be okay with over-offers but not under. However, the quadratic term would treat both as negative if alpha is positive. Wait, if alpha is positive, then the term -alpha*(difference)^2 would always reduce utility, which might not make sense. Alternatively, if the model is U_accept = beta * split_self + alpha * (split_self - 3*token_self). But this would be a linear term. If alpha is positive, then getting more than fair share increases utility, which is plausible. But maybe people have a reference point and are sensitive to both gains and losses around it. But with two parameters, perhaps a better approach is to model the utility as a combination of the monetary value and the ratio of the offer to the fair share.\n\nWait, another approach: the ratio of the offer to the fair share. For example, (split_self / (3*token_self)). If this ratio is 1, it's fair. If it's less than 1, it's unfair. The utility could be beta * split_self + alpha * (split_self / (3*token_self)). But this might not capture nonlinear effects. Alternatively, using a logarithmic function of the ratio, which can model diminishing sensitivity. For example, U = beta * split_self + alpha * ln(split_self / (3*token_self)). But if split_self is less than 3*token_self, the ln would be negative, which could decrease utility. However, this introduces a non-linearity. But with two parameters, beta and alpha, this could work. However, the problem states that the model should not include parameters for general stochasticity, which is handled by a temperature parameter later. So the model needs to be deterministic in its utility calculation.\n\nAlternatively, using the difference between the offered percentage (splitperc_self) and the fair percentage (token_self / (token_self + token_opp) * 100). The fair percentage is (token_self / total_tokens) * 100. The difference between splitperc_self and this fair percentage could be a term. So U_accept = beta * split_self + alpha * (splitperc_self - fair_percent). Here, fair_percent is (token_self / (token_self + token_opp)) * 100. This is similar to Model 2, but Model 2 uses splitperc_self minus fair_percent. Wait, Model 2's specification is: splitperc_self - (token_self/(token_self + token_opp)) * 100. So that's exactly this term. So if we use that, but perhaps square it to model aversion to any deviation, but that would require a different parameterization. However, since Model 2 already exists, we need a different approach.\n\nAnother angle: the participant's contribution (token_self) and the opponent's contribution (token_opp) could influence the utility. For example, the utility could depend on the ratio of the split to the opponent's split. If the responder is getting a smaller share than the proposer, that might be unfair. But in the responder role, the split_self is what's offered to them, and split_opp is what the proposer is keeping. So perhaps the ratio split_self / split_opp. If that's less than 1, it's unfair. But this might not account for the contributions. Alternatively, comparing the split percentages. For example, if the responder's percentage is less than their contribution percentage, that's unfair.\n\nWait, the fair percentage is (token_self / (token_self + token_opp)) * 100. If splitperc_self is less than this, it's unfair. The difference could be a term. So U_accept = beta * split_self + alpha * (splitperc_self - fair_percent). But this is similar to Model 2, which uses splitperc_self - fair_percent. So perhaps to differentiate, we need a different functional form. Maybe instead of a linear term, a threshold or nonlinear function. For example, if the split is below the fair percentage, the term is negative, but if above, it's positive. But with two parameters, perhaps a linear term is the way to go.\n\nAlternatively, consider that the utility is influenced by both the absolute amount (split_self) and the relative fairness compared to the opponent's contribution. For example, the opponent's contribution is token_opp, so their fair share is 3*token_opp. The proposer's split_opp is the amount they keep. The responder might compare their own split_self to the proposer's split_opp. If the proposer is keeping more than their fair share, that could be a negative term. So U_accept = beta * split_self + alpha * (split_opp - 3*token_opp). But split_opp is the amount the proposer keeps, which should be 3*(token_self + token_opp) - split_self. Wait, combined_earning is 3*(token_self + token_opp), so split_opp = combined_earning - split_self. So split_opp = 3*(token_self + token_opp) - split_self. So the proposer's share is split_opp. The fair share for the proposer would be 3*token_opp. So the difference between split_opp and 3*token_opp is (split_opp - 3*token_opp) = [3*(token_self + token_opp) - split_self] - 3*token_opp = 3*token_self - split_self. So this term is the same as the one in Model 1. So if we model the utility as beta * split_self + alpha * (3*token_self - split_self), that's similar to Model 1 but without the max function. However, Model 1 uses max(3*token_self - split_self, 0), which only penalizes when the offer is less than the fair share. If we remove the max, then the term becomes (3*token_self - split_self), which is positive when the offer is less than fair, negative otherwise. So including this term would mean that when the offer is less than fair, the term is positive, multiplied by alpha (which could be negative, leading to a penalty). Wait, if alpha is a learnable parameter, say positive, then (3*token_self - split_self) would be positive when split_self < 3*token_self, leading to alpha * positive, which would add to the utility. But that's the opposite of what we want. So perhaps alpha should be negative. But if alpha is a parameter to be learned, its sign would be determined during fitting. Alternatively, the model could be U_accept = beta * split_self - alpha * (3*token_self - split_self). Then, if split_self < 3*token_self, the term (3*token_self - split_self) is positive, so the utility decreases by alpha * that amount. If split_self is more than fair, the term becomes negative, so subtracting a negative would add to utility. This way, alpha represents the sensitivity to deviations below the fair share. But this is similar to Model 1, which uses max(3*token_self - split_self, 0), so only penalizing below-fair offers. The difference here is that this model would also reward offers above fair, which might not be intended. But perhaps participants do derive positive utility from receiving more than their fair share. However, in Model 1, the penalty is only applied when the offer is below fair. So the current model would have a different dynamic.\n\nAlternatively, using a term that is the difference between split_self and the fair share: (split_self - 3*token_self). If this is positive, the offer is generous; if negative, it's unfair. Then, the utility could be beta * split_self + alpha * (split_self - 3*token_self). This combines the absolute monetary gain with the deviation from the fair share. If alpha is positive, then offers above fair share increase utility, below decrease. But this might conflate the two terms, as split_self is part of both. For example, beta * split_self + alpha * split_self - alpha * 3*token_self = (beta + alpha) * split_self - alpha * 3*token_self. This could lead to identifiability issues between beta and alpha. If beta and alpha are parameters, their sum would be the effective weight on split_self. This might make parameter recovery difficult because of high covariance.\n\nHmm. To avoid this, perhaps the model should separate the terms more distinctly. For example, one term is the absolute offer (split_self), and the other is the fair share (3*token_self). So U_accept = beta * split_self + alpha * 3*token_self. But this would model the utility as a combination of the offer and the fair share. If alpha is positive, then higher fair shares (i.e., higher contributions) increase the utility, which might not make sense. Alternatively, if the participant feels entitled to their fair share, then the utility could be the difference between the offer and the fair share: beta * (split_self - 3*token_self). But this would be a single parameter model. However, the task allows up to two parameters. So perhaps combining both the absolute offer and the difference from fair share.\n\nWait, another angle: the participant's utility could depend on both their own monetary gain and the ratio of their gain to their contribution. For example, U = beta * split_self + alpha * (split_self / (3*token_self)). Here, the second term represents how much they're getting per unit of their contribution. If their contribution is higher, they might expect a higher split_self. This ratio could capture that. If split_self is less than 3*token_self, the ratio is less than 1, which could decrease utility. But this introduces a non-linear term (ratio) which might interact with the absolute term. However, with two parameters, this could be manageable. The parameters beta and alpha would capture the weight on absolute vs. relative gains.\n\nAlternatively, using the ratio of the offer to the fair share: split_self / (3*token_self). If this is 1, it's fair. The utility could be beta * split_self + alpha * (split_self / (3*token_self)). This combines absolute and relative measures. But again, this might lead to parameter covariance, as both terms involve split_self. For example, if split_self is high, both terms increase. However, the parameters could adjust to balance this.\n\nBut perhaps a better approach is to model the utility as a function that combines the monetary gain and a fairness term that is the difference between the offer and the fair share, but with a nonlinear function. For example, using a hyperbolic tangent or sigmoid function to model diminishing sensitivity. However, introducing nonlinearities might complicate parameter recovery, but the problem allows it as long as there are no more than two parameters.\n\nWait, the problem states that the model should have no more than two learnable parameters and should avoid high covariance between them. So perhaps a linear combination of two terms that are not directly proportional. For example, one term is the split_self, and the other is the difference between split_self and the fair share. But as mentioned earlier, this could lead to covariance. Alternatively, using split_self and the fair share as separate terms. So U = beta * split_self + alpha * (3*token_self). This would model the utility as a function of both the offer and what the participant contributed. If alpha is positive, higher contributions increase the utility for the same offer, which might reflect a sense of entitlement. But this might not capture the comparison between offer and contribution. Alternatively, the model could be U = beta * split_self + alpha * (split_self - 3*token_self). This is similar to earlier thoughts. But again, this combines split_self in both terms.\n\nAlternatively, think about the ratio of the offer to the fair share. Let\u2019s define fair_self = 3 * token_self. Then, the ratio is split_self / fair_self. If this ratio is 1, it's fair. The utility could be beta * split_self + alpha * (split_self / fair_self - 1). This way, when the ratio is 1, the second term is zero. If the ratio is greater than 1, the term is positive; if less, negative. This captures proportional deviation from fairness. But this still has split_self in both terms. However, beta and alpha could adjust the weights. For example, beta captures the linear effect of money, and alpha captures the proportional fairness.\n\nBut perhaps a better way is to separate the terms. For example, the utility could be a weighted sum of the absolute offer and the proportional offer relative to the fair share. So U = beta * split_self + alpha * (split_self / fair_self). Here, fair_self is 3*token_self. This way, the first term is the absolute money, and the second term is how much they're getting relative to their fair share. If alpha is positive, getting more relative to fair increases utility. But this might not capture aversion to unfairness when the offer is below fair. Alternatively, if the model is U = beta * split_self + alpha * (split_self >= fair_self ? 1 : 0). But that's a step function, which might be too simplistic and not differentiable, making parameter estimation hard.\n\nAlternatively, using a piecewise linear function with one parameter. For example, U = beta * split_self + alpha * max(fair_self - split_self, 0). This is similar to Model 1, which uses max(3*token_self - split_self, 0). But Model 1 already exists. So to differentiate, perhaps use a different fairness component.\n\nWait, the previous models are:\n\nModel 1: U_accept = beta * split_self - alpha * max(3*token_self - split_self, 0)\n\nModel 2: U_accept = beta * split_self + alpha * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nSo Model 1 uses a one-sided penalty (only when offer is below fair share), and Model 2 uses a proportional fairness term.\n\nTo propose a new model, perhaps consider the difference between the offer and the fair share, but squared, to capture aversion to any deviation from fairness. For example, U = beta * split_self - alpha * (split_self - fair_self)^2. Here, fair_self is 3*token_self. This would penalize both offers below and above the fair share, with the penalty increasing quadratically with the deviation. This assumes that participants prefer offers close to their fair share, whether lower or higher. However, in reality, people might not mind getting more than their fair share. But this model would capture a preference for equity, where any deviation is seen as negative. However, this might not be the case, but it's a different mechanism from the previous models.\n\nThe quadratic term would create a parabolic utility function with a peak at the fair share. So the utility increases with split_self up to the fair share, then decreases beyond that. But this might not align with typical findings where people prefer more money, but the model could still be considered as a possible explanation. However, with two parameters (beta and alpha), this could be feasible. The beta term would capture the linear monetary gain, and alpha the sensitivity to deviations from fairness. If alpha is positive, then the quadratic term reduces utility as the offer deviates from fair_self. If the offer is higher than fair_self, the squared term is positive, so -alpha*(...)^2 would decrease utility. But if participants actually prefer higher offers, this might not fit. Alternatively, if the model is U = beta * split_self + alpha * (split_self - fair_self)^2, then a positive alpha would mean that offers above fair_self increase utility quadratically, which might not be realistic. So the sign of alpha would determine the direction. But in this case, to penalize deviations, alpha should be negative. However, since alpha is a learnable parameter, the model can learn the appropriate sign.\n\nBut the problem states that the model should not include parameters for general stochasticity, which is handled later. So the model must be deterministic. The quadratic model could be a way to capture aversion to any deviation from the fair share. This would be a different psychological mechanism than the previous models, which only consider one-sided penalties or proportional differences.\n\nAnother consideration is the parameter recovery. If the model has parameters that are not orthogonal, like beta and alpha in the quadratic model, they might be harder to recover. But the problem allows two parameters as long as they are not highly covariant. The quadratic model's parameters might be recoverable if the data contains sufficient variation in split_self and fair_self.\n\nSo, putting this together, the model would be:\n\nU_accept = beta * split_self - alpha * (split_self - 3*token_self)^2\n\nHere, beta represents the sensitivity to the absolute monetary gain, and alpha represents the sensitivity to deviations from the fair share (3*token_self). A positive alpha would mean that deviations (both above and below) reduce utility, implying a preference for exact fairness. A negative alpha would imply that deviations increase utility, which is unlikely, but the parameter can be bounded to be positive.\n\nThe variables involved are split_self (from the data) and token_self (from the data). The parameters are beta and alpha, both learnable with appropriate bounds. Beta can be any real number, but likely positive since higher offers should increase utility. Alpha should be non-negative to ensure that deviations are penalized.\n\nFor the variable descriptions:\n\n- split_self: data variable, the proposed share for the participant.\n- token_self: data variable, the participant's contribution.\n- beta: learnable parameter, sensitivity to monetary gain. Range could be from 0 to a large positive number, but to allow for negative values (though unlikely), maybe set min to -10 and max to 10, but with inclusive bounds.\n- alpha: learnable parameter, sensitivity to fairness deviations. Should be non-negative. Range from 0 to a large number, say 10.\n\nBut the problem states that bounds should be generous. So for beta, perhaps min -10 to max 10, inclusive. For alpha, min 0 to max 10, inclusive.\n\nThis model introduces a new mechanism where participants are averse to any deviation from their fair share, whether advantageous or disadvantageous. This is different from Models 1 and 2, which focus on one-sided penalties or proportional fairness. The quadratic term captures a nonlinear aversion to inequity, which could be a testable hypothesis.\n\nAnother check: does this model use variables available in the dataset? Yes, split_self and token_self are present. The combined_earning is 3*(token_self + token_opp), but the model uses 3*token_self, which is derived from token_self. Since token_self is available, this is acceptable.\n\nThe utility function is U_accept = beta * split_self - alpha * (split_self - 3*token_self)^2. This meets the requirements: only responder behavior (since trial_role is 1), uses relevant variables, two learnable parameters, no stochasticity parameters, and maps to observable behavior (acceptance based on monetary gain and fairness).\n\nNow, considering parameter recovery, if the data has cases where offers are both above and below the fair share, the parameters beta and alpha can be identified. For example, when offers are above the fair share, the quadratic term will reduce utility if alpha is positive, countering the monetary gain. The balance between beta and alpha determines whether higher offers are still accepted despite being above fair.\n\nIn summary, this model explores a different psychological mechanism\u2014nonlinear aversion to deviations from a fair share\u2014using two learnable parameters. It's distinct from previous models and uses available variables appropriately.\n</think>\n\n<MODEL>\nU_accept = beta * split_self - alpha * (split_self - 3 * token_self)^2\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to absolute monetary gain\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviations from contribution-based fairness\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's contribution in tokens (each worth \u00a33)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA quadratic utility model combining absolute monetary value with aversion to deviations from contribution-based fairness. The utility function penalizes both advantageous and disadvantageous inequity relative to the participant's fair share (3 * token_self). Two parameters control sensitivity to money (beta) and fairness deviations (alpha), capturing tension between self-interest and nonlinear equity preferences. The quadratic term reflects stronger aversion to large inequities than linear models.\n</SUMMARY>",
  "previous_models": [
    "Specification: U_accept = beta * split_self - alpha * (split_self - 3 * token_self)^2\nSummary: A quadratic utility model combining absolute monetary value with aversion to deviations from contribution-based fairness. The utility function penalizes both advantageous and disadvantageous inequity relative to the participant's fair share (3 * token_self). Two parameters control sensitivity to money (beta) and fairness deviations (alpha), capturing tension between self-interest and nonlinear equity preferences. The quadratic term reflects stronger aversion to large inequities than linear models."
  ],
  "simulation_code": "def simulate_model(trial_data, beta=1, alpha=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        utility = beta * split_self - alpha * math.pow(split_self - 3 * token_self, 2)\n        utility_values.append(utility)\n    return utility_values",
  "fitting_results": [
    {
      "beta": -0.20946489343386054,
      "alpha": 1.057012168463844,
      "success": true,
      "log_likelihood": -9.944811003424789,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.7044905533705937,
      "alpha": 0.2947039573342488,
      "success": true,
      "log_likelihood": -18.468728992332725,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.6904898941071234,
      "alpha": 0.7500146900761088,
      "success": true,
      "log_likelihood": -10.864528077198997,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9176475630060593,
      "alpha": 0.5193625183767383,
      "success": true,
      "log_likelihood": -14.907536053734344,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.4843264314935143,
      "alpha": 1.0778601787007354,
      "success": true,
      "log_likelihood": -9.346025228994849,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.862860469797295,
      "alpha": 0.5410473647898922,
      "success": true,
      "log_likelihood": -11.067117823431111,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228865,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.223470074639882,
      "alpha": 0.6446708834570203,
      "success": true,
      "log_likelihood": -9.491196761292935,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -0.0064721874752288674,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.832167406734929,
      "alpha": 0.6548107634551202,
      "success": true,
      "log_likelihood": -13.626258776047544,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.851269392029741,
      "alpha": 1.8513986358936438,
      "success": true,
      "log_likelihood": -5.179077955952604,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.02631345005785793,
      "alpha": 1.1763324764085437,
      "success": true,
      "log_likelihood": -10.56306258991326,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.3613100930512116,
      "alpha": 1.0427886581640537,
      "success": true,
      "log_likelihood": -9.791479814262312,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228864,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228864,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228867,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228865,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.811386769088886,
      "alpha": 0.32952296541746773,
      "success": true,
      "log_likelihood": -4.906984943340491,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 3.1587831726569644,
      "alpha": 0.2792003511686503,
      "success": true,
      "log_likelihood": -6.105162451518307,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228866,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.8756482499005942,
      "alpha": 0.3345966390458077,
      "success": true,
      "log_likelihood": -17.176262764638302,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.602863242594436,
      "alpha": 0.257215356567957,
      "success": true,
      "log_likelihood": -7.7297881657549485,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.42931831768129,
      "alpha": 0.29024790532148814,
      "success": true,
      "log_likelihood": -19.31774749163873,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.7441694857309574,
      "alpha": 0.23764227282743508,
      "success": true,
      "log_likelihood": -18.973224931223168,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.097241703528579,
      "alpha": 0.6450972101653613,
      "success": true,
      "log_likelihood": -13.141570428434864,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9230736553458044,
      "alpha": 0.2493593566803773,
      "success": true,
      "log_likelihood": -17.681001418347577,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.475189099117529,
      "alpha": 0.5266076579143798,
      "success": true,
      "log_likelihood": -12.861697904218175,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.7082044326258636,
      "alpha": 3.7282969259672925,
      "success": true,
      "log_likelihood": -5.952249970812596,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.1999255630752637,
      "alpha": 0.16576422061824714,
      "success": true,
      "log_likelihood": -8.757564666167944,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.115266997166682,
      "alpha": 0.12105011414000978,
      "success": true,
      "log_likelihood": -16.20370576444084,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228866,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.20946587489459095,
      "alpha": 1.0570152453517565,
      "success": true,
      "log_likelihood": -9.944811003425185,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.332437287801889,
      "alpha": 0.16531951803965267,
      "success": true,
      "log_likelihood": -21.931822630108954,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.5625454725433781,
      "alpha": 0.3871113323592331,
      "success": true,
      "log_likelihood": -17.470968022896194,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228865,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.2641693935694194,
      "alpha": 0.22431139697782432,
      "success": true,
      "log_likelihood": -15.180631635787433,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.7448179972752325,
      "alpha": 0.6400107074911378,
      "success": true,
      "log_likelihood": -7.778921031431264,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.0643295241569155,
      "alpha": 0.5481591348440213,
      "success": true,
      "log_likelihood": -14.191304852489361,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9182485821349164,
      "alpha": 0.19988051054623224,
      "success": true,
      "log_likelihood": -18.029167183073593,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.7047171375036827,
      "alpha": 0.032757059682330746,
      "success": true,
      "log_likelihood": -9.675305417499375,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9083894023343163,
      "alpha": 0.2868025854585108,
      "success": true,
      "log_likelihood": -17.46873223288146,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.11453362668314142,
      "alpha": 0.4870402860739066,
      "success": true,
      "log_likelihood": -15.450711929503191,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 10.0,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -23.013701505177707,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9521830805777535,
      "alpha": 0.29265915559281425,
      "success": true,
      "log_likelihood": -17.145332396670348,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.3883018874197763,
      "alpha": 0.39595035409417983,
      "success": true,
      "log_likelihood": -13.86952857260221,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.1126130772307816,
      "alpha": 0.11839986294208452,
      "success": true,
      "log_likelihood": -16.21040082543137,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9987083471541348,
      "alpha": 0.40056943076207197,
      "success": true,
      "log_likelihood": -15.916165074515286,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.30315008044929326,
      "alpha": 0.18377688812167461,
      "success": true,
      "log_likelihood": -21.591546132381524,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.2071166373765179,
      "alpha": 1.1384614924627436,
      "success": true,
      "log_likelihood": -9.802834737337674,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.27142892720097617,
      "alpha": 0.03684138051715916,
      "success": true,
      "log_likelihood": -24.020241379041487,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.026502370085141,
      "alpha": 0.16146028110145874,
      "success": true,
      "log_likelihood": -17.18631400923891,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.23362493194711717,
      "alpha": 0.3603704525726834,
      "success": true,
      "log_likelihood": -14.855570939345135,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6454090239915484,
      "alpha": 0.1720228825821557,
      "success": true,
      "log_likelihood": -20.324869801826157,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.523684733187457,
      "alpha": 0.06697225316094406,
      "success": true,
      "log_likelihood": -21.966265105840712,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.24599859168526267,
      "alpha": 0.24935685381806832,
      "success": true,
      "log_likelihood": -20.118008347181668,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.183228135689769,
      "alpha": 3.10944015110558,
      "success": true,
      "log_likelihood": -5.6406831569357125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6466276724118788,
      "alpha": 1.1055170256564189,
      "success": true,
      "log_likelihood": -11.276425381032274,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.1376586241655358,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -13.680488206347551,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.08185972760379467,
      "alpha": 0.9735864123102628,
      "success": true,
      "log_likelihood": -11.019626833422283,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.1344630063866656,
      "alpha": 0.03226386023053994,
      "success": true,
      "log_likelihood": -14.668006671162846,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.6863889700698951,
      "alpha": 0.15157954253296183,
      "success": true,
      "log_likelihood": -20.19736354683148,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.8672052041156375,
      "alpha": 0.43127503891133845,
      "success": true,
      "log_likelihood": -16.095276654388424,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.20946326907528348,
      "alpha": 1.0570184927971225,
      "success": true,
      "log_likelihood": -9.944811003389015,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.458782397662915,
      "alpha": 0.19775068423588996,
      "success": true,
      "log_likelihood": -20.966033764137848,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.31581065521543966,
      "alpha": 0.7244860264315542,
      "success": true,
      "log_likelihood": -13.536748139965486,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.936458478772391,
      "alpha": 0.27202411685689937,
      "success": true,
      "log_likelihood": -17.41469061617481,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.9964188594917736,
      "alpha": 0.06742729907877833,
      "success": true,
      "log_likelihood": -8.570610137190906,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.6251850339772385,
      "alpha": 0.11451417963606568,
      "success": true,
      "log_likelihood": -11.72540153312432,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.8870568365143472,
      "alpha": 0.20536792885972177,
      "success": true,
      "log_likelihood": -18.24723908913805,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.35604594460131,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -11.54012448936212,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.1413573656530515,
      "alpha": 2.340443113592195,
      "success": true,
      "log_likelihood": -8.557267171998136,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.10608061009886267,
      "alpha": 0.5734874570237166,
      "success": true,
      "log_likelihood": -13.289931542145528,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9676410556532726,
      "alpha": 0.36253531241020226,
      "success": true,
      "log_likelihood": -16.430930535356428,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.105347597456867,
      "alpha": 0.05650011002247968,
      "success": true,
      "log_likelihood": -15.516798128962543,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.262429379231353,
      "alpha": 0.07142409782649047,
      "success": true,
      "log_likelihood": -23.80536194878912,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.7618512254395473,
      "alpha": 0.4423972637429277,
      "success": true,
      "log_likelihood": -11.691713705114509,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.9845771119826274,
      "alpha": 0.028568809864782663,
      "success": true,
      "log_likelihood": -7.809018491812515,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.5679735833520472,
      "alpha": 0.9257263025780664,
      "success": true,
      "log_likelihood": -7.868125025013484,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.39589071112612606,
      "alpha": 0.009872946678673444,
      "success": true,
      "log_likelihood": -22.743757631337097,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.20945680559183857,
      "alpha": 1.0570315660918306,
      "success": true,
      "log_likelihood": -9.944811003681078,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.36374047983381214,
      "alpha": 0.10639812896636328,
      "success": true,
      "log_likelihood": -22.8611969134098,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.3338931339091373,
      "alpha": 0.0,
      "success": true,
      "log_likelihood": -11.741860654220122,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.006148539858641663,
      "alpha": 1.7663813579932943,
      "success": true,
      "log_likelihood": -9.19952963017371,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 1.5056374916462862,
      "alpha": 0.5615689719195841,
      "success": true,
      "log_likelihood": -12.55671300815782,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.25996717849492496,
      "alpha": 0.014203184810298345,
      "success": true,
      "log_likelihood": -24.01402605752589,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": -0.08015040784253182,
      "alpha": 1.1461323230008331,
      "success": true,
      "log_likelihood": -10.401662407382858,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9491300947996955,
      "alpha": 0.16834120196428773,
      "success": true,
      "log_likelihood": -17.87490334674929,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.1120762712222363,
      "alpha": 0.5616517918617685,
      "success": true,
      "log_likelihood": -14.56844174096131,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 2.8901273819613382,
      "alpha": 0.1140799636164867,
      "success": true,
      "log_likelihood": -5.461023704014408,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.9492748589555069,
      "alpha": 0.322478425488781,
      "success": true,
      "log_likelihood": -16.90339736640358,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "beta": 0.5945491011879395,
      "alpha": 0.042303501080689365,
      "success": true,
      "log_likelihood": -21.0732597705891,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "beta": {
        "r": 0.774832508397215,
        "p": 3.1236052818648035e-21
      },
      "alpha": {
        "r": 0.6519976718469853,
        "p": 2.019620384436651e-13
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 32.75155320518664,
  "bic_Control": 27.73685699824581,
  "bic_Cocaine": 37.24388522390446,
  "bic_error": "'ModelOutput' object has no attribute 'metrics'"
}