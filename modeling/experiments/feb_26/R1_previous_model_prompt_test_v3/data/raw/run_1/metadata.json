{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nPrevious Models:\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer for the responder in the given experiment. The model needs to be based on psychological principles, use the dataset variables appropriately, and have no more than two learnable parameters. Also, it should avoid parameters for stochasticity since that's handled later by a temperature parameter.\n\nFirst, I need to focus on the responder's behavior. The responder decides whether to accept or reject a proposal. The utility of accepting should depend on factors that influence their decision. From the variables provided, key ones might be the fairness of the split, the contributions of both participants (since combined earning varies), and perhaps the trial type indicating who contributed more.\n\nPsychological principles likely involved here are fairness considerations and inequity aversion. People often care about fair outcomes, not just their own payoff. The Fehr-Schmidt model of inequity aversion comes to mind, where individuals dislike unequal outcomes. Alternatively, the model could consider the difference between the proposed split and what's considered fair based on contributions.\n\nLooking at the dataset variables, token_self and token_opp represent each participant's contribution. The combined_earning is 3*(token_self + token_opp), since each token is \u00a33. The split_self and split_opp are the proposed amounts. The split percentages (splitperc_self and splitperc_opp) might also be relevant, but perhaps the actual amounts are more directly tied to utility.\n\nA fair split based on contributions would be each person getting a share proportional to their tokens. For example, if the participant contributed token_self tokens and the opponent token_opp, the fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. The proposed split_self is what's offered. The difference between the proposed and fair share could be a key factor. If the proposed is less than fair, the responder might feel negative utility, leading to rejection.\n\nSo, the utility could be modeled as the proposed amount (split_self) minus some parameter times the difference between the fair share and the proposed. But wait, maybe the fair share is (token_self / total_tokens) * combined_earning. Let's compute that. Let's call the fair share S_fair = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between S_fair and split_self is the inequity. If split_self is less than S_fair, that's unfavorable, and the responder's utility decreases.\n\nAlternatively, the model could use the ratio of split_self to S_fair. If the ratio is less than 1, the responder is getting less than fair, which reduces utility. But how to parameterize this?\n\nAnother angle is to consider the absolute difference between split_self and S_fair. Let's say the utility U = split_self - \u03b1 * max(S_fair - split_self, 0). Here, \u03b1 is a parameter representing aversion to receiving less than the fair share. If the proposed split is less than fair, the utility is reduced by \u03b1 times the deficit. If it's equal or more, then no reduction. This captures the idea that people dislike being shortchanged relative to their contribution.\n\nBut maybe the model should also consider if the responder is getting more than fair. However, the Fehr-Schmidt model differentiates between disadvantageous and advantageous inequity. But since the task is about responder behavior, perhaps only disadvantageous inequity (getting less than fair) matters. Because if they get more, they might just accept it, so the utility would just be split_self. But maybe the model should include both, but that would require two parameters. However, the problem states no more than two parameters. So perhaps we can combine them or focus on one.\n\nAlternatively, the model could use a single parameter for inequity aversion, combining both cases. But given the constraint of two parameters, maybe one for disadvantageous and another for advantageous. But the problem says two parameters max. Let's see.\n\nWait, the problem says the model should have no more than two learnable parameters. So maybe one parameter for the disadvantageous inequity (alpha) and another for advantageous (beta), but that's two. However, in the responder's case, when they are deciding to accept, perhaps advantageous inequity (they get more than fair) doesn't affect their utility negatively. So maybe only disadvantageous inequity is relevant. So one parameter for that. Then, the utility could be split_self + alpha*(split_self - S_fair) if split_self < S_fair. Wait, no. Let me think again.\n\nWait, the utility of accepting is split_self. But if the split is unfair (split_self < S_fair), the responder might have a negative utility from the unfairness. So the total utility could be split_self - alpha*(S_fair - split_self). So if split_self is less than S_fair, the term (S_fair - split_self) is positive, reducing the utility. Alpha is the weight given to this inequity. If alpha is high, even small deficits lead to lower utility, making rejection more likely.\n\nAlternatively, the model could be U = split_self - alpha * max(S_fair - split_self, 0). This way, when the proposed split is less than fair, the utility is reduced by alpha times the deficit. If it's equal or more, utility is just split_self. This uses one parameter, alpha. But maybe another parameter could capture something else, like the weight on the absolute split regardless of fairness. But the problem allows up to two parameters.\n\nAlternatively, the model could include a term for the absolute split_self, and another term for the difference from the fair share. For example, U = beta * split_self - alpha * max(S_fair - split_self, 0). Here, beta is a weight on the monetary gain, and alpha is the inequity aversion. But beta could be fixed to 1 if we assume linear utility of money. However, if beta is a learnable parameter, that's another parameter. But the problem allows two parameters. However, maybe beta is redundant if we assume that utility is linear in money. So perhaps beta is 1, and alpha is the only parameter. But the problem allows two, so maybe include beta as a parameter to capture the sensitivity to monetary gains. But the user wants to avoid high covariance between parameters. If beta and alpha are both free parameters, they might be correlated. So maybe better to fix beta as 1, and have alpha as the only parameter. But the problem allows two parameters. Hmm.\n\nAlternatively, another approach: the responder's utility could depend on both their own share and the opponent's share. For example, U = split_self + gamma * split_opp. If gamma is negative, that could represent envy (disliking the opponent's high share). If gamma is positive, it could represent altruism. But this might not directly account for fairness based on contributions. However, combining this with the fair share concept could be better.\n\nWait, but the fair share is based on contributions. So perhaps the responder compares the split to what's fair. So the model should incorporate the difference between the proposed split and the fair split. Let's formalize that.\n\nCompute S_fair = (token_self / (token_self + token_opp)) * combined_earning.\n\nThen, the difference D = split_self - S_fair. If D is positive, the responder is getting more than fair; if negative, less. The utility could be split_self + alpha * D. But that would mean if D is positive, utility increases, which might not be the case. Alternatively, if the responder dislikes both getting less and more (inequity aversion), the utility could be split_self - alpha * |D|. But that's a different model. However, in the responder's case, getting more than fair might not be a problem, so maybe only negative D matters. So U = split_self - alpha * max(S_fair - split_self, 0). This way, if the split is less than fair, utility is reduced by alpha times the deficit. If it's equal or more, utility is just split_self. This uses one parameter, alpha. But the problem allows two. What else could be considered?\n\nAnother factor could be the trial type (trial_type), which indicates whether contributions are equal, opponent more, or participant more. Maybe the responder's sensitivity to inequity depends on the trial type. For example, if the participant contributed more (trial_type=3), they might be more sensitive to receiving less than fair. But including trial_type would require interaction terms, which might complicate the model. However, the problem states that the model should use variables appropriately but doesn't need to use all. So maybe trial_type isn't necessary here.\n\nAlternatively, the model could include a term that captures the ratio of contributions. For example, the ratio token_self / token_opp. If the responder contributed more, they might expect a higher share. So the utility could be split_self - alpha * (S_fair - split_self) * (token_self / token_opp). But this introduces more variables and parameters. However, the problem allows two parameters. Let's see.\n\nWait, the problem says the model should have no more than two learnable parameters. So perhaps alpha and another parameter. Let's think of another possible factor. Maybe the responder's utility also depends on the absolute amount they are getting, but with diminishing returns. But since the model is linear, perhaps not. Alternatively, the responder might have a reference point based on their own contribution. For example, they might expect to get at least their own contribution's worth. Since each token is \u00a33, their own contribution is token_self * 3. The proposed split_self is in \u00a3. So if split_self is less than 3*token_self, that's a direct loss. But combined_earning is 3*(token_self + token_opp), so the fair share is (token_self/(token_self + token_opp)) * combined_earning = 3*token_self. Wait, that's interesting. Because combined_earning is 3*(token_self + token_opp), so S_fair = (token_self/(token_self + token_opp)) * 3*(token_self + token_opp) ) = 3*token_self. So the fair share is exactly equal to the participant's own contribution in pounds. Because each token is \u00a33, so token_self tokens = 3*token_self \u00a3. Therefore, the fair split_self would be 3*token_self. So if the proposed split_self is less than 3*token_self, that's unfair. So the difference is split_self - 3*token_self. If this is negative, the responder is getting less than their contribution. So the utility could be split_self - alpha * max(3*token_self - split_self, 0). This simplifies the model because S_fair is 3*token_self. That's a key insight. So the fair share is simply the participant's own contribution in monetary terms. Therefore, the model can be based on whether the proposed split_self meets or exceeds their own contribution.\n\nThis is a crucial point. Because combined_earning is 3*(token_self + token_opp), the fair split based on contribution would be each getting their own tokens converted to money. So the fair split_self is 3*token_self, and split_opp is 3*token_opp. Any deviation from this would be considered unfair. So if the proposer offers split_self less than 3*token_self, the responder perceives this as unfair. The utility of accepting would then be split_self (the monetary gain) minus a penalty for the unfairness, which is alpha times the deficit (3*token_self - split_self). \n\nSo the utility function U_accept = split_self - alpha * max(3*token_self - split_self, 0). \n\nThis model has one learnable parameter, alpha, which represents the aversion to receiving less than their own contribution. If alpha is high, even small deficits lead to a significant drop in utility, making rejection more likely. \n\nBut the problem allows up to two parameters. What else could be considered? Maybe the responder also cares about the opponent's share relative to their own contribution. For example, if the opponent's split_opp is more than 3*token_opp, that could be seen as unfair in the other direction. But since the responder is evaluating their own utility for accepting, perhaps their own deficit is the main factor. However, another parameter could capture the aversion to the opponent getting more than their fair share. Let's think.\n\nSuppose the responder also dislikes if the opponent is getting more than their fair share. The opponent's fair share is 3*token_opp. So if split_opp > 3*token_opp, the responder might feel that the opponent is being overcompensated. This could be modeled as another term: beta * max(split_opp - 3*token_opp, 0). If beta is positive, this would decrease utility (since the responder dislikes the opponent getting more than fair), but if beta is negative, it could increase utility (maybe the responder is altruistic). However, this introduces a second parameter, beta. \n\nBut the problem states that the model should predict only responder behavior and build on psychological principles. Inequity aversion often includes both aversion to disadvantageous and advantageous inequity, but in the responder's case, when deciding to accept, advantageous inequity (getting more than fair) might not be a concern. However, the opponent getting more than their fair share could be another source of inequity. \n\nAlternatively, the responder's utility could be influenced by the ratio of their split to the opponent's split, relative to their contributions. For example, the ratio (split_self / split_opp) compared to (token_self / token_opp). If the proposed split ratio is less than the contribution ratio, that's unfair. \n\nBut this might complicate the model. Let's see. The fair ratio is (token_self / token_opp), so the proposed split should have (split_self / split_opp) >= (token_self / token_opp). Taking logs, but perhaps a simpler approach is to compute the product of split_self and token_opp versus split_opp and token_self. If split_self * token_opp < split_opp * token_self, then the split is unfair. This is similar to the fairness condition in the ultimatum game. \n\nBut integrating this into the utility function could be done with a parameter that penalizes deviations from this fairness. For example, U = split_self - alpha * max( (split_opp * token_self) - (split_self * token_opp), 0 ). This term represents the unfairness where the opponent's share relative to their contribution is higher than the responder's. \n\nBut this introduces more variables and might require more parameters. However, with two parameters, perhaps alpha and beta for different aspects. But given the parameter recovery concern, having two parameters that might be correlated could be problematic. \n\nAlternatively, stick with the initial model using one parameter for the responder's own deficit. Then, think of another factor. For example, the responder might have a baseline acceptance tendency, independent of the split. But the problem says not to include parameters for general stochasticity, as that's handled by a temperature parameter later. So perhaps the model should focus on deterministic components of utility.\n\nWait, the example given in the problem statement had a model with U_i = beta + epsilon E + eta N, but the current task is to model the utility of accepting, which will be used in a softmax function with a temperature parameter. So the utility should be deterministic, and the temperature will handle the stochasticity. Therefore, the model should not include any noise parameters (eta N in the example), as per point 5.\n\nSo, back to the initial idea: the utility is split_self minus a penalty for receiving less than their fair share (3*token_self). This uses one parameter. To use two parameters, perhaps include another term, such as the difference between the proposed percentage and the fair percentage. The fair percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. So the difference is splitperc_self - (token_self / (token_self + token_opp))*100. If this is negative, the responder is getting a lower percentage than their contribution. So the utility could be split_self + beta * (splitperc_self - fair_percent). But this introduces another parameter, beta. However, split_self and splitperc_self are related since split_self = (splitperc_self / 100) * combined_earning. So including both might lead to multicollinearity. \n\nAlternatively, the model could use either the absolute amount or the percentage, but not both. Given that combined_earning varies, the percentage might be a more consistent measure of fairness. For example, if the combined earning is higher, the same split_self could represent a different percentage. So perhaps the percentage is a better measure. \n\nBut according to the problem statement, two equal split_self variables do not represent the same fairness because combined_earning varies. Therefore, the fairness should be assessed relative to the combined_earning. The fair percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these two percentages could be a key factor. \n\nSo the utility could be split_self + alpha * (splitperc_self - fair_percent). But split_self is (splitperc_self / 100) * combined_earning. So this might be redundant. Alternatively, the utility could be based on the percentage difference: U = splitperc_self - alpha * max(fair_percent - splitperc_self, 0). Here, alpha is the penalty for receiving a lower percentage than fair. \n\nBut this model uses splitperc_self, which is provided in the dataset. However, splitperc_self is derived from split_self and combined_earning. So using splitperc_self directly might be appropriate. \n\nAlternatively, combining both the absolute and percentage terms. For example, U = beta * split_self - alpha * max(fair_share - split_self, 0). Here, beta is a parameter scaling the utility of the monetary gain, and alpha is the penalty for unfairness. This would use two parameters. However, if beta is fixed to 1 (assuming linear utility of money), then alpha is the only parameter. But the problem allows two parameters, so perhaps beta is a learnable parameter. However, if beta and alpha are both free, they might be correlated, which could complicate parameter recovery. \n\nBut given the constraints, perhaps the best approach is to use two parameters: one for the utility of the monetary gain (beta) and one for the inequity aversion (alpha). So the model would be U = beta * split_self - alpha * max(3*token_self - split_self, 0). This way, beta represents the marginal utility of money, and alpha represents the aversion to receiving less than the fair share. \n\nHowever, if beta is set to 1, the model simplifies to U = split_self - alpha * max(3*token_self - split_self, 0). This uses one parameter. But the problem allows two, so maybe including beta as a learnable parameter allows the model to account for individual differences in the utility of money. For example, some people might value each pound more than others. \n\nBut introducing beta could lead to identifiability issues. For instance, if beta and alpha are both free, a higher beta and lower alpha could produce similar utilities as a lower beta and higher alpha in certain cases. However, with proper bounds and sufficient data, it might still be feasible. \n\nAlternatively, another approach is to model the utility as a function of both the absolute offer and the offer relative to the fair share. For example, U = alpha * split_self + beta * (split_self / S_fair). Here, alpha and beta are parameters. If S_fair is 3*token_self, then split_self / S_fair is the ratio of the offer to the fair share. If this ratio is less than 1, it's unfair. But this model combines both absolute and relative measures. However, this might not directly capture the aversion to unfairness. \n\nAlternatively, the utility could be U = split_self + beta * (split_self - S_fair). This would mean that if the offer is above the fair share, utility increases, and if below, it decreases. Here, beta is a parameter that scales the effect of deviation from fairness. If beta is positive, getting more than fair increases utility; getting less decreases it. But this assumes that deviations in both directions are valued linearly, which might not be the case. \n\nBut considering the problem constraints, perhaps the best model with two parameters is:\n\nU = beta * split_self - alpha * max(S_fair - split_self, 0)\n\nWhere S_fair = 3 * token_self.\n\nThis model has two learnable parameters: beta (marginal utility of money) and alpha (aversion to disadvantageous inequity). The max term ensures that only deficits relative to the fair share are penalized. \n\nHowever, if beta is redundant (e.g., if we assume that utility is linear in money with a coefficient of 1), then beta can be omitted, leaving alpha as the only parameter. But the problem allows two parameters, so including beta might allow more flexibility. However, the problem also mentions that the model should be considerate of high covariance between parameters. If beta and alpha are highly correlated, that's a problem. For example, if in the data, higher split_self is associated with higher S_fair, then beta and alpha might be confounded. \n\nAlternatively, perhaps fixing beta to 1 and using alpha as the only parameter is better for parameter recovery. Let's test this. If beta is 1, then U = split_self - alpha * max(3*token_self - split_self, 0). This model has one parameter, alpha. But the problem allows up to two. So maybe we can include another parameter, such as a term that accounts for the opponent's share relative to their contribution. \n\nFor example, the opponent's fair share is 3*token_opp. If the opponent is getting more than that (split_opp > 3*token_opp), the responder might feel that's unfair. So the utility could be:\n\nU = split_self - alpha * max(3*token_self - split_self, 0) - gamma * max(split_opp - 3*token_opp, 0)\n\nHere, gamma is a second parameter representing aversion to the opponent's overcompensation. But this introduces two parameters, alpha and gamma. However, this might complicate the model and introduce covariance between alpha and gamma. \n\nBut given the dataset variables, split_opp is available. So this is feasible. However, the problem states that the model should predict responder behavior and build on psychological principles. Aversion to advantageous inequity (opponent getting less) is part of some models, but in the responder's case, when deciding to accept, they might primarily care about their own share relative to their contribution and perhaps the opponent's share relative to theirs. \n\nAlternatively, the responder might compare their own split to the opponent's split in light of their contributions. For example, the ratio (split_self / split_opp) compared to (token_self / token_opp). If the ratio is lower, that's unfair. This could be modeled as:\n\nU = split_self - alpha * max( (token_self / token_opp) - (split_self / split_opp), 0 )\n\nBut this requires calculating the ratio, which could be problematic if token_opp is zero (but in the dataset, token_opp is an integer, so maybe it's always positive). However, this introduces more complexity. \n\nGiven the time constraints and the need to limit to two parameters, perhaps the best approach is to use the initial model with one parameter, but the problem allows two. So to use two parameters, perhaps include both the aversion to own deficit and the aversion to opponent's excess. \n\nThus, the model would be:\n\nU = split_self - alpha * max(3*token_self - split_self, 0) - beta * max(split_opp - 3*token_opp, 0)\n\nHere, alpha and beta are learnable parameters. Alpha captures aversion to getting less than fair, beta captures aversion to the opponent getting more than fair. \n\nBut does this align with psychological principles? Inequity aversion models often include both parameters. For example, Fehr-Schmidt's model has parameters for disadvantageous and advantageous inequity. However, in the responder's case, when accepting, advantageous inequity (responder getting more) might not be a factor, but the opponent's overcompensation could be. \n\nHowever, in the Fehr-Schmidt model, the utility function is:\n\nU_i = x_i - alpha * max(x_j - x_i, 0) - beta * max(x_i - x_j, 0)\n\nWhere x_i is the player's payoff, x_j is the other's payoff. But in our case, the fair share is based on contributions, not equal splits. So adapting this, the responder's utility could be:\n\nU = split_self - alpha * max( (split_opp - 3*token_opp) - (split_self - 3*token_self), 0 )\n\nBut this is getting complicated. Alternatively, stick with the simpler model based on own fair share. \n\nGiven the time I've spent, perhaps the best model with two parameters is:\n\nU = split_self - alpha * max(S_fair - split_self, 0) + beta * (split_self / S_fair)\n\nHere, beta captures the sensitivity to the proportion of the fair share received. But this might not be necessary. Alternatively, another approach is to have a non-linear term, but the problem doesn't specify that. \n\nAlternatively, the model could use the difference between split_self and S_fair, scaled by alpha, and the absolute split_self scaled by beta. But that might not make sense. \n\nWait, perhaps the responder's utility has two components: the monetary value of the offer and the fairness of the offer. So U = monetary_component + fairness_component. \n\nMonetary component could be beta * split_self (beta is the utility per pound). Fairness component could be -alpha * (S_fair - split_self) if split_self < S_fair. So:\n\nU = beta * split_self - alpha * max(S_fair - split_self, 0)\n\nThis uses two parameters, beta and alpha. Beta represents the utility per pound received, and alpha represents the penalty per pound deficit from the fair share. \n\nThis model allows for individual differences in both the value of money and the aversion to unfairness. However, if beta and alpha are correlated (e.g., people who value money more also care less about fairness), parameter recovery might be challenging. But given the problem allows two parameters, this is acceptable. \n\nThe variables needed are split_self and token_self (to compute S_fair = 3*token_self). The dataset has both variables. \n\nSo the final model would be:\n\nU = beta * split_self - alpha * max(3 * token_self - split_self, 0)\n\nThis model uses two learnable parameters, alpha and beta. The first term is the utility from the monetary offer, scaled by beta. The second term is the penalty for receiving less than the fair share (3*token_self), scaled by alpha. \n\nThis model aligns with the idea that responders derive utility from the money they receive but also dislike being treated unfairly, i.e., receiving less than their contribution's worth. The parameters alpha and beta can vary between individuals, capturing different sensitivities to money and fairness. \n\nNow, checking the dataset variables: split_self and token_self are available. 3*token_self is calculated from token_self. So the model is feasible with the given data. \n\nRegarding parameter bounds: alpha and beta should be positive. Because a higher alpha means more penalty for unfairness, and beta should be positive as more money is better. The bounds for alpha and beta can be [0, some upper limit], say 5, to keep them within a reasonable range. \n\nSo alpha and beta are learnable parameters with min 0 and max, say, 5. The upper bound is arbitrary but should be generous enough to cover plausible values. \n\nThus, the variables section would have:\n\n- alpha: learnable, range [0, 5], aversion to receiving less than fair share.\n- beta: learnable, range [0, 5], marginal utility of money.\n- split_self: from data.\n- token_self: from data.\n- S_fair: calculated as 3*token_self.\n\nThe utility U is calculated as beta*split_self - alpha*max(S_fair - split_self, 0).\n\nThis model meets all the requirements: predicts responder behavior, uses relevant variables, has two learnable parameters, and is based on psychological principles of fairness and inequity aversion.\n</think>\n\n<MODEL>\nU_accept = beta * split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the proposed split\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Aversion parameter for receiving less than the fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Marginal utility of monetary gain from the proposed split\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant (\u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA two-parameter model capturing responder utility as a trade-off between monetary gain and fairness. The utility combines the proposed split value (scaled by beta) with a penalty (alpha) for offers below the participant's contribution-based fair share (3 * token_self). Alpha quantifies aversion to disadvantageous inequity, while beta represents the marginal utility of money. The model explicitly links fairness expectations to pretask contributions, avoiding covariance issues through distinct parameter roles.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return the utility values for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of utility values for each trial\n\n                Your code will be implemented within the following code block:\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    utility_values = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        utility_values.append(beta + (epsilon * E) + (eta * N))\n    return utility_values\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE UTLITY VALUES.\n                Model Specification:\n                U_accept = beta * split_self - alpha * max(3 * token_self - split_self, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the proposed split\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Aversion parameter for receiving less than the fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Marginal utility of monetary gain from the proposed split\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant (\\u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        fair_share = 3 * token_self\n        utility = beta * split_self - alpha * max(fair_share - split_self, 0)\n        utility_values.append(utility)\n    return utility_values\n\n# Data as list of dictionaries (replace with your actual data loading)\ndata_json = \"\"\"[\n  {\"ID\": 1, \"group\": \"A\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 2, \"token_opp\": 5, \"token_self\": 10, \"combined_earning\": 45, \"split_opp\": 15, \"split_self\": 30, \"splitperc_opp\": 33, \"splitperc_self\": 67, \"accept\": 1, \"accepted_amount\": 30, \"accepted_perc\": 67, \"proposed_perc\": 67, \"proposed_amount\": 30},\n  {\"ID\": 2, \"group\": \"B\", \"trial_number\": 2, \"trial_type\": 2, \"trial_role\": 2, \"token_opp\": 8, \"token_self\": 2, \"combined_earning\": 30, \"split_opp\": 22, \"split_self\": 8, \"splitperc_opp\": 73, \"splitperc_self\": 27, \"accept\": 0, \"accepted_amount\": 0, \"accepted_perc\": 0, \"proposed_perc\": 27, \"proposed_amount\": 8}\n]\"\"\"\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data, alpha=0.5, beta=1.2)\nprint(json.dumps({\"results\": results}))\n```"
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Aversion parameter for receiving less than the fair share (3 * token_self)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Marginal utility of monetary gain from the proposed split",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U_accept = beta * split_self - alpha * max(3 * token_self - split_self, 0)",
  "variable_descriptions": {
    "U_accept": {
      "description": "Utility of accepting the proposed split",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha": {
      "description": "Aversion parameter for receiving less than the fair share (3 * token_self)",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Marginal utility of monetary gain from the proposed split",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed monetary amount for the participant (\u00a3)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant in the pretask",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A two-parameter model capturing responder utility as a trade-off between monetary gain and fairness. The utility combines the proposed split value (scaled by beta) with a penalty (alpha) for offers below the participant's contribution-based fair share (3 * token_self). Alpha quantifies aversion to disadvantageous inequity, while beta represents the marginal utility of money. The model explicitly links fairness expectations to pretask contributions, avoiding covariance issues through distinct parameter roles.",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer for the responder in the given experiment. The model needs to be based on psychological principles, use the dataset variables appropriately, and have no more than two learnable parameters. Also, it should avoid parameters for stochasticity since that's handled later by a temperature parameter.\n\nFirst, I need to focus on the responder's behavior. The responder decides whether to accept or reject a proposal. The utility of accepting should depend on factors that influence their decision. From the variables provided, key ones might be the fairness of the split, the contributions of both participants (since combined earning varies), and perhaps the trial type indicating who contributed more.\n\nPsychological principles likely involved here are fairness considerations and inequity aversion. People often care about fair outcomes, not just their own payoff. The Fehr-Schmidt model of inequity aversion comes to mind, where individuals dislike unequal outcomes. Alternatively, the model could consider the difference between the proposed split and what's considered fair based on contributions.\n\nLooking at the dataset variables, token_self and token_opp represent each participant's contribution. The combined_earning is 3*(token_self + token_opp), since each token is \u00a33. The split_self and split_opp are the proposed amounts. The split percentages (splitperc_self and splitperc_opp) might also be relevant, but perhaps the actual amounts are more directly tied to utility.\n\nA fair split based on contributions would be each person getting a share proportional to their tokens. For example, if the participant contributed token_self tokens and the opponent token_opp, the fair share for the participant would be (token_self / (token_self + token_opp)) * combined_earning. The proposed split_self is what's offered. The difference between the proposed and fair share could be a key factor. If the proposed is less than fair, the responder might feel negative utility, leading to rejection.\n\nSo, the utility could be modeled as the proposed amount (split_self) minus some parameter times the difference between the fair share and the proposed. But wait, maybe the fair share is (token_self / total_tokens) * combined_earning. Let's compute that. Let's call the fair share S_fair = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between S_fair and split_self is the inequity. If split_self is less than S_fair, that's unfavorable, and the responder's utility decreases.\n\nAlternatively, the model could use the ratio of split_self to S_fair. If the ratio is less than 1, the responder is getting less than fair, which reduces utility. But how to parameterize this?\n\nAnother angle is to consider the absolute difference between split_self and S_fair. Let's say the utility U = split_self - \u03b1 * max(S_fair - split_self, 0). Here, \u03b1 is a parameter representing aversion to receiving less than the fair share. If the proposed split is less than fair, the utility is reduced by \u03b1 times the deficit. If it's equal or more, then no reduction. This captures the idea that people dislike being shortchanged relative to their contribution.\n\nBut maybe the model should also consider if the responder is getting more than fair. However, the Fehr-Schmidt model differentiates between disadvantageous and advantageous inequity. But since the task is about responder behavior, perhaps only disadvantageous inequity (getting less than fair) matters. Because if they get more, they might just accept it, so the utility would just be split_self. But maybe the model should include both, but that would require two parameters. However, the problem states no more than two parameters. So perhaps we can combine them or focus on one.\n\nAlternatively, the model could use a single parameter for inequity aversion, combining both cases. But given the constraint of two parameters, maybe one for disadvantageous and another for advantageous. But the problem says two parameters max. Let's see.\n\nWait, the problem says the model should have no more than two learnable parameters. So maybe one parameter for the disadvantageous inequity (alpha) and another for advantageous (beta), but that's two. However, in the responder's case, when they are deciding to accept, perhaps advantageous inequity (they get more than fair) doesn't affect their utility negatively. So maybe only disadvantageous inequity is relevant. So one parameter for that. Then, the utility could be split_self + alpha*(split_self - S_fair) if split_self < S_fair. Wait, no. Let me think again.\n\nWait, the utility of accepting is split_self. But if the split is unfair (split_self < S_fair), the responder might have a negative utility from the unfairness. So the total utility could be split_self - alpha*(S_fair - split_self). So if split_self is less than S_fair, the term (S_fair - split_self) is positive, reducing the utility. Alpha is the weight given to this inequity. If alpha is high, even small deficits lead to lower utility, making rejection more likely.\n\nAlternatively, the model could be U = split_self - alpha * max(S_fair - split_self, 0). This way, when the proposed split is less than fair, the utility is reduced by alpha times the deficit. If it's equal or more, utility is just split_self. This uses one parameter, alpha. But maybe another parameter could capture something else, like the weight on the absolute split regardless of fairness. But the problem allows up to two parameters.\n\nAlternatively, the model could include a term for the absolute split_self, and another term for the difference from the fair share. For example, U = beta * split_self - alpha * max(S_fair - split_self, 0). Here, beta is a weight on the monetary gain, and alpha is the inequity aversion. But beta could be fixed to 1 if we assume linear utility of money. However, if beta is a learnable parameter, that's another parameter. But the problem allows two parameters. However, maybe beta is redundant if we assume that utility is linear in money. So perhaps beta is 1, and alpha is the only parameter. But the problem allows two, so maybe include beta as a parameter to capture the sensitivity to monetary gains. But the user wants to avoid high covariance between parameters. If beta and alpha are both free parameters, they might be correlated. So maybe better to fix beta as 1, and have alpha as the only parameter. But the problem allows two parameters. Hmm.\n\nAlternatively, another approach: the responder's utility could depend on both their own share and the opponent's share. For example, U = split_self + gamma * split_opp. If gamma is negative, that could represent envy (disliking the opponent's high share). If gamma is positive, it could represent altruism. But this might not directly account for fairness based on contributions. However, combining this with the fair share concept could be better.\n\nWait, but the fair share is based on contributions. So perhaps the responder compares the split to what's fair. So the model should incorporate the difference between the proposed split and the fair split. Let's formalize that.\n\nCompute S_fair = (token_self / (token_self + token_opp)) * combined_earning.\n\nThen, the difference D = split_self - S_fair. If D is positive, the responder is getting more than fair; if negative, less. The utility could be split_self + alpha * D. But that would mean if D is positive, utility increases, which might not be the case. Alternatively, if the responder dislikes both getting less and more (inequity aversion), the utility could be split_self - alpha * |D|. But that's a different model. However, in the responder's case, getting more than fair might not be a problem, so maybe only negative D matters. So U = split_self - alpha * max(S_fair - split_self, 0). This way, if the split is less than fair, utility is reduced by alpha times the deficit. If it's equal or more, utility is just split_self. This uses one parameter, alpha. But the problem allows two. What else could be considered?\n\nAnother factor could be the trial type (trial_type), which indicates whether contributions are equal, opponent more, or participant more. Maybe the responder's sensitivity to inequity depends on the trial type. For example, if the participant contributed more (trial_type=3), they might be more sensitive to receiving less than fair. But including trial_type would require interaction terms, which might complicate the model. However, the problem states that the model should use variables appropriately but doesn't need to use all. So maybe trial_type isn't necessary here.\n\nAlternatively, the model could include a term that captures the ratio of contributions. For example, the ratio token_self / token_opp. If the responder contributed more, they might expect a higher share. So the utility could be split_self - alpha * (S_fair - split_self) * (token_self / token_opp). But this introduces more variables and parameters. However, the problem allows two parameters. Let's see.\n\nWait, the problem says the model should have no more than two learnable parameters. So perhaps alpha and another parameter. Let's think of another possible factor. Maybe the responder's utility also depends on the absolute amount they are getting, but with diminishing returns. But since the model is linear, perhaps not. Alternatively, the responder might have a reference point based on their own contribution. For example, they might expect to get at least their own contribution's worth. Since each token is \u00a33, their own contribution is token_self * 3. The proposed split_self is in \u00a3. So if split_self is less than 3*token_self, that's a direct loss. But combined_earning is 3*(token_self + token_opp), so the fair share is (token_self/(token_self + token_opp)) * combined_earning = 3*token_self. Wait, that's interesting. Because combined_earning is 3*(token_self + token_opp), so S_fair = (token_self/(token_self + token_opp)) * 3*(token_self + token_opp) ) = 3*token_self. So the fair share is exactly equal to the participant's own contribution in pounds. Because each token is \u00a33, so token_self tokens = 3*token_self \u00a3. Therefore, the fair split_self would be 3*token_self. So if the proposed split_self is less than 3*token_self, that's unfair. So the difference is split_self - 3*token_self. If this is negative, the responder is getting less than their contribution. So the utility could be split_self - alpha * max(3*token_self - split_self, 0). This simplifies the model because S_fair is 3*token_self. That's a key insight. So the fair share is simply the participant's own contribution in monetary terms. Therefore, the model can be based on whether the proposed split_self meets or exceeds their own contribution.\n\nThis is a crucial point. Because combined_earning is 3*(token_self + token_opp), the fair split based on contribution would be each getting their own tokens converted to money. So the fair split_self is 3*token_self, and split_opp is 3*token_opp. Any deviation from this would be considered unfair. So if the proposer offers split_self less than 3*token_self, the responder perceives this as unfair. The utility of accepting would then be split_self (the monetary gain) minus a penalty for the unfairness, which is alpha times the deficit (3*token_self - split_self). \n\nSo the utility function U_accept = split_self - alpha * max(3*token_self - split_self, 0). \n\nThis model has one learnable parameter, alpha, which represents the aversion to receiving less than their own contribution. If alpha is high, even small deficits lead to a significant drop in utility, making rejection more likely. \n\nBut the problem allows up to two parameters. What else could be considered? Maybe the responder also cares about the opponent's share relative to their own contribution. For example, if the opponent's split_opp is more than 3*token_opp, that could be seen as unfair in the other direction. But since the responder is evaluating their own utility for accepting, perhaps their own deficit is the main factor. However, another parameter could capture the aversion to the opponent getting more than their fair share. Let's think.\n\nSuppose the responder also dislikes if the opponent is getting more than their fair share. The opponent's fair share is 3*token_opp. So if split_opp > 3*token_opp, the responder might feel that the opponent is being overcompensated. This could be modeled as another term: beta * max(split_opp - 3*token_opp, 0). If beta is positive, this would decrease utility (since the responder dislikes the opponent getting more than fair), but if beta is negative, it could increase utility (maybe the responder is altruistic). However, this introduces a second parameter, beta. \n\nBut the problem states that the model should predict only responder behavior and build on psychological principles. Inequity aversion often includes both aversion to disadvantageous and advantageous inequity, but in the responder's case, when deciding to accept, advantageous inequity (getting more than fair) might not be a concern. However, the opponent getting more than their fair share could be another source of inequity. \n\nAlternatively, the responder's utility could be influenced by the ratio of their split to the opponent's split, relative to their contributions. For example, the ratio (split_self / split_opp) compared to (token_self / token_opp). If the proposed split ratio is less than the contribution ratio, that's unfair. \n\nBut this might complicate the model. Let's see. The fair ratio is (token_self / token_opp), so the proposed split should have (split_self / split_opp) >= (token_self / token_opp). Taking logs, but perhaps a simpler approach is to compute the product of split_self and token_opp versus split_opp and token_self. If split_self * token_opp < split_opp * token_self, then the split is unfair. This is similar to the fairness condition in the ultimatum game. \n\nBut integrating this into the utility function could be done with a parameter that penalizes deviations from this fairness. For example, U = split_self - alpha * max( (split_opp * token_self) - (split_self * token_opp), 0 ). This term represents the unfairness where the opponent's share relative to their contribution is higher than the responder's. \n\nBut this introduces more variables and might require more parameters. However, with two parameters, perhaps alpha and beta for different aspects. But given the parameter recovery concern, having two parameters that might be correlated could be problematic. \n\nAlternatively, stick with the initial model using one parameter for the responder's own deficit. Then, think of another factor. For example, the responder might have a baseline acceptance tendency, independent of the split. But the problem says not to include parameters for general stochasticity, as that's handled by a temperature parameter later. So perhaps the model should focus on deterministic components of utility.\n\nWait, the example given in the problem statement had a model with U_i = beta + epsilon E + eta N, but the current task is to model the utility of accepting, which will be used in a softmax function with a temperature parameter. So the utility should be deterministic, and the temperature will handle the stochasticity. Therefore, the model should not include any noise parameters (eta N in the example), as per point 5.\n\nSo, back to the initial idea: the utility is split_self minus a penalty for receiving less than their fair share (3*token_self). This uses one parameter. To use two parameters, perhaps include another term, such as the difference between the proposed percentage and the fair percentage. The fair percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. So the difference is splitperc_self - (token_self / (token_self + token_opp))*100. If this is negative, the responder is getting a lower percentage than their contribution. So the utility could be split_self + beta * (splitperc_self - fair_percent). But this introduces another parameter, beta. However, split_self and splitperc_self are related since split_self = (splitperc_self / 100) * combined_earning. So including both might lead to multicollinearity. \n\nAlternatively, the model could use either the absolute amount or the percentage, but not both. Given that combined_earning varies, the percentage might be a more consistent measure of fairness. For example, if the combined earning is higher, the same split_self could represent a different percentage. So perhaps the percentage is a better measure. \n\nBut according to the problem statement, two equal split_self variables do not represent the same fairness because combined_earning varies. Therefore, the fairness should be assessed relative to the combined_earning. The fair percentage is (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these two percentages could be a key factor. \n\nSo the utility could be split_self + alpha * (splitperc_self - fair_percent). But split_self is (splitperc_self / 100) * combined_earning. So this might be redundant. Alternatively, the utility could be based on the percentage difference: U = splitperc_self - alpha * max(fair_percent - splitperc_self, 0). Here, alpha is the penalty for receiving a lower percentage than fair. \n\nBut this model uses splitperc_self, which is provided in the dataset. However, splitperc_self is derived from split_self and combined_earning. So using splitperc_self directly might be appropriate. \n\nAlternatively, combining both the absolute and percentage terms. For example, U = beta * split_self - alpha * max(fair_share - split_self, 0). Here, beta is a parameter scaling the utility of the monetary gain, and alpha is the penalty for unfairness. This would use two parameters. However, if beta is fixed to 1 (assuming linear utility of money), then alpha is the only parameter. But the problem allows two parameters, so perhaps beta is a learnable parameter. However, if beta and alpha are both free, they might be correlated, which could complicate parameter recovery. \n\nBut given the constraints, perhaps the best approach is to use two parameters: one for the utility of the monetary gain (beta) and one for the inequity aversion (alpha). So the model would be U = beta * split_self - alpha * max(3*token_self - split_self, 0). This way, beta represents the marginal utility of money, and alpha represents the aversion to receiving less than the fair share. \n\nHowever, if beta is set to 1, the model simplifies to U = split_self - alpha * max(3*token_self - split_self, 0). This uses one parameter. But the problem allows two, so maybe including beta as a learnable parameter allows the model to account for individual differences in the utility of money. For example, some people might value each pound more than others. \n\nBut introducing beta could lead to identifiability issues. For instance, if beta and alpha are both free, a higher beta and lower alpha could produce similar utilities as a lower beta and higher alpha in certain cases. However, with proper bounds and sufficient data, it might still be feasible. \n\nAlternatively, another approach is to model the utility as a function of both the absolute offer and the offer relative to the fair share. For example, U = alpha * split_self + beta * (split_self / S_fair). Here, alpha and beta are parameters. If S_fair is 3*token_self, then split_self / S_fair is the ratio of the offer to the fair share. If this ratio is less than 1, it's unfair. But this model combines both absolute and relative measures. However, this might not directly capture the aversion to unfairness. \n\nAlternatively, the utility could be U = split_self + beta * (split_self - S_fair). This would mean that if the offer is above the fair share, utility increases, and if below, it decreases. Here, beta is a parameter that scales the effect of deviation from fairness. If beta is positive, getting more than fair increases utility; getting less decreases it. But this assumes that deviations in both directions are valued linearly, which might not be the case. \n\nBut considering the problem constraints, perhaps the best model with two parameters is:\n\nU = beta * split_self - alpha * max(S_fair - split_self, 0)\n\nWhere S_fair = 3 * token_self.\n\nThis model has two learnable parameters: beta (marginal utility of money) and alpha (aversion to disadvantageous inequity). The max term ensures that only deficits relative to the fair share are penalized. \n\nHowever, if beta is redundant (e.g., if we assume that utility is linear in money with a coefficient of 1), then beta can be omitted, leaving alpha as the only parameter. But the problem allows two parameters, so including beta might allow more flexibility. However, the problem also mentions that the model should be considerate of high covariance between parameters. If beta and alpha are highly correlated, that's a problem. For example, if in the data, higher split_self is associated with higher S_fair, then beta and alpha might be confounded. \n\nAlternatively, perhaps fixing beta to 1 and using alpha as the only parameter is better for parameter recovery. Let's test this. If beta is 1, then U = split_self - alpha * max(3*token_self - split_self, 0). This model has one parameter, alpha. But the problem allows up to two. So maybe we can include another parameter, such as a term that accounts for the opponent's share relative to their contribution. \n\nFor example, the opponent's fair share is 3*token_opp. If the opponent is getting more than that (split_opp > 3*token_opp), the responder might feel that's unfair. So the utility could be:\n\nU = split_self - alpha * max(3*token_self - split_self, 0) - gamma * max(split_opp - 3*token_opp, 0)\n\nHere, gamma is a second parameter representing aversion to the opponent's overcompensation. But this introduces two parameters, alpha and gamma. However, this might complicate the model and introduce covariance between alpha and gamma. \n\nBut given the dataset variables, split_opp is available. So this is feasible. However, the problem states that the model should predict responder behavior and build on psychological principles. Aversion to advantageous inequity (opponent getting less) is part of some models, but in the responder's case, when deciding to accept, they might primarily care about their own share relative to their contribution and perhaps the opponent's share relative to theirs. \n\nAlternatively, the responder might compare their own split to the opponent's split in light of their contributions. For example, the ratio (split_self / split_opp) compared to (token_self / token_opp). If the ratio is lower, that's unfair. This could be modeled as:\n\nU = split_self - alpha * max( (token_self / token_opp) - (split_self / split_opp), 0 )\n\nBut this requires calculating the ratio, which could be problematic if token_opp is zero (but in the dataset, token_opp is an integer, so maybe it's always positive). However, this introduces more complexity. \n\nGiven the time constraints and the need to limit to two parameters, perhaps the best approach is to use the initial model with one parameter, but the problem allows two. So to use two parameters, perhaps include both the aversion to own deficit and the aversion to opponent's excess. \n\nThus, the model would be:\n\nU = split_self - alpha * max(3*token_self - split_self, 0) - beta * max(split_opp - 3*token_opp, 0)\n\nHere, alpha and beta are learnable parameters. Alpha captures aversion to getting less than fair, beta captures aversion to the opponent getting more than fair. \n\nBut does this align with psychological principles? Inequity aversion models often include both parameters. For example, Fehr-Schmidt's model has parameters for disadvantageous and advantageous inequity. However, in the responder's case, when accepting, advantageous inequity (responder getting more) might not be a factor, but the opponent's overcompensation could be. \n\nHowever, in the Fehr-Schmidt model, the utility function is:\n\nU_i = x_i - alpha * max(x_j - x_i, 0) - beta * max(x_i - x_j, 0)\n\nWhere x_i is the player's payoff, x_j is the other's payoff. But in our case, the fair share is based on contributions, not equal splits. So adapting this, the responder's utility could be:\n\nU = split_self - alpha * max( (split_opp - 3*token_opp) - (split_self - 3*token_self), 0 )\n\nBut this is getting complicated. Alternatively, stick with the simpler model based on own fair share. \n\nGiven the time I've spent, perhaps the best model with two parameters is:\n\nU = split_self - alpha * max(S_fair - split_self, 0) + beta * (split_self / S_fair)\n\nHere, beta captures the sensitivity to the proportion of the fair share received. But this might not be necessary. Alternatively, another approach is to have a non-linear term, but the problem doesn't specify that. \n\nAlternatively, the model could use the difference between split_self and S_fair, scaled by alpha, and the absolute split_self scaled by beta. But that might not make sense. \n\nWait, perhaps the responder's utility has two components: the monetary value of the offer and the fairness of the offer. So U = monetary_component + fairness_component. \n\nMonetary component could be beta * split_self (beta is the utility per pound). Fairness component could be -alpha * (S_fair - split_self) if split_self < S_fair. So:\n\nU = beta * split_self - alpha * max(S_fair - split_self, 0)\n\nThis uses two parameters, beta and alpha. Beta represents the utility per pound received, and alpha represents the penalty per pound deficit from the fair share. \n\nThis model allows for individual differences in both the value of money and the aversion to unfairness. However, if beta and alpha are correlated (e.g., people who value money more also care less about fairness), parameter recovery might be challenging. But given the problem allows two parameters, this is acceptable. \n\nThe variables needed are split_self and token_self (to compute S_fair = 3*token_self). The dataset has both variables. \n\nSo the final model would be:\n\nU = beta * split_self - alpha * max(3 * token_self - split_self, 0)\n\nThis model uses two learnable parameters, alpha and beta. The first term is the utility from the monetary offer, scaled by beta. The second term is the penalty for receiving less than the fair share (3*token_self), scaled by alpha. \n\nThis model aligns with the idea that responders derive utility from the money they receive but also dislike being treated unfairly, i.e., receiving less than their contribution's worth. The parameters alpha and beta can vary between individuals, capturing different sensitivities to money and fairness. \n\nNow, checking the dataset variables: split_self and token_self are available. 3*token_self is calculated from token_self. So the model is feasible with the given data. \n\nRegarding parameter bounds: alpha and beta should be positive. Because a higher alpha means more penalty for unfairness, and beta should be positive as more money is better. The bounds for alpha and beta can be [0, some upper limit], say 5, to keep them within a reasonable range. \n\nSo alpha and beta are learnable parameters with min 0 and max, say, 5. The upper bound is arbitrary but should be generous enough to cover plausible values. \n\nThus, the variables section would have:\n\n- alpha: learnable, range [0, 5], aversion to receiving less than fair share.\n- beta: learnable, range [0, 5], marginal utility of money.\n- split_self: from data.\n- token_self: from data.\n- S_fair: calculated as 3*token_self.\n\nThe utility U is calculated as beta*split_self - alpha*max(S_fair - split_self, 0).\n\nThis model meets all the requirements: predicts responder behavior, uses relevant variables, has two learnable parameters, and is based on psychological principles of fairness and inequity aversion.\n</think>\n\n<MODEL>\nU_accept = beta * split_self - alpha * max(3 * token_self - split_self, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the proposed split\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Aversion parameter for receiving less than the fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Marginal utility of monetary gain from the proposed split\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant (\u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA two-parameter model capturing responder utility as a trade-off between monetary gain and fairness. The utility combines the proposed split value (scaled by beta) with a penalty (alpha) for offers below the participant's contribution-based fair share (3 * token_self). Alpha quantifies aversion to disadvantageous inequity, while beta represents the marginal utility of money. The model explicitly links fairness expectations to pretask contributions, avoiding covariance issues through distinct parameter roles.\n</SUMMARY>",
  "previous_models": [
    "Specification: U_accept = beta * split_self - alpha * max(3 * token_self - split_self, 0)\nSummary: A two-parameter model capturing responder utility as a trade-off between monetary gain and fairness. The utility combines the proposed split value (scaled by beta) with a penalty (alpha) for offers below the participant's contribution-based fair share (3 * token_self). Alpha quantifies aversion to disadvantageous inequity, while beta represents the marginal utility of money. The model explicitly links fairness expectations to pretask contributions, avoiding covariance issues through distinct parameter roles."
  ],
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        \n        fair_share = 3 * token_self\n        utility = beta * split_self - alpha * max(fair_share - split_self, 0)\n        utility_values.append(utility)\n    return utility_values",
  "fitting_results": [
    {
      "alpha": 3.8060020131757932,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.931512218249421,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7641630833979796,
      "beta": 1.1798435152545936,
      "success": true,
      "log_likelihood": -16.28126268370752,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.588904074532154,
      "beta": 2.499847416861014,
      "success": true,
      "log_likelihood": -10.046565395822206,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.4612498063041586,
      "beta": 1.41815574662811,
      "success": true,
      "log_likelihood": -13.653156181719693,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.868979227231563,
      "beta": 2.0287385956313524,
      "success": true,
      "log_likelihood": -9.848024472740283,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.540146217021876,
      "beta": 2.419423305147902,
      "success": true,
      "log_likelihood": -11.042332384699295,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.19582658987568802,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.4417320099049693,
      "beta": 2.416915929735709,
      "success": true,
      "log_likelihood": -11.096668637640331,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.19582658987568802,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.275811004605333,
      "beta": 1.5441689522150874,
      "success": true,
      "log_likelihood": -11.166531339812952,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 2.921398714724323,
      "success": true,
      "log_likelihood": -4.1372871797324695,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.4120602809801532,
      "beta": 0.14165940642734137,
      "success": true,
      "log_likelihood": -9.090310470066255,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.416788797235976,
      "beta": 2.2353103354271364,
      "success": true,
      "log_likelihood": -8.81615696470317,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.195826589875688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.195826589875688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.195826589875688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.19582658987568796,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.315106523285386,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -3.2051934882148583,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5225097726528383,
      "beta": 3.571684986610026,
      "success": true,
      "log_likelihood": -6.650035489564317,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.195826589875688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.93005407894704,
      "beta": 1.3997529676207379,
      "success": true,
      "log_likelihood": -15.24028348524522,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5080717111828876,
      "beta": 3.1465055002132765,
      "success": true,
      "log_likelihood": -7.939517179679073,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4190066538760058,
      "beta": 0.6803263776145483,
      "success": true,
      "log_likelihood": -18.32423206924848,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5114599428673925,
      "beta": 1.1922271183545585,
      "success": true,
      "log_likelihood": -16.99121846617474,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.38278524645616,
      "beta": 1.369645153656629,
      "success": true,
      "log_likelihood": -13.939297529877201,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7910064395250167,
      "beta": 1.5707485934346164,
      "success": true,
      "log_likelihood": -14.894718696487335,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.737591974789839,
      "beta": 2.187003477255483,
      "success": true,
      "log_likelihood": -11.62284383669592,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 1.456732704647882,
      "success": true,
      "log_likelihood": -4.943540866913564,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8624556586240811,
      "beta": 2.3820607551878132,
      "success": true,
      "log_likelihood": -9.361863495232047,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7716985690191905,
      "beta": 1.368642666318758,
      "success": true,
      "log_likelihood": -15.902796869744765,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.19582658987568796,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.806015150920835,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.931512218291035,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9385726753776612,
      "beta": 0.5302952811690249,
      "success": true,
      "log_likelihood": -21.0583751784014,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0198789576540332,
      "beta": 0.9859993231143774,
      "success": true,
      "log_likelihood": -15.41219496993816,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.0,
      "success": true,
      "log_likelihood": -0.19582658987568802,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.586729592280968,
      "beta": 1.907388737480131,
      "success": true,
      "log_likelihood": -13.36377464321619,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.6887143158682183,
      "beta": 4.158446448815521,
      "success": true,
      "log_likelihood": -7.009324202079472,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.2782753332929246,
      "beta": 1.4182441060592765,
      "success": true,
      "log_likelihood": -14.2256691281867,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0867850840899118,
      "beta": 1.182431839445841,
      "success": true,
      "log_likelihood": -17.719069088838125,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3197067689343672,
      "beta": 1.9042349302715684,
      "success": true,
      "log_likelihood": -9.541669272845787,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3062944850325708,
      "beta": 1.1277701744785413,
      "success": true,
      "log_likelihood": -17.74625432355557,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.4821585167857294,
      "beta": 0.4485464351679877,
      "success": true,
      "log_likelihood": -12.426179410244394,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 1.8697442910253999,
      "success": true,
      "log_likelihood": -7.745036530262249,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5925267162431775,
      "beta": 1.3558939932988021,
      "success": true,
      "log_likelihood": -16.139319574914552,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.033171403031147,
      "beta": 1.8991310949802223,
      "success": true,
      "log_likelihood": -13.25755328089357,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7958694412526213,
      "beta": 1.396185997690736,
      "success": true,
      "log_likelihood": -15.740340539999844,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9300505475594893,
      "beta": 1.3997494677997429,
      "success": true,
      "log_likelihood": -15.240283488103955,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2985968975586288,
      "beta": 0.6486286437181168,
      "success": true,
      "log_likelihood": -18.99591754549713,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.219846415930505,
      "beta": 1.4370717199218155,
      "success": true,
      "log_likelihood": -11.26989120495144,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3456112775616479,
      "beta": 0.3872185417617592,
      "success": true,
      "log_likelihood": -23.58999699230841,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8528989332277813,
      "beta": 1.2183528547172193,
      "success": true,
      "log_likelihood": -17.337692824312043,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7888089376871328,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -13.305935033762944,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0885885250599676,
      "beta": 0.9537919552031267,
      "success": true,
      "log_likelihood": -19.08444485415414,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4590150246011199,
      "beta": 0.6554069934689232,
      "success": true,
      "log_likelihood": -21.649576318391965,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.233255176099383,
      "beta": 0.4463338185458644,
      "success": true,
      "log_likelihood": -19.145851304825918,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 1.9978465769048743,
      "success": true,
      "log_likelihood": -5.173014313354193,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.421384804476021,
      "beta": 1.0033182703109453,
      "success": true,
      "log_likelihood": -10.386539880506673,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 1.137658461560396,
      "success": true,
      "log_likelihood": -13.680488206347569,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.935698797759387,
      "beta": 0.07468300609093208,
      "success": true,
      "log_likelihood": -9.825736629824613,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.46460271305002443,
      "beta": 1.4313195398034235,
      "success": true,
      "log_likelihood": -14.062138466420835,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8693053007358202,
      "beta": 0.9003140883420536,
      "success": true,
      "log_likelihood": -19.76650471323207,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.143855312751089,
      "beta": 1.330580562685625,
      "success": true,
      "log_likelihood": -14.763159511668606,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.8060007843342407,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.931512218248791,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9772793523661736,
      "beta": 0.6307573571747216,
      "success": true,
      "log_likelihood": -20.702526864568785,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.040142596745839,
      "beta": 0.6965806398728513,
      "success": true,
      "log_likelihood": -11.083994119218326,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9393034812923897,
      "beta": 1.6312751980176063,
      "success": true,
      "log_likelihood": -14.437445103732987,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5413846683720401,
      "beta": 2.3095840573048614,
      "success": true,
      "log_likelihood": -8.407405638298567,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7069880526888913,
      "beta": 1.870066995735982,
      "success": true,
      "log_likelihood": -11.77955863965959,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4703963276046337,
      "beta": 1.415404071725409,
      "success": true,
      "log_likelihood": -16.02928879983105,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 1.3560456636690166,
      "success": true,
      "log_likelihood": -11.540124489362412,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.2774401144581393,
      "success": true,
      "log_likelihood": -3.902180963067038,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9994267830163137,
      "beta": 0.03723825608534831,
      "success": true,
      "log_likelihood": -12.598482049186622,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.817159048772524,
      "beta": 1.3706250715350476,
      "success": true,
      "log_likelihood": -15.60614830649521,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5665096071866726,
      "beta": 1.4028693636996175,
      "success": true,
      "log_likelihood": -14.83372668277399,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6219337707701145,
      "beta": 0.4606028227336162,
      "success": true,
      "log_likelihood": -22.663571894007017,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7135695642300575,
      "beta": 1.8873599738963796,
      "success": true,
      "log_likelihood": -13.469173044825395,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.20438326395950887,
      "beta": 2.079390045042393,
      "success": true,
      "log_likelihood": -7.802173684445504,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.452194089347257,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -8.474686531176056,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2669941774041429,
      "beta": 0.5316281907156895,
      "success": true,
      "log_likelihood": -22.370656052834665,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.8059979319492605,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.931512218249497,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5662303907501742,
      "beta": 0.4684563603652728,
      "success": true,
      "log_likelihood": -22.809114635743622,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 1.333893374163395,
      "success": true,
      "log_likelihood": -11.741860654220266,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.6892646460915035,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -10.063924084619371,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.262585234719323,
      "beta": 2.505700404956922,
      "success": true,
      "log_likelihood": -10.339432462327917,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.19845083020047508,
      "beta": 0.33959313424790394,
      "success": true,
      "log_likelihood": -23.81915563980949,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.314168483698777,
      "beta": 0.16603755098332282,
      "success": true,
      "log_likelihood": -7.785436408188076,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8855299209020554,
      "beta": 1.1454481603909865,
      "success": true,
      "log_likelihood": -17.95964190291066,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.157858943166549,
      "beta": 0.31657181191654965,
      "success": true,
      "log_likelihood": -13.317159706383109,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.675597074096177,
      "beta": 3.1024843773375164,
      "success": true,
      "log_likelihood": -5.817840692317194,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.978797794386235,
      "beta": 1.5459199152592296,
      "success": true,
      "log_likelihood": -14.679368249029604,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3327097112187001,
      "beta": 0.7069640900284464,
      "success": true,
      "log_likelihood": -20.83312331403783,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.8516541265639913,
        "p": 2.9940402802645116e-29
      },
      "beta": {
        "r": 0.8995305521395054,
        "p": 5.052651998566599e-37
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 30.92992705075289,
  "bic_Control": 25.68177497815461,
  "bic_Cocaine": 35.63139661578885,
  "bic_error": "'ModelOutput' object has no attribute 'metrics'"
}