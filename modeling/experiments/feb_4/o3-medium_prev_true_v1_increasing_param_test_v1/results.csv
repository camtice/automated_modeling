run_number,average_bic,model_specification,model_summary,version,alpha_recovery,beta_recovery,gamma_recovery,delta_recovery,phi_recovery,omega_recovery,lambda_param_recovery,kappa_recovery,psi_recovery,theta_recovery
1,,"Let fair_share = (token_self / (token_self + token_opp)) × combined_earning
U_accept = split_self - [ alpha × max( fair_share - split_self, 0 ) + beta × max( split_self - fair_share, 0 ) ]",A fairness-based utility model for responder behavior where the utility of accepting an offer equals the offered monetary gain minus a fairness penalty. The penalty is scaled by one parameter (alpha) when the offer falls short of the contribution-based fair share and by a second parameter (beta) when the offer exceeds it.,v1,0.899165600644108,0.7345186274583931,,,,,,,,
2,,"Let 
 F = (token_self / (token_self + token_opp)) × combined_earning

Then, the utility for accepting an offer is defined as:
 U_accept = split_self − { γ × (F − split_self)² if split_self < F 
                δ × (split_self − F)² otherwise }
  (Represented as a piecewise function)","A fairness-based utility model for responder behavior that computes a fairness benchmark from token contributions. The model subtracts a quadratic penalty from the monetary offer, with two separate sensitivity parameters for disadvantageous (γ) and advantageous (δ) deviations from fairness. This directly produces the utility of accepting an offer.",v1,,,0.8417839448046889,0.6417996538967843,,,,,,
3,,"Let F = (token_self / (token_self + token_opp)) × combined_earning  
U_accept = split_self − phi × |split_self − F|^(omega)",A fairness-based utility model for responders where the utility of accepting an offer equals the monetary share (split_self) minus a penalty for deviating from a fairness benchmark computed from token contributions and combined earnings. The penalty is scaled by a learnable sensitivity parameter (phi) and raised to a power (omega) to capture nonlinear (diminishing or enhanced) sensitivity to fairness deviations.,v1,,,,,0.7931922148533305,0.6545128710622409,,,,
4,,"Let F = (token_self / (token_self + token_opp)) × combined_earning  
U_accept = ln(split_self) − lambda_param × | ln(split_self / F) |","This model computes the utility of accepting an offer for responders by combining subjective monetary value (expressed as the natural log of the offered share) with a penalty for deviating from a fairness benchmark calculated from token contributions. A single learnable parameter (lambda_param) scales the penalty, capturing sensitivity to fairness deviations.",v1,,,,,,,0.8608995055779909,,,
5,,"Let F = (token_self / (token_self + token_opp)) × combined_earning
Let d = (split_self / F) − 1
U_accept = split_self × (1 − kappa × |d|^(psi))","A normalized discrepancy model for responder behavior. The model computes a fairness benchmark from token contributions and combined earnings and derives a normalized deviation of the offered share from this benchmark. The utility of accepting an offer is given by the offered monetary amount, reduced multiplicatively by a penalty that grows nonlinearly with the normalized deviation. Two learnable parameters (kappa and psi) scale and shape the penalty.",v1,,,,,,,,0.3130462565295785,0.5760720272279299,
6,,"Let F = (token_self / (token_self + token_opp)) * combined_earning  
Let d = split_self - F  
U_accept = ln(split_self) - [ theta * (max(-d, 0) / combined_earning) + delta * (max(d, 0) / combined_earning) ]","A fairness-based utility model for responder behavior. The model computes the fairness benchmark (F) from token contributions and combined earnings, and then derives the subjective utility of accepting an offer as the logarithm of the monetary offer (split_self) reduced by normalized fairness penalties. Asymmetric penalties for disadvantageous (θ) versus advantageous (δ) deviations from fairness are applied, capturing potential loss aversion and guilt effects.",v1,,,,0.03144991294337794,,,,,,0.7483170608279727
7,,"Let F = (token_self / (token_self + token_opp)) * combined_earning  
U_accept = split_self - kappa * max(F - split_self, 0)","The model computes a fairness benchmark from token contributions and total earnings, then assigns the utility of an accepted offer as the offered monetary amount decreased by a penalty for disadvantageous deviations from this fairness benchmark. The penalty is active only when the offer is below the fair share, and its magnitude is scaled by a single learnable parameter (kappa).",v1,,,,,,,,,,
8,,"Let F = (token_self / (token_self + token_opp)) × combined_earning  
Let d = ln(split_self / F)  
U_accept = ln(split_self) - kappa × |d|^(phi)","This model computes the utility of a responder accepting an offer as the logarithm of the offered monetary share minus a fairness penalty. The fairness benchmark is derived from the participant's token contribution relative to the total and the combined earnings. The penalty is a nonlinear function of the log deviation between split_self and the fairness benchmark, scaled by two learnable parameters: kappa (penalty strength) and phi (nonlinearity).",v1,,,,,0.6153487943926133,,,0.7715175459586632,,
9,,"Let F = (token_self / (token_self + token_opp)) × combined_earning
Let d = |split_self − F| / F
U_accept = ln(split_self) − kappa × ln(1 + d^(phi))",Utility of accepting an offer is defined as the log-transformed monetary share minus a fairness penalty. The fairness benchmark is computed from the participant’s relative token contribution and combined earnings; the normalized deviation between the offer and this benchmark is penalized via a logarithmic function scaled by two learnable parameters (kappa and phi).,v1,,,,,0.37299197656048155,,,0.4523061799695681,,
10,,"Let 
 F = (token_self / (token_self + token_opp)) × combined_earning
 R = (split_self / F) − 1
Then, the utility of accepting an offer is given by:
 U_accept = ln(split_self) + beta × tanh(alpha × R)",This model computes the utility of accepting an offer by combining the logarithmic valuation of the monetary share with a fairness bonus/penalty. The fairness benchmark is derived from the participant’s token contribution relative to the total and the combined earnings. The relative deviation between the received offer and the benchmark is transformed via a hyperbolic tangent function scaled by a sensitivity parameter (alpha) and further weighted by a magnitude parameter (beta). Two learnable parameters (alpha and beta) modulate the influence of fairness on utility.,v1,0.2563767578005124,0.5810457696862029,,,,,,,,
