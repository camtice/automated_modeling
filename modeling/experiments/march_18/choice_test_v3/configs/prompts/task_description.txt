Participants played 320 games (in four blocks of 80 games) of our Horizon task (see Figure
1A). Each game lasted either five or 10 trials and the two game lengths were interleaved and
counter-balanced such that there were 160 games of each length.
In each game, participants made repeated decisions between two options. Each option paid
out between 1 and 100 points that was sampled (rounded to the nearest integer) from a
Gaussian distribution with a fixed standard deviation of 8 points. The generative means of
the underlying Gaussians were different for the two options and remained stable within a
game. In each game, the mean of one option was set to either 40 or 60 points and the mean
of the other was set relative to the mean of the first, such that the difference between the two
was sampled from 4, 8, 12, 20, and 30. Both the identity and the difference in means were
counterbalanced over the entire experiment.
Participants were instructed in the task with the use of a set of illustrated onscreen
instructions. These explicitly conveyed that the means of the two options were constant over
a game and that the variability in the options was constant over the entire experiment.
Participants were told to maximize the points they earned and that one option was always
better on average. The full text of the instructions is provided in the Supplementary Material.
Choice and outcome history in each game remained onscreen inside each of the slot
machines (Figure 1A). After a particular option was played, the reward on that trial was
added to the slot machine, whereas the corresponding space for the unplayed option was
filled with an “XX.”
The first four trials of each game were forced-choice trials, in which only one of the options
(cued by a green square inside the next available space) was available for participants to
choose. We used these forced-choice trials to manipulate the information participants had
about the two options from experience (Hertwig, Barron, Weber, & Erev, 2004) before their
first free choice, while maintaining their active engagement in the task. The four forcedchoice trials set up two information conditions: “unequal information” (or [1 3]), in which
one option was forced to be played once and the other three times, and “equal information”
(or [2 2]), in which each option was forced to be played twice.
Crucially, this manipulation ensured that participants were exposed to a specified amount of
information about each option, regardless of how rewarding it was. Furthermore, the relative
amount of information provided about each option was independent of the relative difference
in their means. Thus on the first free choice (the fifth trial in each game), the difference in
the number of times each option had been sampled (and hence the difference in available
information) had no effect on the difference in mean payout of that option (repeatedmeasures ANOVA,
F(2, 89) = 0.09,
p = .91) thus removing the reward–information
confound on this trial.
After the forced-choice trials, participants made either one or six free choices (Figure 1B).
At the beginning of each game, the number of upcoming free-choice trials (i.e., the horizon)


Below is the description of each variable in the dataset, which will be followed by information about the data type of the variable:

participant = [Unique identifier for each human subject in the experiment]
task = [Identifier for the specific game/round the participant is playing]
trial = [Sequential number indicating which decision point this is within a game]
choice = [The option (0 or 1) that the participant selected on this trial]
reward = [The number of points (1-100) received after making a choice]
RT = [Reaction time measuring how long it took the participant to make their decision]
expected_reward0 = [The observed mean reward for option 0 based on previous forced/free plays]
expected_reward1 = [The observed mean reward for option 1 based on previous forced/free plays]
forced_choice = [Boolean indicator of whether this was a forced-choice trial (1) or free-choice trial (0)]
horizon = [The number of free choices available in the current game (either 1 or 6)]

Dataset:
{dataset_info}