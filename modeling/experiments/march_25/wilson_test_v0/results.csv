run_number,average_bic,bic_control,bic_cocaine,model_specification,model_summary,version,tau_recovery,bias_recovery,beta_recovery,gamma_recovery,phi_recovery
2,3253.066033327937,,,p(choice=1) = 1/(1 + exp(- beta * ( (expected_reward1 - expected_reward0) + gamma * (horizon - 1) * (1/sqrt(n1) - 1/sqrt(n0)) + bias ) )),A binary choice model in which the utility of each option is the sum of its observed mean reward and an exploration bonus that is inversely proportional to the square root of its sampling count. The bonus is scaled by the horizon (with no bonus when the horizon is 1) and weighted by a learnable exploration parameter gamma. The decision is made via a logistic function applying an inverse temperature beta and an overall bias term.,v0,,0.5286476525175523,-0.02459174206593453,0.11164905090436766,
1,3253.8775861277345,,,p = 1/(1+exp(-(tau*(expected_reward1-expected_reward0)+bias))),"A logistic-choice model where the difference between the observed expected rewards drives a probabilistic decision. Two learnable parameters—an inverse temperature (tau) and a bias—are combined additively, then transformed via a logistic function to predict binary free-choice behavior.",v0,0.26761185847749486,0.15095235663896156,,,
3,3300.1803255841537,,,p(choice=1) = 1/(1 + exp(- beta * ( (expected_reward1 - expected_reward0) + I(horizon=6)*phi*(I_forced_option1 - I_forced_option0) ))),The model computes the decision value as the difference in expected rewards between options augmented by an exploration bonus that is only applied in long-horizon (horizon = 6) games and is based on which option was under-sampled during forced-choice trials. The decision value is scaled by an inverse temperature parameter and transformed via a logistic function to predict the probability of choosing option 1.,v0,,,0.650746442340378,,0.019761473268130905
