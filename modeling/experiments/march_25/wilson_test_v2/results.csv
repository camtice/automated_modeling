run_number,average_bic,bic_control,bic_cocaine,model_specification,model_summary,version,beta_recovery,eta_recovery,xi_recovery,tau_recovery
1,3255.256066831293,,,p = 1/(1 + exp(- (beta*(expected_reward1 - expected_reward0) + eta*H))),"A logistic choice model where the probability of a free-choice decision for option 1 is determined by the weighted difference in observed expected rewards and an exploration bonus that is active in long-horizon (6 free choices) games, using two learnable parameters: a reward sensitivity (beta) and an exploration bonus (eta).",v2,0.0835187654056462,0.1279886622324469,,
2,3255.4481297021944,,,"U_0 = expected_reward0 + xi * (horizon - 1) / sqrt(N_0)
U_1 = expected_reward1 + xi * (horizon - 1) / sqrt(N_1)
P(choice=1) = 1/(1+exp(-tau*(U_1-U_0)))","A binary choice model that computes option utilities from observed expected rewards and an uncertainty bonus scaled by the gameâ€™s horizon. The exploration bonus is inversely proportional to the square-root of the forced-choice sample counts, and the difference in utilities drives a logistic choice function with an inverse temperature parameter.",v2,,,0.037476590746245916,-0.07443905562318089
3,3364.847248124183,,,p = 1/(1+exp(- beta_inv * ((expected_reward1 - expected_reward0) + gamma * ((horizon - 1)/5))),A logistic choice model that combines the difference in expected rewards with an exploration bonus that is activated in long-horizon (free-choice=6) games. The bonus is scaled by the parameter gamma while decision stochasticity is controlled by the inverse temperature parameter beta_inv. The model predicts the binary choice behavior using a weighted sum of reward difference and horizon-modulated exploration.,v2,,,,
