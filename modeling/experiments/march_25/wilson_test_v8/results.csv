run_number,average_bic,bic_control,bic_cocaine,model_specification,model_summary,version,beta0_recovery,beta_value_recovery,beta_info_recovery,beta_bias_recovery,beta_reward_recovery,beta_explore_recovery,beta_exploit_recovery,exploration_weight_recovery
2,29568.15187868654,,,U = beta_bias + beta_reward*(expected_reward1 - expected_reward0) + beta_explore*(horizon - 1)*info_difference,"A linear utility model that combines exploitation and directed exploration. The utility is computed as a weighted sum of the difference in expected rewards and an exploration bonus that is activated when the forced-choice sampling is unequal and scaled by the free-choice horizon. Three parameters (baseline bias, reward sensitivity, and exploration bonus) modulate the decision utility, which is later transformed via a logistic function to predict binary free-choice behavior.",v8,,,,0.12252429475712123,-0.07065484479482553,-0.07200008065487196,,
1,34693.38636780188,,,U = beta0 + beta_value * (expected_reward1 - expected_reward0) + beta_info * H * ((1/n1) - (1/n0)),"A dual-component linear utility model that computes the latent utility difference for a binary decision based on the difference in observed expected rewards and an exploration bonus driven by differences in sampling uncertainty from forced-choice trials. The exploration bonus is active only in long-horizon games. Three learnable parameters (intercept, reward sensitivity, exploration bonus weight) with finite bounds ensure parameters can be reliably recovered.",v8,0.003078771728294705,0.14008067577754496,-0.12275194687796695,,,,,
3,40119.520489664406,,,U = beta_exploit * (expected_reward1 - expected_reward0) + beta_info * ((horizon - 1)/5) * (forced_count0 - forced_count1) + beta_bias,"A dual-process linear utility model for binary free choices that combines exploitation—computed as the weighted difference between the observed mean rewards of the two options—with directed exploration via an uncertainty bonus derived from differences in forced-choice sample counts. The exploration bonus is modulated by the horizon (number of free choices available), and an overall bias term is included. This utility is then passed to a logistic function to predict the binary choice.",v8,,,-0.04456085923723983,0.004541423565440527,,,-0.026666043407456434,
4,40358.28438507579,,,V = (expected_reward1 - expected_reward0) + exploration_weight * horizon * (1/sqrt(n1) - 1/sqrt(n0)),"A binary-choice utility model that combines an exploitation term (difference in expected rewards) with an exploration term (bonus proportional to the inverse square-root of the number of samples, reflecting uncertainty) scaled by the horizon. Only one parameter (exploration_weight) is learnable, ensuring a minimal parameter set and reducing covariance concerns.",v8,,,,,,,,-0.019228693192936165
