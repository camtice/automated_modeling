run_number,average_bic,bic_control,bic_cocaine,model_specification,model_summary,version,phi_param_recovery,beta_param_recovery,alpha_recovery,phi_recovery,fairness_aversion_recovery,gamma_recovery,fairness_sensitivity_recovery,acceptance_bias_recovery,beta_recovery,fairness_weight_recovery,bias_recovery,theta_d_recovery,theta_a_recovery,delta_recovery,alpha_param_recovery
9,30.17290403066776,,,"U = split_self + bias - fairness_weight * max( (token_self / (token_self + token_opp)) * combined_earning - split_self , 0 )","A utility model for responder behavior that computes the accept-action utility as the offered share adjusted by a baseline bias and penalized when the offer is lower than the fair share, determined from the participant’s relative token contribution to the combined earnings. The model contains two learnable parameters: fairness_weight and bias.",v2,,,,,,,,,,0.7698439762052295,0.8112253436632485,,,,
3,30.172974770665242,,,"U_accept = alpha + split_self - phi * max((combined_earning * (token_self / (token_self + token_opp))) - split_self, 0)","A utility model for responder behavior that compares the proposed monetary share (split_self) with a fair share computed from each participant’s token contributions and the trial’s combined earnings. The model applies a fairness penalty (scaled by the learnable parameter phi) when the offer is below the fair expectation, with an overall bias captured by alpha.",v2,,,0.9177875875727215,0.8607012620055148,,,,,,,,,,,
5,30.185236217491255,,,"U = split_self - fairness_sensitivity * max((token_self / (token_self + token_opp)) * combined_earning - split_self, 0) + acceptance_bias","This model computes the utility for accepting an offer by comparing the offered amount (split_self) to a fair share based on relative contributions (token_self and token_opp) and the total communal earnings (combined_earning). If the offer falls short of the fair share, a penalty scaled by the fairness_sensitivity parameter is applied, with an additional acceptance_bias offset. Only responder behavior is modeled, with two learnable parameters subject to finite bounds.",v2,,,,,,,0.7172202875912398,0.8724082025309003,,,,,,,
2,30.720522721702537,,,U_accept = split_self - phi_param * |split_self - ((token_self / (token_self + token_opp)) * combined_earning)| + beta_param,"This model computes the utility of accepting an offer by taking the monetary gain (split_self) and subtracting a penalty proportional to the absolute deviation from a fairness benchmark (the expected fair share derived from token contributions and combined earnings). A baseline bias parameter is added to capture any constant shift in preference. Two learnable parameters (phi_param and beta_param) govern fairness sensitivity and baseline bias, ensuring the model captures key fairness considerations in responder behavior.",v2,0.7702257270599007,0.8912757959228335,,,,,,,,,,,,,
13,31.682620984140165,,,"U = split_self - delta * max((token_self / (token_self + token_opp)) * combined_earning - split_self, 0)","This model computes the utility for the responder by subtracting a fairness penalty from the offered amount. The penalty is applied when the offer falls short of the fair share, where the fair share is based on the participant’s contribution relative to the total contribution. A single learnable parameter delta scales the penalty, capturing sensitivity to disadvantageous inequality.",v2,,,,,,,,,,,,,,0.8821342143773317,
4,32.49566872650589,,,"U = split_self - fairness_aversion * max( ( (gamma * token_self) / (gamma * token_self + token_opp) * combined_earning ) - split_self , 0 )",The model computes a subjective fair share based on the weighted token contributions (modulated by gamma) and the combined earnings. The utility of accepting an offer is defined as the monetary offer (split_self) minus a fairness penalty applied when the offer falls below this fair share. The penalty strength is determined by the fairness_aversion parameter.,v2,,,,,0.6230570656884261,0.5901918783534771,,,,,,,,,
12,33.30512983820434,,,"Let fair_share = (token_self / (token_self + token_opp)) × combined_earning.
Define d = split_self − fair_share.
Then, the utility of accepting an offer is given by:
  U = β + d      if d ≥ 0,
  U = β + (1 + γ) · d if d < 0.","A piecewise linear utility model for responder behavior that computes the utility of accepting an offer. The model first calculates a fairness reference based on the ratio of the participant’s tokens to total tokens, scaled by the combined earning. It then assesses the deviation (d) between the offered amount (split_self) and this fair share. Offers meeting or exceeding the fair share add d linearly to a bias parameter (beta); offers below the fair share incur an amplified penalty proportional to a fairness sensitivity parameter (gamma). The model uses two learnable parameters: beta and gamma, with generous bounds.",v2,,,,,,0.8079668199247795,,,0.8636359722982074,,,,,,
7,35.00456495888595,,,"Let 
  fair_share = (token_self / (token_self + token_opp)) × combined_earning.
Then, the utility of accepting an offer is computed as:
  U = split_self − alpha × max(fair_share − split_self, 0) − beta × max(split_self − fair_share, 0).","A fairness-based utility model for responder behavior where the fair share is determined by the participant’s contribution relative to the opponent. The utility of accepting an offer is the monetary gain (split_self) decreased by penalties for deviating from the fair share. Two learnable parameters, alpha and beta, modulate the sensitivity to disadvantageous and advantageous inequity, respectively.",v2,,,0.9291693316265257,,,,,,0.6436541782004102,,,,,,
14,35.00456495896045,,,"Let F = combined_earning * (token_self / (token_self + token_opp))
U_accept = split_self - alpha * max(F - split_self, 0) - beta * max(split_self - F, 0)",This model computes the utility of accepting a responder’s offer by comparing the offered monetary share (split_self) to the fair share computed from individual token contributions. Deviations from fairness incur a disutility proportional to either a disadvantageous (alpha) or advantageous (beta) inequity penalty. Two learnable parameters (alpha and beta) capture the sensitivity to fairness deviations.,v2,,,0.9091446040539322,,,,,,0.7064201371301806,,,,,,
8,35.0045649590039,,,"U_accept = split_self - [ alpha * max( (token_self / (token_self + token_opp)) * combined_earning - split_self, 0 ) + beta * max( split_self - (token_self / (token_self + token_opp)) * combined_earning, 0 ) ]","A fairness-based inequality aversion model in which the utility for the responder to accept an offer is computed as the monetary share offered minus a penalty for deviations from a fairness benchmark derived from the comparative token contributions. Two learnable parameters (alpha and beta) capture sensitivity to disadvantageous and advantageous inequalities, respectively.",v2,,,0.9113375991304169,,,,,,0.6488040767656224,,,,,,
10,35.004564959261664,,,"U_accept = split_self - alpha * max((combined_earning * (token_self / (token_self + token_opp)) - split_self), 0) - beta * max((split_self - (combined_earning * (token_self / (token_self + token_opp)))), 0)",A utility model for responder behavior that computes the utility of accepting an offer. The model calculates a fair share based on token contributions and penalizes deviations from this fair outcome by two learnable parameters: one for disadvantageous inequity (alpha) and one for advantageous inequity (beta). This formulation is directly applicable to the provided dataset and captures key fairness considerations in social decision-making.,v2,,,0.9342159521062993,,,,,,0.7545115799506374,,,,,,
11,35.004564959602554,,,"Let f = (token_self / (token_self + token_opp)) × combined_earning  
U = split_self − theta_d × max(f − split_self, 0) − theta_a × max(split_self − f, 0)","A utility model for responder behavior that computes the subjective value of an accepted offer by comparing the observed monetary gain (split_self) to a fairness benchmark (f) derived from token contributions and combined earnings. The model imposes distinct linear penalties for disadvantageous and advantageous inequity using two learnable parameters (theta_d and theta_a) with finite bounds, capturing aversion to deviations from fair shares.",v2,,,,,,,,,,,,0.8859292803733276,0.6752483702703177,,
15,35.00456495976901,,,"Let F = (token_self / (token_self + token_opp)) × combined_earning
U_accept = split_self − alpha_param × max(F − split_self, 0) − beta_param × max(split_self − F, 0)",A utility model for responder behavior that computes the utility of accepting an offer by comparing the monetary gain (split_self) with a fair share determined by the relative token contributions. The model penalizes deviations from this fairness benchmark asymmetrically using two learnable parameters: alpha_param for offers below the fair share and beta_param for offers above it.,v2,,0.7163638784035093,,,,,,,,,,,,,0.8893109199977501
1,,,,U = split_self + beta - kappa * | split_self - (combined_earning * token_self / (token_self + token_opp)) |,"This model computes the utility for accepting an offer by combining the direct monetary benefit (split_self) with a penalty for deviation from a fair share computed from the participant’s token contribution relative to the total tokens. Two learnable parameters are included: beta, a baseline bias, and kappa, a fairness sensitivity that scales the penalty. The structure is guided by fairness considerations and is calibrated using observable variables in the dataset.",v2,,,,,,,,,,,,,,,
6,,,,"U_accept = split_self - beta * max(((token_self/(token_self + token_opp)) * combined_earning) - split_self, 0)","The model computes the utility of accepting an offer as the monetary gain (split_self) reduced by a penalty proportional to the shortfall from a fairness reference value. This fair share is derived from the participant’s token contribution relative to the total tokens and the combined earnings. The only learnable parameter, beta, controls sensitivity to unfairness, resulting in lower utility when offers are perceived as below the fair share.",v2,,,,,,,,,,,,,,,
