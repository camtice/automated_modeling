{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. If there are no previous models, start with one learnable parameter, then ensure the space of possible models is thoroughly explored with one parameter before moving to two parameters, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: Utility of accepting an offer combines linear monetary value and proportional fairness. The fairness term quantifies how much the proposed percentage deviates from the contribution-based equitable percentage (tokens_self / total_tokens). Two parameters control sensitivity to absolute monetary gains (alpha) and fairness violations (beta).\nBIC: 35.35895699690902\nParameter Recovery:\n- alpha: r = 0.625\n- beta: r = 0.856\nModel 2:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self)\nSummary: A linear utility model combining absolute monetary gain and contribution-based fairness. The utility of accepting an offer depends on the proposed amount (split_self) and its deviation from the participant's equitable contribution (3 * token_self). Two parameters control sensitivity to money (alpha) and fairness violations (beta), where beta < 0 indicates aversion to offers below one's contribution. The model directly anchors fairness expectations to pretask performance.\nBIC: 30.777436411868965\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.903\nModel 3:\nSpecification: U_accept = alpha * split_self + beta * ((split_self - 3 * token_self) / (3 * token_self))\nSummary: A utility model combining absolute monetary gain and proportional fairness evaluation. The utility of accepting an offer depends linearly on the proposed amount (split_self) and the proportional deviation of this amount from the participant's equitable contribution (3 * token_self). The proportional term captures sensitivity to relative fairness violations, where offers below one's contribution are penalized according to their percentage shortfall. Two parameters control sensitivity to money (alpha) and proportional fairness (beta).\nBIC: 32.24846051603598\nParameter Recovery:\n- alpha: r = 0.767\n- beta: r = 0.562\nModel 4:\nSpecification: U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0)\nSummary: A utility model based on inequity aversion, where the utility of accepting an offer depends on the absolute monetary gain (split_self) and aversion to both disadvantageous (split_opp > split_self) and advantageous (split_self > split_opp) inequality. Two learnable parameters (alpha, beta) quantify sensitivity to these inequalities. The model captures how responders evaluate offers relative to the proposer's share, penalizing unequal outcomes even when beneficial.\nBIC: 34.61528543210206\nParameter Recovery:\n- alpha: r = 0.827\n- beta: r = -0.027\nModel 5:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).\nBIC: 32.20179038739441\nParameter Recovery:\n- alpha: r = 0.750\n- beta: r = 0.855\nModel 6:\nSpecification: U_accept = alpha * split_self + beta * max(3 * token_self - split_self, 0)\nSummary: A utility model combining absolute monetary gain with loss aversion relative to contribution-based fairness. The utility of accepting an offer depends linearly on the proposed amount (split_self) and incorporates a penalty for offers below the participant's equitable contribution (3 * token_self). The penalty term activates only when split_self < 3 * token_self, capturing asymmetric sensitivity to disadvantageous inequality. Two parameters control sensitivity to money (alpha \u2265 0) and fairness violations (beta \u2264 0), with beta quantifying aversion to offers below one's contribution.\nBIC: 31.092498427958233\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.836\nModel 7:\nSpecification: U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\nSummary: A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the participant's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3 * token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\nBIC: 30.92992704863568\nParameter Recovery:\n- alpha: r = 0.868\n- beta: r = 0.865\nModel 8:\nSpecification: U_accept = alpha * split_self + beta * ((splitperc_self - (token_self / (token_self + token_opp)) * 100) / ((token_self / (token_self + token_opp)) * 100))\nSummary: A utility model combining absolute monetary gain with proportional fairness evaluation relative to contribution-based equity. The utility of accepting depends on the proposed amount (split_self) and the proportional deviation of the proposed percentage (splitperc_self) from the participant's fair share percentage (token_self / total_tokens * 100). Two parameters control sensitivity to money (alpha) and relative fairness violations (beta), capturing how responders weigh personal gain against deviations from their entitled share.\nBIC: 32.485483241059136\nParameter Recovery:\n- alpha: r = 0.720\n- beta: r = 0.551\nModel 9:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) / (3 * (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain with contribution-based fairness violations normalized by total earnings. The utility of accepting depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), scaled by the total combined earnings (3*(token_self + token_opp)). This captures sensitivity to both personal gain and fairness violations relative to the total stake, where smaller pots amplify the impact of inequity. Two parameters control sensitivity to money (alpha) and normalized fairness (beta).\nBIC: 38.949711847331244\nParameter Recovery:\n- alpha: r = 0.864\n- beta: r = 0.474\nModel 10:\nSpecification: U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) + max(split_opp - 3 * token_opp, 0))\nSummary: A utility model combining absolute monetary gain with aversion to two fairness violations: the responder receiving less than their contribution (3 * token_self) and the proposer taking more than their contribution (3 * token_opp). The model captures loss aversion relative to one's own contribution and punishment of proposer greed, with two parameters controlling sensitivity to money (alpha \u2265 0) and combined fairness violations (beta \u2264 0). Offers are penalized when split_self < 3*token_self (disadvantageous inequity) or split_opp > 3*token_opp (proposer overcompensation), weighted by a single fairness sensitivity parameter.\nBIC: 31.017025036615987\nParameter Recovery:\n- alpha: r = 0.795\n- beta: r = 0.844",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. If there are no previous models, start with one learnable parameter, then ensure the space of possible models is thoroughly explored with one parameter before moving to two parameters, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: Utility of accepting an offer combines linear monetary value and proportional fairness. The fairness term quantifies how much the proposed percentage deviates from the contribution-based equitable percentage (tokens_self / total_tokens). Two parameters control sensitivity to absolute monetary gains (alpha) and fairness violations (beta).\nBIC: 35.35895699690902\nParameter Recovery:\n- alpha: r = 0.625\n- beta: r = 0.856\nModel 2:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self)\nSummary: A linear utility model combining absolute monetary gain and contribution-based fairness. The utility of accepting an offer depends on the proposed amount (split_self) and its deviation from the participant's equitable contribution (3 * token_self). Two parameters control sensitivity to money (alpha) and fairness violations (beta), where beta < 0 indicates aversion to offers below one's contribution. The model directly anchors fairness expectations to pretask performance.\nBIC: 30.777436411868965\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.903\nModel 3:\nSpecification: U_accept = alpha * split_self + beta * ((split_self - 3 * token_self) / (3 * token_self))\nSummary: A utility model combining absolute monetary gain and proportional fairness evaluation. The utility of accepting an offer depends linearly on the proposed amount (split_self) and the proportional deviation of this amount from the participant's equitable contribution (3 * token_self). The proportional term captures sensitivity to relative fairness violations, where offers below one's contribution are penalized according to their percentage shortfall. Two parameters control sensitivity to money (alpha) and proportional fairness (beta).\nBIC: 32.24846051603598\nParameter Recovery:\n- alpha: r = 0.767\n- beta: r = 0.562\nModel 4:\nSpecification: U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0)\nSummary: A utility model based on inequity aversion, where the utility of accepting an offer depends on the absolute monetary gain (split_self) and aversion to both disadvantageous (split_opp > split_self) and advantageous (split_self > split_opp) inequality. Two learnable parameters (alpha, beta) quantify sensitivity to these inequalities. The model captures how responders evaluate offers relative to the proposer's share, penalizing unequal outcomes even when beneficial.\nBIC: 34.61528543210206\nParameter Recovery:\n- alpha: r = 0.827\n- beta: r = -0.027\nModel 5:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).\nBIC: 32.20179038739441\nParameter Recovery:\n- alpha: r = 0.750\n- beta: r = 0.855\nModel 6:\nSpecification: U_accept = alpha * split_self + beta * max(3 * token_self - split_self, 0)\nSummary: A utility model combining absolute monetary gain with loss aversion relative to contribution-based fairness. The utility of accepting an offer depends linearly on the proposed amount (split_self) and incorporates a penalty for offers below the participant's equitable contribution (3 * token_self). The penalty term activates only when split_self < 3 * token_self, capturing asymmetric sensitivity to disadvantageous inequality. Two parameters control sensitivity to money (alpha \u2265 0) and fairness violations (beta \u2264 0), with beta quantifying aversion to offers below one's contribution.\nBIC: 31.092498427958233\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.836\nModel 7:\nSpecification: U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\nSummary: A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the participant's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3 * token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\nBIC: 30.92992704863568\nParameter Recovery:\n- alpha: r = 0.868\n- beta: r = 0.865\nModel 8:\nSpecification: U_accept = alpha * split_self + beta * ((splitperc_self - (token_self / (token_self + token_opp)) * 100) / ((token_self / (token_self + token_opp)) * 100))\nSummary: A utility model combining absolute monetary gain with proportional fairness evaluation relative to contribution-based equity. The utility of accepting depends on the proposed amount (split_self) and the proportional deviation of the proposed percentage (splitperc_self) from the participant's fair share percentage (token_self / total_tokens * 100). Two parameters control sensitivity to money (alpha) and relative fairness violations (beta), capturing how responders weigh personal gain against deviations from their entitled share.\nBIC: 32.485483241059136\nParameter Recovery:\n- alpha: r = 0.720\n- beta: r = 0.551\nModel 9:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) / (3 * (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain with contribution-based fairness violations normalized by total earnings. The utility of accepting depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), scaled by the total combined earnings (3*(token_self + token_opp)). This captures sensitivity to both personal gain and fairness violations relative to the total stake, where smaller pots amplify the impact of inequity. Two parameters control sensitivity to money (alpha) and normalized fairness (beta).\nBIC: 38.949711847331244\nParameter Recovery:\n- alpha: r = 0.864\n- beta: r = 0.474\nModel 10:\nSpecification: U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) + max(split_opp - 3 * token_opp, 0))\nSummary: A utility model combining absolute monetary gain with aversion to two fairness violations: the responder receiving less than their contribution (3 * token_self) and the proposer taking more than their contribution (3 * token_opp). The model captures loss aversion relative to one's own contribution and punishment of proposer greed, with two parameters controlling sensitivity to money (alpha \u2265 0) and combined fairness violations (beta \u2264 0). Offers are penalized when split_self < 3*token_self (disadvantageous inequity) or split_opp > 3*token_opp (proposer overcompensation), weighted by a single fairness sensitivity parameter.\nBIC: 31.017025036615987\nParameter Recovery:\n- alpha: r = 0.795\n- beta: r = 0.844\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer in the responder role, based on the given dataset. The model needs to adhere to the constraints provided, such as using only the responder behavior, having no more than two learnable parameters, and avoiding parameters for stochasticity.\n\nFirst, I need to consider the psychological principles that might influence a responder's decision. Previous models have explored concepts like inequity aversion, contribution-based fairness, and comparisons between proposed shares and equitable contributions. The existing models have parameters for sensitivity to monetary gain (alpha) and fairness (beta), often in various forms. \n\nLooking at the previous models, Models 2, 6, 7, and 10 incorporate the participant's contribution (3 * token_self) as a reference point. Model 10 even considers both the responder's and proposer's contributions. However, the best-performing models in terms of BIC and parameter recovery are Models 2, 6, 7, and 10. Model 7 introduces the idea of penalizing the proposer's greed by checking if their share exceeds their contribution. This seems to have good parameter recovery for both alpha and beta.\n\nBut maybe there's a different angle. Let's think about the relative comparison between the split percentages and the contribution percentages. The split percentage (splitperc_self) versus the contribution percentage (token_self / (token_self + token_opp) * 100). If the proposed percentage is less than the contribution percentage, that's a fairness violation. But how does this compare to using absolute values (like split_self vs 3*token_self)?\n\nAnother angle is the ratio of the proposed share to the contribution. For example, if the participant contributed 40% of the tokens, their \"fair\" share would be 40% of the combined earnings. If the proposed percentage is 30%, that's a 25% reduction (30/40 = 0.75). Maybe the utility is sensitive to this ratio. \n\nBut the problem is that the previous models have already tried proportional fairness terms. For example, Model 8 uses ((splitperc_self - fair_percentage)/fair_percentage), which is a proportional deviation. However, its parameter recovery for beta was lower (0.551). So maybe that's not the best approach.\n\nAlternatively, considering the difference between the proposed percentage and the fair percentage, scaled by the fair percentage. That's similar to Model 8. But perhaps a different scaling could help. Or maybe using a non-linear function, like a logistic curve, but that might complicate things and require more parameters.\n\nWait, the task says the model should have no more than two learnable parameters. So we need to keep it simple. Let's look at the variables again. The key variables are split_self (the amount offered to the participant), token_self (their contribution), token_opp (opponent's contribution), combined_earning (total money), splitperc_self (percentage offered), etc.\n\nThe combined_earning is 3*(token_self + token_opp). So split_self is splitperc_self/100 * combined_earning. But split_self is already in the dataset. So maybe using split_self and splitperc_self is redundant. \n\nThe fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. Which is 3*token_self. Because combined_earning is 3*(tokens), so (token_self / total_tokens) * 3*total_tokens = 3*token_self. So split_self compared to 3*token_self is the same as comparing the proposed amount to their contribution-based fair share. \n\nSo Models 2, 6, 7, 10 use this 3*token_self as a reference. Model 2 uses split_self - 3*token_self, which is the deviation from fair share. Model 6 uses max(3*token_self - split_self, 0), which only penalizes when the offer is below the fair share. Model 7 looks at the proposer's share exceeding their contribution. Model 10 combines both.\n\nBut perhaps there's a way to combine both the participant's and proposer's fairness in a single term. For example, the participant's fairness is split_self - 3*token_self, and the proposer's is split_opp - 3*token_opp. But split_opp is the amount the proposer is taking, so if split_opp > 3*token_opp, that's the proposer taking more than their contribution. \n\nBut in the responder's role, the proposer's split_opp is their own share. So if the proposer is keeping more than their contribution, that's unfair. The responder might be averse to that. So the utility could be split_self (monetary gain) plus a term that penalizes when the proposer's share exceeds their contribution. \n\nModel 7 does exactly this: beta * max(split_opp - 3*token_opp, 0). So when the proposer's share is more than their contribution, the utility decreases. This model has good BIC and parameter recovery. But maybe combining both the participant's and proposer's fairness into a single parameter. However, that would require two parameters (one for each), but the model is limited to two parameters. \n\nWait, the current models have two parameters: alpha for split_self and beta for the fairness term. So if we want to capture both the participant's underpayment and the proposer's overpayment, but with a single beta parameter, that's possible. For example, the fairness term could be (split_self - 3*token_self) + (split_opp - 3*token_opp). But since split_self + split_opp = combined_earning, which is 3*(token_self + token_opp), then (split_self - 3t_self) + (split_opp -3t_opp) = (split_self + split_opp) -3(t_self + t_opp) = combined_earning -3(t_self + t_opp) = 0. Because combined_earning is exactly 3*(t_self + t_opp). So that sum is zero. Therefore, that approach might not work. \n\nAlternatively, maybe the difference between the two: (split_self -3t_self) - (split_opp -3t_opp). But since split_self + split_opp = 3(t_self + t_opp), then split_opp = 3(t_self + t_opp) - split_self. So substituting, the difference becomes (split_self -3t_self) - (3(t_self + t_opp) - split_self -3t_opp) = split_self -3t_self -3t_self -3t_opp + split_self +3t_opp = 2split_self -6t_self. Which simplifies to 2(split_self -3t_self). So that's just a scaled version of the participant's fairness term. So that approach might not add new information.\n\nAlternatively, perhaps the ratio of the participant's split to their contribution and the proposer's split to their contribution. For example, (split_self / (3t_self)) / (split_opp / (3t_opp)). If this ratio is less than 1, the participant is getting less than their contribution relative to the proposer. But this could be a multiplicative term, which might complicate things. Also, handling division by zero if t_self or t_opp is zero, but in the dataset, tokens are found in the pretask, so they should be positive.\n\nBut using ratios might introduce non-linearity. However, the model needs to be linear in parameters to keep it simple. So perhaps a better approach is to consider both the participant's underpayment and the proposer's overpayment as separate terms but combine them into a single parameter. For example, beta * [max(3t_self - split_self, 0) + max(split_opp -3t_opp, 0)]. This is what Model 10 does. However, Model 10 uses two max terms added together, multiplied by a single beta. This model has a BIC of 31.017, which is better than some others. Its parameter recovery for beta is 0.844, which is good. But maybe there's a way to improve upon this.\n\nAlternatively, perhaps the responder cares about the difference between their split percentage and their contribution percentage. For example, splitperc_self - (t_self/(t_self + t_opp))*100. If this is negative, the responder is getting less than their fair share. This is similar to Model 1. But Model 1's BIC is 35.35, which is worse than others. So maybe absolute terms are better than percentage terms.\n\nAnother angle: the responder's utility could depend on the ratio of their split to their contribution. For example, split_self / (3t_self). If this ratio is 1, it's fair. If less than 1, unfair. Then, the utility could be alpha * split_self + beta * (split_self / (3t_self)). But this introduces a non-linear term (split_self divided by t_self), but since t_self is a data variable, it's a known value. However, this might lead to issues if t_self is zero, but the dataset probably doesn't have that. Alternatively, using the ratio as a multiplier: split_self * (split_self / (3t_self)) = (split_self)^2 / (3t_self). But this introduces a quadratic term, which complicates the model and may not be linear in parameters.\n\nAlternatively, the model could use the difference between split_self and the fair share, scaled by the fair share. For example, (split_self - 3t_self)/(3t_self). This represents the proportional deviation from the fair share. This is similar to Model 3, which uses (split_self -3t_self)/(3t_self). Model 3's BIC is 32.24, which is higher than Models 2 and 6. So maybe this isn't the best approach.\n\nLooking back at the best-performing models, Model 2 has a BIC of 30.77 and parameter recovery of 0.836 and 0.903. Model 6 has BIC 31.09 and recovery 0.836 for both. Model 7 has BIC 30.93 and recovery 0.868 and 0.865. Model 10 has BIC 31.01 and recovery 0.795 and 0.844. So Model 7 is quite good. Its specification is U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0). The idea here is that responders dislike when the proposer takes more than their own contribution. So even if the responder's share is fair, if the proposer is being greedy, the responder might reject. This model seems to capture a different aspect of fairness.\n\nBut perhaps combining both the responder's underpayment and the proposer's overpayment into a single model. However, that would require two parameters (one for each term), but the model is allowed up to two parameters. Wait, the existing Model 10 uses a single beta for both terms. Let me check Model 10's specification: U_accept = alpha * split_self + beta * (max(3t_self - split_self, 0) + max(split_opp -3t_opp, 0)). So here, beta is a single parameter that scales both the responder's underpayment and the proposer's overpayment. But maybe these two terms should have separate parameters. However, the constraint is to have no more than two learnable parameters. So if we have alpha for split_self and beta for the combined term, that's two parameters. But if we split the fairness term into two separate parameters (beta1 and beta2), that would exceed the two-parameter limit. So Model 10 is using a single beta for both fairness violations.\n\nBut according to the BIC scores, Model 10 is slightly worse than Model 7. Model 7's BIC is 30.93, Model 10's is 31.01. So Model 7 is better. So perhaps focusing on the proposer's greed is a better predictor.\n\nAlternatively, maybe the responder cares about both their own split relative to their contribution and the proposer's split relative to theirs. But given the parameter limit, we can only have two terms. So alpha * split_self (monetary gain) and beta * (split_self - 3t_self) + gamma * (split_opp -3t_opp). But that would require three parameters. So that's not allowed.\n\nWait, but split_opp is equal to combined_earning - split_self. And combined_earning is 3*(t_self + t_opp). So split_opp = 3(t_self + t_opp) - split_self. Therefore, split_opp -3t_opp = 3t_self +3t_opp - split_self -3t_opp = 3t_self - split_self. So split_opp -3t_opp = 3t_self - split_self. Therefore, the term (split_opp -3t_opp) is equivalent to (3t_self - split_self). So the two terms (split_self -3t_self) and (split_opp -3t_opp) are negatives of each other. Therefore, including both would be redundant. For example, if split_self is less than 3t_self, then split_opp is more than 3t_opp. So the proposer's overpayment is directly related to the responder's underpayment. Therefore, models that include both terms are actually capturing the same underlying variable but in opposite directions. \n\nThis is an important insight. So if split_self < 3t_self, then split_opp = 3(t_self + t_opp) - split_self > 3t_opp. Therefore, the proposer's share exceeds their contribution exactly when the responder's share is less than their contribution. So the two fairness terms are perfectly negatively correlated. Therefore, including both terms in a model would lead to multicollinearity, making parameter recovery difficult. Hence, models that include both terms (like Model 10) might not be adding new information, but rather capturing the same effect twice. \n\nTherefore, focusing on either the responder's underpayment or the proposer's overpayment is sufficient. Models 2, 6, and 7 each focus on one of these aspects. Model 7, which penalizes proposer greed (split_opp >3t_opp), has good performance. But since split_opp >3t_opp is equivalent to split_self <3t_self, the two terms are interchangeable. So choosing one or the other is a matter of interpretation. \n\nBut perhaps responders are more sensitive to the proposer's greed than their own underpayment. For example, even if the responder's share is acceptable, if the proposer is taking more than their fair share, the responder might reject out of spite. This is a different psychological mechanism than just focusing on their own share. \n\nModel 7's specification is U_accept = alpha * split_self + beta * max(split_opp -3t_opp, 0). Here, beta is expected to be negative, as higher proposer overpayment reduces utility. The parameter recovery for beta in Model 7 is 0.865, which is good. So this model seems to capture a valid mechanism.\n\nBut perhaps combining the proposer's overpayment with the responder's underpayment in a way that's multiplicative. For example, the product of the two terms. But that would introduce a non-linear term and complicate the model. Also, with two parameters, it's not clear. \n\nAlternatively, the responder's utility could depend on both the absolute monetary gain and the ratio of the proposer's share to their contribution. For example, split_opp / (3t_opp). If this ratio is greater than 1, the proposer is taking more than their contribution. So the utility could be alpha * split_self + beta * (split_opp / (3t_opp)). But this ratio could be a continuous measure of proposer greed. However, this introduces a division by a variable, which might lead to scaling issues. But since t_opp is known, this could be manageable. \n\nAlternatively, using the proposer's split percentage (splitperc_opp) compared to their contribution percentage (t_opp/(t_self + t_opp)*100). The difference between these could be a fairness term. For example, splitperc_opp - (t_opp/(t_self + t_opp)*100). If this is positive, the proposer is taking a higher percentage than their contribution. This is similar to Model 1's approach but applied to the proposer's side. \n\nBut splitperc_opp is already in the dataset. However, using splitperc_opp might not add new information beyond split_opp and token_opp. Since split_opp = (splitperc_opp /100) * combined_earning, and combined_earning is 3*(t_self + t_opp), split_opp is directly calculable from splitperc_opp. So using splitperc_opp or split_opp is equivalent. \n\nBut the key is to find a model that hasn't been tried yet, given the previous models. Let's look at the previous models again. Models 1-10 have covered various combinations of split_self, split_opp, token_self, token_opp, and their relationships. \n\nAnother angle could be the difference between the proposed split percentages and the contribution percentages for both parties. For example, (splitperc_self - fair_percent_self) - (splitperc_opp - fair_percent_opp). But since splitperc_self + splitperc_opp = 100, and fair_percent_self + fair_percent_opp = 100, this difference would be (splitperc_self - fair_self) - (100 - splitperc_self - (100 - fair_self)) ) = 2*(splitperc_self - fair_self). So again, this reduces to twice the responder's percentage deviation. So it's redundant.\n\nAlternatively, the responder might compare their proposed percentage to the proposer's proposed percentage. For example, if splitperc_self is much lower than splitperc_opp, that's unfair. But since splitperc_self + splitperc_opp = 100, this is equivalent to splitperc_self being less than 50% if the contributions are equal. But if contributions are unequal, the fair percentage would differ. \n\nWait, the fair percentage for the responder is (token_self / (token_self + token_opp)) * 100. The proposer's fair percentage is similarly (token_opp / total) *100. So the difference between splitperc_self and the fair percentage is a measure of unfairness. Similarly for the proposer's split. \n\nBut again, since splitperc_opp = 100 - splitperc_self, and the fair percentages sum to 100, the difference between splitperc_self and fair_self is the negative of the difference between splitperc_opp and fair_opp. So they are perfectly negatively correlated. Therefore, including both terms would be redundant. \n\nGiven all this, perhaps the best approach is to consider a model that hasn't been tried yet, which combines the absolute monetary gain with a term that captures the proposer's greed relative to their contribution, but in a different mathematical form. \n\nWait, Model 7 uses max(split_opp -3t_opp, 0), which is the proposer's overpayment. But split_opp is the proposer's share. So when the proposer takes more than their contribution, the responder's utility decreases. This is a form of punishment for greed. \n\nAnother possibility is that the responder's utility is influenced by the ratio of the proposer's share to their contribution. For example, (split_opp / (3t_opp)) - 1. If this is positive, the proposer is taking more than their contribution. The utility could then be alpha * split_self + beta * ((split_opp / (3t_opp)) - 1). This term would be zero when the proposer takes exactly their contribution, positive when they take more, and negative when they take less. If beta is negative, this would penalize proposer greed. \n\nBut this introduces a division, which might lead to scaling issues. However, since 3t_opp is a fixed value per trial, this could be manageable. For example, if t_opp is 10, then 3t_opp is 30. If split_opp is 40, then (40/30 -1) = 0.333. So beta * 0.333 would reduce the utility. \n\nThis model hasn't been tried in the previous models. Let's see how this compares. The utility would be:\n\nU_accept = alpha * split_self + beta * (split_opp / (3 * token_opp) - 1)\n\nBut split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self. So substituting:\n\nsplit_opp/(3*token_opp) = [3(t_self + t_opp) - split_self]/(3t_opp) = (t_self + t_opp)/t_opp - split_self/(3t_opp) = 1 + t_self/t_opp - split_self/(3t_opp)\n\nSo the term becomes (1 + t_self/t_opp - split_self/(3t_opp)) -1 = t_self/t_opp - split_self/(3t_opp) = (3t_self - split_self)/(3t_opp)\n\nSo the model can be rewritten as:\n\nU_accept = alpha * split_self + beta * (3t_self - split_self)/(3t_opp)\n\nBut 3t_self is the responder's fair share. So (3t_self - split_self) is the deviation from the fair share. Then, dividing by 3t_opp scales this deviation by the proposer's contribution. \n\nThis is an interesting approach. The fairness term is the responder's underpayment (3t_self - split_self) scaled by the proposer's contribution. This means that the same underpayment is penalized more if the proposer contributed less. For example, if the proposer contributed few tokens (small t_opp), then the scaling factor (1/(3t_opp)) is larger, making the penalty term larger. This could model a sense of entitlement: if the proposer didn't contribute much, the responder is more upset about being underpaid. \n\nAlternatively, this could be interpreted as the responder's underpayment relative to the proposer's contribution. So if the proposer contributed little, the responder expects a larger share, and any underpayment is more salient. \n\nThis model would have two parameters: alpha (sensitivity to monetary gain) and beta (sensitivity to scaled underpayment). The term (3t_self - split_self) is the same as in Model 2 and 6, but scaled by 1/(3t_opp). \n\nThis introduces an interaction between the responder's underpayment and the proposer's contribution. For example, if the proposer contributed very little (small t_opp), then even a small underpayment (3t_self - split_self) would be amplified by the scaling factor. This could capture a psychological mechanism where responders are more sensitive to unfairness when the proposer hasn't contributed much. \n\nThis model hasn't been tried in the previous models, so it's a novel approach. Let's check if this fits within the constraints. The model uses split_self, token_self, and token_opp from the dataset. The parameters are alpha and beta. The utility is linear in parameters. The scaled term (3t_self - split_self)/(3t_opp) is a data-derived variable. \n\nThe mathematical form would be:\n\nU_accept = alpha * split_self + beta * ( (3 * token_self - split_self) / (3 * token_opp) )\n\nThis model uses two parameters, alpha and beta. The term (3t_self - split_self) is the underpayment, and dividing by 3t_opp scales it based on the proposer's contribution. \n\nIf beta is negative, then higher underpayment (when split_self <3t_self) reduces utility. The scaling by token_opp means that for a given underpayment, the penalty is larger when token_opp is smaller. \n\nThis model could explain behavior where responders are more sensitive to unfair offers when the proposer contributed less. For example, if the proposer found few tokens (low token_opp), the responder expects a larger share, so any deviation from their fair share (3t_self) is more impactful. \n\nThis model meets all the constraints: two parameters, uses available variables, no stochastic parameters, and focuses on responder behavior. \n\nNow, checking for covariance between parameters. If alpha and beta are both positive, they might be correlated, but the scaling term could help differentiate their effects. The parameter recovery would depend on how these terms vary across trials. \n\nIn terms of variable ranges, token_opp is an integer >=0 (but in practice, since it's from a pretask, it's at least 1). So 3*token_opp is at least 3. The term (3t_self - split_self) can be positive or negative. But since the model is for the utility of accepting, and we're not modeling reject utility, negative values of the fairness term would not penalize the utility. However, if split_self exceeds 3t_self, the term becomes negative, which would increase utility if beta is positive. But since beta is a learnable parameter, the model can adapt. However, if the hypothesis is that responders penalize underpayment, beta should be negative. So when split_self <3t_self, the term is positive, multiplied by a negative beta, leading to lower utility. When split_self >3t_self, the term is negative, multiplied by a negative beta, leading to higher utility. But this might not align with the intended direction. \n\nWait, if beta is negative, then:\n\nIf split_self <3t_self: (3t_self - split_self) is positive. Divided by 3t_opp (positive), the term is positive. Multiply by beta (negative) gives a negative contribution to utility. So lower utility.\n\nIf split_self >3t_self: (3t_self - split_self) is negative. Divided by 3t_opp (positive), the term is negative. Multiply by beta (negative) gives a positive contribution to utility. So higher utility.\n\nThis makes sense. So beta is expected to be negative, penalizing underpayment and rewarding overpayment. But responders might not care about overpayment (getting more than their fair share), so perhaps the model should only penalize underpayment. To do that, we can take the max of (3t_self - split_self, 0), similar to Model 6. \n\nSo modifying the model to:\n\nU_accept = alpha * split_self + beta * ( max(3 * token_self - split_self, 0) / (3 * token_opp) )\n\nThis way, only underpayment is penalized. The term is zero when split_self >=3t_self. This might make the model more aligned with loss aversion, where only losses (underpayment) are penalized. \n\nThis model would have two parameters: alpha (sensitivity to monetary gain) and beta (sensitivity to scaled underpayment). The scaling by token_opp introduces a novel interaction between the proposer's contribution and the responder's underpayment. \n\nThis model hasn't been tried in the previous models. Let's check the previous models again. Model 6 uses max(3t_self - split_self, 0) without scaling. Model 10 combines this with the proposer's overpayment. Model 7 uses the proposer's overpayment. This new model scales the underpayment by the proposer's contribution. \n\nThis could capture a scenario where the responder is more upset about being underpaid when the proposer contributed less. For example, if the proposer found only a few tokens, the responder expects them to take less, so any underpayment feels more unfair. \n\nIn terms of parameter recovery, if the scaling by token_opp introduces variability in the fairness term, it might help in distinguishing alpha and beta. For example, trials with the same underpayment but different token_opp would have different fairness terms, allowing beta to be estimated more accurately. \n\nThe mathematical form is:\n\nU_accept = \u03b1 * split_self + \u03b2 * ( max(3 * token_self - split_self, 0) / (3 * token_opp) )\n\nHere, \u03b1 is the sensitivity to the absolute monetary gain, and \u03b2 (expected to be negative) is the sensitivity to the scaled underpayment. \n\nThis model uses variables available in the dataset: split_self, token_self, token_opp. It has two learnable parameters, \u03b1 and \u03b2. The utility is calculated only for the responder's accept action. \n\nNow, checking the constraints:\n\n1. Predicts responder behavior: Yes, as trial_role is responder.\n2. Builds on psychological principles: Loss aversion scaled by proposer's contribution.\n3. Considers high covariance: The scaling by token_opp may reduce covariance between \u03b1 and \u03b2.\n4. Uses dataset variables: Yes.\n5. No stochastic parameters: Correct.\n6. Two learnable parameters: Yes.\n7. Utility can be negative: Yes.\n\nThe variable descriptions would be:\n\n- \u03b1: sensitivity to monetary gain (split_self), learnable, range could be [0, some upper bound], but since it's a linear term, perhaps [-inf, inf], but the model may need to bound it. However, the user says bounds should be generous. So maybe alpha in [-10, 10], inclusive.\n\n- \u03b2: sensitivity to scaled underpayment, learnable, expected to be negative. Bounds could be [-10, 10], inclusive.\n\nThe variables in the model:\n\n- split_self: from data.\n- token_self: from data.\n- token_opp: from data.\n- max(3*token_self - split_self, 0): calculated.\n- divided by 3*token_opp: calculated.\n\nSo the variable descriptions in JSON would include alpha and beta as learnable parameters with appropriate bounds. \n\nThis model introduces a new mechanism not present in previous models: scaling the underpayment by the proposer's contribution. This could explain variations in responder behavior based on the proposer's contribution level, which hasn't been explicitly modeled before. \n\nThe summary would highlight that the utility depends on the absolute monetary gain and the responder's underpayment scaled by the proposer's contribution, capturing how the proposer's contribution modulates sensitivity to unfairness.\n</think>\n\n<MODEL>\nU_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from the proposed split_self\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to scaled underpayment relative to contribution. Expected negative values indicate aversion to offers below 3*token_self, scaled by proposer's contribution (3*token_opp)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nUtility of accepting depends on absolute monetary gain (split_self) and contribution-scaled underpayment aversion. The fairness term penalizes offers below the participant's equitable contribution (3*token_self), scaled by the proposer's contribution (3*token_opp). This captures heightened sensitivity to unfairness when proposers contribute less, modulating penalty strength via reciprocal entitlement expectations.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return the utility values for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of utility values for each trial\n\n                Your code will be implemented within the following code block:\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    utility_values = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        utility_values.append(beta + (epsilon * E) + (eta * N))\n    return utility_values\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE UTLITY VALUES.\n                Model Specification:\n                U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from the proposed split_self\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to scaled underpayment relative to contribution. Expected negative values indicate aversion to offers below 3*token_self, scaled by proposer's contribution (3*token_opp)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 1) #avoid division by zero\n\n        if token_opp == 0:\n            utility = alpha * split_self\n        else:\n            utility = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))\n        utility_values.append(utility)\n    return utility_values\n\n# Data as list of dictionaries\ndata_json = \"\"\"\n[\n  {\"ID\": 1, \"group\": \"A\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 2, \"token_opp\": 10, \"token_self\": 20, \"combined_earning\": 90, \"split_opp\": 30, \"split_self\": 60, \"splitperc_opp\": 33, \"splitperc_self\": 67, \"accept\": 1, \"accepted_amount\": 60, \"accepted_perc\": 67, \"proposed_perc\": 67, \"proposed_amount\": 60},\n  {\"ID\": 2, \"group\": \"B\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 2, \"token_opp\": 5, \"token_self\": 15, \"combined_earning\": 60, \"split_opp\": 40, \"split_self\": 20, \"splitperc_opp\": 67, \"splitperc_self\": 33, \"accept\": 0, \"accepted_amount\": 0, \"accepted_perc\": 0, \"proposed_perc\": 33, \"proposed_amount\": 20},\n  {\"ID\":3, \"group\": \"A\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 2, \"token_opp\": 0, \"token_self\": 10, \"combined_earning\": 30, \"split_opp\": 20, \"split_self\": 10, \"splitperc_opp\": 67, \"splitperc_self\": 33, \"accept\": 1, \"accepted_amount\": 10, \"accepted_perc\": 33, \"proposed_perc\": 33, \"proposed_amount\": 10}\n]\n\"\"\"\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data, alpha=0.5, beta=-1)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```"
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to absolute monetary gain from the proposed split_self",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to scaled underpayment relative to contribution. Expected negative values indicate aversion to offers below 3*token_self, scaled by proposer's contribution (3*token_opp)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))",
  "variable_descriptions": {
    "U_accept": {
      "description": "Utility of accepting the offer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha": {
      "description": "Sensitivity to absolute monetary gain from the proposed split_self",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to scaled underpayment relative to contribution. Expected negative values indicate aversion to offers below 3*token_self, scaled by proposer's contribution (3*token_opp)",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform"
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed monetary amount for the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant in the pretask",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent in the pretask",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "Utility of accepting depends on absolute monetary gain (split_self) and contribution-scaled underpayment aversion. The fairness term penalizes offers below the participant's equitable contribution (3*token_self), scaled by the proposer's contribution (3*token_opp). This captures heightened sensitivity to unfairness when proposers contribute less, modulating penalty strength via reciprocal entitlement expectations.",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer in the responder role, based on the given dataset. The model needs to adhere to the constraints provided, such as using only the responder behavior, having no more than two learnable parameters, and avoiding parameters for stochasticity.\n\nFirst, I need to consider the psychological principles that might influence a responder's decision. Previous models have explored concepts like inequity aversion, contribution-based fairness, and comparisons between proposed shares and equitable contributions. The existing models have parameters for sensitivity to monetary gain (alpha) and fairness (beta), often in various forms. \n\nLooking at the previous models, Models 2, 6, 7, and 10 incorporate the participant's contribution (3 * token_self) as a reference point. Model 10 even considers both the responder's and proposer's contributions. However, the best-performing models in terms of BIC and parameter recovery are Models 2, 6, 7, and 10. Model 7 introduces the idea of penalizing the proposer's greed by checking if their share exceeds their contribution. This seems to have good parameter recovery for both alpha and beta.\n\nBut maybe there's a different angle. Let's think about the relative comparison between the split percentages and the contribution percentages. The split percentage (splitperc_self) versus the contribution percentage (token_self / (token_self + token_opp) * 100). If the proposed percentage is less than the contribution percentage, that's a fairness violation. But how does this compare to using absolute values (like split_self vs 3*token_self)?\n\nAnother angle is the ratio of the proposed share to the contribution. For example, if the participant contributed 40% of the tokens, their \"fair\" share would be 40% of the combined earnings. If the proposed percentage is 30%, that's a 25% reduction (30/40 = 0.75). Maybe the utility is sensitive to this ratio. \n\nBut the problem is that the previous models have already tried proportional fairness terms. For example, Model 8 uses ((splitperc_self - fair_percentage)/fair_percentage), which is a proportional deviation. However, its parameter recovery for beta was lower (0.551). So maybe that's not the best approach.\n\nAlternatively, considering the difference between the proposed percentage and the fair percentage, scaled by the fair percentage. That's similar to Model 8. But perhaps a different scaling could help. Or maybe using a non-linear function, like a logistic curve, but that might complicate things and require more parameters.\n\nWait, the task says the model should have no more than two learnable parameters. So we need to keep it simple. Let's look at the variables again. The key variables are split_self (the amount offered to the participant), token_self (their contribution), token_opp (opponent's contribution), combined_earning (total money), splitperc_self (percentage offered), etc.\n\nThe combined_earning is 3*(token_self + token_opp). So split_self is splitperc_self/100 * combined_earning. But split_self is already in the dataset. So maybe using split_self and splitperc_self is redundant. \n\nThe fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. Which is 3*token_self. Because combined_earning is 3*(tokens), so (token_self / total_tokens) * 3*total_tokens = 3*token_self. So split_self compared to 3*token_self is the same as comparing the proposed amount to their contribution-based fair share. \n\nSo Models 2, 6, 7, 10 use this 3*token_self as a reference. Model 2 uses split_self - 3*token_self, which is the deviation from fair share. Model 6 uses max(3*token_self - split_self, 0), which only penalizes when the offer is below the fair share. Model 7 looks at the proposer's share exceeding their contribution. Model 10 combines both.\n\nBut perhaps there's a way to combine both the participant's and proposer's fairness in a single term. For example, the participant's fairness is split_self - 3*token_self, and the proposer's is split_opp - 3*token_opp. But split_opp is the amount the proposer is taking, so if split_opp > 3*token_opp, that's the proposer taking more than their contribution. \n\nBut in the responder's role, the proposer's split_opp is their own share. So if the proposer is keeping more than their contribution, that's unfair. The responder might be averse to that. So the utility could be split_self (monetary gain) plus a term that penalizes when the proposer's share exceeds their contribution. \n\nModel 7 does exactly this: beta * max(split_opp - 3*token_opp, 0). So when the proposer's share is more than their contribution, the utility decreases. This model has good BIC and parameter recovery. But maybe combining both the participant's and proposer's fairness into a single parameter. However, that would require two parameters (one for each), but the model is limited to two parameters. \n\nWait, the current models have two parameters: alpha for split_self and beta for the fairness term. So if we want to capture both the participant's underpayment and the proposer's overpayment, but with a single beta parameter, that's possible. For example, the fairness term could be (split_self - 3*token_self) + (split_opp - 3*token_opp). But since split_self + split_opp = combined_earning, which is 3*(token_self + token_opp), then (split_self - 3t_self) + (split_opp -3t_opp) = (split_self + split_opp) -3(t_self + t_opp) = combined_earning -3(t_self + t_opp) = 0. Because combined_earning is exactly 3*(t_self + t_opp). So that sum is zero. Therefore, that approach might not work. \n\nAlternatively, maybe the difference between the two: (split_self -3t_self) - (split_opp -3t_opp). But since split_self + split_opp = 3(t_self + t_opp), then split_opp = 3(t_self + t_opp) - split_self. So substituting, the difference becomes (split_self -3t_self) - (3(t_self + t_opp) - split_self -3t_opp) = split_self -3t_self -3t_self -3t_opp + split_self +3t_opp = 2split_self -6t_self. Which simplifies to 2(split_self -3t_self). So that's just a scaled version of the participant's fairness term. So that approach might not add new information.\n\nAlternatively, perhaps the ratio of the participant's split to their contribution and the proposer's split to their contribution. For example, (split_self / (3t_self)) / (split_opp / (3t_opp)). If this ratio is less than 1, the participant is getting less than their contribution relative to the proposer. But this could be a multiplicative term, which might complicate things. Also, handling division by zero if t_self or t_opp is zero, but in the dataset, tokens are found in the pretask, so they should be positive.\n\nBut using ratios might introduce non-linearity. However, the model needs to be linear in parameters to keep it simple. So perhaps a better approach is to consider both the participant's underpayment and the proposer's overpayment as separate terms but combine them into a single parameter. For example, beta * [max(3t_self - split_self, 0) + max(split_opp -3t_opp, 0)]. This is what Model 10 does. However, Model 10 uses two max terms added together, multiplied by a single beta. This model has a BIC of 31.017, which is better than some others. Its parameter recovery for beta is 0.844, which is good. But maybe there's a way to improve upon this.\n\nAlternatively, perhaps the responder cares about the difference between their split percentage and their contribution percentage. For example, splitperc_self - (t_self/(t_self + t_opp))*100. If this is negative, the responder is getting less than their fair share. This is similar to Model 1. But Model 1's BIC is 35.35, which is worse than others. So maybe absolute terms are better than percentage terms.\n\nAnother angle: the responder's utility could depend on the ratio of their split to their contribution. For example, split_self / (3t_self). If this ratio is 1, it's fair. If less than 1, unfair. Then, the utility could be alpha * split_self + beta * (split_self / (3t_self)). But this introduces a non-linear term (split_self divided by t_self), but since t_self is a data variable, it's a known value. However, this might lead to issues if t_self is zero, but the dataset probably doesn't have that. Alternatively, using the ratio as a multiplier: split_self * (split_self / (3t_self)) = (split_self)^2 / (3t_self). But this introduces a quadratic term, which complicates the model and may not be linear in parameters.\n\nAlternatively, the model could use the difference between split_self and the fair share, scaled by the fair share. For example, (split_self - 3t_self)/(3t_self). This represents the proportional deviation from the fair share. This is similar to Model 3, which uses (split_self -3t_self)/(3t_self). Model 3's BIC is 32.24, which is higher than Models 2 and 6. So maybe this isn't the best approach.\n\nLooking back at the best-performing models, Model 2 has a BIC of 30.77 and parameter recovery of 0.836 and 0.903. Model 6 has BIC 31.09 and recovery 0.836 for both. Model 7 has BIC 30.93 and recovery 0.868 and 0.865. Model 10 has BIC 31.01 and recovery 0.795 and 0.844. So Model 7 is quite good. Its specification is U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0). The idea here is that responders dislike when the proposer takes more than their own contribution. So even if the responder's share is fair, if the proposer is being greedy, the responder might reject. This model seems to capture a different aspect of fairness.\n\nBut perhaps combining both the responder's underpayment and the proposer's overpayment into a single model. However, that would require two parameters (one for each term), but the model is allowed up to two parameters. Wait, the existing Model 10 uses a single beta for both terms. Let me check Model 10's specification: U_accept = alpha * split_self + beta * (max(3t_self - split_self, 0) + max(split_opp -3t_opp, 0)). So here, beta is a single parameter that scales both the responder's underpayment and the proposer's overpayment. But maybe these two terms should have separate parameters. However, the constraint is to have no more than two learnable parameters. So if we have alpha for split_self and beta for the combined term, that's two parameters. But if we split the fairness term into two separate parameters (beta1 and beta2), that would exceed the two-parameter limit. So Model 10 is using a single beta for both fairness violations.\n\nBut according to the BIC scores, Model 10 is slightly worse than Model 7. Model 7's BIC is 30.93, Model 10's is 31.01. So Model 7 is better. So perhaps focusing on the proposer's greed is a better predictor.\n\nAlternatively, maybe the responder cares about both their own split relative to their contribution and the proposer's split relative to theirs. But given the parameter limit, we can only have two terms. So alpha * split_self (monetary gain) and beta * (split_self - 3t_self) + gamma * (split_opp -3t_opp). But that would require three parameters. So that's not allowed.\n\nWait, but split_opp is equal to combined_earning - split_self. And combined_earning is 3*(t_self + t_opp). So split_opp = 3(t_self + t_opp) - split_self. Therefore, split_opp -3t_opp = 3t_self +3t_opp - split_self -3t_opp = 3t_self - split_self. So split_opp -3t_opp = 3t_self - split_self. Therefore, the term (split_opp -3t_opp) is equivalent to (3t_self - split_self). So the two terms (split_self -3t_self) and (split_opp -3t_opp) are negatives of each other. Therefore, including both would be redundant. For example, if split_self is less than 3t_self, then split_opp is more than 3t_opp. So the proposer's overpayment is directly related to the responder's underpayment. Therefore, models that include both terms are actually capturing the same underlying variable but in opposite directions. \n\nThis is an important insight. So if split_self < 3t_self, then split_opp = 3(t_self + t_opp) - split_self > 3t_opp. Therefore, the proposer's share exceeds their contribution exactly when the responder's share is less than their contribution. So the two fairness terms are perfectly negatively correlated. Therefore, including both terms in a model would lead to multicollinearity, making parameter recovery difficult. Hence, models that include both terms (like Model 10) might not be adding new information, but rather capturing the same effect twice. \n\nTherefore, focusing on either the responder's underpayment or the proposer's overpayment is sufficient. Models 2, 6, and 7 each focus on one of these aspects. Model 7, which penalizes proposer greed (split_opp >3t_opp), has good performance. But since split_opp >3t_opp is equivalent to split_self <3t_self, the two terms are interchangeable. So choosing one or the other is a matter of interpretation. \n\nBut perhaps responders are more sensitive to the proposer's greed than their own underpayment. For example, even if the responder's share is acceptable, if the proposer is taking more than their fair share, the responder might reject out of spite. This is a different psychological mechanism than just focusing on their own share. \n\nModel 7's specification is U_accept = alpha * split_self + beta * max(split_opp -3t_opp, 0). Here, beta is expected to be negative, as higher proposer overpayment reduces utility. The parameter recovery for beta in Model 7 is 0.865, which is good. So this model seems to capture a valid mechanism.\n\nBut perhaps combining the proposer's overpayment with the responder's underpayment in a way that's multiplicative. For example, the product of the two terms. But that would introduce a non-linear term and complicate the model. Also, with two parameters, it's not clear. \n\nAlternatively, the responder's utility could depend on both the absolute monetary gain and the ratio of the proposer's share to their contribution. For example, split_opp / (3t_opp). If this ratio is greater than 1, the proposer is taking more than their contribution. So the utility could be alpha * split_self + beta * (split_opp / (3t_opp)). But this ratio could be a continuous measure of proposer greed. However, this introduces a division by a variable, which might lead to scaling issues. But since t_opp is known, this could be manageable. \n\nAlternatively, using the proposer's split percentage (splitperc_opp) compared to their contribution percentage (t_opp/(t_self + t_opp)*100). The difference between these could be a fairness term. For example, splitperc_opp - (t_opp/(t_self + t_opp)*100). If this is positive, the proposer is taking a higher percentage than their contribution. This is similar to Model 1's approach but applied to the proposer's side. \n\nBut splitperc_opp is already in the dataset. However, using splitperc_opp might not add new information beyond split_opp and token_opp. Since split_opp = (splitperc_opp /100) * combined_earning, and combined_earning is 3*(t_self + t_opp), split_opp is directly calculable from splitperc_opp. So using splitperc_opp or split_opp is equivalent. \n\nBut the key is to find a model that hasn't been tried yet, given the previous models. Let's look at the previous models again. Models 1-10 have covered various combinations of split_self, split_opp, token_self, token_opp, and their relationships. \n\nAnother angle could be the difference between the proposed split percentages and the contribution percentages for both parties. For example, (splitperc_self - fair_percent_self) - (splitperc_opp - fair_percent_opp). But since splitperc_self + splitperc_opp = 100, and fair_percent_self + fair_percent_opp = 100, this difference would be (splitperc_self - fair_self) - (100 - splitperc_self - (100 - fair_self)) ) = 2*(splitperc_self - fair_self). So again, this reduces to twice the responder's percentage deviation. So it's redundant.\n\nAlternatively, the responder might compare their proposed percentage to the proposer's proposed percentage. For example, if splitperc_self is much lower than splitperc_opp, that's unfair. But since splitperc_self + splitperc_opp = 100, this is equivalent to splitperc_self being less than 50% if the contributions are equal. But if contributions are unequal, the fair percentage would differ. \n\nWait, the fair percentage for the responder is (token_self / (token_self + token_opp)) * 100. The proposer's fair percentage is similarly (token_opp / total) *100. So the difference between splitperc_self and the fair percentage is a measure of unfairness. Similarly for the proposer's split. \n\nBut again, since splitperc_opp = 100 - splitperc_self, and the fair percentages sum to 100, the difference between splitperc_self and fair_self is the negative of the difference between splitperc_opp and fair_opp. So they are perfectly negatively correlated. Therefore, including both terms would be redundant. \n\nGiven all this, perhaps the best approach is to consider a model that hasn't been tried yet, which combines the absolute monetary gain with a term that captures the proposer's greed relative to their contribution, but in a different mathematical form. \n\nWait, Model 7 uses max(split_opp -3t_opp, 0), which is the proposer's overpayment. But split_opp is the proposer's share. So when the proposer takes more than their contribution, the responder's utility decreases. This is a form of punishment for greed. \n\nAnother possibility is that the responder's utility is influenced by the ratio of the proposer's share to their contribution. For example, (split_opp / (3t_opp)) - 1. If this is positive, the proposer is taking more than their contribution. The utility could then be alpha * split_self + beta * ((split_opp / (3t_opp)) - 1). This term would be zero when the proposer takes exactly their contribution, positive when they take more, and negative when they take less. If beta is negative, this would penalize proposer greed. \n\nBut this introduces a division, which might lead to scaling issues. However, since 3t_opp is a fixed value per trial, this could be manageable. For example, if t_opp is 10, then 3t_opp is 30. If split_opp is 40, then (40/30 -1) = 0.333. So beta * 0.333 would reduce the utility. \n\nThis model hasn't been tried in the previous models. Let's see how this compares. The utility would be:\n\nU_accept = alpha * split_self + beta * (split_opp / (3 * token_opp) - 1)\n\nBut split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self. So substituting:\n\nsplit_opp/(3*token_opp) = [3(t_self + t_opp) - split_self]/(3t_opp) = (t_self + t_opp)/t_opp - split_self/(3t_opp) = 1 + t_self/t_opp - split_self/(3t_opp)\n\nSo the term becomes (1 + t_self/t_opp - split_self/(3t_opp)) -1 = t_self/t_opp - split_self/(3t_opp) = (3t_self - split_self)/(3t_opp)\n\nSo the model can be rewritten as:\n\nU_accept = alpha * split_self + beta * (3t_self - split_self)/(3t_opp)\n\nBut 3t_self is the responder's fair share. So (3t_self - split_self) is the deviation from the fair share. Then, dividing by 3t_opp scales this deviation by the proposer's contribution. \n\nThis is an interesting approach. The fairness term is the responder's underpayment (3t_self - split_self) scaled by the proposer's contribution. This means that the same underpayment is penalized more if the proposer contributed less. For example, if the proposer contributed few tokens (small t_opp), then the scaling factor (1/(3t_opp)) is larger, making the penalty term larger. This could model a sense of entitlement: if the proposer didn't contribute much, the responder is more upset about being underpaid. \n\nAlternatively, this could be interpreted as the responder's underpayment relative to the proposer's contribution. So if the proposer contributed little, the responder expects a larger share, and any underpayment is more salient. \n\nThis model would have two parameters: alpha (sensitivity to monetary gain) and beta (sensitivity to scaled underpayment). The term (3t_self - split_self) is the same as in Model 2 and 6, but scaled by 1/(3t_opp). \n\nThis introduces an interaction between the responder's underpayment and the proposer's contribution. For example, if the proposer contributed very little (small t_opp), then even a small underpayment (3t_self - split_self) would be amplified by the scaling factor. This could capture a psychological mechanism where responders are more sensitive to unfairness when the proposer hasn't contributed much. \n\nThis model hasn't been tried in the previous models, so it's a novel approach. Let's check if this fits within the constraints. The model uses split_self, token_self, and token_opp from the dataset. The parameters are alpha and beta. The utility is linear in parameters. The scaled term (3t_self - split_self)/(3t_opp) is a data-derived variable. \n\nThe mathematical form would be:\n\nU_accept = alpha * split_self + beta * ( (3 * token_self - split_self) / (3 * token_opp) )\n\nThis model uses two parameters, alpha and beta. The term (3t_self - split_self) is the underpayment, and dividing by 3t_opp scales it based on the proposer's contribution. \n\nIf beta is negative, then higher underpayment (when split_self <3t_self) reduces utility. The scaling by token_opp means that for a given underpayment, the penalty is larger when token_opp is smaller. \n\nThis model could explain behavior where responders are more sensitive to unfair offers when the proposer contributed less. For example, if the proposer found few tokens (low token_opp), the responder expects a larger share, so any deviation from their fair share (3t_self) is more impactful. \n\nThis model meets all the constraints: two parameters, uses available variables, no stochastic parameters, and focuses on responder behavior. \n\nNow, checking for covariance between parameters. If alpha and beta are both positive, they might be correlated, but the scaling term could help differentiate their effects. The parameter recovery would depend on how these terms vary across trials. \n\nIn terms of variable ranges, token_opp is an integer >=0 (but in practice, since it's from a pretask, it's at least 1). So 3*token_opp is at least 3. The term (3t_self - split_self) can be positive or negative. But since the model is for the utility of accepting, and we're not modeling reject utility, negative values of the fairness term would not penalize the utility. However, if split_self exceeds 3t_self, the term becomes negative, which would increase utility if beta is positive. But since beta is a learnable parameter, the model can adapt. However, if the hypothesis is that responders penalize underpayment, beta should be negative. So when split_self <3t_self, the term is positive, multiplied by a negative beta, leading to lower utility. When split_self >3t_self, the term is negative, multiplied by a negative beta, leading to higher utility. But this might not align with the intended direction. \n\nWait, if beta is negative, then:\n\nIf split_self <3t_self: (3t_self - split_self) is positive. Divided by 3t_opp (positive), the term is positive. Multiply by beta (negative) gives a negative contribution to utility. So lower utility.\n\nIf split_self >3t_self: (3t_self - split_self) is negative. Divided by 3t_opp (positive), the term is negative. Multiply by beta (negative) gives a positive contribution to utility. So higher utility.\n\nThis makes sense. So beta is expected to be negative, penalizing underpayment and rewarding overpayment. But responders might not care about overpayment (getting more than their fair share), so perhaps the model should only penalize underpayment. To do that, we can take the max of (3t_self - split_self, 0), similar to Model 6. \n\nSo modifying the model to:\n\nU_accept = alpha * split_self + beta * ( max(3 * token_self - split_self, 0) / (3 * token_opp) )\n\nThis way, only underpayment is penalized. The term is zero when split_self >=3t_self. This might make the model more aligned with loss aversion, where only losses (underpayment) are penalized. \n\nThis model would have two parameters: alpha (sensitivity to monetary gain) and beta (sensitivity to scaled underpayment). The scaling by token_opp introduces a novel interaction between the proposer's contribution and the responder's underpayment. \n\nThis model hasn't been tried in the previous models. Let's check the previous models again. Model 6 uses max(3t_self - split_self, 0) without scaling. Model 10 combines this with the proposer's overpayment. Model 7 uses the proposer's overpayment. This new model scales the underpayment by the proposer's contribution. \n\nThis could capture a scenario where the responder is more upset about being underpaid when the proposer contributed less. For example, if the proposer found only a few tokens, the responder expects them to take less, so any underpayment feels more unfair. \n\nIn terms of parameter recovery, if the scaling by token_opp introduces variability in the fairness term, it might help in distinguishing alpha and beta. For example, trials with the same underpayment but different token_opp would have different fairness terms, allowing beta to be estimated more accurately. \n\nThe mathematical form is:\n\nU_accept = \u03b1 * split_self + \u03b2 * ( max(3 * token_self - split_self, 0) / (3 * token_opp) )\n\nHere, \u03b1 is the sensitivity to the absolute monetary gain, and \u03b2 (expected to be negative) is the sensitivity to the scaled underpayment. \n\nThis model uses variables available in the dataset: split_self, token_self, token_opp. It has two learnable parameters, \u03b1 and \u03b2. The utility is calculated only for the responder's accept action. \n\nNow, checking the constraints:\n\n1. Predicts responder behavior: Yes, as trial_role is responder.\n2. Builds on psychological principles: Loss aversion scaled by proposer's contribution.\n3. Considers high covariance: The scaling by token_opp may reduce covariance between \u03b1 and \u03b2.\n4. Uses dataset variables: Yes.\n5. No stochastic parameters: Correct.\n6. Two learnable parameters: Yes.\n7. Utility can be negative: Yes.\n\nThe variable descriptions would be:\n\n- \u03b1: sensitivity to monetary gain (split_self), learnable, range could be [0, some upper bound], but since it's a linear term, perhaps [-inf, inf], but the model may need to bound it. However, the user says bounds should be generous. So maybe alpha in [-10, 10], inclusive.\n\n- \u03b2: sensitivity to scaled underpayment, learnable, expected to be negative. Bounds could be [-10, 10], inclusive.\n\nThe variables in the model:\n\n- split_self: from data.\n- token_self: from data.\n- token_opp: from data.\n- max(3*token_self - split_self, 0): calculated.\n- divided by 3*token_opp: calculated.\n\nSo the variable descriptions in JSON would include alpha and beta as learnable parameters with appropriate bounds. \n\nThis model introduces a new mechanism not present in previous models: scaling the underpayment by the proposer's contribution. This could explain variations in responder behavior based on the proposer's contribution level, which hasn't been explicitly modeled before. \n\nThe summary would highlight that the utility depends on the absolute monetary gain and the responder's underpayment scaled by the proposer's contribution, capturing how the proposer's contribution modulates sensitivity to unfairness.\n</think>\n\n<MODEL>\nU_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from the proposed split_self\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to scaled underpayment relative to contribution. Expected negative values indicate aversion to offers below 3*token_self, scaled by proposer's contribution (3*token_opp)\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nUtility of accepting depends on absolute monetary gain (split_self) and contribution-scaled underpayment aversion. The fairness term penalizes offers below the participant's equitable contribution (3*token_self), scaled by the proposer's contribution (3*token_opp). This captures heightened sensitivity to unfairness when proposers contribute less, modulating penalty strength via reciprocal entitlement expectations.\n</SUMMARY>",
  "current_model": "Specification: U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))\nSummary: Utility of accepting depends on absolute monetary gain (split_self) and contribution-scaled underpayment aversion. The fairness term penalizes offers below the participant's equitable contribution (3*token_self), scaled by the proposer's contribution (3*token_opp). This captures heightened sensitivity to unfairness when proposers contribute less, modulating penalty strength via reciprocal entitlement expectations.",
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        token_opp = trial.get(\"token_opp\", 1) #avoid division by zero\n\n        if token_opp == 0:\n            utility = alpha * split_self\n        else:\n            utility = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))\n        utility_values.append(utility)\n    return utility_values",
  "fitting_results": [
    {
      "alpha": -0.24817841450169228,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -9.983137633113923,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7068380287199469,
      "beta": -5.411106200836799,
      "success": true,
      "log_likelihood": -19.337057935525344,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2915966208279885,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -10.8207807073702,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9668420417891102,
      "beta": -9.653483274979392,
      "success": true,
      "log_likelihood": -15.55481026452458,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8921229083818598,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -11.417969537967851,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2594091805552299,
      "beta": -5.753507119351671,
      "success": true,
      "log_likelihood": -15.939514743761837,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.0002892804838027336,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.193368686914902,
      "beta": -4.926836996918168,
      "success": true,
      "log_likelihood": -16.618686858129585,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.0002892804838027336,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.782858734771603,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -13.510097150635138,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0283552955267141,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -10.160216286160294,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.09998661960772176,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -10.850886887825443,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8517058041748815,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -15.220600676174278,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.00028928048380273376,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.0002892804838027337,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.00028928048380273376,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.0002892804838027337,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.317062114852516,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -4.424582918559815,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.1832638415411942,
      "beta": -5.619682638233594,
      "success": true,
      "log_likelihood": -7.4616919127691945,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.0002892804838027336,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9529531358867617,
      "beta": -6.882907726097936,
      "success": true,
      "log_likelihood": -17.22750352172449,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.193397937611964,
      "beta": -3.71498917159176,
      "success": true,
      "log_likelihood": -10.13906621945599,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4899095227587518,
      "beta": -5.877070460597757,
      "success": true,
      "log_likelihood": -19.378491648739875,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7932192540412755,
      "beta": -4.886390052502841,
      "success": true,
      "log_likelihood": -19.29845882852931,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7755104486788474,
      "beta": -7.532718514157013,
      "success": true,
      "log_likelihood": -17.428051730369873,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.296047148418109,
      "beta": -7.767470647197724,
      "success": true,
      "log_likelihood": -15.019595569169873,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9353881069829498,
      "beta": -5.146962057779682,
      "success": true,
      "log_likelihood": -18.29592782854603,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4256669308019739,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -10.872315451960613,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.12794728890279,
      "beta": -3.083873813119701,
      "success": true,
      "log_likelihood": -9.982101024870628,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1921116088150525,
      "beta": -2.8437472883476262,
      "success": true,
      "log_likelihood": -16.484743726344757,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.0002892804838027337,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.24817831217601383,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -9.983137633113774,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3408692320899855,
      "beta": -3.170789486372098,
      "success": true,
      "log_likelihood": -22.56882856545183,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5983277367002102,
      "beta": -7.145222323015922,
      "success": true,
      "log_likelihood": -18.028518799841734,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.00028928048380273366,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4053997869130244,
      "beta": -5.160330364118162,
      "success": true,
      "log_likelihood": -15.11998626597214,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0858915521176606,
      "beta": -8.247581237162162,
      "success": true,
      "log_likelihood": -11.0036630911293,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7708488058614504,
      "beta": -6.609127041147609,
      "success": true,
      "log_likelihood": -18.153540482365795,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8124231087632595,
      "beta": -3.0051237419758015,
      "success": true,
      "log_likelihood": -19.93104829363535,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7103819785105434,
      "beta": -0.6938043159415866,
      "success": true,
      "log_likelihood": -9.741636989444324,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6152706375568016,
      "beta": -2.790567419305776,
      "success": true,
      "log_likelihood": -21.549776427029197,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2453533030243628,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -12.496813233712787,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -2.029992573369531,
      "success": true,
      "log_likelihood": -23.01197123200895,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7778452950693903,
      "beta": -3.967381402832843,
      "success": true,
      "log_likelihood": -19.91808961975984,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2073552172890214,
      "beta": -6.0039972382083935,
      "success": true,
      "log_likelihood": -16.189249279228356,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1689070344699164,
      "beta": -2.672303672157917,
      "success": true,
      "log_likelihood": -16.603038164563596,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6387743492688925,
      "beta": -3.907225755504083,
      "success": true,
      "log_likelihood": -20.811946937216455,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5197430564808033,
      "beta": -5.927575277929811,
      "success": true,
      "log_likelihood": -19.308286426526102,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7327604221145304,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -14.430756008627544,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3895889739991337,
      "beta": -1.8872253689130105,
      "success": true,
      "log_likelihood": -23.343529554022396,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8721513074333361,
      "beta": -2.074650724174335,
      "success": true,
      "log_likelihood": -19.150485932626673,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.14434136965106795,
      "beta": -7.252986053726446,
      "success": true,
      "log_likelihood": -14.350055476195722,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7238673182085842,
      "beta": -3.964870025530148,
      "success": true,
      "log_likelihood": -20.273499529470843,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.612853056245635,
      "beta": -2.084223877166687,
      "success": true,
      "log_likelihood": -21.659775610826124,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6004983847685769,
      "beta": -9.31941709404721,
      "success": true,
      "log_likelihood": -16.029040823988655,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6302708323033084,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -10.662774279076164,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.43596077851564236,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -13.108715875366135,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4969846486589781,
      "beta": 7.414080411368794,
      "success": true,
      "log_likelihood": -10.539607727870287,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.11173978916710244,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -11.410403498301497,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2818700759714683,
      "beta": -1.4858773944894017,
      "success": true,
      "log_likelihood": -14.437935406073061,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6250834767418573,
      "beta": -2.397836544567694,
      "success": true,
      "log_likelihood": -21.55513138012066,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6646482193567652,
      "beta": -5.555592951518369,
      "success": true,
      "log_likelihood": -19.371716589576298,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.24817840458487148,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -9.983137633113904,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5774589741310122,
      "beta": -4.918497507921725,
      "success": true,
      "log_likelihood": -20.225907431027853,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3199069421166595,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -13.322174493495101,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9774515201253412,
      "beta": -5.43524993534042,
      "success": true,
      "log_likelihood": -17.888032297032723,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7614282778957777,
      "beta": -0.452732640724364,
      "success": true,
      "log_likelihood": -9.047034724907485,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7301916034421814,
      "beta": -2.750259414308305,
      "success": true,
      "log_likelihood": -12.122294531929962,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0928258718693118,
      "beta": -5.517347525424706,
      "success": true,
      "log_likelihood": -17.112954657346236,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3397955183515842,
      "beta": 0.09322408286666681,
      "success": true,
      "log_likelihood": -11.539040256194646,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.10388955977766752,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -7.926574934586753,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.1587499086367629,
      "beta": -6.5562439193918,
      "success": true,
      "log_likelihood": -15.010159974261313,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9453080703902237,
      "beta": -6.453755289149322,
      "success": true,
      "log_likelihood": -17.515481599814752,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3821852652701254,
      "beta": -2.6646522854390366,
      "success": true,
      "log_likelihood": -14.720696821794053,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.31405836721200636,
      "beta": -1.9203314364566755,
      "success": true,
      "log_likelihood": -23.695283089928886,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9308114196320266,
      "beta": -2.726161354924763,
      "success": true,
      "log_likelihood": -18.842322616840363,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.3559627201283138,
      "beta": -1.9156399257439973,
      "success": true,
      "log_likelihood": -7.579757032982284,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.6467604993720211,
      "beta": -7.720854628375912,
      "success": true,
      "log_likelihood": -8.866556600716862,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5430084278240663,
      "beta": -1.5161193534803352,
      "success": true,
      "log_likelihood": -22.17474000384988,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.2481782587514965,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -9.983137633113763,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.30567160928175086,
      "beta": -1.4739900871654463,
      "success": true,
      "log_likelihood": -23.91880211225764,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8178462661753472,
      "beta": 4.44592405788199,
      "success": true,
      "log_likelihood": -10.295195569383548,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.15612313957598412,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -11.716724883106021,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4571545936877481,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -12.570949040832282,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.31394972093891466,
      "beta": -0.8164384838324691,
      "success": true,
      "log_likelihood": -23.87014351477391,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.14379924127667118,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -10.333796411579652,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8426183917769852,
      "beta": -2.4820573050838686,
      "success": true,
      "log_likelihood": -19.608453420300577,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2013409728069664,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -12.672913636614473,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.6280220117534743,
      "beta": -4.311420302550626,
      "success": true,
      "log_likelihood": -5.531661018762988,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.255397124987758,
      "beta": -8.747961854066206,
      "success": true,
      "log_likelihood": -14.885720574362164,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5650465632354793,
      "beta": -0.5933153477647314,
      "success": true,
      "log_likelihood": -21.30625187877163,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.8281606510938897,
        "p": 2.212324722583043e-26
      },
      "beta": {
        "r": 0.5526062606344757,
        "p": 2.5074500934860123e-09
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 34.51731809918489,
  "bic_error": "'ModelOutput' object has no attribute 'metrics'",
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.828\n- beta: r = 0.553",
  "previous_models": [
    "Specification: U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))\nSummary: Utility of accepting depends on absolute monetary gain (split_self) and contribution-scaled underpayment aversion. The fairness term penalizes offers below the participant's equitable contribution (3*token_self), scaled by the proposer's contribution (3*token_opp). This captures heightened sensitivity to unfairness when proposers contribute less, modulating penalty strength via reciprocal entitlement expectations.\nBIC: 34.51731809918489\n\nParameter Recovery:\n- alpha: r = 0.828\n- beta: r = 0.553"
  ]
}