{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. If there are no previous models, start with one learnable parameter, then ensure the space of possible models is thoroughly explored with one parameter before moving to two parameters, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: Utility of accepting an offer combines linear monetary value and proportional fairness. The fairness term quantifies how much the proposed percentage deviates from the contribution-based equitable percentage (tokens_self / total_tokens). Two parameters control sensitivity to absolute monetary gains (alpha) and fairness violations (beta).\nBIC: 35.35895699690902\nParameter Recovery:\n- alpha: r = 0.625\n- beta: r = 0.856\nModel 2:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self)\nSummary: A linear utility model combining absolute monetary gain and contribution-based fairness. The utility of accepting an offer depends on the proposed amount (split_self) and its deviation from the participant's equitable contribution (3 * token_self). Two parameters control sensitivity to money (alpha) and fairness violations (beta), where beta < 0 indicates aversion to offers below one's contribution. The model directly anchors fairness expectations to pretask performance.\nBIC: 30.777436411868965\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.903\nModel 3:\nSpecification: U_accept = alpha * split_self + beta * ((split_self - 3 * token_self) / (3 * token_self))\nSummary: A utility model combining absolute monetary gain and proportional fairness evaluation. The utility of accepting an offer depends linearly on the proposed amount (split_self) and the proportional deviation of this amount from the participant's equitable contribution (3 * token_self). The proportional term captures sensitivity to relative fairness violations, where offers below one's contribution are penalized according to their percentage shortfall. Two parameters control sensitivity to money (alpha) and proportional fairness (beta).\nBIC: 32.24846051603598\nParameter Recovery:\n- alpha: r = 0.767\n- beta: r = 0.562\nModel 4:\nSpecification: U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0)\nSummary: A utility model based on inequity aversion, where the utility of accepting an offer depends on the absolute monetary gain (split_self) and aversion to both disadvantageous (split_opp > split_self) and advantageous (split_self > split_opp) inequality. Two learnable parameters (alpha, beta) quantify sensitivity to these inequalities. The model captures how responders evaluate offers relative to the proposer's share, penalizing unequal outcomes even when beneficial.\nBIC: 34.61528543210206\nParameter Recovery:\n- alpha: r = 0.827\n- beta: r = -0.027\nModel 5:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).\nBIC: 32.20179038739441\nParameter Recovery:\n- alpha: r = 0.750\n- beta: r = 0.855\nModel 6:\nSpecification: U_accept = alpha * split_self + beta * max(3 * token_self - split_self, 0)\nSummary: A utility model combining absolute monetary gain with loss aversion relative to contribution-based fairness. The utility of accepting an offer depends linearly on the proposed amount (split_self) and incorporates a penalty for offers below the participant's equitable contribution (3 * token_self). The penalty term activates only when split_self < 3 * token_self, capturing asymmetric sensitivity to disadvantageous inequality. Two parameters control sensitivity to money (alpha \u2265 0) and fairness violations (beta \u2264 0), with beta quantifying aversion to offers below one's contribution.\nBIC: 31.092498427958233\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.836\nModel 7:\nSpecification: U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\nSummary: A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the participant's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3 * token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\nBIC: 30.92992704863568\nParameter Recovery:\n- alpha: r = 0.868\n- beta: r = 0.865\nModel 8:\nSpecification: U_accept = alpha * split_self + beta * ((splitperc_self - (token_self / (token_self + token_opp)) * 100) / ((token_self / (token_self + token_opp)) * 100))\nSummary: A utility model combining absolute monetary gain with proportional fairness evaluation relative to contribution-based equity. The utility of accepting depends on the proposed amount (split_self) and the proportional deviation of the proposed percentage (splitperc_self) from the participant's fair share percentage (token_self / total_tokens * 100). Two parameters control sensitivity to money (alpha) and relative fairness violations (beta), capturing how responders weigh personal gain against deviations from their entitled share.\nBIC: 32.485483241059136\nParameter Recovery:\n- alpha: r = 0.720\n- beta: r = 0.551\nModel 9:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) / (3 * (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain with contribution-based fairness violations normalized by total earnings. The utility of accepting depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), scaled by the total combined earnings (3*(token_self + token_opp)). This captures sensitivity to both personal gain and fairness violations relative to the total stake, where smaller pots amplify the impact of inequity. Two parameters control sensitivity to money (alpha) and normalized fairness (beta).\nBIC: 38.949711847331244\nParameter Recovery:\n- alpha: r = 0.864\n- beta: r = 0.474\nModel 10:\nSpecification: U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) + max(split_opp - 3 * token_opp, 0))\nSummary: A utility model combining absolute monetary gain with aversion to two fairness violations: the responder receiving less than their contribution (3 * token_self) and the proposer taking more than their contribution (3 * token_opp). The model captures loss aversion relative to one's own contribution and punishment of proposer greed, with two parameters controlling sensitivity to money (alpha \u2265 0) and combined fairness violations (beta \u2264 0). Offers are penalized when split_self < 3*token_self (disadvantageous inequity) or split_opp > 3*token_opp (proposer overcompensation), weighted by a single fairness sensitivity parameter.\nBIC: 31.017025036615987\nParameter Recovery:\n- alpha: r = 0.795\n- beta: r = 0.844\nModel 11:\nSpecification: U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))\nSummary: Utility of accepting depends on absolute monetary gain (split_self) and contribution-scaled underpayment aversion. The fairness term penalizes offers below the participant's equitable contribution (3*token_self), scaled by the proposer's contribution (3*token_opp). This captures heightened sensitivity to unfairness when proposers contribute less, modulating penalty strength via reciprocal entitlement expectations.\nBIC: 34.51731809918489\nParameter Recovery:\n- alpha: r = 0.828\n- beta: r = 0.553",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. If there are no previous models, start with one learnable parameter, then ensure the space of possible models is thoroughly explored with one parameter before moving to two parameters, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: Utility of accepting an offer combines linear monetary value and proportional fairness. The fairness term quantifies how much the proposed percentage deviates from the contribution-based equitable percentage (tokens_self / total_tokens). Two parameters control sensitivity to absolute monetary gains (alpha) and fairness violations (beta).\nBIC: 35.35895699690902\nParameter Recovery:\n- alpha: r = 0.625\n- beta: r = 0.856\nModel 2:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self)\nSummary: A linear utility model combining absolute monetary gain and contribution-based fairness. The utility of accepting an offer depends on the proposed amount (split_self) and its deviation from the participant's equitable contribution (3 * token_self). Two parameters control sensitivity to money (alpha) and fairness violations (beta), where beta < 0 indicates aversion to offers below one's contribution. The model directly anchors fairness expectations to pretask performance.\nBIC: 30.777436411868965\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.903\nModel 3:\nSpecification: U_accept = alpha * split_self + beta * ((split_self - 3 * token_self) / (3 * token_self))\nSummary: A utility model combining absolute monetary gain and proportional fairness evaluation. The utility of accepting an offer depends linearly on the proposed amount (split_self) and the proportional deviation of this amount from the participant's equitable contribution (3 * token_self). The proportional term captures sensitivity to relative fairness violations, where offers below one's contribution are penalized according to their percentage shortfall. Two parameters control sensitivity to money (alpha) and proportional fairness (beta).\nBIC: 32.24846051603598\nParameter Recovery:\n- alpha: r = 0.767\n- beta: r = 0.562\nModel 4:\nSpecification: U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0)\nSummary: A utility model based on inequity aversion, where the utility of accepting an offer depends on the absolute monetary gain (split_self) and aversion to both disadvantageous (split_opp > split_self) and advantageous (split_self > split_opp) inequality. Two learnable parameters (alpha, beta) quantify sensitivity to these inequalities. The model captures how responders evaluate offers relative to the proposer's share, penalizing unequal outcomes even when beneficial.\nBIC: 34.61528543210206\nParameter Recovery:\n- alpha: r = 0.827\n- beta: r = -0.027\nModel 5:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).\nBIC: 32.20179038739441\nParameter Recovery:\n- alpha: r = 0.750\n- beta: r = 0.855\nModel 6:\nSpecification: U_accept = alpha * split_self + beta * max(3 * token_self - split_self, 0)\nSummary: A utility model combining absolute monetary gain with loss aversion relative to contribution-based fairness. The utility of accepting an offer depends linearly on the proposed amount (split_self) and incorporates a penalty for offers below the participant's equitable contribution (3 * token_self). The penalty term activates only when split_self < 3 * token_self, capturing asymmetric sensitivity to disadvantageous inequality. Two parameters control sensitivity to money (alpha \u2265 0) and fairness violations (beta \u2264 0), with beta quantifying aversion to offers below one's contribution.\nBIC: 31.092498427958233\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.836\nModel 7:\nSpecification: U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\nSummary: A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the participant's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3 * token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\nBIC: 30.92992704863568\nParameter Recovery:\n- alpha: r = 0.868\n- beta: r = 0.865\nModel 8:\nSpecification: U_accept = alpha * split_self + beta * ((splitperc_self - (token_self / (token_self + token_opp)) * 100) / ((token_self / (token_self + token_opp)) * 100))\nSummary: A utility model combining absolute monetary gain with proportional fairness evaluation relative to contribution-based equity. The utility of accepting depends on the proposed amount (split_self) and the proportional deviation of the proposed percentage (splitperc_self) from the participant's fair share percentage (token_self / total_tokens * 100). Two parameters control sensitivity to money (alpha) and relative fairness violations (beta), capturing how responders weigh personal gain against deviations from their entitled share.\nBIC: 32.485483241059136\nParameter Recovery:\n- alpha: r = 0.720\n- beta: r = 0.551\nModel 9:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) / (3 * (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain with contribution-based fairness violations normalized by total earnings. The utility of accepting depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), scaled by the total combined earnings (3*(token_self + token_opp)). This captures sensitivity to both personal gain and fairness violations relative to the total stake, where smaller pots amplify the impact of inequity. Two parameters control sensitivity to money (alpha) and normalized fairness (beta).\nBIC: 38.949711847331244\nParameter Recovery:\n- alpha: r = 0.864\n- beta: r = 0.474\nModel 10:\nSpecification: U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) + max(split_opp - 3 * token_opp, 0))\nSummary: A utility model combining absolute monetary gain with aversion to two fairness violations: the responder receiving less than their contribution (3 * token_self) and the proposer taking more than their contribution (3 * token_opp). The model captures loss aversion relative to one's own contribution and punishment of proposer greed, with two parameters controlling sensitivity to money (alpha \u2265 0) and combined fairness violations (beta \u2264 0). Offers are penalized when split_self < 3*token_self (disadvantageous inequity) or split_opp > 3*token_opp (proposer overcompensation), weighted by a single fairness sensitivity parameter.\nBIC: 31.017025036615987\nParameter Recovery:\n- alpha: r = 0.795\n- beta: r = 0.844\nModel 11:\nSpecification: U_accept = alpha * split_self + beta * (max(3 * token_self - split_self, 0) / (3 * token_opp))\nSummary: Utility of accepting depends on absolute monetary gain (split_self) and contribution-scaled underpayment aversion. The fairness term penalizes offers below the participant's equitable contribution (3*token_self), scaled by the proposer's contribution (3*token_opp). This captures heightened sensitivity to unfairness when proposers contribute less, modulating penalty strength via reciprocal entitlement expectations.\nBIC: 34.51731809918489\nParameter Recovery:\n- alpha: r = 0.828\n- beta: r = 0.553\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer for responders in the given experiment. The model needs to be based on psychological principles, use the dataset variables appropriately, and have no more than two learnable parameters. Also, it should avoid parameters for stochasticity since that's handled later by a temperature parameter.\n\nFirst, I need to consider the previous models to ensure I'm not repeating the same approaches. The existing models have explored various fairness mechanisms, such as comparing the split to one's own contribution (3*token_self), inequity aversion between split_self and split_opp, and combinations of absolute gain with different fairness terms. The best-performing models in terms of BIC and parameter recovery are Models 2, 6, 7, and 10. These models often combine absolute monetary gain (split_self) with some form of fairness violation relative to contributions.\n\nLooking at the dataset variables, key variables for responders are token_self, token_opp, split_self, split_opp, and combined_earning. The trial_type indicates whether contributions are equal, opponent more, or participant more. The responder's decision (accept) depends on the utility of the offer.\n\nPsychological principles involved here might include fairness considerations, loss aversion, and reciprocity. For example, people might feel entitled to a share proportional to their contribution. If the offer is below their contribution, they might perceive it as unfair. Alternatively, they might compare their split to the opponent's, leading to inequity aversion.\n\nBut previous models have already used terms like (split_self - 3*token_self) and max(3*token_self - split_self, 0). Model 10 even combines underpayment and proposer greed. However, Model 7 introduces a term where the responder penalizes the proposer if the proposer's split exceeds their contribution. That seems like a different angle, focusing on the proposer's fairness.\n\nAnother angle could be the relative comparison between the splits and contributions. For instance, if the responder's split is less than their contribution, and the proposer's split is more than their contribution, that might be a double unfairness. But Model 10 already combines these two terms.\n\nWait, Model 10's specification is: max(3*token_self - split_self, 0) + max(split_opp - 3*token_opp, 0). So it's adding two penalties: one for the responder getting less than their contribution, and another for the proposer getting more than theirs. The beta parameter then scales the combined penalty. That model has a BIC of ~31, which is better than some others. But maybe there's a way to combine these two terms in a multiplicative way or another form that hasn't been tried yet.\n\nAlternatively, perhaps the responder cares about the ratio of their split to their contribution compared to the proposer's split to their contribution. For example, if the responder's split_self/(3*token_self) is less than the proposer's split_opp/(3*token_opp), that could be a fairness violation. This ratio comparison might capture a different aspect of fairness, where the responder expects the splits to be proportional to contributions.\n\nLet me think: Suppose the responder's fair share is (3*token_self) / combined_earning * 100% (since combined_earning is 3*(token_self + token_opp)). But splitperc_self is the percentage the responder is offered. The difference between splitperc_self and their contribution percentage could be a fairness term. However, Model 1 uses (splitperc_self - (token_self/(token_self + token_opp))*100), which is exactly that. But Model 1's BIC is higher (worse) than some others, so maybe that's not the best approach.\n\nAlternatively, the responder might compare their split to both their own contribution and the proposer's contribution. For example, if the responder's split is less than their contribution, and the proposer's split is more than their contribution, that's a double inequity. But Model 10 adds these two terms. Maybe a product of these terms would capture a synergistic effect, but that would introduce a non-linear term which might complicate parameter recovery. However, the problem states that the model should have no more than two parameters, so adding a product term would still be within the limit if the parameters are kept to two.\n\nWait, but the current models have tried additive terms. A multiplicative interaction might be a new direction. For example, the utility could be:\n\nU_accept = alpha * split_self + beta * (max(3*token_self - split_self, 0)) * (max(split_opp - 3*token_opp, 0))\n\nHere, the fairness term is the product of two penalties: the responder being underpaid and the proposer being overpaid. This would mean that the penalty is only applied when both conditions are true. If either one is not violated, the penalty is zero. But maybe that's too restrictive. Alternatively, using the product of the two max terms would create a term that's non-zero only when both underpayment and overpayment occur. However, this might not capture cases where only one of the two is violated. Alternatively, maybe the sum of the two max terms multiplied together? Not sure.\n\nAlternatively, the responder might care about the ratio of their split to their contribution versus the proposer's split to their contribution. For example, if (split_self / (3*token_self)) < (split_opp / (3*token_opp)), that's unfair. The difference between these ratios could be a fairness term. But this would require calculating both ratios and their difference. However, this might be similar to existing models but in a different form.\n\nAnother angle: the responder's perception of fairness could be based on the difference between the actual split and the equitable split. The equitable split would be each party getting their contribution (3*token_self for responder, 3*token_opp for proposer), and any surplus (combined_earning - 3*(token_self + token_opp)) is split somehow. Wait, but combined_earning is 3*(token_self + token_opp), since each token is \u00a33. So the total is exactly 3*(sum of tokens). Therefore, the equitable split would be each getting their own contribution. So any split where split_self is less than 3*token_self is taking from the responder's contribution. Similarly, split_opp being more than 3*token_opp would mean the proposer is taking more than their contribution.\n\nBut in the task description, it's mentioned that the combined earning is the total, so split_self + split_opp = combined_earning. Therefore, if split_self < 3*token_self, then split_opp must be > 3*token_opp, because 3*(token_self + token_opp) = combined_earning. So if the responder is getting less than their contribution, the proposer is automatically getting more than theirs. Therefore, the two max terms in Model 10 are actually capturing the same underlying condition. Because if split_self < 3*token_self, then split_opp = combined_earning - split_self > 3*token_opp. So the two terms in Model 10 are redundant. That's a key insight. Because combined_earning is fixed as 3*(token_self + token_opp), then split_self + split_opp = 3*(token_self + token_opp). Therefore, split_self = 3*token_self + delta implies split_opp = 3*token_opp - delta. So if delta is negative (split_self < 3*token_self), then split_opp = 3*token_opp + |delta|. Therefore, whenever the responder is underpaid, the proposer is overpaid by the same amount. Therefore, the two terms in Model 10 are perfectly correlated. That means Model 10's two max terms are actually capturing the same thing, which might lead to multicollinearity issues and make parameter recovery difficult. \n\nThis is a crucial point. Because split_self and split_opp are dependent variables (they sum to a fixed total), any model that includes both terms related to split_self and split_opp might have high covariance between parameters. The problem statement mentions that the model should be considerate of high covariance between parameters to pass parameter recovery tests. Therefore, including both terms that are perfectly correlated (like underpayment of self and overpayment of opp) would lead to parameters that are not recoverable because they are colinear.\n\nTherefore, models that include both terms (like Model 10) might have issues with parameter recovery. However, in Model 10's parameter recovery, beta has a high r (0.844), which suggests that perhaps the model is still manageable. But maybe there's a better way to model this.\n\nGiven that split_self < 3*token_self implies split_opp > 3*token_opp, the two terms are perfectly inversely related. Therefore, including both terms in the model is redundant. So perhaps the best approach is to model only one of them. For example, the underpayment of self (split_self < 3*token_self) is equivalent to the overpayment of the opponent. Therefore, a model that includes only one of these terms would capture the same effect. \n\nLooking back at previous models, Model 6 uses max(3*token_self - split_self, 0), which is the underpayment term. Model 7 uses max(split_opp - 3*token_opp, 0), which is the overpayment term. But since these are two sides of the same coin, the parameters in these models would capture similar effects. However, depending on how participants perceive the unfairness, they might focus on their own underpayment or the proposer's overpayment. \n\nBut according to the task description, the proposer's split_opp is the amount the opponent would take. So when the responder is deciding to accept or reject, they see both their own split and the proposer's split. The key is whether the responder's utility is affected by how much the proposer is taking relative to their contribution. \n\nIn Model 7, the utility includes a term for when the proposer's split exceeds their contribution. The idea is that responders might reject offers not just because they're getting less, but because the proposer is being greedy. However, given the dependency between split_self and split_opp, this term is equivalent to the responder's underpayment. \n\nBut perhaps participants don't realize this dependency and evaluate both terms separately. Alternatively, they might focus on their own share relative to their contribution, regardless of the proposer's share. \n\nAnother angle is the ratio of the split to the contribution. For example, split_self / (3*token_self) versus split_opp / (3*token_opp). If these ratios are unequal, that could be a fairness violation. For instance, if the responder's ratio is lower than the proposer's, that's unfair. The difference between these ratios could be a term in the utility function. \n\nSo the fairness term could be (split_self/(3*token_self) - split_opp/(3*token_opp)). If this is negative, it indicates the responder is getting a worse ratio than the proposer. However, since split_self + split_opp = 3*(token_self + token_opp), this ratio difference might simplify in a way that's captured by existing terms. Let's see:\n\nsplit_self/(3*token_self) - split_opp/(3*token_opp) = [split_self * token_opp - split_opp * token_self] / [3*token_self*token_opp]\n\nBut since split_opp = 3*(token_self + token_opp) - split_self, substituting:\n\n= [split_self * token_opp - (3*(token_self + token_opp) - split_self) * token_self] / [3*token_self*token_opp]\n\nSimplifying numerator:\n\nsplit_self * token_opp - 3*(token_self + token_opp)*token_self + split_self * token_self\n\n= split_self (token_opp + token_self) - 3*token_self*(token_self + token_opp)\n\n= (split_self - 3*token_self) * (token_self + token_opp)\n\nSo the ratio difference term simplifies to (split_self - 3*token_self) * (token_self + token_opp) / [3*token_self*token_opp]\n\nWhich is proportional to (split_self - 3*token_self). Therefore, this ratio difference term is essentially capturing the same underpayment term as in Model 2. So using this ratio difference might not add new information beyond existing models.\n\nHmm. So maybe the key is to find a different way to represent the fairness term that hasn't been captured in previous models. Let's think about the psychological principles again. \n\nOne principle is loss aversion: people are more sensitive to losses than gains. In this context, getting less than one's contribution is a loss, and the disutility from that loss is greater than the utility from a gain. However, Model 6 uses a max term for underpayment, which captures loss aversion relative to the contribution. \n\nAnother principle is reciprocity: responders might be more lenient if the proposer is also being fair in some way. But given that the proposer's split is determined by the offer, which is fixed in each trial, perhaps responders evaluate the proposer's intent based on their own contribution. \n\nWait, the proposer's contribution is token_opp (the number of tokens they found in the pretask). So if the proposer contributed more, maybe the responder expects them to take a larger share. But in the task, the proposer's split_opp is their proposed share. So if the proposer is taking more than their contribution, that's seen as unfair. But if they contributed more, maybe it's acceptable. \n\nAlternatively, the responder's perception of fairness could depend on both their own and the proposer's contributions. For example, the fair split could be proportional to contributions: split_self / split_opp = token_self / token_opp. If the actual split deviates from this ratio, that's unfair. \n\nThe fair percentage for the responder would be token_self / (token_self + token_opp) * 100, which is what splitperc_self should be if it's proportional. The variable splitperc_self is provided in the dataset, so the difference between splitperc_self and the fair percentage could be a term. Model 1 uses this difference. However, Model 1's BIC is higher than others, suggesting it's not the best model. \n\nAlternatively, the disutility could be proportional to the squared difference, introducing a non-linear term. But the problem states that the model should have no more than two parameters, and we need to keep it simple. Also, non-linear terms might complicate parameter recovery. \n\nAnother angle: the utility could depend on the absolute difference between the proposed split and the fair split. The fair split for the responder is 3*token_self, so the difference is (split_self - 3*token_self). Model 2 uses this term linearly. Model 6 uses a max function to capture only underpayment. \n\nBut perhaps the responder's sensitivity to this difference depends on the total combined earning. For example, a \u00a31 difference in a small pot might matter more than in a large pot. Model 9 scales the difference by the total combined earning (3*(token_self + token_opp)), but its BIC is higher, indicating worse performance. \n\nAlternatively, the responder might have reference points based on both their own contribution and the proposer's. For example, the sum of the deviations: (split_self - 3*token_self) + (split_opp - 3*token_opp). But since split_self + split_opp = 3*(token_self + token_opp), this sum is zero. Therefore, this term is always zero, which is not useful. \n\nWait, that's right. Because split_self + split_opp = 3*(token_self + token_opp), so (split_self - 3*token_self) + (split_opp - 3*token_opp) = 0. So any model that uses the sum of these two terms would be zero, making it irrelevant. \n\nTherefore, the key is that any unfairness in one direction is exactly offset in the other. So models that focus on one side (either responder's underpayment or proposer's overpayment) are sufficient. \n\nGiven that, perhaps the best approach is to use a term that captures the responder's underpayment relative to their contribution, as in Model 6. However, Model 6's BIC is 31.09, which is better than some but not the best. The best BIC is Model 7 with 30.93. Model 7 uses the proposer's overpayment as the fairness term. \n\nBut why does Model 7 perform well? It's possible that responders are more sensitive to the proposer's greed (taking more than their contribution) than their own underpayment. This could be a form of inequity aversion focused on the proposer's behavior. \n\nAlternatively, combining both terms (own underpayment and proposer's overpayment) as in Model 10, but given the dependency between the two terms, this might not add value. However, Model 10's BIC is 31.01, which is slightly worse than Model 7. \n\nSo perhaps a better approach is to consider a different psychological mechanism. For example, the responder's utility might depend on the ratio of their split to the proposer's split, adjusted by their contributions. If the ratio of splits is different from the ratio of contributions, that's unfair. \n\nMathematically, the fair ratio would be (split_self / split_opp) = (token_self / token_opp). If the actual ratio deviates from this, the utility decreases. \n\nTo model this, we could take the difference between (split_self / split_opp) and (token_self / token_opp). However, this introduces a non-linear term. Alternatively, use the log ratio to make it linear, but that complicates things. \n\nAlternatively, compute the product of split_self and token_opp versus split_opp and token_self. The fair condition is split_self * token_opp = split_opp * token_self. The difference (split_self * token_opp - split_opp * token_self) could be a fairness term. If this is positive, the responder is getting more than their fair share; if negative, less. \n\nBut since split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self, substituting into the term:\n\nsplit_self * token_opp - (3*(token_self + token_opp) - split_self) * token_self \n\n= split_self * token_opp - 3*token_self*(token_self + token_opp) + split_self * token_self \n\n= split_self (token_opp + token_self) - 3*token_self*(token_self + token_opp)\n\n= (split_self - 3*token_self) * (token_self + token_opp)\n\nSo this term is proportional to (split_self - 3*token_self), which is the same as Model 2's fairness term. Therefore, this approach doesn't add new information. \n\nHmm. It seems like many of these approaches boil down to the same underlying fairness term. Therefore, perhaps the best way to proceed is to consider a model that has shown good parameter recovery and BIC, but hasn't been explored in a certain variation. \n\nLooking at Model 7, which uses the proposer's overpayment (split_opp - 3*token_opp), it has a BIC of 30.93 and good parameter recovery (r=0.868 and 0.865). This suggests that responders are sensitive to the proposer's share exceeding their contribution. \n\nBut why would that be? Perhaps responders are not just focused on their own share but also on whether the proposer is taking more than they deserve. This could be a form of fairness where the responder punishes the proposer for being greedy, even if their own share is acceptable. \n\nHowever, given the dependency between split_self and split_opp, when split_opp exceeds 3*token_opp, split_self must be less than 3*token_self. So Model 7's term is equivalent to Model 6's term but framed differently. \n\nBut the parameter recovery for Model 7 is good, suggesting that this framing works well. Maybe participants are more attuned to the proposer's share than their own in terms of contribution. \n\nAlternatively, combining both terms in a way that hasn't been done before. For example, using the product of the responder's underpayment and the proposer's overpayment. But as discussed earlier, since these are perfectly correlated, the product would be the square of one term. \n\nWait, if split_self < 3*token_self, then split_opp = 3*(token_self + token_opp) - split_self > 3*token_opp. So the proposer's overpayment is exactly equal to the responder's underpayment. Let me check:\n\nsplit_opp - 3*token_opp = (3*(token_self + token_opp) - split_self) - 3*token_opp = 3*token_self - split_self.\n\nSo split_opp - 3*token_opp = 3*token_self - split_self. Therefore, the two terms are exactly equal. Therefore, any model that uses either term is equivalent. For example, Model 6's term max(3*token_self - split_self, 0) is the same as Model 7's term max(split_opp - 3*token_opp, 0). Therefore, these models are mathematically identical. The difference is just in how the term is framed. \n\nThis is a critical realization. Therefore, Models 6 and 7 are actually the same model, just expressed differently. This explains why their BIC and parameter recovery are similar. \n\nGiven this, perhaps the best approach is to consider a model that hasn't been tried yet, which might involve a different psychological mechanism. \n\nAnother angle could be the relative difference between the proposed split and the fair split. For example, the utility could depend on the percentage difference from the fair share. \n\nThe fair share for the responder is 3*token_self. The proposed split_self could be compared to this. The percentage difference is (split_self - 3*token_self)/ (3*token_self). This term would capture proportional deviations from the fair share. If split_self is 10% below the fair share, the term is -0.1; if it's 20% above, +0.2. \n\nModel 3 uses a similar term: (split_self - 3*token_self)/(3*token_self). However, Model 3's BIC is 32.25, which is worse than Models 2, 6, 7, and 10. So this approach might not be better. \n\nAlternatively, the responder might have a non-linear utility for money. For example, diminishing sensitivity to larger amounts. But the problem states that the model should not include parameters for stochasticity, but it doesn't prohibit non-linear transformations. However, introducing non-linear terms might require more parameters, but we're limited to two. \n\nAnother idea: the responder's utility could depend on the difference between their split and the proposer's split, scaled by their contributions. For example, (split_self - split_opp) / (3*(token_self - token_opp)). But this might not make sense if contributions are equal. \n\nAlternatively, the responder might compare their split to the proposer's split in light of their respective contributions. For example, the difference (split_self - split_opp) compared to the difference in contributions (3*token_self - 3*token_opp). If the split difference is less than the contribution difference, that's unfair. \n\nThe term could be (split_self - split_opp) - (3*token_self - 3*token_opp). If this is negative, the responder is getting less than their contribution-based entitlement relative to the proposer. \n\nSimplifying, this term is (split_self - 3*token_self) - (split_opp - 3*token_opp). But since split_self + split_opp = 3*(token_self + token_opp), this term becomes (split_self - 3*token_self) - (3*(token_self + token_opp) - split_self - 3*token_opp) = 2*(split_self - 3*token_self). So this term is just twice the underpayment term. Therefore, this approach is equivalent to Model 2's term. \n\nHmm. It seems challenging to find a new term that isn't redundant with existing models. Given that, perhaps the best approach is to build on the best-performing models and see if a slight variation can be introduced. \n\nModel 2 has a BIC of 30.77, which is better than Model 7's 30.93. Model 2's specification is U_accept = alpha * split_self + beta * (split_self - 3 * token_self). Here, beta would be positive if higher deviations above the fair share increase utility, but since responders are likely to reject when split_self is below the fair share, beta should be positive to penalize negative values. Wait, no: if split_self is less than 3*token_self, then (split_self - 3*token_self) is negative. If beta is positive, this term would decrease the utility, making acceptance less likely. So beta should be positive to penalize underpayment. \n\nModel 2's parameter recovery for beta is r=0.903, which is very good. So this model is performing well. However, perhaps there's a way to improve it by incorporating another aspect. \n\nWait, the problem states that the model should have no more than two learnable parameters. Model 2 already uses two parameters: alpha and beta. So we can't add more. \n\nBut maybe a different combination. For example, combining the absolute split_self with a term that is the product of split_self and the fairness term. But that would require more parameters. \n\nAlternatively, using a non-linear combination, but with the same two parameters. For example, U_accept = alpha * split_self + beta * (split_self / (3*token_self)). This would make the fairness term a ratio, capturing how much the split is relative to the fair share. If split_self is equal to 3*token_self, the ratio is 1. If it's less, the ratio is less than 1. But this is similar to Model 3's approach, which had a worse BIC. \n\nAlternatively, using a reference point other than the fair share. For example, the responder might compare their split to the proposer's split, irrespective of contributions. This is the inequity aversion model (Model 4), which has a BIC of 34.61, worse than Model 2. \n\nAnother angle: the responder's utility could depend on whether the split is above or below the fair share, with different sensitivities. For example, a piecewise function where the penalty for underpayment is larger than the reward for overpayment. But this would require more parameters. \n\nWait, but the problem allows up to two parameters. Model 6 uses a max function for underpayment, which is a form of loss aversion. It has a BIC of 31.09, which is slightly worse than Model 2. \n\nGiven that Model 2 is performing well, perhaps a better approach is to consider that the fairness term (split_self - 3*token_self) could interact with the trial_type. For example, in trials where the participant contributed more (trial_type=3), they might be more sensitive to underpayment. However, trial_type is a categorical variable, and incorporating it would require additional parameters or interactions, which might exceed the two-parameter limit. \n\nAlternatively, the model could weight the fairness term by the participant's contribution relative to the proposer's. For example, (split_self - 3*token_self) * (token_self / (token_self + token_opp)). This would mean that participants who contributed more are more sensitive to fairness violations. Model 5 uses a similar approach, weighting the fairness term by the contribution proportion. Model 5's BIC is 32.20, which is worse than Model 2. \n\nAnother idea: the utility could depend on the absolute amount of the fairness violation, not just the direction. For example, a quadratic term (split_self - 3*token_self)^2. This would penalize both under and overpayment, but since overpayment is rare (proposers are unlikely to offer more than the fair share), this might not be useful. Also, it introduces a non-linear term with the same parameter, which might not improve the model. \n\nAlternatively, using the absolute value of the fairness violation: |split_self - 3*token_self|. This would penalize any deviation from the fair share, but responders are likely only penalizing underpayment. Model 2's linear term allows for different effects of under vs overpayment, but if beta is positive, underpayment (negative term) reduces utility, while overpayment (positive term) increases it. However, in reality, responders might not care about overpayment (getting more than their contribution), so the model might benefit from only penalizing underpayment. \n\nThis is what Model 6 does: max(3*token_self - split_self, 0). This term is zero when split_self >= fair share, and positive otherwise. If beta is negative, this term reduces utility when there's underpayment. However, in Model 6's specification, beta is a learnable parameter, so the sign is determined during fitting. \n\nWait, in Model 6's specification: U_accept = alpha * split_self + beta * max(3*token_self - split_self, 0). If beta is negative, then when there's underpayment (max term positive), the utility decreases. So beta should be negative to penalize underpayment. However, in the parameter recovery for Model 6, beta has r=0.836, which suggests that the fitted beta values are positive. That would imply that underpayment increases utility, which is counterintuitive. This might indicate a mistake in the model's formulation or in the parameter recovery reporting. \n\nWait, perhaps there's a sign error. If the model is U_accept = alpha * split_self + beta * max(3*token_self - split_self, 0), then when split_self is less than 3*token_self, the max term is positive. If beta is negative, this term subtracts from the utility, making acceptance less likely. So beta should be negative. However, in the parameter recovery, beta's r is 0.836, which is a high positive correlation. This suggests that in the simulated data, beta was positive, leading to higher utility when underpayment occurs, which doesn't make sense. This inconsistency might indicate that Model 6's formulation is flawed or that the parameter recovery was incorrectly reported. \n\nThis is a critical point. If the parameter recovery for beta in Model 6 is positive, but the term should penalize underpayment, then there's a problem. Perhaps the model should have a negative sign before beta. For example, U_accept = alpha * split_self - beta * max(3*token_self - split_self, 0), with beta constrained to be positive. This way, underpayment reduces utility. But in the given Model 6, the specification is as written, and the parameter recovery for beta is positive. This suggests that in the model, a positive beta increases utility when underpayment occurs, which is the opposite of what's expected. This could be a mistake in the model's design. \n\nGiven this confusion, perhaps the best approach is to revisit the model formulation. For example, in Model 2, the term (split_self - 3*token_self) can be positive or negative. If beta is positive, then when split_self is below the fair share, the term is negative, reducing utility. When above, it's positive, increasing utility. This could explain why responders accept offers above their fair share and reject below. However, in reality, responders might not derive extra utility from overpayment; they might simply not penalize it. So Model 2's linear term might not capture the asymmetry of loss aversion. \n\nModel 6's max term addresses this by only penalizing underpayment. But if the parameter recovery for beta is positive, that suggests that the model is incorrectly specified. Therefore, perhaps the correct formulation should have a negative beta. \n\nThis highlights the importance of carefully considering the sign of parameters in the model. To avoid such issues, the model should be structured so that a positive beta corresponds to a penalty for underpayment. \n\nGiven all this, perhaps the best approach is to propose a model that combines the absolute monetary gain (split_self) with a term that penalizes underpayment relative to the fair share, using a max function to capture loss aversion. This is similar to Model 6 but ensuring the parameter has the correct sign. \n\nHowever, since Model 6 already exists and has a certain BIC, perhaps a better approach is to consider a different psychological mechanism. For example, the responder might evaluate the offer based on the proposer's intention, which could be inferred from the proposer's own contribution. \n\nFor instance, if the proposer contributed less (token_opp is low), the responder might expect them to take a smaller share. If the proposer takes a large share despite contributing little, the responder might perceive this as unfair. This is similar to Model 7, where the term is max(split_opp - 3*token_opp, 0). \n\nBut given that split_opp - 3*token_opp is equal to 3*token_self - split_self, this term is equivalent to the underpayment term. Therefore, Models 6 and 7 are two sides of the same coin. \n\nGiven that, perhaps the best new model is one that combines the absolute monetary gain with a term that captures the proposer's greed relative to their contribution, but in a way that hasn't been modeled yet. \n\nWait, another idea: the responder's utility could depend on the ratio of the proposer's split to their contribution. For example, (split_opp / (3*token_opp)) - 1. If this is positive, the proposer is taking more than their contribution. The term could be max((split_opp / (3*token_opp)) - 1, 0), which captures the proposer's greed as a percentage over their contribution. \n\nThis term would be zero if the proposer takes exactly their contribution or less, and positive otherwise. The utility function would then be:\n\nU_accept = alpha * split_self - beta * max((split_opp / (3*token_opp)) - 1, 0)\n\nHere, alpha is the sensitivity to monetary gain, and beta is the sensitivity to the proposer's greed. This model hasn't been tried before and introduces a new fairness mechanism: responders penalize offers where the proposer takes a higher percentage of their contribution. \n\nHowever, since split_opp = 3*(token_self + token_opp) - split_self, this term can be rewritten in terms of split_self and token_self. But perhaps participants perceive it as the proposer's greed, making this a psychologically distinct mechanism. \n\nThis model would have two parameters: alpha and beta. The term max((split_opp / (3*token_opp)) - 1, 0) is zero when split_opp <= 3*token_opp, and positive otherwise. This captures how much the proposer is taking relative to their own contribution. \n\nFor example, if the proposer contributed 10 tokens (3*10=\u00a330), and takes \u00a340, then the term is (40/30 -1) = 0.333. Beta scales how much this affects the utility. \n\nThis model is different from previous ones because it focuses on the proposer's share relative to their own contribution, not the responder's. This could tap into a sense of fairness where the responder expects the proposer to not take more than they contributed, regardless of the responder's own share. \n\nHowever, given the dependency between split_self and split_opp, this term is still related to the responder's underpayment. But psychologically, it's a different evaluation. For example, a responder might accept an offer that's slightly below their fair share if the proposer isn't being greedy, but reject it if the proposer is taking way more than their contribution. \n\nThis model could explain behavior that isn't captured by models focusing solely on the responder's share. The parameter recovery would indicate whether this proposer-greed term is a significant predictor. \n\nGiven that this model hasn't been tried in the previous models, it's a viable candidate. The BIC would depend on how well it fits the data, but since it's a new mechanism, it's worth exploring. \n\nTherefore, the proposed model is:\n\nU_accept = alpha * split_self - beta * max((split_opp / (3 * token_opp)) - 1, 0)\n\nHere, alpha is the sensitivity to monetary gain (positive), and beta is the sensitivity to the proposer's greed (positive, as higher values increase the penalty). \n\nThis model uses two learnable parameters, alpha and beta, and the variables split_self, split_opp, and token_opp from the dataset. The max function ensures that only cases where the proposer takes more than their contribution are penalized. \n\nThis model builds on the psychological principle of aversion to proposer greed, where responders dislike offers where the proposer keeps more than their fair share based on their contribution. It also includes the absolute monetary gain, which is a standard economic factor. \n\nThe variables involved are:\n\n- split_self: the responder's proposed share (monetary gain).\n- split_opp: the proposer's share.\n- token_opp: the proposer's contribution in tokens.\n\nThe term (split_opp / (3 * token_opp)) calculates the proposer's share relative to their contribution. Subtracting 1 gives the percentage over their contribution. The max function ensures we only consider cases where the proposer takes more than their contribution. \n\nThis model meets all the requirements: it predicts responder behavior, uses relevant psychological principles (aversion to proposer greed), has two learnable parameters, and uses variables available in the dataset. It also avoids high covariance between parameters since alpha and beta scale different terms. \n\nThe parameter ranges can be set as alpha >= 0 (since more money should increase utility) and beta >= 0 (since proposer greed should decrease utility). The bounds can be generous, like alpha in [0, 10] and beta in [0, 10], allowing the model to capture varying sensitivities. \n\nThis model hasn't been tested in the previous models, so it adds a new explanation to the space of possible models. If the data shows that responders are sensitive to the proposer's greed beyond their own underpayment, this model could perform well.\n</think>\n\n<MODEL>\nU_accept = alpha * split_self - beta * max((split_opp / (3 * token_opp)) - 1, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from accepting\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to proposer greed (taking more than their contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the responder\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed monetary amount for the proposer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent (proposer's contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the responder's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3*token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return the utility values for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of utility values for each trial\n\n                Your code will be implemented within the following code block:\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    utility_values = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        utility_values.append(beta + (epsilon * E) + (eta * N))\n    return utility_values\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE UTLITY VALUES.\n                Model Specification:\n                U_accept = alpha * split_self - beta * max((split_opp / (3 * token_opp)) - 1, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from accepting\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to proposer greed (taking more than their contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the responder\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed monetary amount for the proposer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent (proposer's contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        token_opp = trial.get(\"token_opp\", 1) #Handle potential division by zero\n\n        if token_opp == 0:\n            proposer_greed_penalty = 0 # Avoid division by zero\n        else:\n            proposer_greed_penalty = max((split_opp / (3 * token_opp)) - 1, 0)\n\n        utility = alpha * split_self - beta * proposer_greed_penalty\n        utility_values.append(utility)\n    return utility_values\n\n# Data as list of dictionaries (example data)\ndata_json = '[{\"split_self\": 10, \"split_opp\": 20, \"token_opp\": 5}, {\"split_self\": 15, \"split_opp\": 15, \"token_opp\": 5}, {\"split_self\": 5, \"split_opp\": 25, \"token_opp\": 0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data, alpha=2, beta=1)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n\n```"
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to absolute monetary gain from accepting",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to proposer greed (taking more than their contribution)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U_accept = alpha * split_self - beta * max((split_opp / (3 * token_opp)) - 1, 0)",
  "variable_descriptions": {
    "U_accept": {
      "description": "Utility of accepting the offer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha": {
      "description": "Sensitivity to absolute monetary gain from accepting",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to proposer greed (taking more than their contribution)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed monetary amount for the responder",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_opp": {
      "description": "Proposed monetary amount for the proposer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent (proposer's contribution)",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the responder's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3*token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer for responders in the given experiment. The model needs to be based on psychological principles, use the dataset variables appropriately, and have no more than two learnable parameters. Also, it should avoid parameters for stochasticity since that's handled later by a temperature parameter.\n\nFirst, I need to consider the previous models to ensure I'm not repeating the same approaches. The existing models have explored various fairness mechanisms, such as comparing the split to one's own contribution (3*token_self), inequity aversion between split_self and split_opp, and combinations of absolute gain with different fairness terms. The best-performing models in terms of BIC and parameter recovery are Models 2, 6, 7, and 10. These models often combine absolute monetary gain (split_self) with some form of fairness violation relative to contributions.\n\nLooking at the dataset variables, key variables for responders are token_self, token_opp, split_self, split_opp, and combined_earning. The trial_type indicates whether contributions are equal, opponent more, or participant more. The responder's decision (accept) depends on the utility of the offer.\n\nPsychological principles involved here might include fairness considerations, loss aversion, and reciprocity. For example, people might feel entitled to a share proportional to their contribution. If the offer is below their contribution, they might perceive it as unfair. Alternatively, they might compare their split to the opponent's, leading to inequity aversion.\n\nBut previous models have already used terms like (split_self - 3*token_self) and max(3*token_self - split_self, 0). Model 10 even combines underpayment and proposer greed. However, Model 7 introduces a term where the responder penalizes the proposer if the proposer's split exceeds their contribution. That seems like a different angle, focusing on the proposer's fairness.\n\nAnother angle could be the relative comparison between the splits and contributions. For instance, if the responder's split is less than their contribution, and the proposer's split is more than their contribution, that might be a double unfairness. But Model 10 already combines these two terms.\n\nWait, Model 10's specification is: max(3*token_self - split_self, 0) + max(split_opp - 3*token_opp, 0). So it's adding two penalties: one for the responder getting less than their contribution, and another for the proposer getting more than theirs. The beta parameter then scales the combined penalty. That model has a BIC of ~31, which is better than some others. But maybe there's a way to combine these two terms in a multiplicative way or another form that hasn't been tried yet.\n\nAlternatively, perhaps the responder cares about the ratio of their split to their contribution compared to the proposer's split to their contribution. For example, if the responder's split_self/(3*token_self) is less than the proposer's split_opp/(3*token_opp), that could be a fairness violation. This ratio comparison might capture a different aspect of fairness, where the responder expects the splits to be proportional to contributions.\n\nLet me think: Suppose the responder's fair share is (3*token_self) / combined_earning * 100% (since combined_earning is 3*(token_self + token_opp)). But splitperc_self is the percentage the responder is offered. The difference between splitperc_self and their contribution percentage could be a fairness term. However, Model 1 uses (splitperc_self - (token_self/(token_self + token_opp))*100), which is exactly that. But Model 1's BIC is higher (worse) than some others, so maybe that's not the best approach.\n\nAlternatively, the responder might compare their split to both their own contribution and the proposer's contribution. For example, if the responder's split is less than their contribution, and the proposer's split is more than their contribution, that's a double inequity. But Model 10 adds these two terms. Maybe a product of these terms would capture a synergistic effect, but that would introduce a non-linear term which might complicate parameter recovery. However, the problem states that the model should have no more than two parameters, so adding a product term would still be within the limit if the parameters are kept to two.\n\nWait, but the current models have tried additive terms. A multiplicative interaction might be a new direction. For example, the utility could be:\n\nU_accept = alpha * split_self + beta * (max(3*token_self - split_self, 0)) * (max(split_opp - 3*token_opp, 0))\n\nHere, the fairness term is the product of two penalties: the responder being underpaid and the proposer being overpaid. This would mean that the penalty is only applied when both conditions are true. If either one is not violated, the penalty is zero. But maybe that's too restrictive. Alternatively, using the product of the two max terms would create a term that's non-zero only when both underpayment and overpayment occur. However, this might not capture cases where only one of the two is violated. Alternatively, maybe the sum of the two max terms multiplied together? Not sure.\n\nAlternatively, the responder might care about the ratio of their split to their contribution versus the proposer's split to their contribution. For example, if (split_self / (3*token_self)) < (split_opp / (3*token_opp)), that's unfair. The difference between these ratios could be a fairness term. But this would require calculating both ratios and their difference. However, this might be similar to existing models but in a different form.\n\nAnother angle: the responder's perception of fairness could be based on the difference between the actual split and the equitable split. The equitable split would be each party getting their contribution (3*token_self for responder, 3*token_opp for proposer), and any surplus (combined_earning - 3*(token_self + token_opp)) is split somehow. Wait, but combined_earning is 3*(token_self + token_opp), since each token is \u00a33. So the total is exactly 3*(sum of tokens). Therefore, the equitable split would be each getting their own contribution. So any split where split_self is less than 3*token_self is taking from the responder's contribution. Similarly, split_opp being more than 3*token_opp would mean the proposer is taking more than their contribution.\n\nBut in the task description, it's mentioned that the combined earning is the total, so split_self + split_opp = combined_earning. Therefore, if split_self < 3*token_self, then split_opp must be > 3*token_opp, because 3*(token_self + token_opp) = combined_earning. So if the responder is getting less than their contribution, the proposer is automatically getting more than theirs. Therefore, the two max terms in Model 10 are actually capturing the same underlying condition. Because if split_self < 3*token_self, then split_opp = combined_earning - split_self > 3*token_opp. So the two terms in Model 10 are redundant. That's a key insight. Because combined_earning is fixed as 3*(token_self + token_opp), then split_self + split_opp = 3*(token_self + token_opp). Therefore, split_self = 3*token_self + delta implies split_opp = 3*token_opp - delta. So if delta is negative (split_self < 3*token_self), then split_opp = 3*token_opp + |delta|. Therefore, whenever the responder is underpaid, the proposer is overpaid by the same amount. Therefore, the two terms in Model 10 are perfectly correlated. That means Model 10's two max terms are actually capturing the same thing, which might lead to multicollinearity issues and make parameter recovery difficult. \n\nThis is a crucial point. Because split_self and split_opp are dependent variables (they sum to a fixed total), any model that includes both terms related to split_self and split_opp might have high covariance between parameters. The problem statement mentions that the model should be considerate of high covariance between parameters to pass parameter recovery tests. Therefore, including both terms that are perfectly correlated (like underpayment of self and overpayment of opp) would lead to parameters that are not recoverable because they are colinear.\n\nTherefore, models that include both terms (like Model 10) might have issues with parameter recovery. However, in Model 10's parameter recovery, beta has a high r (0.844), which suggests that perhaps the model is still manageable. But maybe there's a better way to model this.\n\nGiven that split_self < 3*token_self implies split_opp > 3*token_opp, the two terms are perfectly inversely related. Therefore, including both terms in the model is redundant. So perhaps the best approach is to model only one of them. For example, the underpayment of self (split_self < 3*token_self) is equivalent to the overpayment of the opponent. Therefore, a model that includes only one of these terms would capture the same effect. \n\nLooking back at previous models, Model 6 uses max(3*token_self - split_self, 0), which is the underpayment term. Model 7 uses max(split_opp - 3*token_opp, 0), which is the overpayment term. But since these are two sides of the same coin, the parameters in these models would capture similar effects. However, depending on how participants perceive the unfairness, they might focus on their own underpayment or the proposer's overpayment. \n\nBut according to the task description, the proposer's split_opp is the amount the opponent would take. So when the responder is deciding to accept or reject, they see both their own split and the proposer's split. The key is whether the responder's utility is affected by how much the proposer is taking relative to their contribution. \n\nIn Model 7, the utility includes a term for when the proposer's split exceeds their contribution. The idea is that responders might reject offers not just because they're getting less, but because the proposer is being greedy. However, given the dependency between split_self and split_opp, this term is equivalent to the responder's underpayment. \n\nBut perhaps participants don't realize this dependency and evaluate both terms separately. Alternatively, they might focus on their own share relative to their contribution, regardless of the proposer's share. \n\nAnother angle is the ratio of the split to the contribution. For example, split_self / (3*token_self) versus split_opp / (3*token_opp). If these ratios are unequal, that could be a fairness violation. For instance, if the responder's ratio is lower than the proposer's, that's unfair. The difference between these ratios could be a term in the utility function. \n\nSo the fairness term could be (split_self/(3*token_self) - split_opp/(3*token_opp)). If this is negative, it indicates the responder is getting a worse ratio than the proposer. However, since split_self + split_opp = 3*(token_self + token_opp), this ratio difference might simplify in a way that's captured by existing terms. Let's see:\n\nsplit_self/(3*token_self) - split_opp/(3*token_opp) = [split_self * token_opp - split_opp * token_self] / [3*token_self*token_opp]\n\nBut since split_opp = 3*(token_self + token_opp) - split_self, substituting:\n\n= [split_self * token_opp - (3*(token_self + token_opp) - split_self) * token_self] / [3*token_self*token_opp]\n\nSimplifying numerator:\n\nsplit_self * token_opp - 3*(token_self + token_opp)*token_self + split_self * token_self\n\n= split_self (token_opp + token_self) - 3*token_self*(token_self + token_opp)\n\n= (split_self - 3*token_self) * (token_self + token_opp)\n\nSo the ratio difference term simplifies to (split_self - 3*token_self) * (token_self + token_opp) / [3*token_self*token_opp]\n\nWhich is proportional to (split_self - 3*token_self). Therefore, this ratio difference term is essentially capturing the same underpayment term as in Model 2. So using this ratio difference might not add new information beyond existing models.\n\nHmm. So maybe the key is to find a different way to represent the fairness term that hasn't been captured in previous models. Let's think about the psychological principles again. \n\nOne principle is loss aversion: people are more sensitive to losses than gains. In this context, getting less than one's contribution is a loss, and the disutility from that loss is greater than the utility from a gain. However, Model 6 uses a max term for underpayment, which captures loss aversion relative to the contribution. \n\nAnother principle is reciprocity: responders might be more lenient if the proposer is also being fair in some way. But given that the proposer's split is determined by the offer, which is fixed in each trial, perhaps responders evaluate the proposer's intent based on their own contribution. \n\nWait, the proposer's contribution is token_opp (the number of tokens they found in the pretask). So if the proposer contributed more, maybe the responder expects them to take a larger share. But in the task, the proposer's split_opp is their proposed share. So if the proposer is taking more than their contribution, that's seen as unfair. But if they contributed more, maybe it's acceptable. \n\nAlternatively, the responder's perception of fairness could depend on both their own and the proposer's contributions. For example, the fair split could be proportional to contributions: split_self / split_opp = token_self / token_opp. If the actual split deviates from this ratio, that's unfair. \n\nThe fair percentage for the responder would be token_self / (token_self + token_opp) * 100, which is what splitperc_self should be if it's proportional. The variable splitperc_self is provided in the dataset, so the difference between splitperc_self and the fair percentage could be a term. Model 1 uses this difference. However, Model 1's BIC is higher than others, suggesting it's not the best model. \n\nAlternatively, the disutility could be proportional to the squared difference, introducing a non-linear term. But the problem states that the model should have no more than two parameters, and we need to keep it simple. Also, non-linear terms might complicate parameter recovery. \n\nAnother angle: the utility could depend on the absolute difference between the proposed split and the fair split. The fair split for the responder is 3*token_self, so the difference is (split_self - 3*token_self). Model 2 uses this term linearly. Model 6 uses a max function to capture only underpayment. \n\nBut perhaps the responder's sensitivity to this difference depends on the total combined earning. For example, a \u00a31 difference in a small pot might matter more than in a large pot. Model 9 scales the difference by the total combined earning (3*(token_self + token_opp)), but its BIC is higher, indicating worse performance. \n\nAlternatively, the responder might have reference points based on both their own contribution and the proposer's. For example, the sum of the deviations: (split_self - 3*token_self) + (split_opp - 3*token_opp). But since split_self + split_opp = 3*(token_self + token_opp), this sum is zero. Therefore, this term is always zero, which is not useful. \n\nWait, that's right. Because split_self + split_opp = 3*(token_self + token_opp), so (split_self - 3*token_self) + (split_opp - 3*token_opp) = 0. So any model that uses the sum of these two terms would be zero, making it irrelevant. \n\nTherefore, the key is that any unfairness in one direction is exactly offset in the other. So models that focus on one side (either responder's underpayment or proposer's overpayment) are sufficient. \n\nGiven that, perhaps the best approach is to use a term that captures the responder's underpayment relative to their contribution, as in Model 6. However, Model 6's BIC is 31.09, which is better than some but not the best. The best BIC is Model 7 with 30.93. Model 7 uses the proposer's overpayment as the fairness term. \n\nBut why does Model 7 perform well? It's possible that responders are more sensitive to the proposer's greed (taking more than their contribution) than their own underpayment. This could be a form of inequity aversion focused on the proposer's behavior. \n\nAlternatively, combining both terms (own underpayment and proposer's overpayment) as in Model 10, but given the dependency between the two terms, this might not add value. However, Model 10's BIC is 31.01, which is slightly worse than Model 7. \n\nSo perhaps a better approach is to consider a different psychological mechanism. For example, the responder's utility might depend on the ratio of their split to the proposer's split, adjusted by their contributions. If the ratio of splits is different from the ratio of contributions, that's unfair. \n\nMathematically, the fair ratio would be (split_self / split_opp) = (token_self / token_opp). If the actual ratio deviates from this, the utility decreases. \n\nTo model this, we could take the difference between (split_self / split_opp) and (token_self / token_opp). However, this introduces a non-linear term. Alternatively, use the log ratio to make it linear, but that complicates things. \n\nAlternatively, compute the product of split_self and token_opp versus split_opp and token_self. The fair condition is split_self * token_opp = split_opp * token_self. The difference (split_self * token_opp - split_opp * token_self) could be a fairness term. If this is positive, the responder is getting more than their fair share; if negative, less. \n\nBut since split_opp = combined_earning - split_self = 3*(token_self + token_opp) - split_self, substituting into the term:\n\nsplit_self * token_opp - (3*(token_self + token_opp) - split_self) * token_self \n\n= split_self * token_opp - 3*token_self*(token_self + token_opp) + split_self * token_self \n\n= split_self (token_opp + token_self) - 3*token_self*(token_self + token_opp)\n\n= (split_self - 3*token_self) * (token_self + token_opp)\n\nSo this term is proportional to (split_self - 3*token_self), which is the same as Model 2's fairness term. Therefore, this approach doesn't add new information. \n\nHmm. It seems like many of these approaches boil down to the same underlying fairness term. Therefore, perhaps the best way to proceed is to consider a model that has shown good parameter recovery and BIC, but hasn't been explored in a certain variation. \n\nLooking at Model 7, which uses the proposer's overpayment (split_opp - 3*token_opp), it has a BIC of 30.93 and good parameter recovery (r=0.868 and 0.865). This suggests that responders are sensitive to the proposer's share exceeding their contribution. \n\nBut why would that be? Perhaps responders are not just focused on their own share but also on whether the proposer is taking more than they deserve. This could be a form of fairness where the responder punishes the proposer for being greedy, even if their own share is acceptable. \n\nHowever, given the dependency between split_self and split_opp, when split_opp exceeds 3*token_opp, split_self must be less than 3*token_self. So Model 7's term is equivalent to Model 6's term but framed differently. \n\nBut the parameter recovery for Model 7 is good, suggesting that this framing works well. Maybe participants are more attuned to the proposer's share than their own in terms of contribution. \n\nAlternatively, combining both terms in a way that hasn't been done before. For example, using the product of the responder's underpayment and the proposer's overpayment. But as discussed earlier, since these are perfectly correlated, the product would be the square of one term. \n\nWait, if split_self < 3*token_self, then split_opp = 3*(token_self + token_opp) - split_self > 3*token_opp. So the proposer's overpayment is exactly equal to the responder's underpayment. Let me check:\n\nsplit_opp - 3*token_opp = (3*(token_self + token_opp) - split_self) - 3*token_opp = 3*token_self - split_self.\n\nSo split_opp - 3*token_opp = 3*token_self - split_self. Therefore, the two terms are exactly equal. Therefore, any model that uses either term is equivalent. For example, Model 6's term max(3*token_self - split_self, 0) is the same as Model 7's term max(split_opp - 3*token_opp, 0). Therefore, these models are mathematically identical. The difference is just in how the term is framed. \n\nThis is a critical realization. Therefore, Models 6 and 7 are actually the same model, just expressed differently. This explains why their BIC and parameter recovery are similar. \n\nGiven this, perhaps the best approach is to consider a model that hasn't been tried yet, which might involve a different psychological mechanism. \n\nAnother angle could be the relative difference between the proposed split and the fair split. For example, the utility could depend on the percentage difference from the fair share. \n\nThe fair share for the responder is 3*token_self. The proposed split_self could be compared to this. The percentage difference is (split_self - 3*token_self)/ (3*token_self). This term would capture proportional deviations from the fair share. If split_self is 10% below the fair share, the term is -0.1; if it's 20% above, +0.2. \n\nModel 3 uses a similar term: (split_self - 3*token_self)/(3*token_self). However, Model 3's BIC is 32.25, which is worse than Models 2, 6, 7, and 10. So this approach might not be better. \n\nAlternatively, the responder might have a non-linear utility for money. For example, diminishing sensitivity to larger amounts. But the problem states that the model should not include parameters for stochasticity, but it doesn't prohibit non-linear transformations. However, introducing non-linear terms might require more parameters, but we're limited to two. \n\nAnother idea: the responder's utility could depend on the difference between their split and the proposer's split, scaled by their contributions. For example, (split_self - split_opp) / (3*(token_self - token_opp)). But this might not make sense if contributions are equal. \n\nAlternatively, the responder might compare their split to the proposer's split in light of their respective contributions. For example, the difference (split_self - split_opp) compared to the difference in contributions (3*token_self - 3*token_opp). If the split difference is less than the contribution difference, that's unfair. \n\nThe term could be (split_self - split_opp) - (3*token_self - 3*token_opp). If this is negative, the responder is getting less than their contribution-based entitlement relative to the proposer. \n\nSimplifying, this term is (split_self - 3*token_self) - (split_opp - 3*token_opp). But since split_self + split_opp = 3*(token_self + token_opp), this term becomes (split_self - 3*token_self) - (3*(token_self + token_opp) - split_self - 3*token_opp) = 2*(split_self - 3*token_self). So this term is just twice the underpayment term. Therefore, this approach is equivalent to Model 2's term. \n\nHmm. It seems challenging to find a new term that isn't redundant with existing models. Given that, perhaps the best approach is to build on the best-performing models and see if a slight variation can be introduced. \n\nModel 2 has a BIC of 30.77, which is better than Model 7's 30.93. Model 2's specification is U_accept = alpha * split_self + beta * (split_self - 3 * token_self). Here, beta would be positive if higher deviations above the fair share increase utility, but since responders are likely to reject when split_self is below the fair share, beta should be positive to penalize negative values. Wait, no: if split_self is less than 3*token_self, then (split_self - 3*token_self) is negative. If beta is positive, this term would decrease the utility, making acceptance less likely. So beta should be positive to penalize underpayment. \n\nModel 2's parameter recovery for beta is r=0.903, which is very good. So this model is performing well. However, perhaps there's a way to improve it by incorporating another aspect. \n\nWait, the problem states that the model should have no more than two learnable parameters. Model 2 already uses two parameters: alpha and beta. So we can't add more. \n\nBut maybe a different combination. For example, combining the absolute split_self with a term that is the product of split_self and the fairness term. But that would require more parameters. \n\nAlternatively, using a non-linear combination, but with the same two parameters. For example, U_accept = alpha * split_self + beta * (split_self / (3*token_self)). This would make the fairness term a ratio, capturing how much the split is relative to the fair share. If split_self is equal to 3*token_self, the ratio is 1. If it's less, the ratio is less than 1. But this is similar to Model 3's approach, which had a worse BIC. \n\nAlternatively, using a reference point other than the fair share. For example, the responder might compare their split to the proposer's split, irrespective of contributions. This is the inequity aversion model (Model 4), which has a BIC of 34.61, worse than Model 2. \n\nAnother angle: the responder's utility could depend on whether the split is above or below the fair share, with different sensitivities. For example, a piecewise function where the penalty for underpayment is larger than the reward for overpayment. But this would require more parameters. \n\nWait, but the problem allows up to two parameters. Model 6 uses a max function for underpayment, which is a form of loss aversion. It has a BIC of 31.09, which is slightly worse than Model 2. \n\nGiven that Model 2 is performing well, perhaps a better approach is to consider that the fairness term (split_self - 3*token_self) could interact with the trial_type. For example, in trials where the participant contributed more (trial_type=3), they might be more sensitive to underpayment. However, trial_type is a categorical variable, and incorporating it would require additional parameters or interactions, which might exceed the two-parameter limit. \n\nAlternatively, the model could weight the fairness term by the participant's contribution relative to the proposer's. For example, (split_self - 3*token_self) * (token_self / (token_self + token_opp)). This would mean that participants who contributed more are more sensitive to fairness violations. Model 5 uses a similar approach, weighting the fairness term by the contribution proportion. Model 5's BIC is 32.20, which is worse than Model 2. \n\nAnother idea: the utility could depend on the absolute amount of the fairness violation, not just the direction. For example, a quadratic term (split_self - 3*token_self)^2. This would penalize both under and overpayment, but since overpayment is rare (proposers are unlikely to offer more than the fair share), this might not be useful. Also, it introduces a non-linear term with the same parameter, which might not improve the model. \n\nAlternatively, using the absolute value of the fairness violation: |split_self - 3*token_self|. This would penalize any deviation from the fair share, but responders are likely only penalizing underpayment. Model 2's linear term allows for different effects of under vs overpayment, but if beta is positive, underpayment (negative term) reduces utility, while overpayment (positive term) increases it. However, in reality, responders might not care about overpayment (getting more than their contribution), so the model might benefit from only penalizing underpayment. \n\nThis is what Model 6 does: max(3*token_self - split_self, 0). This term is zero when split_self >= fair share, and positive otherwise. If beta is negative, this term reduces utility when there's underpayment. However, in Model 6's specification, beta is a learnable parameter, so the sign is determined during fitting. \n\nWait, in Model 6's specification: U_accept = alpha * split_self + beta * max(3*token_self - split_self, 0). If beta is negative, then when there's underpayment (max term positive), the utility decreases. So beta should be negative to penalize underpayment. However, in the parameter recovery for Model 6, beta has r=0.836, which suggests that the fitted beta values are positive. That would imply that underpayment increases utility, which is counterintuitive. This might indicate a mistake in the model's formulation or in the parameter recovery reporting. \n\nWait, perhaps there's a sign error. If the model is U_accept = alpha * split_self + beta * max(3*token_self - split_self, 0), then when split_self is less than 3*token_self, the max term is positive. If beta is negative, this term subtracts from the utility, making acceptance less likely. So beta should be negative. However, in the parameter recovery, beta's r is 0.836, which is a high positive correlation. This suggests that in the simulated data, beta was positive, leading to higher utility when underpayment occurs, which doesn't make sense. This inconsistency might indicate that Model 6's formulation is flawed or that the parameter recovery was incorrectly reported. \n\nThis is a critical point. If the parameter recovery for beta in Model 6 is positive, but the term should penalize underpayment, then there's a problem. Perhaps the model should have a negative sign before beta. For example, U_accept = alpha * split_self - beta * max(3*token_self - split_self, 0), with beta constrained to be positive. This way, underpayment reduces utility. But in the given Model 6, the specification is as written, and the parameter recovery for beta is positive. This suggests that in the model, a positive beta increases utility when underpayment occurs, which is the opposite of what's expected. This could be a mistake in the model's design. \n\nGiven this confusion, perhaps the best approach is to revisit the model formulation. For example, in Model 2, the term (split_self - 3*token_self) can be positive or negative. If beta is positive, then when split_self is below the fair share, the term is negative, reducing utility. When above, it's positive, increasing utility. This could explain why responders accept offers above their fair share and reject below. However, in reality, responders might not derive extra utility from overpayment; they might simply not penalize it. So Model 2's linear term might not capture the asymmetry of loss aversion. \n\nModel 6's max term addresses this by only penalizing underpayment. But if the parameter recovery for beta is positive, that suggests that the model is incorrectly specified. Therefore, perhaps the correct formulation should have a negative beta. \n\nThis highlights the importance of carefully considering the sign of parameters in the model. To avoid such issues, the model should be structured so that a positive beta corresponds to a penalty for underpayment. \n\nGiven all this, perhaps the best approach is to propose a model that combines the absolute monetary gain (split_self) with a term that penalizes underpayment relative to the fair share, using a max function to capture loss aversion. This is similar to Model 6 but ensuring the parameter has the correct sign. \n\nHowever, since Model 6 already exists and has a certain BIC, perhaps a better approach is to consider a different psychological mechanism. For example, the responder might evaluate the offer based on the proposer's intention, which could be inferred from the proposer's own contribution. \n\nFor instance, if the proposer contributed less (token_opp is low), the responder might expect them to take a smaller share. If the proposer takes a large share despite contributing little, the responder might perceive this as unfair. This is similar to Model 7, where the term is max(split_opp - 3*token_opp, 0). \n\nBut given that split_opp - 3*token_opp is equal to 3*token_self - split_self, this term is equivalent to the underpayment term. Therefore, Models 6 and 7 are two sides of the same coin. \n\nGiven that, perhaps the best new model is one that combines the absolute monetary gain with a term that captures the proposer's greed relative to their contribution, but in a way that hasn't been modeled yet. \n\nWait, another idea: the responder's utility could depend on the ratio of the proposer's split to their contribution. For example, (split_opp / (3*token_opp)) - 1. If this is positive, the proposer is taking more than their contribution. The term could be max((split_opp / (3*token_opp)) - 1, 0), which captures the proposer's greed as a percentage over their contribution. \n\nThis term would be zero if the proposer takes exactly their contribution or less, and positive otherwise. The utility function would then be:\n\nU_accept = alpha * split_self - beta * max((split_opp / (3*token_opp)) - 1, 0)\n\nHere, alpha is the sensitivity to monetary gain, and beta is the sensitivity to the proposer's greed. This model hasn't been tried before and introduces a new fairness mechanism: responders penalize offers where the proposer takes a higher percentage of their contribution. \n\nHowever, since split_opp = 3*(token_self + token_opp) - split_self, this term can be rewritten in terms of split_self and token_self. But perhaps participants perceive it as the proposer's greed, making this a psychologically distinct mechanism. \n\nThis model would have two parameters: alpha and beta. The term max((split_opp / (3*token_opp)) - 1, 0) is zero when split_opp <= 3*token_opp, and positive otherwise. This captures how much the proposer is taking relative to their own contribution. \n\nFor example, if the proposer contributed 10 tokens (3*10=\u00a330), and takes \u00a340, then the term is (40/30 -1) = 0.333. Beta scales how much this affects the utility. \n\nThis model is different from previous ones because it focuses on the proposer's share relative to their own contribution, not the responder's. This could tap into a sense of fairness where the responder expects the proposer to not take more than they contributed, regardless of the responder's own share. \n\nHowever, given the dependency between split_self and split_opp, this term is still related to the responder's underpayment. But psychologically, it's a different evaluation. For example, a responder might accept an offer that's slightly below their fair share if the proposer isn't being greedy, but reject it if the proposer is taking way more than their contribution. \n\nThis model could explain behavior that isn't captured by models focusing solely on the responder's share. The parameter recovery would indicate whether this proposer-greed term is a significant predictor. \n\nGiven that this model hasn't been tried in the previous models, it's a viable candidate. The BIC would depend on how well it fits the data, but since it's a new mechanism, it's worth exploring. \n\nTherefore, the proposed model is:\n\nU_accept = alpha * split_self - beta * max((split_opp / (3 * token_opp)) - 1, 0)\n\nHere, alpha is the sensitivity to monetary gain (positive), and beta is the sensitivity to the proposer's greed (positive, as higher values increase the penalty). \n\nThis model uses two learnable parameters, alpha and beta, and the variables split_self, split_opp, and token_opp from the dataset. The max function ensures that only cases where the proposer takes more than their contribution are penalized. \n\nThis model builds on the psychological principle of aversion to proposer greed, where responders dislike offers where the proposer keeps more than their fair share based on their contribution. It also includes the absolute monetary gain, which is a standard economic factor. \n\nThe variables involved are:\n\n- split_self: the responder's proposed share (monetary gain).\n- split_opp: the proposer's share.\n- token_opp: the proposer's contribution in tokens.\n\nThe term (split_opp / (3 * token_opp)) calculates the proposer's share relative to their contribution. Subtracting 1 gives the percentage over their contribution. The max function ensures we only consider cases where the proposer takes more than their contribution. \n\nThis model meets all the requirements: it predicts responder behavior, uses relevant psychological principles (aversion to proposer greed), has two learnable parameters, and uses variables available in the dataset. It also avoids high covariance between parameters since alpha and beta scale different terms. \n\nThe parameter ranges can be set as alpha >= 0 (since more money should increase utility) and beta >= 0 (since proposer greed should decrease utility). The bounds can be generous, like alpha in [0, 10] and beta in [0, 10], allowing the model to capture varying sensitivities. \n\nThis model hasn't been tested in the previous models, so it adds a new explanation to the space of possible models. If the data shows that responders are sensitive to the proposer's greed beyond their own underpayment, this model could perform well.\n</think>\n\n<MODEL>\nU_accept = alpha * split_self - beta * max((split_opp / (3 * token_opp)) - 1, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from accepting\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to proposer greed (taking more than their contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the responder\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed monetary amount for the proposer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent (proposer's contribution)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the responder's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3*token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\n</SUMMARY>",
  "current_model": "Specification: U_accept = alpha * split_self - beta * max((split_opp / (3 * token_opp)) - 1, 0)\nSummary: A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the responder's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3*token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.",
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        token_opp = trial.get(\"token_opp\", 1) #Handle potential division by zero\n\n        if token_opp == 0:\n            proposer_greed_penalty = 0 # Avoid division by zero\n        else:\n            proposer_greed_penalty = max((split_opp / (3 * token_opp)) - 1, 0)\n\n        utility = alpha * split_self - beta * proposer_greed_penalty\n        utility_values.append(utility)\n    return utility_values",
  "fitting_results": [
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.5333648843746,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7068367254849455,
      "beta": 5.4111200528287515,
      "success": true,
      "log_likelihood": -19.337057935398704,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2915965022161469,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.82078070737004,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9665610397502404,
      "beta": 9.651721102779453,
      "success": true,
      "log_likelihood": -15.554810471453264,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.892122957896729,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -11.417969537967856,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.259409458881977,
      "beta": 5.753472644784024,
      "success": true,
      "log_likelihood": -15.93951474329903,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228865,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1933697992674883,
      "beta": 4.926837167374591,
      "success": true,
      "log_likelihood": -16.618686858130143,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.0064721874752288674,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7828587579407351,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -13.510097150635138,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.028355279648471,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.160216286160292,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.950031550955503,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8517061297273707,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -15.220600676174367,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228864,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228864,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228867,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228865,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.3170621894718835,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -4.424582918559798,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.1832371965537525,
      "beta": 5.619679803115006,
      "success": true,
      "log_likelihood": -7.461691911144007,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228866,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.952940934240574,
      "beta": 6.882842336464838,
      "success": true,
      "log_likelihood": -17.22750352213909,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.193368142333269,
      "beta": 3.7148993315131325,
      "success": true,
      "log_likelihood": -10.139066218723217,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.48991178959086995,
      "beta": 5.87712679004793,
      "success": true,
      "log_likelihood": -19.37849164913186,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7932518249438419,
      "beta": 4.886462816530256,
      "success": true,
      "log_likelihood": -19.298458826320005,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7755025314105299,
      "beta": 7.532751341928188,
      "success": true,
      "log_likelihood": -17.428051727725652,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2960345427388429,
      "beta": 7.76734384695997,
      "success": true,
      "log_likelihood": -15.019595570492543,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9353712586387414,
      "beta": 5.146925651469092,
      "success": true,
      "log_likelihood": -18.295927827014832,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.42566694171291014,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.872315451960613,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.127945400651143,
      "beta": 3.08387007997722,
      "success": true,
      "log_likelihood": -9.982101024869673,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.192105252207518,
      "beta": 2.843696358549843,
      "success": true,
      "log_likelihood": -16.48474372589908,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228866,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.533364884374603,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3408590633215519,
      "beta": 3.170736220315857,
      "success": true,
      "log_likelihood": -22.568828564454293,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5983189675425848,
      "beta": 7.145212713568689,
      "success": true,
      "log_likelihood": -18.02851879913564,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.006472187475228865,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4053434863612648,
      "beta": 5.160148465902216,
      "success": true,
      "log_likelihood": -15.119986258884422,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0857311407602306,
      "beta": 8.246399633618745,
      "success": true,
      "log_likelihood": -11.003662996383158,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7708479065878922,
      "beta": 6.609119004636469,
      "success": true,
      "log_likelihood": -18.15354048237281,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8124331677083464,
      "beta": 3.0051679870633077,
      "success": true,
      "log_likelihood": -19.931048292901863,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7103837718733979,
      "beta": 0.6938163217234299,
      "success": true,
      "log_likelihood": -9.741636989448587,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6152863474028271,
      "beta": 2.790595313292621,
      "success": true,
      "log_likelihood": -21.549776424355283,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.24535330902375063,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -12.496813233712787,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 2.0067503663207673,
      "success": true,
      "log_likelihood": -23.01197175907107,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7778450437942851,
      "beta": 3.9673745203002673,
      "success": true,
      "log_likelihood": -19.91808961976779,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.207334396311325,
      "beta": 6.003786107184702,
      "success": true,
      "log_likelihood": -16.18924928616695,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1689123623061528,
      "beta": 2.6723079429255545,
      "success": true,
      "log_likelihood": -16.603038164691924,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6387816497478197,
      "beta": 3.907254786472152,
      "success": true,
      "log_likelihood": -20.811946936944196,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5197436097903427,
      "beta": 5.92758581671849,
      "success": true,
      "log_likelihood": -19.308286426523644,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7327604249096584,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -14.430756008627542,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3895879931927936,
      "beta": 1.8872164195024081,
      "success": true,
      "log_likelihood": -23.3435295540637,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8721524154604363,
      "beta": 2.0746333611663603,
      "success": true,
      "log_likelihood": -19.150485932811325,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 8.317419984750998,
      "success": true,
      "log_likelihood": -14.50758195413268,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7238668571389684,
      "beta": 3.964843922937092,
      "success": true,
      "log_likelihood": -20.27349952937636,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6128653928401591,
      "beta": 2.0841905317437615,
      "success": true,
      "log_likelihood": -21.659775608039485,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6004930981900567,
      "beta": 9.319325111458795,
      "success": true,
      "log_likelihood": -16.02904082437075,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.630270529155826,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.66277427907523,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4359608100522727,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -13.108715875366144,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1376584492883153,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -13.680488206347578,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -11.533364884139559,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.281870678023239,
      "beta": 1.4858808310231726,
      "success": true,
      "log_likelihood": -14.437935406072636,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.625077000390529,
      "beta": 2.3978170993733903,
      "success": true,
      "log_likelihood": -21.555131380410398,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6645830663575131,
      "beta": 5.555252245986592,
      "success": true,
      "log_likelihood": -19.371716564219522,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.533364884374603,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5774514408650353,
      "beta": 4.918496853240993,
      "success": true,
      "log_likelihood": -20.225907431956497,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.31990694381501517,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -13.3221744934951,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.977452668846765,
      "beta": 5.435200505828894,
      "success": true,
      "log_likelihood": -17.888032296238055,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7614210848054885,
      "beta": 0.4526466592097786,
      "success": true,
      "log_likelihood": -9.047034723887698,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7301925057482437,
      "beta": 2.750252917377606,
      "success": true,
      "log_likelihood": -12.122294531938868,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0927432141840958,
      "beta": 5.517118522415152,
      "success": true,
      "log_likelihood": -17.11295464605986,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3560471959470721,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.540124489367187,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -8.033364885584644,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 7.656169929460981,
      "success": true,
      "log_likelihood": -15.205618955349031,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9453211867977239,
      "beta": 6.453929851731165,
      "success": true,
      "log_likelihood": -17.51548159740869,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3821872156863315,
      "beta": 2.664660265104513,
      "success": true,
      "log_likelihood": -14.720696821805028,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.31405777006484203,
      "beta": 1.9203292819266646,
      "success": true,
      "log_likelihood": -23.695283089934232,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9308139971711468,
      "beta": 2.7261539060507007,
      "success": true,
      "log_likelihood": -18.842322616867197,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.3559427518759226,
      "beta": 1.915665021755781,
      "success": true,
      "log_likelihood": -7.579757034081533,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -11.033364883581067,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5430075794344474,
      "beta": 1.51612529224299,
      "success": true,
      "log_likelihood": -22.174740003946958,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.533364884374603,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3056539118463888,
      "beta": 1.4738470723716697,
      "success": true,
      "log_likelihood": -23.9188021084984,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3338932849528353,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.74186065422016,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -11.950031550720462,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4571546536416944,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -12.570949040832309,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3139623863299736,
      "beta": 0.8165131492825595,
      "success": true,
      "log_likelihood": -23.870143516736043,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -10.533364884374599,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.842609377535937,
      "beta": 2.4820733989929193,
      "success": true,
      "log_likelihood": -19.608453419129027,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.20134097197367354,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -12.67291363661447,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.6280218454872033,
      "beta": 4.311420759357534,
      "success": true,
      "log_likelihood": -5.5316610187627475,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2553803510831023,
      "beta": 8.74786629536459,
      "success": true,
      "log_likelihood": -14.885720572219762,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.565045302957679,
      "beta": 0.5933021345181717,
      "success": true,
      "log_likelihood": -21.306251878821573,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.9239739231634538,
        "p": 1.075832730265513e-42
      },
      "beta": {
        "r": 0.6105601724346882,
        "p": 1.5343020917882244e-11
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 34.739884042639666,
  "bic_error": "'ModelOutput' object has no attribute 'metrics'",
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.924\n- beta: r = 0.611",
  "previous_models": [
    "Specification: U_accept = alpha * split_self - beta * max((split_opp / (3 * token_opp)) - 1, 0)\nSummary: A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the responder's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3*token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\nBIC: 34.739884042639666\n\nParameter Recovery:\n- alpha: r = 0.924\n- beta: r = 0.611"
  ]
}