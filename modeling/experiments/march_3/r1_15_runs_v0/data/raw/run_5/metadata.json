{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. If there are no previous models, start with one learnable parameter, then ensure the space of possible models is thoroughly explored with one parameter before moving to two parameters, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: Utility of accepting an offer combines linear monetary value and proportional fairness. The fairness term quantifies how much the proposed percentage deviates from the contribution-based equitable percentage (tokens_self / total_tokens). Two parameters control sensitivity to absolute monetary gains (alpha) and fairness violations (beta).\nBIC: 35.35895699690902\nParameter Recovery:\n- alpha: r = 0.625\n- beta: r = 0.856\nModel 2:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self)\nSummary: A linear utility model combining absolute monetary gain and contribution-based fairness. The utility of accepting an offer depends on the proposed amount (split_self) and its deviation from the participant's equitable contribution (3 * token_self). Two parameters control sensitivity to money (alpha) and fairness violations (beta), where beta < 0 indicates aversion to offers below one's contribution. The model directly anchors fairness expectations to pretask performance.\nBIC: 30.777436411868965\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.903\nModel 3:\nSpecification: U_accept = alpha * split_self + beta * ((split_self - 3 * token_self) / (3 * token_self))\nSummary: A utility model combining absolute monetary gain and proportional fairness evaluation. The utility of accepting an offer depends linearly on the proposed amount (split_self) and the proportional deviation of this amount from the participant's equitable contribution (3 * token_self). The proportional term captures sensitivity to relative fairness violations, where offers below one's contribution are penalized according to their percentage shortfall. Two parameters control sensitivity to money (alpha) and proportional fairness (beta).\nBIC: 32.24846051603598\nParameter Recovery:\n- alpha: r = 0.767\n- beta: r = 0.562\nModel 4:\nSpecification: U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0)\nSummary: A utility model based on inequity aversion, where the utility of accepting an offer depends on the absolute monetary gain (split_self) and aversion to both disadvantageous (split_opp > split_self) and advantageous (split_self > split_opp) inequality. Two learnable parameters (alpha, beta) quantify sensitivity to these inequalities. The model captures how responders evaluate offers relative to the proposer's share, penalizing unequal outcomes even when beneficial.\nBIC: 34.61528543210206\nParameter Recovery:\n- alpha: r = 0.827\n- beta: r = -0.027",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. If there are no previous models, start with one learnable parameter, then ensure the space of possible models is thoroughly explored with one parameter before moving to two parameters, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: Utility of accepting an offer combines linear monetary value and proportional fairness. The fairness term quantifies how much the proposed percentage deviates from the contribution-based equitable percentage (tokens_self / total_tokens). Two parameters control sensitivity to absolute monetary gains (alpha) and fairness violations (beta).\nBIC: 35.35895699690902\nParameter Recovery:\n- alpha: r = 0.625\n- beta: r = 0.856\nModel 2:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self)\nSummary: A linear utility model combining absolute monetary gain and contribution-based fairness. The utility of accepting an offer depends on the proposed amount (split_self) and its deviation from the participant's equitable contribution (3 * token_self). Two parameters control sensitivity to money (alpha) and fairness violations (beta), where beta < 0 indicates aversion to offers below one's contribution. The model directly anchors fairness expectations to pretask performance.\nBIC: 30.777436411868965\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.903\nModel 3:\nSpecification: U_accept = alpha * split_self + beta * ((split_self - 3 * token_self) / (3 * token_self))\nSummary: A utility model combining absolute monetary gain and proportional fairness evaluation. The utility of accepting an offer depends linearly on the proposed amount (split_self) and the proportional deviation of this amount from the participant's equitable contribution (3 * token_self). The proportional term captures sensitivity to relative fairness violations, where offers below one's contribution are penalized according to their percentage shortfall. Two parameters control sensitivity to money (alpha) and proportional fairness (beta).\nBIC: 32.24846051603598\nParameter Recovery:\n- alpha: r = 0.767\n- beta: r = 0.562\nModel 4:\nSpecification: U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0)\nSummary: A utility model based on inequity aversion, where the utility of accepting an offer depends on the absolute monetary gain (split_self) and aversion to both disadvantageous (split_opp > split_self) and advantageous (split_self > split_opp) inequality. Two learnable parameters (alpha, beta) quantify sensitivity to these inequalities. The model captures how responders evaluate offers relative to the proposer's share, penalizing unequal outcomes even when beneficial.\nBIC: 34.61528543210206\nParameter Recovery:\n- alpha: r = 0.827\n- beta: r = -0.027\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer in the responder role, based on the given dataset. The model needs to adhere to the constraints provided, such as using only the available variables, having no more than two learnable parameters, and avoiding parameters for stochasticity.\n\nFirst, I need to understand the key psychological principles that might influence a responder's decision. Previous models have explored fairness based on contributions (token_self vs. token_opp), inequity aversion (comparing split_self and split_opp), and absolute monetary gains. The existing models have parameters for sensitivity to money and fairness, but maybe there's another angle.\n\nLooking at the previous models:\n\n1. Model 1 uses a fairness term comparing the proposed percentage to the contribution-based percentage. \n2. Model 2 compares split_self to 3*token_self (since each token is \u00a33), which directly ties to the participant's contribution.\n3. Model 3 uses a proportional deviation from the contribution.\n4. Model 4 uses inequity aversion by comparing split_self and split_opp.\n\nThe BIC scores suggest that Model 2 and Model 3 perform better, but Model 4's parameter recovery for alpha is good, beta is not. Maybe there's a different way to model fairness that hasn't been considered yet.\n\nThe task mentions that two equal split_self values don't represent the same fairness because combined_earning varies. So, the fairness might depend on the relative split compared to the total. For example, if the combined earning is higher, the same split_self might be perceived differently.\n\nAnother angle could be the concept of \"intentions\" or \"reciprocity\". If the proposer is offering a split that's fair relative to their own contribution, the responder might care about that. For example, if the proposer contributed more (token_opp), maybe the responder expects them to take a larger share. Alternatively, the responder might evaluate whether the proposer's split is fair given both contributions.\n\nWait, the trial_type variable indicates whether contributions are equal, opponent more, or participant more. Maybe the responder's perception of fairness depends on the trial_type. For example, in trials where the opponent contributed more, the responder might expect a lower split_self, and vice versa. But trial_type is a categorical variable (1,2,3), which might complicate the model. However, the model should use the variables appropriately. Maybe instead of using trial_type directly, we can derive a measure based on token_self and token_opp.\n\nThe key variables available are token_self, token_opp, split_self, split_opp, combined_earning, etc. The responder's utility might depend on how the proposed split (split_self) compares to what's considered fair based on both their own and the opponent's contributions.\n\nA fair split could be calculated as (token_self / (token_self + token_opp)) * combined_earning. Because each token is \u00a33, combined_earning is 3*(token_self + token_opp). Wait, combined_earning is given as a variable, but according to the note, each token is \u00a33. So combined_earning should be 3*(token_self + token_opp). But the note says that in each trial, the combined earning is variable. Wait, maybe that's not the case. Wait, the note says: \"Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same 'fairness'.\" Hmm, that's confusing. Wait, if each token is \u00a33, then combined_earning should be 3*(token_self + token_opp). But the note says that combined_earning varies. Maybe there's a miscalculation here. Wait, maybe the combined_earning is not directly 3*(token_self + token_opp). Let me check the dataset variables again. The variables are token_opp (number of tokens found by opponent), token_self (participant's tokens), combined_earning (total money). The note says each token found is \u00a33, so combined_earning should be 3*(token_self + token_opp). But the note says that combined_earning varies. Wait, maybe the combined_earning is not fixed per token. Maybe the task has some other way of calculating it. But according to the dataset description, combined_earning is the total money earned, with each token worth \u00a33. So combined_earning = 3*(token_self + token_opp). But the note says that two equal split_self values don't represent the same fairness because combined_earning varies. So if split_self is the same in two trials but combined_earning is different, the percentage would be different. For example, split_self=\u00a36 in a combined_earning of \u00a312 (50%) vs. \u00a318 (33.3%). So the fairness is relative to the total.\n\nBut previous models have considered splitperc_self, which is the percentage. However, Model 1 uses splitperc_self minus the contribution-based percentage. Maybe another approach is to model the utility based on the difference between the proposed percentage and the contribution-based percentage. For example, the fair percentage would be (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these could be a fairness term.\n\nBut Model 1 already uses (splitperc_self - (token_self / (token_self + token_opp)) * 100) as a term. However, in Model 1, the BIC is higher than Model 2 and 3. Maybe there's a different way to model this.\n\nAlternatively, the utility could be a function of the absolute difference between the proposed split and the fair split. The fair split would be (token_self / (token_self + token_opp)) * combined_earning. So split_self_fair = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and split_self_fair could be a term. If split_self is less than split_self_fair, that's unfair, and the utility decreases. If it's more, maybe the responder doesn't care as much (or maybe they do, depending on the model).\n\nSo the utility could be: U = alpha * split_self + beta * (split_self - split_self_fair). But split_self_fair is (token_self/(token_self + token_opp)) * combined_earning. Since combined_earning is 3*(token_self + token_opp), split_self_fair would be 3*token_self. Because combined_earning is 3*(t_self + t_opp), so (t_self/(t_self + t_opp)) * 3*(t_self + t_opp) = 3*t_self. So split_self_fair is 3*token_self. Therefore, split_self - split_self_fair is split_self - 3*token_self. Which is exactly the term used in Model 2. So Model 2's fairness term is split_self - 3*t_self. So if beta is positive, then when split_self is higher than 3*t_self, utility increases. But in Model 2, beta is a learnable parameter. However, in Model 2, the BIC is 30.77, which is better than Model 1. So maybe that's a good approach.\n\nBut the previous models have already considered this. The user wants to explore different mechanisms. Let's think of other psychological principles. For example, maybe the responder cares about the ratio of the split to their contribution. Or perhaps they compare their split to the opponent's split in absolute terms (inequity aversion). Model 4 uses split_opp and split_self, but the parameter recovery for beta (advantageous inequity) was poor. Maybe responders are more sensitive to getting less than their fair share, but not to getting more. So perhaps only disadvantageous inequity matters.\n\nAlternatively, the responder might evaluate the offer based on both their own contribution and the opponent's contribution. For example, if the opponent contributed more, the responder might accept a lower split_self. But how to model that?\n\nAnother angle: the utility could depend on the ratio of split_self to the opponent's split. For example, if split_self / split_opp is lower than some fair ratio, utility decreases. The fair ratio could be based on contributions. For example, (token_self / token_opp). So the fair ratio would be (t_self / t_opp), and the actual ratio is (split_self / split_opp). The difference between these could be a term. But this might introduce more complexity and parameters.\n\nAlternatively, the responder's utility could be influenced by the difference between their split and the opponent's split, scaled by their contributions. For example, (split_self - split_opp) compared to (3*t_self - 3*t_opp). But this might be similar to previous models.\n\nWait, another thought: the responder's perception of fairness might depend on whether the proposer is keeping a share that's proportional to their contribution. So the proposer's share (split_opp) should be proportional to their contribution (t_opp). So the fair split_opp would be 3*t_opp. If the proposer is keeping more than that, the responder might perceive it as unfair. So the responder's utility could decrease if the proposer's split_opp exceeds 3*t_opp. This introduces a term that captures the proposer's fairness. For example, the utility could be:\n\nU = alpha * split_self + beta * (split_opp - 3*token_opp)\n\nIf split_opp is greater than 3*token_opp, that means the proposer is taking more than their contribution, which might be seen as unfair, reducing the utility. But if beta is negative, then higher (split_opp - 3*t_opp) would decrease utility. Alternatively, if beta is positive, but the term is (3*t_opp - split_opp), then higher values (proposer taking less) would increase utility. But this is a different angle: the responder cares not just about their own split relative to their contribution, but also about the proposer's split relative to their contribution. This could be a form of reciprocity or intention-based fairness.\n\nSo combining both terms: the responder's utility depends on their own split compared to their contribution, and the proposer's split compared to theirs. For example:\n\nU = alpha*(split_self - 3*t_self) + beta*(split_opp - 3*t_opp)\n\nBut this would have two parameters. However, in this case, if both terms are positive, the responder gains utility. If split_self is more than their contribution (good) and split_opp is more than the proposer's contribution (proposer is being greedy), but wait, if the proposer is taking more than their contribution, that might be seen as unfair, so the responder might dislike that. So maybe the second term should be negative. For example:\n\nU = alpha*(split_self - 3*t_self) - beta*(split_opp - 3*t_opp)\n\nHere, if the proposer takes more than their contribution (split_opp > 3*t_opp), that's bad, so beta scales how much that affects utility. But this model would have two parameters. However, the problem is that split_opp and split_self are linked because split_self + split_opp = combined_earning. Since combined_earning is 3*(t_self + t_opp), split_opp = combined_earning - split_self. So split_opp - 3*t_opp = (3*(t_self + t_opp) - split_self) - 3*t_opp = 3*t_self - split_self. So split_opp - 3*t_opp = 3*t_self - split_self. Therefore, the term (split_opp - 3*t_opp) is equivalent to (3*t_self - split_self). So the model U = alpha*(split_self - 3*t_self) - beta*(3*t_self - split_self) can be rewritten as U = (alpha + beta)*(split_self - 3*t_self). Which reduces to a single parameter. So this approach might not add new information.\n\nAlternatively, maybe the responder cares about both their own fairness and the proposer's fairness. For example, if the proposer is keeping a fair share (split_opp \u2248 3*t_opp), then the responder is more likely to accept even if their own split is slightly below fair. But if the proposer is being unfair (split_opp much higher than 3*t_opp), the responder might reject even if their own split is fair. This interaction could be modeled as a product term or another combination.\n\nBut given the constraint of two parameters, perhaps a model that combines the responder's own fairness and the proposer's fairness as separate terms. For example:\n\nU = alpha*(split_self - 3*t_self) + beta*(3*t_opp - split_opp)\n\nHere, alpha is sensitivity to own under/overpayment, and beta is sensitivity to proposer's over/underpayment. If the proposer takes less than their contribution (split_opp < 3*t_opp), that's good for the responder (positive term if beta is positive). But this might not make sense. Alternatively, if the proposer takes more than their contribution (split_opp > 3*t_opp), the responder dislikes it, so beta would be negative. For example:\n\nU = alpha*(split_self - 3*t_self) - beta*(split_opp - 3*t_opp)\n\nBut as earlier, split_opp - 3*t_opp = 3*t_self - split_self. So this model becomes U = alpha*(split_self - 3*t_self) - beta*(3*t_self - split_self) = (alpha + beta)*(split_self - 3*t_self). Which is equivalent to a single parameter model. Hence, this approach might not add value.\n\nAlternatively, maybe the responder's utility is influenced by the difference between their split and the proposer's split, relative to their contributions. For example, (split_self / split_opp) vs (t_self / t_opp). The ratio of splits compared to the ratio of contributions. If the split ratio is less than the contribution ratio, that's unfair. So the term could be (split_self / split_opp) - (t_self / t_opp). But this introduces division, which could lead to instability if split_opp is zero, but in the dataset, split_opp is a float, so maybe it's always positive. However, this might complicate the model and introduce non-linearities.\n\nAlternatively, using the difference in percentages: splitperc_self vs (t_self/(t_self + t_opp))*100. This is what Model 1 does. But Model 1's BIC is higher than others. Maybe a different functional form for the fairness term.\n\nAnother idea: the responder's utility could have a non-linear response to fairness violations. For example, a quadratic term for deviations from fairness. But the problem states that the model should not include parameters for stochasticity, but the utility itself can be any form. However, adding non-linear terms might require more parameters, but we're limited to two.\n\nWait, the user wants the model to have no more than two learnable parameters. So perhaps combining two key factors: absolute monetary gain and a fairness term that hasn't been explored yet.\n\nLooking back at the previous models, Model 4 uses inequity aversion between self and opponent's splits. But the parameter recovery for beta was poor. Maybe responders only care about disadvantageous inequality, not advantageous. So Model 4 could be simplified by setting beta=0, but that would reduce to a one-parameter model. However, the user wants to explore new models, so perhaps a different approach.\n\nAnother psychological principle is the notion of \"entitlements\" based on contributions. The responder feels entitled to a share proportional to their contribution, and any deviation from that affects utility. Model 2 captures this with split_self - 3*t_self. But maybe the responder also considers the total combined earning. Wait, but combined_earning is fixed as 3*(t_self + t_opp), so split_self + split_opp = 3*(t_self + t_opp). So split_opp = 3*(t_self + t_opp) - split_self. Therefore, any term involving split_opp can be rewritten in terms of split_self and tokens. This might mean that including split_opp doesn't add new information beyond split_self and tokens. So perhaps models that use split_opp are redundant because it's a linear combination of split_self and tokens.\n\nGiven that, maybe the key variables are split_self, token_self, and token_opp. But token_opp and token_self determine the combined_earning. So the fair split_self is 3*t_self. The proposed split_self's deviation from this fair value is a key term, as in Model 2.\n\nBut Model 2 has good BIC and parameter recovery. To propose a new model, perhaps consider a different fairness evaluation. For example, the responder might evaluate the proposer's intention by how much they deviate from the fair split. If the proposer could have kept more but chose to offer a fair split, that's different from if they had to offer it. Wait, but in this task, the proposer's role is to propose a split, and the responder accepts or rejects. The proposer's own contribution (token_opp) might influence what's considered a fair offer.\n\nAlternatively, the responder's utility could depend on the ratio of the proposed split to their fair share. For example, split_self / (3*t_self). If this ratio is less than 1, the responder is getting less than fair, which reduces utility. The utility could be modeled as:\n\nU = alpha * split_self + beta * (split_self / (3*t_self) - 1)\n\nHere, the term (split_self/(3*t_self) -1) represents the proportional deviation from the fair share. If split_self is exactly 3*t_self, the term is zero. If it's higher, positive; lower, negative. Beta would then capture sensitivity to proportional fairness. This is similar to Model 3, which uses (split_self - 3*t_self)/(3*t_self), which is the same as (split_self/(3*t_self) -1). So Model 3's term is equivalent. Therefore, this approach is already covered.\n\nHmm. Another angle: the responder might have different sensitivity to fairness violations depending on whether the proposer contributed more or less. For example, in trials where the proposer contributed more (trial_type=2), the responder might be more tolerant of lower offers. But trial_type is derived from token_self and token_opp. So instead of using trial_type, we can model it using the relative contributions.\n\nFor example, the fairness term could be weighted by the relative contribution. If the proposer contributed more, the responder expects less, so the fairness term is adjusted. But how to model that with existing variables.\n\nAlternatively, the model could include an interaction term between the fairness violation and the proposer's contribution. For example:\n\nU = alpha * split_self + beta * (split_self - 3*t_self) * (token_opp / token_self)\n\nHere, if the proposer contributed more (token_opp > token_self), the fairness term is scaled by the ratio of their contributions. This introduces a non-linear interaction. However, this adds complexity and might require careful parameter bounds. But with two parameters (alpha and beta), this could be feasible.\n\nBut would this model make psychological sense? If the proposer contributed more, the responder might be more accepting of a lower split_self. So the term (split_self - 3*t_self) is multiplied by (token_opp / token_self). If token_opp is higher, this ratio is greater than 1, so a negative (split_self - 3*t_self) (i.e., unfair offer) would be scaled up, making the utility more negative. Wait, that might not align. Let me think: if the proposer contributed more, maybe the responder expects to get less than their fair share. So if the proposer contributed more, the responder is more tolerant of getting less. So the disutility from (split_self - 3*t_self) being negative is reduced when token_opp is higher. So the beta term could be multiplied by (token_self / token_opp), so that when token_opp is higher, the weight on the fairness term is lower. For example:\n\nU = alpha * split_self + beta * (split_self - 3*t_self) * (token_self / (token_self + token_opp))\n\nThis way, when the opponent contributed more (higher token_opp), the weight on the fairness term decreases, making the responder less sensitive to fairness violations. This introduces an interaction between the fairness term and the relative contribution. This could be a novel approach not covered in previous models.\n\nThe term (token_self / (token_self + token_opp)) represents the participant's relative contribution. If they contributed less, this term is smaller, so the effect of the fairness violation (split_self - 3*t_self) is scaled down. This means that when the participant contributed less, they are less sensitive to receiving less than their fair share, which makes psychological sense. Conversely, when they contributed more, they are more sensitive to fairness violations.\n\nThis model would have two parameters: alpha (sensitivity to monetary gain) and beta (sensitivity to scaled fairness violations). The fairness term is modulated by the participant's relative contribution. This could capture scenarios where the responder's entitlement depends on their own contribution relative to the proposer's.\n\nAnother possibility is to use the difference in contributions. For example, (token_self - token_opp) as a modifier. But this might not scale as well.\n\nAlternatively, the model could use the ratio of contributions (token_self / token_opp) to weight the fairness term. For example:\n\nU = alpha * split_self + beta * (split_self - 3*t_self) * (token_self / token_opp)\n\nBut this could lead to division by zero if token_opp is zero, which is unlikely given the task description. However, in practice, token_opp is the opponent's tokens, which I assume is at least 1 in the dataset. But this introduces a non-linear term and might complicate parameter recovery.\n\nConsidering the constraints, the model needs to use variables available in the dataset. The variables token_self and token_opp are available, so using them in an interaction term is feasible.\n\nSo the proposed model is:\n\nU_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\n\nHere, alpha captures the sensitivity to the absolute monetary gain, and beta captures the sensitivity to the fairness violation scaled by the participant's relative contribution. When the participant's contribution is a larger fraction of the total (token_self/(token_self + token_opp) is high), the fairness term has a larger weight, meaning the responder is more sensitive to deviations from their fair share. Conversely, if the participant contributed less, the fairness term's impact is reduced.\n\nThis model introduces an interaction between the fairness violation and the relative contribution, which hasn't been explored in previous models. It uses two learnable parameters (alpha and beta), which is within the limit. The variables used are split_self, token_self, and token_opp, all of which are present in the dataset.\n\nNow, checking if this model meets all the requirements:\n\n1. Predicts responder behavior: Yes, as it's based on the responder's evaluation.\n2. Builds on psychological principles: Combines monetary gain and fairness adjusted by contribution.\n3. Considers parameter covariance: By having two parameters, but the interaction term may reduce covariance if the terms are orthogonal.\n4. Uses dataset variables: split_self, token_self, token_opp are all available.\n5. No stochastic parameters: Correct, only alpha and beta are learnable.\n6. Two parameters: Yes.\n7. Utility can be negative: Yes, depending on the terms.\n\nThe variables in the model are:\n\n- split_self: from data.\n- token_self: from data.\n- token_opp: from data.\n- alpha: learnable parameter, range to be determined.\n- beta: learnable parameter, range to be determined.\n\nFor the ranges, alpha could be positive since higher split_self should increase utility. Beta's sign depends on the model: if (split_self - 3*token_self) is negative (unfair), and the term is multiplied by beta, then beta should be positive to penalize unfairness. For example, if split_self < 3*token_self, then (split_self - 3*token_self) is negative. If beta is positive, then the entire fairness term becomes negative, reducing utility. So beta should be positive to penalize unfair offers. The scaling by (token_self/(token_self + token_opp)) means that when the participant contributed more, the penalty is larger.\n\nSo alpha's range could be [0, some upper bound], and beta's range could be [0, upper bound]. But to allow for possible negative sensitivities (though unlikely), maybe set the min to -something, but given the psychological rationale, it's more plausible that alpha and beta are positive. However, to be safe, we can set alpha to be unconstrained (allowing negative values, though unlikely), and beta similarly. But given the task's nature, responders are expected to prefer higher split_self (alpha positive) and dislike unfair splits (beta positive). However, to allow the model to learn, we can set generous bounds, like alpha in [-10, 10], beta in [-10, 10], but inclusive. But the exact bounds can be determined based on plausible values.\n\nAlternatively, since split_self is in \u00a3, and token_self is in tokens (each \u00a33), the terms could be scaled. For example, split_self ranges from 0 to combined_earning (which is 3*(t_self + t_opp)). The term (split_self - 3*t_self) can range from -3*t_self to 3*t_opp. The scaling factor (token_self/(t_self + t_opp)) ranges from 0 to 1. So the fairness term is (split_self - 3*t_self) * (t_self/(t_self + t_opp)). This term's maximum possible value depends on the tokens. For example, if t_self is 10 and t_opp is 10, then the scaling factor is 0.5. If split_self is 3*10 + x, then the term is x * 0.5. But in reality, split_self can't exceed combined_earning, which is 3*(t_self + t_opp). So the maximum (split_self - 3*t_self) is 3*t_opp. So the fairness term can be up to 3*t_opp * (t_self/(t_self + t_opp)) = 3 * t_self * t_opp / (t_self + t_opp). This varies per trial.\n\nGiven that, the parameters alpha and beta need to be scaled appropriately. To avoid parameters being too large or small, setting bounds of, say, [-10, 10] for both, inclusive, would be generous. However, given that alpha is multiplied by split_self (which can be up to, say, 3*(t_self + t_opp)), and beta is multiplied by the fairness term (which varies), the parameters might need to be small to prevent utility from being too large. But since the utility is passed through a softmax function later (with a temperature parameter), the actual scale might be managed by the temperature. Therefore, setting the bounds as [-10, 10] is reasonable.\n\nSo the variable descriptions would be:\n\nalpha: sensitivity to monetary gain, range [-10, 10], learnable.\nbeta: sensitivity to scaled fairness violation, range [-10, 10], learnable.\n\nThe other variables (split_self, token_self, token_opp) are from the data.\n\nThis model introduces a new interaction term not present in previous models, potentially capturing how the responder's sensitivity to fairness depends on their relative contribution. This could explain behavior where responders are more forgiving of unfair offers when they contributed less, and less forgiving when they contributed more.\n</think>\n\n<MODEL>\nU_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from the proposed split\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to fairness violations scaled by relative contribution\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant (in \u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return the utility values for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of utility values for each trial\n\n                Your code will be implemented within the following code block:\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    utility_values = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        utility_values.append(beta + (epsilon * E) + (eta * N))\n    return utility_values\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE UTLITY VALUES.\n                Model Specification:\n                U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from the proposed split\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to fairness violations scaled by relative contribution\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant (in \\u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=0, beta=0):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 1) #Adding a default to avoid ZeroDivisionError\n        token_opp = trial.get(\"token_opp\", 1)  #Adding a default to avoid ZeroDivisionError\n\n        if token_self + token_opp == 0:\n            utility = alpha * split_self\n        else:\n            utility = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\n        utility_values.append(utility)\n    return utility_values\n\n# Example usage (replace with your actual data)\ndata_json = '[{\"split_self\": 6, \"token_self\": 2, \"token_opp\": 2}, {\"split_self\": 9, \"token_self\": 3, \"token_opp\": 1}]'\ndata = json.loads(data_json)\n\n# Set alpha and beta\nalpha_value = 0.5\nbeta_value = 0.8\n\n# Get results for the data\nresults = simulate_model(data, alpha=alpha_value, beta=beta_value)\nprint(json.dumps({\"results\": results}))\n```"
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to absolute monetary gain from the proposed split",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to fairness violations scaled by relative contribution",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))",
  "variable_descriptions": {
    "U_accept": {
      "description": "Utility of accepting the offer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha": {
      "description": "Sensitivity to absolute monetary gain from the proposed split",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to fairness violations scaled by relative contribution",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed monetary amount for the participant (in \u00a3)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant in the pretask",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Number of tokens found by the opponent in the pretask",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer in the responder role, based on the given dataset. The model needs to adhere to the constraints provided, such as using only the available variables, having no more than two learnable parameters, and avoiding parameters for stochasticity.\n\nFirst, I need to understand the key psychological principles that might influence a responder's decision. Previous models have explored fairness based on contributions (token_self vs. token_opp), inequity aversion (comparing split_self and split_opp), and absolute monetary gains. The existing models have parameters for sensitivity to money and fairness, but maybe there's another angle.\n\nLooking at the previous models:\n\n1. Model 1 uses a fairness term comparing the proposed percentage to the contribution-based percentage. \n2. Model 2 compares split_self to 3*token_self (since each token is \u00a33), which directly ties to the participant's contribution.\n3. Model 3 uses a proportional deviation from the contribution.\n4. Model 4 uses inequity aversion by comparing split_self and split_opp.\n\nThe BIC scores suggest that Model 2 and Model 3 perform better, but Model 4's parameter recovery for alpha is good, beta is not. Maybe there's a different way to model fairness that hasn't been considered yet.\n\nThe task mentions that two equal split_self values don't represent the same fairness because combined_earning varies. So, the fairness might depend on the relative split compared to the total. For example, if the combined earning is higher, the same split_self might be perceived differently.\n\nAnother angle could be the concept of \"intentions\" or \"reciprocity\". If the proposer is offering a split that's fair relative to their own contribution, the responder might care about that. For example, if the proposer contributed more (token_opp), maybe the responder expects them to take a larger share. Alternatively, the responder might evaluate whether the proposer's split is fair given both contributions.\n\nWait, the trial_type variable indicates whether contributions are equal, opponent more, or participant more. Maybe the responder's perception of fairness depends on the trial_type. For example, in trials where the opponent contributed more, the responder might expect a lower split_self, and vice versa. But trial_type is a categorical variable (1,2,3), which might complicate the model. However, the model should use the variables appropriately. Maybe instead of using trial_type directly, we can derive a measure based on token_self and token_opp.\n\nThe key variables available are token_self, token_opp, split_self, split_opp, combined_earning, etc. The responder's utility might depend on how the proposed split (split_self) compares to what's considered fair based on both their own and the opponent's contributions.\n\nA fair split could be calculated as (token_self / (token_self + token_opp)) * combined_earning. Because each token is \u00a33, combined_earning is 3*(token_self + token_opp). Wait, combined_earning is given as a variable, but according to the note, each token is \u00a33. So combined_earning should be 3*(token_self + token_opp). But the note says that in each trial, the combined earning is variable. Wait, maybe that's not the case. Wait, the note says: \"Note that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same 'fairness'.\" Hmm, that's confusing. Wait, if each token is \u00a33, then combined_earning should be 3*(token_self + token_opp). But the note says that combined_earning varies. Maybe there's a miscalculation here. Wait, maybe the combined_earning is not directly 3*(token_self + token_opp). Let me check the dataset variables again. The variables are token_opp (number of tokens found by opponent), token_self (participant's tokens), combined_earning (total money). The note says each token found is \u00a33, so combined_earning should be 3*(token_self + token_opp). But the note says that combined_earning varies. Wait, maybe the combined_earning is not fixed per token. Maybe the task has some other way of calculating it. But according to the dataset description, combined_earning is the total money earned, with each token worth \u00a33. So combined_earning = 3*(token_self + token_opp). But the note says that two equal split_self values don't represent the same fairness because combined_earning varies. So if split_self is the same in two trials but combined_earning is different, the percentage would be different. For example, split_self=\u00a36 in a combined_earning of \u00a312 (50%) vs. \u00a318 (33.3%). So the fairness is relative to the total.\n\nBut previous models have considered splitperc_self, which is the percentage. However, Model 1 uses splitperc_self minus the contribution-based percentage. Maybe another approach is to model the utility based on the difference between the proposed percentage and the contribution-based percentage. For example, the fair percentage would be (token_self / (token_self + token_opp)) * 100. The proposed percentage is splitperc_self. The difference between these could be a fairness term.\n\nBut Model 1 already uses (splitperc_self - (token_self / (token_self + token_opp)) * 100) as a term. However, in Model 1, the BIC is higher than Model 2 and 3. Maybe there's a different way to model this.\n\nAlternatively, the utility could be a function of the absolute difference between the proposed split and the fair split. The fair split would be (token_self / (token_self + token_opp)) * combined_earning. So split_self_fair = (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and split_self_fair could be a term. If split_self is less than split_self_fair, that's unfair, and the utility decreases. If it's more, maybe the responder doesn't care as much (or maybe they do, depending on the model).\n\nSo the utility could be: U = alpha * split_self + beta * (split_self - split_self_fair). But split_self_fair is (token_self/(token_self + token_opp)) * combined_earning. Since combined_earning is 3*(token_self + token_opp), split_self_fair would be 3*token_self. Because combined_earning is 3*(t_self + t_opp), so (t_self/(t_self + t_opp)) * 3*(t_self + t_opp) = 3*t_self. So split_self_fair is 3*token_self. Therefore, split_self - split_self_fair is split_self - 3*token_self. Which is exactly the term used in Model 2. So Model 2's fairness term is split_self - 3*t_self. So if beta is positive, then when split_self is higher than 3*t_self, utility increases. But in Model 2, beta is a learnable parameter. However, in Model 2, the BIC is 30.77, which is better than Model 1. So maybe that's a good approach.\n\nBut the previous models have already considered this. The user wants to explore different mechanisms. Let's think of other psychological principles. For example, maybe the responder cares about the ratio of the split to their contribution. Or perhaps they compare their split to the opponent's split in absolute terms (inequity aversion). Model 4 uses split_opp and split_self, but the parameter recovery for beta (advantageous inequity) was poor. Maybe responders are more sensitive to getting less than their fair share, but not to getting more. So perhaps only disadvantageous inequity matters.\n\nAlternatively, the responder might evaluate the offer based on both their own contribution and the opponent's contribution. For example, if the opponent contributed more, the responder might accept a lower split_self. But how to model that?\n\nAnother angle: the utility could depend on the ratio of split_self to the opponent's split. For example, if split_self / split_opp is lower than some fair ratio, utility decreases. The fair ratio could be based on contributions. For example, (token_self / token_opp). So the fair ratio would be (t_self / t_opp), and the actual ratio is (split_self / split_opp). The difference between these could be a term. But this might introduce more complexity and parameters.\n\nAlternatively, the responder's utility could be influenced by the difference between their split and the opponent's split, scaled by their contributions. For example, (split_self - split_opp) compared to (3*t_self - 3*t_opp). But this might be similar to previous models.\n\nWait, another thought: the responder's perception of fairness might depend on whether the proposer is keeping a share that's proportional to their contribution. So the proposer's share (split_opp) should be proportional to their contribution (t_opp). So the fair split_opp would be 3*t_opp. If the proposer is keeping more than that, the responder might perceive it as unfair. So the responder's utility could decrease if the proposer's split_opp exceeds 3*t_opp. This introduces a term that captures the proposer's fairness. For example, the utility could be:\n\nU = alpha * split_self + beta * (split_opp - 3*token_opp)\n\nIf split_opp is greater than 3*token_opp, that means the proposer is taking more than their contribution, which might be seen as unfair, reducing the utility. But if beta is negative, then higher (split_opp - 3*t_opp) would decrease utility. Alternatively, if beta is positive, but the term is (3*t_opp - split_opp), then higher values (proposer taking less) would increase utility. But this is a different angle: the responder cares not just about their own split relative to their contribution, but also about the proposer's split relative to their contribution. This could be a form of reciprocity or intention-based fairness.\n\nSo combining both terms: the responder's utility depends on their own split compared to their contribution, and the proposer's split compared to theirs. For example:\n\nU = alpha*(split_self - 3*t_self) + beta*(split_opp - 3*t_opp)\n\nBut this would have two parameters. However, in this case, if both terms are positive, the responder gains utility. If split_self is more than their contribution (good) and split_opp is more than the proposer's contribution (proposer is being greedy), but wait, if the proposer is taking more than their contribution, that might be seen as unfair, so the responder might dislike that. So maybe the second term should be negative. For example:\n\nU = alpha*(split_self - 3*t_self) - beta*(split_opp - 3*t_opp)\n\nHere, if the proposer takes more than their contribution (split_opp > 3*t_opp), that's bad, so beta scales how much that affects utility. But this model would have two parameters. However, the problem is that split_opp and split_self are linked because split_self + split_opp = combined_earning. Since combined_earning is 3*(t_self + t_opp), split_opp = combined_earning - split_self. So split_opp - 3*t_opp = (3*(t_self + t_opp) - split_self) - 3*t_opp = 3*t_self - split_self. So split_opp - 3*t_opp = 3*t_self - split_self. Therefore, the term (split_opp - 3*t_opp) is equivalent to (3*t_self - split_self). So the model U = alpha*(split_self - 3*t_self) - beta*(3*t_self - split_self) can be rewritten as U = (alpha + beta)*(split_self - 3*t_self). Which reduces to a single parameter. So this approach might not add new information.\n\nAlternatively, maybe the responder cares about both their own fairness and the proposer's fairness. For example, if the proposer is keeping a fair share (split_opp \u2248 3*t_opp), then the responder is more likely to accept even if their own split is slightly below fair. But if the proposer is being unfair (split_opp much higher than 3*t_opp), the responder might reject even if their own split is fair. This interaction could be modeled as a product term or another combination.\n\nBut given the constraint of two parameters, perhaps a model that combines the responder's own fairness and the proposer's fairness as separate terms. For example:\n\nU = alpha*(split_self - 3*t_self) + beta*(3*t_opp - split_opp)\n\nHere, alpha is sensitivity to own under/overpayment, and beta is sensitivity to proposer's over/underpayment. If the proposer takes less than their contribution (split_opp < 3*t_opp), that's good for the responder (positive term if beta is positive). But this might not make sense. Alternatively, if the proposer takes more than their contribution (split_opp > 3*t_opp), the responder dislikes it, so beta would be negative. For example:\n\nU = alpha*(split_self - 3*t_self) - beta*(split_opp - 3*t_opp)\n\nBut as earlier, split_opp - 3*t_opp = 3*t_self - split_self. So this model becomes U = alpha*(split_self - 3*t_self) - beta*(3*t_self - split_self) = (alpha + beta)*(split_self - 3*t_self). Which is equivalent to a single parameter model. Hence, this approach might not add value.\n\nAlternatively, maybe the responder's utility is influenced by the difference between their split and the proposer's split, relative to their contributions. For example, (split_self / split_opp) vs (t_self / t_opp). The ratio of splits compared to the ratio of contributions. If the split ratio is less than the contribution ratio, that's unfair. So the term could be (split_self / split_opp) - (t_self / t_opp). But this introduces division, which could lead to instability if split_opp is zero, but in the dataset, split_opp is a float, so maybe it's always positive. However, this might complicate the model and introduce non-linearities.\n\nAlternatively, using the difference in percentages: splitperc_self vs (t_self/(t_self + t_opp))*100. This is what Model 1 does. But Model 1's BIC is higher than others. Maybe a different functional form for the fairness term.\n\nAnother idea: the responder's utility could have a non-linear response to fairness violations. For example, a quadratic term for deviations from fairness. But the problem states that the model should not include parameters for stochasticity, but the utility itself can be any form. However, adding non-linear terms might require more parameters, but we're limited to two.\n\nWait, the user wants the model to have no more than two learnable parameters. So perhaps combining two key factors: absolute monetary gain and a fairness term that hasn't been explored yet.\n\nLooking back at the previous models, Model 4 uses inequity aversion between self and opponent's splits. But the parameter recovery for beta was poor. Maybe responders only care about disadvantageous inequality, not advantageous. So Model 4 could be simplified by setting beta=0, but that would reduce to a one-parameter model. However, the user wants to explore new models, so perhaps a different approach.\n\nAnother psychological principle is the notion of \"entitlements\" based on contributions. The responder feels entitled to a share proportional to their contribution, and any deviation from that affects utility. Model 2 captures this with split_self - 3*t_self. But maybe the responder also considers the total combined earning. Wait, but combined_earning is fixed as 3*(t_self + t_opp), so split_self + split_opp = 3*(t_self + t_opp). So split_opp = 3*(t_self + t_opp) - split_self. Therefore, any term involving split_opp can be rewritten in terms of split_self and tokens. This might mean that including split_opp doesn't add new information beyond split_self and tokens. So perhaps models that use split_opp are redundant because it's a linear combination of split_self and tokens.\n\nGiven that, maybe the key variables are split_self, token_self, and token_opp. But token_opp and token_self determine the combined_earning. So the fair split_self is 3*t_self. The proposed split_self's deviation from this fair value is a key term, as in Model 2.\n\nBut Model 2 has good BIC and parameter recovery. To propose a new model, perhaps consider a different fairness evaluation. For example, the responder might evaluate the proposer's intention by how much they deviate from the fair split. If the proposer could have kept more but chose to offer a fair split, that's different from if they had to offer it. Wait, but in this task, the proposer's role is to propose a split, and the responder accepts or rejects. The proposer's own contribution (token_opp) might influence what's considered a fair offer.\n\nAlternatively, the responder's utility could depend on the ratio of the proposed split to their fair share. For example, split_self / (3*t_self). If this ratio is less than 1, the responder is getting less than fair, which reduces utility. The utility could be modeled as:\n\nU = alpha * split_self + beta * (split_self / (3*t_self) - 1)\n\nHere, the term (split_self/(3*t_self) -1) represents the proportional deviation from the fair share. If split_self is exactly 3*t_self, the term is zero. If it's higher, positive; lower, negative. Beta would then capture sensitivity to proportional fairness. This is similar to Model 3, which uses (split_self - 3*t_self)/(3*t_self), which is the same as (split_self/(3*t_self) -1). So Model 3's term is equivalent. Therefore, this approach is already covered.\n\nHmm. Another angle: the responder might have different sensitivity to fairness violations depending on whether the proposer contributed more or less. For example, in trials where the proposer contributed more (trial_type=2), the responder might be more tolerant of lower offers. But trial_type is derived from token_self and token_opp. So instead of using trial_type, we can model it using the relative contributions.\n\nFor example, the fairness term could be weighted by the relative contribution. If the proposer contributed more, the responder expects less, so the fairness term is adjusted. But how to model that with existing variables.\n\nAlternatively, the model could include an interaction term between the fairness violation and the proposer's contribution. For example:\n\nU = alpha * split_self + beta * (split_self - 3*t_self) * (token_opp / token_self)\n\nHere, if the proposer contributed more (token_opp > token_self), the fairness term is scaled by the ratio of their contributions. This introduces a non-linear interaction. However, this adds complexity and might require careful parameter bounds. But with two parameters (alpha and beta), this could be feasible.\n\nBut would this model make psychological sense? If the proposer contributed more, the responder might be more accepting of a lower split_self. So the term (split_self - 3*t_self) is multiplied by (token_opp / token_self). If token_opp is higher, this ratio is greater than 1, so a negative (split_self - 3*t_self) (i.e., unfair offer) would be scaled up, making the utility more negative. Wait, that might not align. Let me think: if the proposer contributed more, maybe the responder expects to get less than their fair share. So if the proposer contributed more, the responder is more tolerant of getting less. So the disutility from (split_self - 3*t_self) being negative is reduced when token_opp is higher. So the beta term could be multiplied by (token_self / token_opp), so that when token_opp is higher, the weight on the fairness term is lower. For example:\n\nU = alpha * split_self + beta * (split_self - 3*t_self) * (token_self / (token_self + token_opp))\n\nThis way, when the opponent contributed more (higher token_opp), the weight on the fairness term decreases, making the responder less sensitive to fairness violations. This introduces an interaction between the fairness term and the relative contribution. This could be a novel approach not covered in previous models.\n\nThe term (token_self / (token_self + token_opp)) represents the participant's relative contribution. If they contributed less, this term is smaller, so the effect of the fairness violation (split_self - 3*t_self) is scaled down. This means that when the participant contributed less, they are less sensitive to receiving less than their fair share, which makes psychological sense. Conversely, when they contributed more, they are more sensitive to fairness violations.\n\nThis model would have two parameters: alpha (sensitivity to monetary gain) and beta (sensitivity to scaled fairness violations). The fairness term is modulated by the participant's relative contribution. This could capture scenarios where the responder's entitlement depends on their own contribution relative to the proposer's.\n\nAnother possibility is to use the difference in contributions. For example, (token_self - token_opp) as a modifier. But this might not scale as well.\n\nAlternatively, the model could use the ratio of contributions (token_self / token_opp) to weight the fairness term. For example:\n\nU = alpha * split_self + beta * (split_self - 3*t_self) * (token_self / token_opp)\n\nBut this could lead to division by zero if token_opp is zero, which is unlikely given the task description. However, in practice, token_opp is the opponent's tokens, which I assume is at least 1 in the dataset. But this introduces a non-linear term and might complicate parameter recovery.\n\nConsidering the constraints, the model needs to use variables available in the dataset. The variables token_self and token_opp are available, so using them in an interaction term is feasible.\n\nSo the proposed model is:\n\nU_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\n\nHere, alpha captures the sensitivity to the absolute monetary gain, and beta captures the sensitivity to the fairness violation scaled by the participant's relative contribution. When the participant's contribution is a larger fraction of the total (token_self/(token_self + token_opp) is high), the fairness term has a larger weight, meaning the responder is more sensitive to deviations from their fair share. Conversely, if the participant contributed less, the fairness term's impact is reduced.\n\nThis model introduces an interaction between the fairness violation and the relative contribution, which hasn't been explored in previous models. It uses two learnable parameters (alpha and beta), which is within the limit. The variables used are split_self, token_self, and token_opp, all of which are present in the dataset.\n\nNow, checking if this model meets all the requirements:\n\n1. Predicts responder behavior: Yes, as it's based on the responder's evaluation.\n2. Builds on psychological principles: Combines monetary gain and fairness adjusted by contribution.\n3. Considers parameter covariance: By having two parameters, but the interaction term may reduce covariance if the terms are orthogonal.\n4. Uses dataset variables: split_self, token_self, token_opp are all available.\n5. No stochastic parameters: Correct, only alpha and beta are learnable.\n6. Two parameters: Yes.\n7. Utility can be negative: Yes, depending on the terms.\n\nThe variables in the model are:\n\n- split_self: from data.\n- token_self: from data.\n- token_opp: from data.\n- alpha: learnable parameter, range to be determined.\n- beta: learnable parameter, range to be determined.\n\nFor the ranges, alpha could be positive since higher split_self should increase utility. Beta's sign depends on the model: if (split_self - 3*token_self) is negative (unfair), and the term is multiplied by beta, then beta should be positive to penalize unfairness. For example, if split_self < 3*token_self, then (split_self - 3*token_self) is negative. If beta is positive, then the entire fairness term becomes negative, reducing utility. So beta should be positive to penalize unfair offers. The scaling by (token_self/(token_self + token_opp)) means that when the participant contributed more, the penalty is larger.\n\nSo alpha's range could be [0, some upper bound], and beta's range could be [0, upper bound]. But to allow for possible negative sensitivities (though unlikely), maybe set the min to -something, but given the psychological rationale, it's more plausible that alpha and beta are positive. However, to be safe, we can set alpha to be unconstrained (allowing negative values, though unlikely), and beta similarly. But given the task's nature, responders are expected to prefer higher split_self (alpha positive) and dislike unfair splits (beta positive). However, to allow the model to learn, we can set generous bounds, like alpha in [-10, 10], beta in [-10, 10], but inclusive. But the exact bounds can be determined based on plausible values.\n\nAlternatively, since split_self is in \u00a3, and token_self is in tokens (each \u00a33), the terms could be scaled. For example, split_self ranges from 0 to combined_earning (which is 3*(t_self + t_opp)). The term (split_self - 3*t_self) can range from -3*t_self to 3*t_opp. The scaling factor (token_self/(t_self + t_opp)) ranges from 0 to 1. So the fairness term is (split_self - 3*t_self) * (t_self/(t_self + t_opp)). This term's maximum possible value depends on the tokens. For example, if t_self is 10 and t_opp is 10, then the scaling factor is 0.5. If split_self is 3*10 + x, then the term is x * 0.5. But in reality, split_self can't exceed combined_earning, which is 3*(t_self + t_opp). So the maximum (split_self - 3*t_self) is 3*t_opp. So the fairness term can be up to 3*t_opp * (t_self/(t_self + t_opp)) = 3 * t_self * t_opp / (t_self + t_opp). This varies per trial.\n\nGiven that, the parameters alpha and beta need to be scaled appropriately. To avoid parameters being too large or small, setting bounds of, say, [-10, 10] for both, inclusive, would be generous. However, given that alpha is multiplied by split_self (which can be up to, say, 3*(t_self + t_opp)), and beta is multiplied by the fairness term (which varies), the parameters might need to be small to prevent utility from being too large. But since the utility is passed through a softmax function later (with a temperature parameter), the actual scale might be managed by the temperature. Therefore, setting the bounds as [-10, 10] is reasonable.\n\nSo the variable descriptions would be:\n\nalpha: sensitivity to monetary gain, range [-10, 10], learnable.\nbeta: sensitivity to scaled fairness violation, range [-10, 10], learnable.\n\nThe other variables (split_self, token_self, token_opp) are from the data.\n\nThis model introduces a new interaction term not present in previous models, potentially capturing how the responder's sensitivity to fairness depends on their relative contribution. This could explain behavior where responders are more forgiving of unfair offers when they contributed less, and less forgiving when they contributed more.\n</think>\n\n<MODEL>\nU_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from the proposed split\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to fairness violations scaled by relative contribution\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant (in \u00a3)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Number of tokens found by the opponent in the pretask\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).\n</SUMMARY>",
  "current_model": "Specification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).",
  "simulation_code": "def simulate_model(trial_data, alpha=0, beta=0):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 1) #Adding a default to avoid ZeroDivisionError\n        token_opp = trial.get(\"token_opp\", 1)  #Adding a default to avoid ZeroDivisionError\n\n        if token_self + token_opp == 0:\n            utility = alpha * split_self\n        else:\n            utility = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\n        utility_values.append(utility)\n    return utility_values",
  "fitting_results": [
    {
      "alpha": -0.1872794518001584,
      "beta": 5.472141645445595,
      "success": true,
      "log_likelihood": -8.872499611442736,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9429547215694055,
      "beta": 2.5356446338536203,
      "success": true,
      "log_likelihood": -17.61639842025033,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.8148341669963126,
      "beta": 7.5150973732189,
      "success": true,
      "log_likelihood": -8.540473727554392,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1665532026083625,
      "beta": 3.756329141844024,
      "success": true,
      "log_likelihood": -14.658956006435718,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.7634893336457242,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -6.591027932789732,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9202928744964431,
      "beta": 3.519802264239558,
      "success": true,
      "log_likelihood": -12.524942470288897,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -5.726782213652584e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9405012834842228,
      "beta": 3.4122049903189295,
      "success": true,
      "log_likelihood": -12.463508246306635,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -5.7267822136525876e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2232147670718347,
      "beta": 4.961357432054577,
      "success": true,
      "log_likelihood": -12.320050004232431,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.0965244326177834,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -4.179375139772224,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.022104425197790642,
      "beta": 5.237571082711948,
      "success": true,
      "log_likelihood": -9.943366252660159,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6167330061259866,
      "beta": 6.055976585151831,
      "success": true,
      "log_likelihood": -10.727964996318212,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -5.7267822136525856e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -5.726782213652583e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -5.7267822136525856e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -5.726782213652584e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 6.728682043179759,
      "beta": 5.370417917700775,
      "success": true,
      "log_likelihood": -3.9183620877821648,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.36116046032028,
      "beta": 3.451072109809348,
      "success": true,
      "log_likelihood": -5.752633470078518,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -5.726782213652583e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2834389967397468,
      "beta": 3.227931745064374,
      "success": true,
      "log_likelihood": -15.375674044933067,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.93910358434117,
      "beta": 2.376906624923187,
      "success": true,
      "log_likelihood": -8.285499517908283,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5334599837727901,
      "beta": 2.1078938656892734,
      "success": true,
      "log_likelihood": -19.351372619225245,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9623341502191973,
      "beta": 2.127332179700189,
      "success": true,
      "log_likelihood": -18.32264285215746,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4040324224259095,
      "beta": 4.56907738975502,
      "success": true,
      "log_likelihood": -12.997112681064383,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.40493702524293,
      "beta": 2.85781122721791,
      "success": true,
      "log_likelihood": -15.330699131734569,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4203789557712359,
      "beta": 3.03492423184778,
      "success": true,
      "log_likelihood": -15.09703293659976,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4149319385957595,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -6.216173648476772,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.5625518431703806,
      "beta": 1.750858234884187,
      "success": true,
      "log_likelihood": -8.844654927921189,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3283591049479186,
      "beta": 1.3065134368635294,
      "success": true,
      "log_likelihood": -15.873616074584712,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -5.726782213652584e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.1872810687211918,
      "beta": 5.472176858211578,
      "success": true,
      "log_likelihood": -8.872499611343427,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.39215948346245955,
      "beta": 1.2537810777351868,
      "success": true,
      "log_likelihood": -22.23971541452803,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7133607170173207,
      "beta": 2.775473827559791,
      "success": true,
      "log_likelihood": -17.262889070975824,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -5.726782213652583e-05,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5413499787683325,
      "beta": 2.1098976974460673,
      "success": true,
      "log_likelihood": -14.851822962775278,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.2041636981015107,
      "beta": 5.008075919851218,
      "success": true,
      "log_likelihood": -7.806205532046608,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2678529414894681,
      "beta": 3.760468533152823,
      "success": true,
      "log_likelihood": -14.532725191855182,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0757989448486909,
      "beta": 1.7143097801868603,
      "success": true,
      "log_likelihood": -18.08349732982625,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8030246009063318,
      "beta": 0.41754155478484056,
      "success": true,
      "log_likelihood": -9.645359340990376,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9621495434438189,
      "beta": 1.9403903823275284,
      "success": true,
      "log_likelihood": -18.598945951543044,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3112464103487287,
      "beta": 4.0497564347504955,
      "success": true,
      "log_likelihood": -13.020762345396205,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": -10.0,
      "success": true,
      "log_likelihood": -23.02590597741901,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.035817100787848,
      "beta": 2.0599646834225616,
      "success": true,
      "log_likelihood": -18.0498931234443,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6000146291995436,
      "beta": 3.0024644461515746,
      "success": true,
      "log_likelihood": -14.270717958805697,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.243648142754556,
      "beta": 1.1195572359475197,
      "success": true,
      "log_likelihood": -16.32241740121593,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0034372393776034,
      "beta": 2.407722355301293,
      "success": true,
      "log_likelihood": -17.673136084657273,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5284479461499428,
      "beta": 1.986749731535318,
      "success": true,
      "log_likelihood": -19.760468906429438,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5675804297823677,
      "beta": 6.713419713751217,
      "success": true,
      "log_likelihood": -9.89556910646171,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3666849696966766,
      "beta": 0.5689382941437423,
      "success": true,
      "log_likelihood": -23.625484889985337,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0978554447956625,
      "beta": 1.2694422681522561,
      "success": true,
      "log_likelihood": -17.82354085389009,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.14444292872390538,
      "beta": 2.6405675551830585,
      "success": true,
      "log_likelihood": -14.132510161702116,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7770937280672345,
      "beta": 1.4959459295654867,
      "success": true,
      "log_likelihood": -20.227818473380506,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5913308908753704,
      "beta": 0.6566794396338508,
      "success": true,
      "log_likelihood": -21.93311611749882,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.534844642014647,
      "beta": 2.7459388625109793,
      "success": true,
      "log_likelihood": -17.185262202954295,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.047311699338676,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -5.792726741160918,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7604569077027692,
      "beta": 5.396794056797045,
      "success": true,
      "log_likelihood": -11.22383128853091,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5437716512715739,
      "beta": -2.51726701544986,
      "success": true,
      "log_likelihood": -10.694666916686497,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.0736278435350089,
      "beta": 4.510274403641877,
      "success": true,
      "log_likelihood": -10.66086954084039,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2773250721258218,
      "beta": 0.5383444750766332,
      "success": true,
      "log_likelihood": -14.452016635613141,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.783628557358178,
      "beta": 1.2710067200334014,
      "success": true,
      "log_likelihood": -20.39901099248303,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0916482857458747,
      "beta": 3.2236640378469503,
      "success": true,
      "log_likelihood": -15.839727253723053,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.18728146521531297,
      "beta": 5.472198238094991,
      "success": true,
      "log_likelihood": -8.872499611413229,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.58362816419697,
      "beta": 1.6553108473770306,
      "success": true,
      "log_likelihood": -20.718497968397816,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4550213715751557,
      "beta": 4.455258061040193,
      "success": true,
      "log_likelihood": -12.498698561282993,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.196196806147751,
      "beta": 2.415602637944825,
      "success": true,
      "log_likelihood": -16.796321077024558,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.1143560593208863,
      "beta": 0.7086332739850861,
      "success": true,
      "log_likelihood": -8.669940372400573,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8588308168461596,
      "beta": 1.2401486095279735,
      "success": true,
      "log_likelihood": -11.684538756440501,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.143986719429063,
      "beta": 2.003514426972308,
      "success": true,
      "log_likelihood": -17.467873764160956,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3286569986577785,
      "beta": -0.059400694214814885,
      "success": true,
      "log_likelihood": -11.536836689872361,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.13003688352712092,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -3.757671247233759,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.09370745575607312,
      "beta": 2.9423491501919816,
      "success": true,
      "log_likelihood": -13.689306196780523,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2280354198723071,
      "beta": 2.945186417462666,
      "success": true,
      "log_likelihood": -15.97597025212592,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3216179012586415,
      "beta": 0.8608902891518578,
      "success": true,
      "log_likelihood": -15.031050810983857,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3538640027296708,
      "beta": 0.7894698139172843,
      "success": true,
      "log_likelihood": -23.456304449678793,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.475591365062854,
      "beta": 2.2105491121933984,
      "success": true,
      "log_likelihood": -15.272684175231099,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.2697605007735535,
      "beta": 0.6292137403483732,
      "success": true,
      "log_likelihood": -7.610936253485124,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.6366060892925981,
      "beta": 3.4574685107464362,
      "success": true,
      "log_likelihood": -8.221885500947073,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5067755176440479,
      "beta": 0.41524983287563016,
      "success": true,
      "log_likelihood": -22.428310975833163,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.18727737818525736,
      "beta": 5.472185038296505,
      "success": true,
      "log_likelihood": -8.8724996114126,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4095692990932649,
      "beta": 0.8508951077510224,
      "success": true,
      "log_likelihood": -23.129659174070312,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9734366525280377,
      "beta": -1.0064413746838814,
      "success": true,
      "log_likelihood": -11.042261896589032,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.18351005927841643,
      "beta": 3.5677897540105707,
      "success": true,
      "log_likelihood": -11.556681596955347,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.867340805741012,
      "beta": 4.3327838108236545,
      "success": true,
      "log_likelihood": -12.367609505720377,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3336416630174223,
      "beta": 0.34568167967665403,
      "success": true,
      "log_likelihood": -23.805586175298377,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": -0.015123017005896175,
      "beta": 6.795553722212179,
      "success": true,
      "log_likelihood": -8.445185215076602,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0845459865352958,
      "beta": 1.4625439193920744,
      "success": true,
      "log_likelihood": -18.040210952504324,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1893680182036697,
      "beta": 3.4197701386843247,
      "success": true,
      "log_likelihood": -14.09955664394713,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.4587075528911146,
      "beta": 1.5344908597569364,
      "success": true,
      "log_likelihood": -5.3586674118505915,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2536319725228062,
      "beta": 2.838230549367886,
      "success": true,
      "log_likelihood": -16.025727992309072,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.668119181132519,
      "beta": 0.5017634084891265,
      "success": true,
      "log_likelihood": -20.94060079883413,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.7502701593412572,
        "p": 2.599511731865597e-19
      },
      "beta": {
        "r": 0.8548325489134752,
        "p": 1.122731725334228e-29
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 32.20179038739441,
  "bic_error": "'ModelOutput' object has no attribute 'metrics'",
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.750\n- beta: r = 0.855",
  "previous_models": [
    "Specification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).\nBIC: 32.20179038739441\n\nParameter Recovery:\n- alpha: r = 0.750\n- beta: r = 0.855"
  ]
}