{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. If there are no previous models, start with one learnable parameter, then ensure the space of possible models is thoroughly explored with one parameter before moving to two parameters, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: Utility of accepting an offer combines linear monetary value and proportional fairness. The fairness term quantifies how much the proposed percentage deviates from the contribution-based equitable percentage (tokens_self / total_tokens). Two parameters control sensitivity to absolute monetary gains (alpha) and fairness violations (beta).\nBIC: 35.35895699690902\nParameter Recovery:\n- alpha: r = 0.625\n- beta: r = 0.856\nModel 2:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self)\nSummary: A linear utility model combining absolute monetary gain and contribution-based fairness. The utility of accepting an offer depends on the proposed amount (split_self) and its deviation from the participant's equitable contribution (3 * token_self). Two parameters control sensitivity to money (alpha) and fairness violations (beta), where beta < 0 indicates aversion to offers below one's contribution. The model directly anchors fairness expectations to pretask performance.\nBIC: 30.777436411868965\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.903\nModel 3:\nSpecification: U_accept = alpha * split_self + beta * ((split_self - 3 * token_self) / (3 * token_self))\nSummary: A utility model combining absolute monetary gain and proportional fairness evaluation. The utility of accepting an offer depends linearly on the proposed amount (split_self) and the proportional deviation of this amount from the participant's equitable contribution (3 * token_self). The proportional term captures sensitivity to relative fairness violations, where offers below one's contribution are penalized according to their percentage shortfall. Two parameters control sensitivity to money (alpha) and proportional fairness (beta).\nBIC: 32.24846051603598\nParameter Recovery:\n- alpha: r = 0.767\n- beta: r = 0.562\nModel 4:\nSpecification: U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0)\nSummary: A utility model based on inequity aversion, where the utility of accepting an offer depends on the absolute monetary gain (split_self) and aversion to both disadvantageous (split_opp > split_self) and advantageous (split_self > split_opp) inequality. Two learnable parameters (alpha, beta) quantify sensitivity to these inequalities. The model captures how responders evaluate offers relative to the proposer's share, penalizing unequal outcomes even when beneficial.\nBIC: 34.61528543210206\nParameter Recovery:\n- alpha: r = 0.827\n- beta: r = -0.027\nModel 5:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).\nBIC: 32.20179038739441\nParameter Recovery:\n- alpha: r = 0.750\n- beta: r = 0.855\nModel 6:\nSpecification: U_accept = alpha * split_self + beta * max(3 * token_self - split_self, 0)\nSummary: A utility model combining absolute monetary gain with loss aversion relative to contribution-based fairness. The utility of accepting an offer depends linearly on the proposed amount (split_self) and incorporates a penalty for offers below the participant's equitable contribution (3 * token_self). The penalty term activates only when split_self < 3 * token_self, capturing asymmetric sensitivity to disadvantageous inequality. Two parameters control sensitivity to money (alpha \u2265 0) and fairness violations (beta \u2264 0), with beta quantifying aversion to offers below one's contribution.\nBIC: 31.092498427958233\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.836",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the given task. The model should obtain the utility, (not the probability of choosing an action).  Your goal is to eventually explore the entire space of possible explainations and mechanisms by which the observed behavior arises.\n\nYour model should use the given variables appropriately, but does not necessary need to use all or even most of the variables. Do not include the model for the utility of rejecting an offer. \n\nThe model should:\n1. Predict only responder behavior.\n2. Build on relevant psychological principles.\n3. Be considerate of high covariance between learnable parameters as the model will have to pass parameter recovery tests.\n4. Be applicable to the dataset given within the task description (i.e. the model should be able to be run using only the provided dataset structure).\n5. Not include parameters to simulate general stochasticity of behavior, because this will be handled later by a temperature parameter.\n6. Should have no more than two learnable parameters.\n7. This specific model should calculate the utility of an action, which will then be used to determine the probability of choosing that action. Negative values should represent a negative utility, and thus an unlikely action.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Use clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'. DO NOT USE 'lambda' AS A PARAMETER NAME.\n\nYou will be given the complete list of models that you have already created. Analyze the models, and notice the absence of possible explanations of behaviors that should be tested. You should start with simple obvious explanations, and as you notice these being thoroughly explored you should shift to fundamentally different models of behavior. Pay less attention to the names of the variables, and how they are actually interacting with the given behavioral data. If there are no previous models, start with one learnable parameter, then ensure the space of possible models is thoroughly explored with one parameter before moving to two parameters, etc.\n\nYour results will be scored on a combination of BIC with a penalization for number of learned parameters, the ability to recover those learned parameters, and the ability of those parameters to account for the differences in behaviors among varying groups of people.\n\nPrevious Models:\nModel 1:\nSpecification: U_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\nSummary: Utility of accepting an offer combines linear monetary value and proportional fairness. The fairness term quantifies how much the proposed percentage deviates from the contribution-based equitable percentage (tokens_self / total_tokens). Two parameters control sensitivity to absolute monetary gains (alpha) and fairness violations (beta).\nBIC: 35.35895699690902\nParameter Recovery:\n- alpha: r = 0.625\n- beta: r = 0.856\nModel 2:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self)\nSummary: A linear utility model combining absolute monetary gain and contribution-based fairness. The utility of accepting an offer depends on the proposed amount (split_self) and its deviation from the participant's equitable contribution (3 * token_self). Two parameters control sensitivity to money (alpha) and fairness violations (beta), where beta < 0 indicates aversion to offers below one's contribution. The model directly anchors fairness expectations to pretask performance.\nBIC: 30.777436411868965\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.903\nModel 3:\nSpecification: U_accept = alpha * split_self + beta * ((split_self - 3 * token_self) / (3 * token_self))\nSummary: A utility model combining absolute monetary gain and proportional fairness evaluation. The utility of accepting an offer depends linearly on the proposed amount (split_self) and the proportional deviation of this amount from the participant's equitable contribution (3 * token_self). The proportional term captures sensitivity to relative fairness violations, where offers below one's contribution are penalized according to their percentage shortfall. Two parameters control sensitivity to money (alpha) and proportional fairness (beta).\nBIC: 32.24846051603598\nParameter Recovery:\n- alpha: r = 0.767\n- beta: r = 0.562\nModel 4:\nSpecification: U_accept = split_self - alpha * max(split_opp - split_self, 0) - beta * max(split_self - split_opp, 0)\nSummary: A utility model based on inequity aversion, where the utility of accepting an offer depends on the absolute monetary gain (split_self) and aversion to both disadvantageous (split_opp > split_self) and advantageous (split_self > split_opp) inequality. Two learnable parameters (alpha, beta) quantify sensitivity to these inequalities. The model captures how responders evaluate offers relative to the proposer's share, penalizing unequal outcomes even when beneficial.\nBIC: 34.61528543210206\nParameter Recovery:\n- alpha: r = 0.827\n- beta: r = -0.027\nModel 5:\nSpecification: U_accept = alpha * split_self + beta * (split_self - 3 * token_self) * (token_self / (token_self + token_opp))\nSummary: A utility model combining absolute monetary gain and contribution-scaled fairness evaluation. The utility of accepting an offer depends on the proposed amount (split_self) and the deviation from the participant's equitable contribution (3 * token_self), weighted by their relative contribution (token_self / total_tokens). This captures how sensitivity to fairness violations depends on one's share of the total contribution, allowing differential responses based on entitlement strength. Two parameters control sensitivity to money (alpha) and scaled fairness (beta).\nBIC: 32.20179038739441\nParameter Recovery:\n- alpha: r = 0.750\n- beta: r = 0.855\nModel 6:\nSpecification: U_accept = alpha * split_self + beta * max(3 * token_self - split_self, 0)\nSummary: A utility model combining absolute monetary gain with loss aversion relative to contribution-based fairness. The utility of accepting an offer depends linearly on the proposed amount (split_self) and incorporates a penalty for offers below the participant's equitable contribution (3 * token_self). The penalty term activates only when split_self < 3 * token_self, capturing asymmetric sensitivity to disadvantageous inequality. Two parameters control sensitivity to money (alpha \u2265 0) and fairness violations (beta \u2264 0), with beta quantifying aversion to offers below one's contribution.\nBIC: 31.092498427958233\nParameter Recovery:\n- alpha: r = 0.836\n- beta: r = 0.836\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer for a responder in the given experiment. The model needs to be based on psychological principles, use the dataset variables appropriately, and have no more than two learnable parameters. Also, it should avoid parameters for stochasticity since that's handled later by a temperature parameter.\n\nFirst, let's review the previous models to see what's been covered. \n\nModel 1 used split_self and a fairness term comparing the proposed percentage to the contribution-based equitable percentage. Model 2 focused on split_self and deviation from 3*token_self. Model 3 added a proportional term to Model 2's fairness. Model 4 was about inequity aversion between split_self and split_opp. Model 5 scaled the fairness term by the contribution ratio. Model 6 introduced loss aversion, penalizing only when split_self is below 3*token_self.\n\nLooking at these, Models 2, 3, 5, and 6 all involve the participant's contribution (token_self) converted to monetary terms (3*token_self). The split_self is compared to this value. However, Model 4 considered the opponent's split (split_opp) instead. \n\nThe BIC scores show that Model 2 and Model 6 have better scores (lower BIC is better). Their parameter recovery is also decent. But maybe there's another angle to explore. \n\nThe task mentions that each trial has a variable combined_earning, so two equal split_self values don't represent the same fairness. This suggests that the absolute split_self might not be sufficient; perhaps the proportion relative to the total combined earning matters. \n\nWait, the splitperc_self is already a percentage, but maybe participants evaluate fairness based on their contribution relative to the total tokens. For example, if the participant contributed more tokens, they might expect a higher percentage of the combined earnings. \n\nIn previous models, the fairness term often uses (split_self - 3*token_self) or similar. But maybe participants are comparing the proposed percentage (splitperc_self) to their contribution percentage (token_self / (token_self + token_opp) * 100). That's what Model 1 did. However, Model 1's BIC was higher (worse) than others. But maybe combining this with another term could help.\n\nAlternatively, perhaps participants are sensitive to both their own contribution and the opponent's contribution. For example, if the opponent contributed more, maybe the participant is more willing to accept a lower split_self. But how to model that?\n\nAnother angle is the ratio of split_self to their contribution. For instance, split_self / (3*token_self) could represent how much they're getting relative to their own contribution. If this ratio is below 1, it's a loss, which might be penalized more. This is similar to Model 3 but using a ratio instead of a difference.\n\nWait, Model 3 used (split_self - 3*token_self)/(3*token_self), which is the proportional deviation. But maybe participants are more sensitive to the ratio itself. For example, if they get less than their contribution, the utility decreases.\n\nAlternatively, combining the absolute monetary gain (split_self) with a term that represents the ratio of their split to their contribution. So, U = alpha * split_self + beta * (split_self / (3 * token_self)). Here, beta would capture how much they value getting a proportion relative to their contribution. If beta is positive, higher ratios (more than their contribution) increase utility. But if beta is negative, maybe they dislike getting more than their contribution? Hmm, not sure. Alternatively, maybe beta is a weight on the ratio, but if the ratio is less than 1, it's a penalty.\n\nWait, but in Model 6, they used max(3*token_self - split_self, 0), which only penalizes when split_self is below their contribution. That's loss aversion. Maybe combining that with a term that also considers the opponent's split. But the problem says to ignore the utility of rejecting, so focus on the utility of accepting.\n\nAnother thought: the combined_earning is token_self*3 + token_opp*3. So split_self + split_opp = combined_earning. The split percentages are splitperc_self and splitperc_opp. If the participant's contribution is token_self, their \"fair\" share would be (token_self / (token_self + token_opp)) * combined_earning. So split_self - fair_share would be the deviation from the contribution-based fair amount. \n\nWait, this is similar to Model 1's fairness term but in absolute terms instead of percentage. Let's compute fair_share as (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and fair_share is split_self - (token_self/(token_self + token_opp)) * combined_earning. \n\nBut combined_earning is 3*(token_self + token_opp), so substituting that in, fair_share becomes 3*token_self. Wait, because (token_self/(token_self + token_opp)) * 3*(token_self + token_opp) = 3*token_self. So split_self - 3*token_self is the same as split_self - fair_share. So Models 2, 3, 5, 6 are all using this difference. \n\nBut in the task description, it's noted that two equal split_self values don't represent the same fairness because combined_earning varies. Wait, but if split_self is compared to 3*token_self, which is fixed based on their own contribution, then even if combined_earning varies, the fair share is 3*token_self. For example, if combined_earning is higher because the opponent contributed more, then the fair share for the participant is still 3*token_self, regardless of the opponent's contribution. So maybe participants are comparing their split_self to their own contribution (3*token_self), not considering the opponent's contribution. But the task says that the fairness isn't the same when combined_earning varies. Hmm, maybe the split percentage (splitperc_self) compared to their contribution percentage (token_self/(total tokens)*100) is a better measure. \n\nWait, let's think. Suppose in trial A, token_self=2, token_opp=2. Combined_earning=12. The fair split_self would be 6 (3*2). If split_self is 6, splitperc_self is 50%, which matches their contribution (50% of tokens). In trial B, token_self=2, token_opp=4. Combined_earning=18. Fair split_self is 6 (3*2), but splitperc_self would be 6/18=33.3%, which matches their token contribution (2/6=33.3%). So in both cases, split_self=6 is fair, but in trial A, splitperc_self is 50%, and in trial B, 33.3%. So if the model uses split_self - 3*token_self, then in both cases, the fairness term is zero. But according to the task, two equal split_self values (e.g., 6) in different trials (different combined_earning) don't represent the same fairness. Wait, but in this example, split_self=6 is exactly fair in both cases. So maybe the task is referring to cases where split_self is the same, but combined_earning is different, leading to different split percentages. For example, if in trial C, token_self=2, token_opp=1. Combined_earning=9. split_self=6 (splitperc_self=66.6%), which is more than their contribution (2/3=66.6%). So split_self=6 here is fair. But if in another trial, token_self=4, token_opp=2. Combined_earning=18. split_self=6 (splitperc_self=33.3%), which is less than their contribution (4/6=66.6%). So split_self=6 here is unfair. So in this case, the same split_self can be fair or unfair depending on the combined_earning. Therefore, models that use split_self - 3*token_self would not capture this, because 3*token_self would be 12 in the second case, so split_self=6 is way below. Wait, no. Wait, in the second case, token_self=4, so 3*token_self=12. split_self=6 is 6 less than 12. So the model would capture that as a large negative fairness term. But in the first case, token_self=2, 3*token_self=6, split_self=6, so fairness term is zero. So the model would correctly capture that split_self=6 is fair in the first case and unfair in the second. So maybe the split_self - 3*token_self term does account for the varying combined_earning. \n\nBut the task says that two equal split_self variables do not represent the same fairness. So perhaps the split_self is not the right variable to use in the model. Maybe the split percentage (splitperc_self) compared to their contribution percentage is better. For example, if splitperc_self is higher than their contribution percentage, it's favorable. If lower, unfavorable. \n\nSo, the fairness term could be splitperc_self - (token_self/(token_self + token_opp))*100. This is exactly what Model 1 used. However, Model 1's BIC was higher than others. But maybe combining this with another term, like the absolute split_self, could improve the model. \n\nAlternatively, perhaps participants care about both the absolute amount (split_self) and the relative fairness (splitperc_self vs contribution percentage). So the utility could be a combination of these two factors. \n\nBut the previous models have tried combinations of split_self and some fairness term. The challenge is to find a model that hasn't been tried yet, which might explain the behavior better. \n\nAnother angle: the participant's contribution (token_self) might influence their entitlement. For example, if they contributed more tokens, they might have a higher expectation, so the same split_self might be less acceptable. But how to model that. \n\nWait, in Model 6, the fairness term is max(3*token_self - split_self, 0), which penalizes when split_self is below their contribution. This is loss aversion. But maybe participants also consider the opponent's contribution. For example, if the opponent contributed more, the participant might be more tolerant of a lower split_self. \n\nBut how to incorporate that. Let's think: the difference between the participant's contribution and the opponent's. If token_self > token_opp, the participant might expect a higher split. If token_opp > token_self, they might accept a lower split. \n\nSo, perhaps the fairness term could be (split_self - 3*token_self) * (token_self - token_opp). This way, if the participant contributed more, the effect of the split_self deviation is amplified. But this introduces an interaction term. However, this would require a new parameter. But the model is allowed two parameters. \n\nAlternatively, the model could have split_self and a term that is (split_self - fair_share) where fair_share is based on contribution. But that's similar to previous models. \n\nWait, another idea: participants might evaluate the offer based on both their own gain and the proposer's gain. Inequity aversion models (like Model 4) consider both, but Model 4's BIC was higher (worse) than Models 2 and 6. However, Model 4's parameter recovery for beta was poor (r=-0.027), suggesting that advantageous inequality (split_self > split_opp) isn't a significant factor. \n\nBut maybe participants are only averse to disadvantageous inequality (split_self < split_opp). So the utility could be split_self - alpha * max(split_opp - split_self, 0). This is similar to Model 4 but without the beta term for advantageous inequality. However, this would have only one parameter (alpha), but the current task allows up to two. \n\nAlternatively, combine split_self with a term that captures the ratio of split_self to split_opp. For example, split_self / split_opp. If this ratio is low, the participant is getting less than the proposer, which might decrease utility. But how to model that. \n\nAlternatively, the model could be U = alpha * split_self + beta * (split_self / split_opp). If beta is positive, a higher ratio (more than 1) increases utility. But if beta is negative, a lower ratio (split_self < split_opp) decreases utility. But this might not capture the fairness based on contributions. \n\nAlternatively, the model could compare the split percentages to the contribution percentages. For example, the difference between splitperc_self and (token_self / total_tokens)*100. This is the fairness term in Model 1. But Model 1's BIC was higher. However, combining this with split_self might help. \n\nWait, let's think of a model that uses both the absolute split_self and the proportional fairness term. For example:\n\nU_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nThis is exactly Model 1. But since Model 1's BIC was higher, maybe this isn't the best approach. \n\nAlternatively, perhaps participants are more sensitive to the ratio of their split to their contribution. For example, split_self / (3 * token_self). If this ratio is 1, it's fair. If less than 1, it's unfair. So the utility could be:\n\nU_accept = alpha * split_self + beta * (split_self / (3 * token_self))\n\nHere, beta would capture the sensitivity to the ratio. If beta is positive, higher ratios (more than 1) increase utility. But if the ratio is less than 1, it would decrease utility. However, this might not capture the aversion to being below the contribution. \n\nAlternatively, use a non-linear function of the ratio. For example, if the ratio is below 1, apply a penalty scaled by how much it's below. But the model needs to be linear in parameters. \n\nAnother angle: the participant's utility could depend on the difference between their split and the proposer's split, scaled by their contribution. For example, (split_self - split_opp) * (token_self / token_opp). But this might complicate things. \n\nWait, considering that the proposer's contribution (token_opp) affects the participant's perception. If the proposer contributed more, maybe the participant is more accepting of a lower split_self. So the fairness term could be (split_self - 3 * token_self) * (token_opp / token_self). This way, if the opponent contributed more (higher token_opp), the effect of the split_self being below their contribution is reduced. \n\nBut this introduces an interaction term between the split difference and the contribution ratio. However, this would require a parameter to scale this term. \n\nAlternatively, the model could have two terms: one for the absolute split_self and another for the split_self relative to the opponent's contribution. For example:\n\nU_accept = alpha * split_self + beta * (split_self - 3 * token_opp)\n\nBut this might not make sense. The opponent's contribution is token_opp, so 3*token_opp is their contribution in money. If the participant's split_self is compared to the opponent's contribution, that might not align with fairness. \n\nAlternatively, the model could compare the split_self to the average contribution. For example, (split_self - 3 * (token_self + token_opp)/2). But this is a different fairness concept. \n\nHmm. Let's recap. The key is to find a model that hasn't been tried yet, which could explain the behavior better. Previous models have used split_self compared to their own contribution (3*token_self), split_self compared to split_opp, and split percentage compared to contribution percentage. \n\nAnother possible angle is the ratio of the split to the combined earning. For example, split_self / combined_earning. But combined_earning is 3*(token_self + token_opp), so this is the same as splitperc_self / 100. So that's redundant with splitperc_self. \n\nWait, but maybe participants care about the absolute amount (split_self) and the relative split (splitperc_self) compared to their contribution. For example, someone might prefer a higher absolute amount even if it's a lower percentage, or vice versa. \n\nBut how to model that. Let's say:\n\nU_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nThis is Model 1. But since Model 1's BIC was higher, maybe this isn't the best approach. \n\nAlternatively, perhaps participants have a reference point based on their contribution and the opponent's contribution. For example, the fair split could be the average of their contribution and the opponent's. But I'm not sure. \n\nWait, another thought: the participant's contribution (token_self) and the opponent's contribution (token_opp) might influence their sense of entitlement. For example, if the participant contributed more, they might expect a higher split, but if the opponent contributed more, they might accept a lower split. \n\nSo, the model could include a term that weights the split_self by the participant's contribution relative to the total. For example:\n\nU_accept = alpha * split_self + beta * (split_self * (token_self / (token_self + token_opp)))\n\nThis term would adjust the value of split_self based on their contribution. If they contributed more, the same split_self is valued more. But this might not capture fairness. \n\nAlternatively, the model could compare split_self to the combined earning scaled by their contribution. For example:\n\nU_accept = alpha * split_self + beta * (split_self - (token_self / (token_self + token_opp)) * combined_earning)\n\nBut combined_earning is 3*(token_self + token_opp), so this term becomes split_self - 3*token_self, which is the same as previous models. So this doesn't add anything new. \n\nHmm. Let's think of a different psychological principle. Maybe the participant evaluates the offer based on the proposer's generosity. The proposer's share is split_opp, which is combined_earning - split_self. The proposer could keep more or less. If the proposer is keeping a lot (high split_opp), the participant might feel the offer is unfair. \n\nSo, the utility could be split_self minus some term related to split_opp. For example:\n\nU_accept = alpha * split_self - beta * split_opp\n\nBut this is similar to inequity aversion but in a different form. If beta is positive, higher split_opp (proposer keeping more) reduces utility. \n\nAlternatively, the ratio of split_self to split_opp. If the participant gets a smaller share, they might dislike it. So:\n\nU_accept = alpha * split_self + beta * (split_self / split_opp)\n\nBut this could be problematic if split_opp is zero, but in the dataset, split_opp is a float, so maybe it's always positive. \n\nAlternatively, the difference between split_self and split_opp. If the participant gets less, the difference is negative, which reduces utility. \n\nU_accept = alpha * split_self + beta * (split_self - split_opp)\n\nBut this is similar to Model 4 but without the max functions. \n\nModel 4 used max(split_opp - split_self, 0) and max(split_self - split_opp, 0), which captures disadvantageous and advantageous inequality. But the parameter recovery for beta (advantageous) was poor. \n\nSo maybe participants only care about disadvantageous inequality. So the model could be:\n\nU_accept = split_self - alpha * max(split_opp - split_self, 0)\n\nThis has one parameter (alpha), but we can add another term. For example, include the split_self compared to their contribution:\n\nU_accept = split_self - alpha * max(split_opp - split_self, 0) + beta * (split_self - 3 * token_self)\n\nBut this would have two parameters. However, this combines inequity aversion with contribution-based fairness. \n\nBut the problem is that the model should have no more than two learnable parameters. So this is possible. \n\nAlternatively, use a single parameter that combines both effects. But the task allows two. \n\nAnother angle: the participant's utility could depend on how much the proposer is keeping relative to their own contribution. For example, if the proposer's split_opp is more than their contribution (3*token_opp), that might be seen as unfair. \n\nSo, the term could be (split_opp - 3*token_opp). If the proposer is keeping more than their contribution, the participant might dislike it. \n\nSo the model could be:\n\nU_accept = alpha * split_self + beta * (split_opp - 3 * token_opp)\n\nBut this introduces the opponent's contribution into the model. If beta is positive, then the participant's utility increases if the proposer is keeping more than their contribution, which doesn't make sense. Maybe beta should be negative, so if the proposer is keeping more than their contribution, the utility decreases. \n\nBut this is a new angle not covered in previous models. Let's think: the proposer's share (split_opp) compared to their contribution (3*token_opp). If split_opp > 3*token_opp, the proposer is gaining more than their contribution, which might be seen as greedy. The participant might penalize this. \n\nSo the model would be:\n\nU_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n\nHere, beta is expected to be negative, so if the proposer is taking more than their contribution, the utility decreases. \n\nThis model combines the participant's own gain (split_self) with a term that penalizes the proposer for taking more than their fair share. This is a different fairness consideration: not just about the participant's own split, but also about the proposer's split relative to their contribution. \n\nThis hasn't been tried in previous models. Let's check the variables. split_opp is available in the dataset, as is token_opp. So this model is feasible. \n\nThe two parameters are alpha (sensitivity to own split) and beta (sensitivity to proposer's excess). \n\nThis model could explain behavior where participants reject offers not just because their own split is low, but also because the proposer is being greedy. \n\nThis seems plausible. Let's formalize this model:\n\nU_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n\nHere, max(split_opp - 3*token_opp, 0) captures the amount by which the proposer's share exceeds their contribution. If the proposer is keeping more than their contribution, this term is positive, and if beta is negative, it reduces the utility of accepting. \n\nThis model uses two parameters, alpha and beta. The variables used are split_self, split_opp, and token_opp. All are present in the dataset. \n\nThis model hasn't been tried before, according to the previous models listed. It introduces a new psychological mechanism: aversion to the proposer's greediness. \n\nNow, checking the task constraints: the model should predict responder behavior, use relevant psychological principles (here, fairness based on both own and proposer's contribution), avoid high covariance between parameters (by having distinct terms), and use variables available in the dataset. \n\nThe parameters alpha and beta would have ranges. For alpha, since split_self is in pounds, we expect alpha to be positive (more money is better). The range could be [0, some upper bound]. Beta is expected to be negative, as higher proposer greed reduces utility. So beta's range could be [-something, 0]. \n\nThis model meets all the requirements: two learnable parameters, uses available variables, builds on fairness principles, and avoids parameters for stochasticity. \n\nSo the final model would be:\n\nU_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n\nVariables:\n- alpha: sensitivity to own monetary gain (split_self)\n- beta: sensitivity to proposer's excess over their contribution (split_opp - 3*token_opp)\n\nThis model introduces a new fairness consideration, which is the proposer's share relative to their own contribution. This could explain rejections where the participant's own split is acceptable, but the proposer is seen as taking too much, which violates fairness norms.\n</think>\n\n<MODEL>\nU_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from accepting\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to proposer's share exceeding their contribution\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed monetary amount for the proposer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Opponent's token count (contribution)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the participant's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3 * token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return the utility values for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of utility values for each trial\n\n                Your code will be implemented within the following code block:\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    utility_values = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        utility_values.append(beta + (epsilon * E) + (eta * N))\n    return utility_values\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE UTLITY VALUES.\n                Model Specification:\n                U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from accepting\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to proposer's share exceeding their contribution\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed monetary amount for the proposer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Opponent's token count (contribution)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        utility = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n        utility_values.append(utility)\n    return utility_values\n\n# Data as list of dictionaries\ndata_json = \"\"\"\n[\n  {\"split_self\": 10, \"split_opp\": 20, \"token_opp\": 5},\n  {\"split_self\": 5, \"split_opp\": 15, \"token_opp\": 3},\n  {\"split_self\": 15, \"split_opp\": 10, \"token_opp\": 2}\n]\n\"\"\"\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data, alpha=0.5, beta=-0.2)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```"
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to absolute monetary gain from accepting",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to proposer's share exceeding their contribution",
      "range": {
        "min": -5,
        "max": 0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)",
  "variable_descriptions": {
    "U_accept": {
      "description": "Utility of accepting the offer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha": {
      "description": "Sensitivity to absolute monetary gain from accepting",
      "range": {
        "min": 0,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to proposer's share exceeding their contribution",
      "range": {
        "min": -5,
        "max": 0,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": {
        "type": "uniform",
        "parameters": {}
      },
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed monetary amount for the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_opp": {
      "description": "Proposed monetary amount for the proposer",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_opp": {
      "description": "Opponent's token count (contribution)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the participant's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3 * token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts the utility of accepting an offer for a responder in the given experiment. The model needs to be based on psychological principles, use the dataset variables appropriately, and have no more than two learnable parameters. Also, it should avoid parameters for stochasticity since that's handled later by a temperature parameter.\n\nFirst, let's review the previous models to see what's been covered. \n\nModel 1 used split_self and a fairness term comparing the proposed percentage to the contribution-based equitable percentage. Model 2 focused on split_self and deviation from 3*token_self. Model 3 added a proportional term to Model 2's fairness. Model 4 was about inequity aversion between split_self and split_opp. Model 5 scaled the fairness term by the contribution ratio. Model 6 introduced loss aversion, penalizing only when split_self is below 3*token_self.\n\nLooking at these, Models 2, 3, 5, and 6 all involve the participant's contribution (token_self) converted to monetary terms (3*token_self). The split_self is compared to this value. However, Model 4 considered the opponent's split (split_opp) instead. \n\nThe BIC scores show that Model 2 and Model 6 have better scores (lower BIC is better). Their parameter recovery is also decent. But maybe there's another angle to explore. \n\nThe task mentions that each trial has a variable combined_earning, so two equal split_self values don't represent the same fairness. This suggests that the absolute split_self might not be sufficient; perhaps the proportion relative to the total combined earning matters. \n\nWait, the splitperc_self is already a percentage, but maybe participants evaluate fairness based on their contribution relative to the total tokens. For example, if the participant contributed more tokens, they might expect a higher percentage of the combined earnings. \n\nIn previous models, the fairness term often uses (split_self - 3*token_self) or similar. But maybe participants are comparing the proposed percentage (splitperc_self) to their contribution percentage (token_self / (token_self + token_opp) * 100). That's what Model 1 did. However, Model 1's BIC was higher (worse) than others. But maybe combining this with another term could help.\n\nAlternatively, perhaps participants are sensitive to both their own contribution and the opponent's contribution. For example, if the opponent contributed more, maybe the participant is more willing to accept a lower split_self. But how to model that?\n\nAnother angle is the ratio of split_self to their contribution. For instance, split_self / (3*token_self) could represent how much they're getting relative to their own contribution. If this ratio is below 1, it's a loss, which might be penalized more. This is similar to Model 3 but using a ratio instead of a difference.\n\nWait, Model 3 used (split_self - 3*token_self)/(3*token_self), which is the proportional deviation. But maybe participants are more sensitive to the ratio itself. For example, if they get less than their contribution, the utility decreases.\n\nAlternatively, combining the absolute monetary gain (split_self) with a term that represents the ratio of their split to their contribution. So, U = alpha * split_self + beta * (split_self / (3 * token_self)). Here, beta would capture how much they value getting a proportion relative to their contribution. If beta is positive, higher ratios (more than their contribution) increase utility. But if beta is negative, maybe they dislike getting more than their contribution? Hmm, not sure. Alternatively, maybe beta is a weight on the ratio, but if the ratio is less than 1, it's a penalty.\n\nWait, but in Model 6, they used max(3*token_self - split_self, 0), which only penalizes when split_self is below their contribution. That's loss aversion. Maybe combining that with a term that also considers the opponent's split. But the problem says to ignore the utility of rejecting, so focus on the utility of accepting.\n\nAnother thought: the combined_earning is token_self*3 + token_opp*3. So split_self + split_opp = combined_earning. The split percentages are splitperc_self and splitperc_opp. If the participant's contribution is token_self, their \"fair\" share would be (token_self / (token_self + token_opp)) * combined_earning. So split_self - fair_share would be the deviation from the contribution-based fair amount. \n\nWait, this is similar to Model 1's fairness term but in absolute terms instead of percentage. Let's compute fair_share as (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and fair_share is split_self - (token_self/(token_self + token_opp)) * combined_earning. \n\nBut combined_earning is 3*(token_self + token_opp), so substituting that in, fair_share becomes 3*token_self. Wait, because (token_self/(token_self + token_opp)) * 3*(token_self + token_opp) = 3*token_self. So split_self - 3*token_self is the same as split_self - fair_share. So Models 2, 3, 5, 6 are all using this difference. \n\nBut in the task description, it's noted that two equal split_self values don't represent the same fairness because combined_earning varies. Wait, but if split_self is compared to 3*token_self, which is fixed based on their own contribution, then even if combined_earning varies, the fair share is 3*token_self. For example, if combined_earning is higher because the opponent contributed more, then the fair share for the participant is still 3*token_self, regardless of the opponent's contribution. So maybe participants are comparing their split_self to their own contribution (3*token_self), not considering the opponent's contribution. But the task says that the fairness isn't the same when combined_earning varies. Hmm, maybe the split percentage (splitperc_self) compared to their contribution percentage (token_self/(total tokens)*100) is a better measure. \n\nWait, let's think. Suppose in trial A, token_self=2, token_opp=2. Combined_earning=12. The fair split_self would be 6 (3*2). If split_self is 6, splitperc_self is 50%, which matches their contribution (50% of tokens). In trial B, token_self=2, token_opp=4. Combined_earning=18. Fair split_self is 6 (3*2), but splitperc_self would be 6/18=33.3%, which matches their token contribution (2/6=33.3%). So in both cases, split_self=6 is fair, but in trial A, splitperc_self is 50%, and in trial B, 33.3%. So if the model uses split_self - 3*token_self, then in both cases, the fairness term is zero. But according to the task, two equal split_self values (e.g., 6) in different trials (different combined_earning) don't represent the same fairness. Wait, but in this example, split_self=6 is exactly fair in both cases. So maybe the task is referring to cases where split_self is the same, but combined_earning is different, leading to different split percentages. For example, if in trial C, token_self=2, token_opp=1. Combined_earning=9. split_self=6 (splitperc_self=66.6%), which is more than their contribution (2/3=66.6%). So split_self=6 here is fair. But if in another trial, token_self=4, token_opp=2. Combined_earning=18. split_self=6 (splitperc_self=33.3%), which is less than their contribution (4/6=66.6%). So split_self=6 here is unfair. So in this case, the same split_self can be fair or unfair depending on the combined_earning. Therefore, models that use split_self - 3*token_self would not capture this, because 3*token_self would be 12 in the second case, so split_self=6 is way below. Wait, no. Wait, in the second case, token_self=4, so 3*token_self=12. split_self=6 is 6 less than 12. So the model would capture that as a large negative fairness term. But in the first case, token_self=2, 3*token_self=6, split_self=6, so fairness term is zero. So the model would correctly capture that split_self=6 is fair in the first case and unfair in the second. So maybe the split_self - 3*token_self term does account for the varying combined_earning. \n\nBut the task says that two equal split_self variables do not represent the same fairness. So perhaps the split_self is not the right variable to use in the model. Maybe the split percentage (splitperc_self) compared to their contribution percentage is better. For example, if splitperc_self is higher than their contribution percentage, it's favorable. If lower, unfavorable. \n\nSo, the fairness term could be splitperc_self - (token_self/(token_self + token_opp))*100. This is exactly what Model 1 used. However, Model 1's BIC was higher than others. But maybe combining this with another term, like the absolute split_self, could improve the model. \n\nAlternatively, perhaps participants care about both the absolute amount (split_self) and the relative fairness (splitperc_self vs contribution percentage). So the utility could be a combination of these two factors. \n\nBut the previous models have tried combinations of split_self and some fairness term. The challenge is to find a model that hasn't been tried yet, which might explain the behavior better. \n\nAnother angle: the participant's contribution (token_self) might influence their entitlement. For example, if they contributed more tokens, they might have a higher expectation, so the same split_self might be less acceptable. But how to model that. \n\nWait, in Model 6, the fairness term is max(3*token_self - split_self, 0), which penalizes when split_self is below their contribution. This is loss aversion. But maybe participants also consider the opponent's contribution. For example, if the opponent contributed more, the participant might be more tolerant of a lower split_self. \n\nBut how to incorporate that. Let's think: the difference between the participant's contribution and the opponent's. If token_self > token_opp, the participant might expect a higher split. If token_opp > token_self, they might accept a lower split. \n\nSo, perhaps the fairness term could be (split_self - 3*token_self) * (token_self - token_opp). This way, if the participant contributed more, the effect of the split_self deviation is amplified. But this introduces an interaction term. However, this would require a new parameter. But the model is allowed two parameters. \n\nAlternatively, the model could have split_self and a term that is (split_self - fair_share) where fair_share is based on contribution. But that's similar to previous models. \n\nWait, another idea: participants might evaluate the offer based on both their own gain and the proposer's gain. Inequity aversion models (like Model 4) consider both, but Model 4's BIC was higher (worse) than Models 2 and 6. However, Model 4's parameter recovery for beta was poor (r=-0.027), suggesting that advantageous inequality (split_self > split_opp) isn't a significant factor. \n\nBut maybe participants are only averse to disadvantageous inequality (split_self < split_opp). So the utility could be split_self - alpha * max(split_opp - split_self, 0). This is similar to Model 4 but without the beta term for advantageous inequality. However, this would have only one parameter (alpha), but the current task allows up to two. \n\nAlternatively, combine split_self with a term that captures the ratio of split_self to split_opp. For example, split_self / split_opp. If this ratio is low, the participant is getting less than the proposer, which might decrease utility. But how to model that. \n\nAlternatively, the model could be U = alpha * split_self + beta * (split_self / split_opp). If beta is positive, a higher ratio (more than 1) increases utility. But if beta is negative, a lower ratio (split_self < split_opp) decreases utility. But this might not capture the fairness based on contributions. \n\nAlternatively, the model could compare the split percentages to the contribution percentages. For example, the difference between splitperc_self and (token_self / total_tokens)*100. This is the fairness term in Model 1. But Model 1's BIC was higher. However, combining this with split_self might help. \n\nWait, let's think of a model that uses both the absolute split_self and the proportional fairness term. For example:\n\nU_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nThis is exactly Model 1. But since Model 1's BIC was higher, maybe this isn't the best approach. \n\nAlternatively, perhaps participants are more sensitive to the ratio of their split to their contribution. For example, split_self / (3 * token_self). If this ratio is 1, it's fair. If less than 1, it's unfair. So the utility could be:\n\nU_accept = alpha * split_self + beta * (split_self / (3 * token_self))\n\nHere, beta would capture the sensitivity to the ratio. If beta is positive, higher ratios (more than 1) increase utility. But if the ratio is less than 1, it would decrease utility. However, this might not capture the aversion to being below the contribution. \n\nAlternatively, use a non-linear function of the ratio. For example, if the ratio is below 1, apply a penalty scaled by how much it's below. But the model needs to be linear in parameters. \n\nAnother angle: the participant's utility could depend on the difference between their split and the proposer's split, scaled by their contribution. For example, (split_self - split_opp) * (token_self / token_opp). But this might complicate things. \n\nWait, considering that the proposer's contribution (token_opp) affects the participant's perception. If the proposer contributed more, maybe the participant is more accepting of a lower split_self. So the fairness term could be (split_self - 3 * token_self) * (token_opp / token_self). This way, if the opponent contributed more (higher token_opp), the effect of the split_self being below their contribution is reduced. \n\nBut this introduces an interaction term between the split difference and the contribution ratio. However, this would require a parameter to scale this term. \n\nAlternatively, the model could have two terms: one for the absolute split_self and another for the split_self relative to the opponent's contribution. For example:\n\nU_accept = alpha * split_self + beta * (split_self - 3 * token_opp)\n\nBut this might not make sense. The opponent's contribution is token_opp, so 3*token_opp is their contribution in money. If the participant's split_self is compared to the opponent's contribution, that might not align with fairness. \n\nAlternatively, the model could compare the split_self to the average contribution. For example, (split_self - 3 * (token_self + token_opp)/2). But this is a different fairness concept. \n\nHmm. Let's recap. The key is to find a model that hasn't been tried yet, which could explain the behavior better. Previous models have used split_self compared to their own contribution (3*token_self), split_self compared to split_opp, and split percentage compared to contribution percentage. \n\nAnother possible angle is the ratio of the split to the combined earning. For example, split_self / combined_earning. But combined_earning is 3*(token_self + token_opp), so this is the same as splitperc_self / 100. So that's redundant with splitperc_self. \n\nWait, but maybe participants care about the absolute amount (split_self) and the relative split (splitperc_self) compared to their contribution. For example, someone might prefer a higher absolute amount even if it's a lower percentage, or vice versa. \n\nBut how to model that. Let's say:\n\nU_accept = alpha * split_self + beta * (splitperc_self - (token_self / (token_self + token_opp)) * 100)\n\nThis is Model 1. But since Model 1's BIC was higher, maybe this isn't the best approach. \n\nAlternatively, perhaps participants have a reference point based on their contribution and the opponent's contribution. For example, the fair split could be the average of their contribution and the opponent's. But I'm not sure. \n\nWait, another thought: the participant's contribution (token_self) and the opponent's contribution (token_opp) might influence their sense of entitlement. For example, if the participant contributed more, they might expect a higher split, but if the opponent contributed more, they might accept a lower split. \n\nSo, the model could include a term that weights the split_self by the participant's contribution relative to the total. For example:\n\nU_accept = alpha * split_self + beta * (split_self * (token_self / (token_self + token_opp)))\n\nThis term would adjust the value of split_self based on their contribution. If they contributed more, the same split_self is valued more. But this might not capture fairness. \n\nAlternatively, the model could compare split_self to the combined earning scaled by their contribution. For example:\n\nU_accept = alpha * split_self + beta * (split_self - (token_self / (token_self + token_opp)) * combined_earning)\n\nBut combined_earning is 3*(token_self + token_opp), so this term becomes split_self - 3*token_self, which is the same as previous models. So this doesn't add anything new. \n\nHmm. Let's think of a different psychological principle. Maybe the participant evaluates the offer based on the proposer's generosity. The proposer's share is split_opp, which is combined_earning - split_self. The proposer could keep more or less. If the proposer is keeping a lot (high split_opp), the participant might feel the offer is unfair. \n\nSo, the utility could be split_self minus some term related to split_opp. For example:\n\nU_accept = alpha * split_self - beta * split_opp\n\nBut this is similar to inequity aversion but in a different form. If beta is positive, higher split_opp (proposer keeping more) reduces utility. \n\nAlternatively, the ratio of split_self to split_opp. If the participant gets a smaller share, they might dislike it. So:\n\nU_accept = alpha * split_self + beta * (split_self / split_opp)\n\nBut this could be problematic if split_opp is zero, but in the dataset, split_opp is a float, so maybe it's always positive. \n\nAlternatively, the difference between split_self and split_opp. If the participant gets less, the difference is negative, which reduces utility. \n\nU_accept = alpha * split_self + beta * (split_self - split_opp)\n\nBut this is similar to Model 4 but without the max functions. \n\nModel 4 used max(split_opp - split_self, 0) and max(split_self - split_opp, 0), which captures disadvantageous and advantageous inequality. But the parameter recovery for beta (advantageous) was poor. \n\nSo maybe participants only care about disadvantageous inequality. So the model could be:\n\nU_accept = split_self - alpha * max(split_opp - split_self, 0)\n\nThis has one parameter (alpha), but we can add another term. For example, include the split_self compared to their contribution:\n\nU_accept = split_self - alpha * max(split_opp - split_self, 0) + beta * (split_self - 3 * token_self)\n\nBut this would have two parameters. However, this combines inequity aversion with contribution-based fairness. \n\nBut the problem is that the model should have no more than two learnable parameters. So this is possible. \n\nAlternatively, use a single parameter that combines both effects. But the task allows two. \n\nAnother angle: the participant's utility could depend on how much the proposer is keeping relative to their own contribution. For example, if the proposer's split_opp is more than their contribution (3*token_opp), that might be seen as unfair. \n\nSo, the term could be (split_opp - 3*token_opp). If the proposer is keeping more than their contribution, the participant might dislike it. \n\nSo the model could be:\n\nU_accept = alpha * split_self + beta * (split_opp - 3 * token_opp)\n\nBut this introduces the opponent's contribution into the model. If beta is positive, then the participant's utility increases if the proposer is keeping more than their contribution, which doesn't make sense. Maybe beta should be negative, so if the proposer is keeping more than their contribution, the utility decreases. \n\nBut this is a new angle not covered in previous models. Let's think: the proposer's share (split_opp) compared to their contribution (3*token_opp). If split_opp > 3*token_opp, the proposer is gaining more than their contribution, which might be seen as greedy. The participant might penalize this. \n\nSo the model would be:\n\nU_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n\nHere, beta is expected to be negative, so if the proposer is taking more than their contribution, the utility decreases. \n\nThis model combines the participant's own gain (split_self) with a term that penalizes the proposer for taking more than their fair share. This is a different fairness consideration: not just about the participant's own split, but also about the proposer's split relative to their contribution. \n\nThis hasn't been tried in previous models. Let's check the variables. split_opp is available in the dataset, as is token_opp. So this model is feasible. \n\nThe two parameters are alpha (sensitivity to own split) and beta (sensitivity to proposer's excess). \n\nThis model could explain behavior where participants reject offers not just because their own split is low, but also because the proposer is being greedy. \n\nThis seems plausible. Let's formalize this model:\n\nU_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n\nHere, max(split_opp - 3*token_opp, 0) captures the amount by which the proposer's share exceeds their contribution. If the proposer is keeping more than their contribution, this term is positive, and if beta is negative, it reduces the utility of accepting. \n\nThis model uses two parameters, alpha and beta. The variables used are split_self, split_opp, and token_opp. All are present in the dataset. \n\nThis model hasn't been tried before, according to the previous models listed. It introduces a new psychological mechanism: aversion to the proposer's greediness. \n\nNow, checking the task constraints: the model should predict responder behavior, use relevant psychological principles (here, fairness based on both own and proposer's contribution), avoid high covariance between parameters (by having distinct terms), and use variables available in the dataset. \n\nThe parameters alpha and beta would have ranges. For alpha, since split_self is in pounds, we expect alpha to be positive (more money is better). The range could be [0, some upper bound]. Beta is expected to be negative, as higher proposer greed reduces utility. So beta's range could be [-something, 0]. \n\nThis model meets all the requirements: two learnable parameters, uses available variables, builds on fairness principles, and avoids parameters for stochasticity. \n\nSo the final model would be:\n\nU_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n\nVariables:\n- alpha: sensitivity to own monetary gain (split_self)\n- beta: sensitivity to proposer's excess over their contribution (split_opp - 3*token_opp)\n\nThis model introduces a new fairness consideration, which is the proposer's share relative to their own contribution. This could explain rejections where the participant's own split is acceptable, but the proposer is seen as taking too much, which violates fairness norms.\n</think>\n\n<MODEL>\nU_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_accept\": {\n      \"description\": \"Utility of accepting the offer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to absolute monetary gain from accepting\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to proposer's share exceeding their contribution\",\n      \"range\": {\n        \"min\": -5,\n        \"max\": 0,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": {\n        \"type\": \"uniform\",\n        \"parameters\": {}\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed monetary amount for the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed monetary amount for the proposer\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_opp\": {\n      \"description\": \"Opponent's token count (contribution)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the participant's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3 * token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\n</SUMMARY>",
  "current_model": "Specification: U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\nSummary: A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the participant's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3 * token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.",
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    utility_values = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        token_opp = trial.get(\"token_opp\", 0)\n        utility = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\n        utility_values.append(utility)\n    return utility_values",
  "fitting_results": [
    {
      "alpha": 0.0,
      "beta": -3.8059999603083927,
      "success": true,
      "log_likelihood": -7.931512218248688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.179860178246637,
      "beta": -1.764184397914078,
      "success": true,
      "log_likelihood": -16.2812626834979,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.499876463768511,
      "beta": -3.5889602293360703,
      "success": true,
      "log_likelihood": -10.046565397149049,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4181557032974472,
      "beta": -2.461251202464816,
      "success": true,
      "log_likelihood": -13.653156181715362,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.028682805775713,
      "beta": -3.8689134004005563,
      "success": true,
      "log_likelihood": -9.848024476410474,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.419619051748138,
      "beta": -2.5402916711190375,
      "success": true,
      "log_likelihood": -11.042332344704928,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.19582658987568802,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.4169262704087466,
      "beta": -2.4417457431425995,
      "success": true,
      "log_likelihood": -11.096668637605786,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.19582658987568802,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5441684560729385,
      "beta": -3.275809787122966,
      "success": true,
      "log_likelihood": -11.166531339816157,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.921398721567089,
      "beta": -5.0,
      "success": true,
      "log_likelihood": -4.137287179732469,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.14166069077679455,
      "beta": -3.4120646265626573,
      "success": true,
      "log_likelihood": -9.09031047006817,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.2353558604010466,
      "beta": -4.4168382677193145,
      "success": true,
      "log_likelihood": -8.81615696452492,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.195826589875688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.195826589875688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.195826589875688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.19582658987568796,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": -2.31510650363324,
      "success": true,
      "log_likelihood": -3.20519348821486,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.5717659527836343,
      "beta": -1.5225802968298086,
      "success": true,
      "log_likelihood": -6.650035494647619,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.195826589875688,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3997608472552212,
      "beta": -1.930056483145573,
      "success": true,
      "log_likelihood": -15.240283485592922,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.146505235697465,
      "beta": -1.50807109929539,
      "success": true,
      "log_likelihood": -7.939517179680118,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.680344472554193,
      "beta": -1.4189994700841018,
      "success": true,
      "log_likelihood": -18.32423207486282,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1922188224646073,
      "beta": -1.5114455869206418,
      "success": true,
      "log_likelihood": -16.991218465700292,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3696450827361115,
      "beta": -2.3827831460142077,
      "success": true,
      "log_likelihood": -13.939297529893352,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5707559436259895,
      "beta": -1.7910083774664998,
      "success": true,
      "log_likelihood": -14.894718696212966,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.1869968401763016,
      "beta": -2.73762047157312,
      "success": true,
      "log_likelihood": -11.622843834687156,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4567327094776241,
      "beta": -5.0,
      "success": true,
      "log_likelihood": -4.943540866913561,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.382063452922204,
      "beta": -0.8624535093669168,
      "success": true,
      "log_likelihood": -9.36186349530019,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3686496590131616,
      "beta": -0.7717189804383786,
      "success": true,
      "log_likelihood": -15.902796867870572,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.19582658987568796,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -3.805999775567921,
      "success": true,
      "log_likelihood": -7.931512218248698,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5302945713188714,
      "beta": -0.9385712611235884,
      "success": true,
      "log_likelihood": -21.058375178396204,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9859977975792263,
      "beta": -2.0198345669993634,
      "success": true,
      "log_likelihood": -15.412194974285821,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -0.19582658987568802,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9073780374745704,
      "beta": -1.586710095092658,
      "success": true,
      "log_likelihood": -13.363774643483255,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 4.158398255885182,
      "beta": -3.688716788749488,
      "success": true,
      "log_likelihood": -7.00932420988806,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.418191135477205,
      "beta": -2.278216745150123,
      "success": true,
      "log_likelihood": -14.225669132830507,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1824986336464707,
      "beta": -1.0867999032952422,
      "success": true,
      "log_likelihood": -17.719069113335134,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9042247318469834,
      "beta": -0.3196662727016288,
      "success": true,
      "log_likelihood": -9.541669265957474,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.127774998544272,
      "beta": -1.3063039886741352,
      "success": true,
      "log_likelihood": -17.74625432361635,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4485550863136761,
      "beta": -2.4821202663912305,
      "success": true,
      "log_likelihood": -12.42617940645236,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.869743364625454,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.745036530259494,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.355891913190429,
      "beta": -1.5925215410471338,
      "success": true,
      "log_likelihood": -16.139319574860778,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8991325080715427,
      "beta": -2.033164468502561,
      "success": true,
      "log_likelihood": -13.257553281202895,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.396197884971533,
      "beta": -0.7958782746925749,
      "success": true,
      "log_likelihood": -15.740340539685487,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3997527311480507,
      "beta": -1.930053111573929,
      "success": true,
      "log_likelihood": -15.240283488079102,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6486287096735255,
      "beta": -1.2985954150436334,
      "success": true,
      "log_likelihood": -18.995917545514143,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4370658116345643,
      "beta": -3.2198475593730214,
      "success": true,
      "log_likelihood": -11.269891205199936,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3872490405578507,
      "beta": -0.3456418909250422,
      "success": true,
      "log_likelihood": -23.58999698435937,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.218359988681188,
      "beta": -0.8529243482308592,
      "success": true,
      "log_likelihood": -17.337692820600754,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -1.7887915325748103,
      "success": true,
      "log_likelihood": -13.305935033356803,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9537737772079585,
      "beta": -1.0885755279393186,
      "success": true,
      "log_likelihood": -19.08444485547621,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6553995054203055,
      "beta": -0.45901392340949265,
      "success": true,
      "log_likelihood": -21.64957631896387,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.44634886055651557,
      "beta": -1.2332589664760023,
      "success": true,
      "log_likelihood": -19.14585130502706,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9978465996720531,
      "beta": -5.0,
      "success": true,
      "log_likelihood": -5.173014313354185,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.003318481319102,
      "beta": -3.4213849022691063,
      "success": true,
      "log_likelihood": -10.38653988050772,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1376595997017909,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -13.680488206352592,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.07468549661563968,
      "beta": -2.9357259175460637,
      "success": true,
      "log_likelihood": -9.82573662982226,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4313192567331852,
      "beta": -0.4645923297914396,
      "success": true,
      "log_likelihood": -14.062138467406053,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9002520826885564,
      "beta": -0.8692266707147146,
      "success": true,
      "log_likelihood": -19.766504731230853,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3305945200827094,
      "beta": -2.1438623368132275,
      "success": true,
      "log_likelihood": -14.763159512099715,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -3.805988493532354,
      "success": true,
      "log_likelihood": -7.931512218273312,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6307149521222778,
      "beta": -0.977132900378039,
      "success": true,
      "log_likelihood": -20.702526780253738,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.696588881920166,
      "beta": -3.040135696954845,
      "success": true,
      "log_likelihood": -11.083994119876074,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6313026427733721,
      "beta": -1.9393337847982022,
      "success": true,
      "log_likelihood": -14.437445103922549,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.3095928598899795,
      "beta": -0.5413838333626454,
      "success": true,
      "log_likelihood": -8.407405638168063,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8700576732602523,
      "beta": -0.7069735469174533,
      "success": true,
      "log_likelihood": -11.779558638963245,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4154031436470018,
      "beta": -1.4703950341504466,
      "success": true,
      "log_likelihood": -16.02928879982901,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.356048331415623,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.54012448938081,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.27744009637581146,
      "beta": -5.0,
      "success": true,
      "log_likelihood": -3.9021809630670385,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0372372426157823,
      "beta": -1.9994312141808452,
      "success": true,
      "log_likelihood": -12.598482049169455,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3706249120886222,
      "beta": -1.8171488219234146,
      "success": true,
      "log_likelihood": -15.60614830602808,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.402876051320787,
      "beta": -0.5665092552577825,
      "success": true,
      "log_likelihood": -14.833726682962608,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4606020182058357,
      "beta": -0.6219328706560064,
      "success": true,
      "log_likelihood": -22.66357189401066,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8873650914146023,
      "beta": -1.7135747150276193,
      "success": true,
      "log_likelihood": -13.469173044845283,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0793629033017016,
      "beta": -0.20435623718228557,
      "success": true,
      "log_likelihood": -7.802173685730109,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -3.452203665868535,
      "success": true,
      "log_likelihood": -8.474686531175818,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5316283026678761,
      "beta": -0.2669976097209713,
      "success": true,
      "log_likelihood": -22.370656052675578,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -3.806003176121485,
      "success": true,
      "log_likelihood": -7.931512218250525,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4684677847432848,
      "beta": -0.5662275417993238,
      "success": true,
      "log_likelihood": -22.809114638238167,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3338931296014027,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.741860654220119,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": -2.689264619308009,
      "success": true,
      "log_likelihood": -10.063924084619336,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.505777983893916,
      "beta": -3.2626948902347337,
      "success": true,
      "log_likelihood": -10.339432463306364,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3395906935283058,
      "beta": -0.19844744311666082,
      "success": true,
      "log_likelihood": -23.81915564002951,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1660377790465024,
      "beta": -4.314169645311047,
      "success": true,
      "log_likelihood": -7.785436408188614,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1454626666427878,
      "beta": -0.8855445412944696,
      "success": true,
      "log_likelihood": -17.959641902197113,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.31651415885258644,
      "beta": -2.1578428526460764,
      "success": true,
      "log_likelihood": -13.317159678560134,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.1024974327765755,
      "beta": -0.6755993086741082,
      "success": true,
      "log_likelihood": -5.817840692325917,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5459318494998873,
      "beta": -1.978813035919211,
      "success": true,
      "log_likelihood": -14.679368249288004,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7069640670162792,
      "beta": -0.33271214741516564,
      "success": true,
      "log_likelihood": -20.833123314077618,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.8684419217004457,
        "p": 1.2715488928668786e-31
      },
      "beta": {
        "r": 0.8652177146644024,
        "p": 3.840626486641212e-31
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 30.92992704863568,
  "bic_error": "'ModelOutput' object has no attribute 'metrics'",
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.868\n- beta: r = 0.865",
  "previous_models": [
    "Specification: U_accept = alpha * split_self + beta * max(split_opp - 3 * token_opp, 0)\nSummary: A utility model combining absolute monetary gain with aversion to proposer greed. The utility of accepting depends on the participant's proposed share (split_self) and a penalty when the proposer's share exceeds their equitable contribution (3 * token_opp). Alpha captures sensitivity to personal gain, while beta quantifies aversion to unfair proposer advantages. The model introduces a novel fairness mechanism where responders evaluate both their own benefit and the proposer's adherence to contribution-based norms.\nBIC: 30.92992704863568\n\nParameter Recovery:\n- alpha: r = 0.868\n- beta: r = 0.865"
  ]
}