run_number,average_bic,bic_control,bic_cocaine,model_specification,model_summary,version,alpha_recovery,learning_rate_recovery
3,113.09426901861116,,,"For a responder on trial t:
 U_accept(t) = split_self(t) – alpha * max( expected_share(t) * combined_earning(t) – split_self(t), 0 )
 expected_share(t+1) = expected_share(t) + learning_rate * ( (split_self(t)/combined_earning(t)) – expected_share(t) )","This model captures the respondent’s utility for accepting an offer in the ultimatum game by integrating the monetary payoff with a fairness penalty based on a dynamically learned expectation of a fair share. Utility is reduced whenever the actual offer falls below the expected fair share, with the impact controlled by the envy parameter (alpha). The expected fair share is updated trial‐by‐trial using a simple learning rule with a designated learning rate.",v5,0.8972225275009486,0.698589909560674
1,,,,"U_response = split_self - alpha * max((f_est * combined_earning) - split_self, 0)

f_est(new) = f_est + learning_rate * ((splitperc_self/100) - f_est)","This model is a variant of the Fehr-Schmidt model for the Ultimatum Game applied only to responder behavior. The responder’s utility for accepting an offer is given by their monetary payoff reduced by a penalty proportional to alpha when the payoff is below the monetary equivalent of a learned fairness estimate (f_est multiplied by combined earnings). The fairness estimate is updated on each trial based on the difference between the observed offer proportion and the current estimate, with a learning_rate governing the update. Two learnable parameters, alpha and learning_rate, are calibrated within finite bounds.",v5,,
2,,,,"For each trial t (t = 1,...,T), let F_prev denote the fairness estimate carried from the previous trial (with F₀ = 0.5). Then, the utility of accepting the offer is given by:
  U_t = split_self_t − alpha × max((F_prev × combined_earning_t) − split_self_t, 0)

Following the evaluation of the offer, the fairness estimate is updated as:
  F_new = F_prev + learning_rate × ((split_self_t / combined_earning_t) − F_prev)","A variant of the Fehr–Schmidt model for the Ultimatum Game in which the responder’s utility for accepting an offer is defined as the monetary amount received minus a penalty for offers falling below a dynamically learned fairness expectation. This fairness expectation, initially set at 0.5 of the combined earnings, is updated on every trial using a simple learning rule with a learnable learning rate. The model includes two learnable parameters: alpha, which scales the disutility from falling short of the expectation, and learning_rate, which determines the update speed of the fairness norm.",v5,,
