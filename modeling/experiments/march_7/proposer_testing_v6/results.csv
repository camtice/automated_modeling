run_number,average_bic,bic_control,bic_cocaine,model_specification,model_summary,version,alpha_recovery,learning_rate_recovery
3,113.13466926709077,,,"U_accept = split_self - alpha * max(n * combined_earning - split_self, 0)

n_next = n + learning_rate * ((split_self / combined_earning) - n)","The model computes the utility of accepting an ultimatum offer by combining the monetary gain with a penalty applied if the offer is below a dynamically learned fairness norm. The fairness norm (n) starts at 0.5 and is updated each trial based on the discrepancy between the observed offer share and the current norm, governed by the learning_rate. The only two learnable parameters are alpha (the sensitivity to fairness violations) and learning_rate.",v6,0.9212989145778633,-0.03351154953050779
1,,,,"For trial t, the utility of accepting the offer is given by:
  U_accept(t) = split_self(t) − alpha * max( combined_earning(t) * fairness(t) − split_self(t) , 0 )
The fairness estimate is updated across trials as:
  fairness(t+1) = fairness(t) + learning_rate * [ (split_self(t) / combined_earning(t)) − fairness(t) ]
with the initial fairness estimate set at:
  fairness(1) = 0.5","A modified Fehr–Schmidt model for the ultimatum game where responders compute the utility of accepting an offer as the monetary share received minus a penalty for falling below a learned fairness norm. The fairness norm is dynamically updated via a simple delta learning rule with an initial value of 0.5. The model uses two learnable parameters: alpha, which scales the disutility of receiving less than the anticipated fair share, and the learning_rate, which determines the speed of updating the fairness expectation.",v6,,
2,,,,"U_accept(t) = split_self(t) - alpha * max((f(t) * combined_earning(t) - split_self(t)), 0)

f(t+1) = f(t) + learning_rate * ((splitperc_self(t)/100) - f(t))","This model is an amended Fehr–Schmidt utility formulation for the Ultimatum Game in which responders compute the utility of accepting an offer as their monetary gain minus a fairness penalty when the offer falls short of a dynamically learned fairness norm. The fairness norm, initially set at 0.5 (50%), is updated across trials using a prediction error driven by the actual offer (splitperc_self). Two learnable parameters, alpha (inequality aversion) and learning_rate, calibrate the sensitivity to fairness deviations and the pace of norm updating, respectively.",v6,,
