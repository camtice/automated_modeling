run_number,average_bic,bic_control,bic_cocaine,model_specification,model_summary,version,alpha_recovery,learning_rate_recovery
2,113.49109511232878,,,"U_accept_t = split_self_t - alpha * max(V_t * combined_earning_t - split_self_t, 0)
V_t+1 = V_t + learning_rate * ((split_self_t / combined_earning_t) - V_t)","A two-parameter model of responder behavior in the ultimatum game where the utility for accepting an offer is computed as the monetary payoff diminished by an inequality aversion penalty. This penalty is proportional to the shortfall between the offer and a dynamically updated, learned expectation of fairness. The envy sensitivity (alpha) and learning rate are the only learnable parameters.",v7,0.9505157146741368,0.054615890779070676
1,,,,"U_accept = split_self - alpha * max(m - (split_self/combined_earning), 0);
m_next = m + learning_rate * ((split_self/combined_earning) - m)","A model for responder behavior where the utility of accepting an offer is given by the monetary payoff reduced by a penalty for receiving less than a learned fairness norm. The fairness norm is continuously updated using a simple delta-rule, with two learnable parameters: alpha determining sensitivity to unfairness and learning_rate governing the update speed.",v7,,
3,,,,"U_t = split_self_t - alpha * max(f_t * combined_earning_t - split_self_t, 0)
f_t+1 = f_t + learning_rate * ((split_self_t / combined_earning_t) - f_t)","A dynamic utility model for responder behavior in the Ultimatum Game where the utility of accepting an offer is the received monetary amount reduced by a penalty proportional to the shortfall from a learned fairness expectation. The fairness expectation is updated trial-by-trial using a learning rule, with two learnable parameters governing fairness sensitivity (alpha) and adaptation (learning_rate).",v7,,
