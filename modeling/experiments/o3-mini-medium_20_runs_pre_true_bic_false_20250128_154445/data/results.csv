run_number,average_bic,model_specification,model_summary,timestamp,mu_recovery,lambda_param_recovery,gamma_recovery,μ_recovery,λ_recovery
9,30.4236540492718,"Let r = combined_earning × (token_self/(token_self + token_opp))
U = μ + ln(split_self) – λ × max{ ln(r) – ln(split_self), 0 }","A logarithmic utility model for responders that compares the logarithm of the offered monetary share to the logarithm of a fairness benchmark computed from token contributions. The utility is given by an additive baseline bias plus the log offer, with a penalty (scaled by a fairness sensitivity parameter) applied when the offer is below the fair share.",20250128_154445,,,,0.8882642398680476,0.7452554583268612
3,30.720451816094016,U = mu + split_self - lambda_param * |(combined_earning * (token_self / (token_self + token_opp))) - split_self|,"A fairness-based utility model for responders that calculates the utility of accepting an offer as the sum of the offered monetary share and a baseline bias, minus a linear penalty for any deviation (in absolute terms) from a fairness benchmark determined by the participant’s relative token contribution. Two learnable parameters (mu and lambda_param) calibrate the overall bias and sensitivity to inequity.",20250128_154445,0.7244368665729877,0.6698131724245773,,,
13,32.24973063757358,"Let r = combined_earning × (token_self / (token_self + token_opp))
U = mu + split_self − lambda_param × ((max(r − split_self, 0))² / r)","A fairness-normalized quadratic utility model for responders. The utility of accepting an offer is given by a baseline bias (mu) plus the offered share, from which a penalty is subtracted if the offer falls short of the fairness benchmark (defined by token proportions). The penalty scales with the square of the shortfall normalized by the fair share, with lambda_param controlling sensitivity.",20250128_154445,0.7960619078525117,0.6827106215329408,,,
6,39.504899105260115,"Let r = combined_earning * (token_self / (token_self + token_opp))
Let error = |split_self - r| / r
U = mu + split_self - lambda_param * ln(1 + error)","A reference-dependent, logarithmic fairness model for responder evaluations. The model computes a baseline-adjusted offered share and subtracts a penalty that increases with the relative deviation from a fairness benchmark derived from token contributions. Two learnable parameters, mu and lambda_param, determine baseline utility and sensitivity to fairness deviations.",20250128_154445,0.7615574060614343,0.23430419311807588,,,
5,42.35811169260152,"Let r = combined_earning * (token_self / (token_self + token_opp))
Let gap = max(r - split_self, 0)
Then, the utility is given by:
  U = mu + split_self - lambda_param * ln(1 + gap)",A utility model for responders where the offered share is augmented by a baseline bias and diminished by a logarithmically scaled penalty whenever the offer falls short of a fairness benchmark computed from token contributions. The model includes two learnable parameters: a baseline bias (mu) and a fairness sensitivity (lambda_param). This structure uses available monetary and token data to infer the fairness gap and its impact on responder utility.,20250128_154445,0.9153393690053836,0.4687225683553184,,,
2,45.038627759918896,U = mu + split_self - lambda_param * (combined_earning * (token_self / (token_self + token_opp)) - split_self)^2,"A fairness-sensitive utility model for responders that calculates the utility of accepting an offer as the sum of the offered monetary share and an intercept, reduced by a quadratic penalty for deviations from a fair share based on token contributions. Two learnable parameters (mu and lambda_param) are used to adjust baseline acceptance bias and sensitivity to fairness deviations.",20250128_154445,0.8805992160072723,0.39378170476858776,,,
20,45.36140274285818,"Let 
  r = combined_earning × (token_self / (token_self + token_opp))
Then, the utility of accepting an offer is given by:
  U = mu + split_self − lambda_param × (token_opp / (token_self + token_opp)) × |split_self − r|","The model computes a fairness benchmark (r) as the participant’s proportional share of the combined earning based on token contributions. The utility of accepting an offer is the sum of a baseline preference (mu) and the offered monetary share (split_self), minus a penalty that is proportional to the absolute deviation of the offer from the fairness benchmark. This penalty is weighted by the relative contribution of the opponent (token_opp/(token_self+token_opp)), reflecting increased sensitivity to unfairness when the opponent’s contribution is high. Only two parameters (mu and lambda_param) are learned.",20250128_154445,0.8840098392610869,-0.049397801733135245,,,
4,48.131679163543986,"Let 
  r = combined_earning * (token_self / (token_self + token_opp))
  diff = split_self - r

Then the utility of accepting an offer is given by:

  U = { (diff)^gamma    if diff ≥ 0 
      - lambda_param * ((-diff)^gamma)  if diff < 0 }","A prospect theory–inspired utility model for responders that measures deviations from a fairness benchmark based on token contributions. The model transforms the difference between the offered split and the fair share with a non‐linear power function. Gains (offers above fairness) are valued with diminishing sensitivity (exponent parameter gamma), whereas losses (offers below fairness) are further penalized by a loss‐aversion factor (lambda_param). This yields a utility, which when negative indicates an unlikely action.",20250128_154445,,0.7066631891408373,0.43297711038454584,,
11,67.63512228227661,"Let r = combined_earning × (token_self / (token_self + token_opp))
U = mu + split_self × (split_self / r)^(lambda_param)","A fairness-driven responder utility model in which the fairness benchmark is computed from token contributions. The offered monetary share is multiplicatively adjusted by a power transformation of the ratio between the offer and the fairness benchmark, with an additive baseline parameter. Two learnable parameters (mu and lambda_param) determine baseline acceptance bias and sensitivity to fairness deviations.",20250128_154445,0.38700135571994304,0.07368940897478032,,,
1,70.3672841130665,"U = mu + split_self - lambda_param * max((combined_earning * (token_self / (token_self + token_opp))) - split_self, 0)","A model of responder behavior based on fairness considerations. The fair share is determined by the participant’s relative token contribution, and the utility of accepting an offer is computed as the offered monetary amount combined with an overall bias, reduced by a penalty proportional to any shortfall relative to the fair share. Two learnable parameters (mu and lambda_param) calibrate baseline utility and sensitivity to inequity.",20250128_154445,0.8849528928390058,0.5824548712444066,,,
12,79.64019223974574,"Let p = token_self / (token_self + token_opp)
Let q = split_self / combined_earning
U = mu + split_self - lambda_param * [max(p - q, 0)]^2 * combined_earning","A utility model for responders that rewards the monetary offer and penalizes deviations below a fairness benchmark. The fair share is defined as the participant’s token contribution proportion relative to total tokens, while the actual offer is considered as the proportion of the combined earnings. The penalty is applied quadratically to any shortfall (only when the actual offered proportion is below the fair share) and is scaled by the combined earning. Two learnable parameters—the baseline bias (mu) and fairness sensitivity (lambda_param)—govern the overall utility.",20250128_154445,0.794311368330593,0.32352124032773744,,,
10,82.35404708168619,"U = mu + split_self - lambda_param * (1 - exp(- max( (combined_earning * (token_self/(token_self + token_opp))) - split_self, 0 ) ))","A utility model for responders that computes the attractiveness of an offer as the sum of a baseline bias and the offered monetary share, reduced by an exponential penalty if the offer falls short of a fairness benchmark derived from the relative token contributions. The penalty saturates for larger deviations, capturing diminishing sensitivity to extreme unfairness. Two learnable parameters—mu (baseline bias) and lambda_param (fairness sensitivity)—govern the model.",20250128_154445,0.8171544742683307,-0.0162189431028463,,,
18,90.9724229436148,"Let 
  r = combined_earning × (token_self/(token_self + token_opp))
  ratio = split_self / r
Then the utility of accepting an offer is given by: 
  U = mu + r · [ (ratio)^(lambda_param) × (ratio − 1) ]","A fairness‐based responder utility model that computes the fairness benchmark from token contributions and scales the monetary offer nonlinearly based on its ratio to the benchmark. Deviations from fairness are transformed via a power function modulated by a sensitivity parameter, and added to a baseline utility.",20250128_154445,0.8160615367807376,0.13509362370444172,,,
7,,"Let r = combined_earning × (token_self / (token_self + token_opp))  
U = mu + split_self + lambda × r × ln(split_self / r)","This model computes the utility of accepting an offer based on an additive combination of the offered monetary share and a fairness adjustment. The fairness benchmark is calculated from the participant’s relative token contribution, and deviations from this benchmark are transformed via a logarithmic function scaled by a sensitivity parameter. Two learnable parameters – a baseline utility parameter (mu) and a fairness sensitivity parameter (lambda) – capture intrinsic acceptance bias and responsiveness to fairness deviations.",20250128_154445,,,,,
8,,"Let r = combined_earning × (token_self / (token_self + token_opp))
Let gap = max(r − split_self, 0)
Then, the utility is:
  U_i = (split_self)^(alpha) − lambda × (gap)^(alpha)","The model calculates the responder’s utility based on a nonlinear transformation of the offered share, while penalizing offers that fall short of a fairness reference computed from token contributions. Two learnable parameters govern the curvature of monetary rewards (alpha) and the sensitivity to fairness deviations (lambda).",20250128_154445,,,,,
14,,"Let r = combined_earning × (token_self/(token_self + token_opp))
Then, define:
  Utility U = mu + r × ((split_self/r)^(lambda_param) − 1)","This model computes the utility of a responder’s offer by first determining a fairness benchmark defined as the expected monetary share based on token contributions. The offered share is compared to this benchmark via a ratio that is exponentiated with a fairness sensitivity parameter. The resulting scaled difference is added to a baseline bias, such that offers exceeding the benchmark yield positive utility and offers falling short incur a penalty.",20250128_154445,,,,,
15,,"Let r = combined_earning × ( token_self / (token_self + token_opp) )
Then, define the normalized shortfall as: S = max((r − split_self) / r, 0)
The utility for an offer is:
 U = mu + split_self − lambda × S","The model computes responder utility as a combination of a baseline bias (mu) and the offered monetary share (split_self), diminished by a penalty that is proportional to the relative shortfall from a fairness benchmark. The fairness benchmark is defined as the proportionate share of the combined earning based on the participant’s token contribution. The penalty, scaled by the fairness sensitivity parameter (lambda), applies only when the offer falls below the fair share.",20250128_154445,,,,,
16,,"Let r = combined_earning × (token_self / (token_self + token_opp))
Let D = (split_self - r) / r
U = mu + split_self + lambda × tanh(D)","A responder utility model that adds the offered monetary split to a baseline bias and adjusts this sum by a bounded fairness factor. The fairness benchmark is computed from the participant's token share of combined tokens, and the normalized deviation (D) is transformed via a hyperbolic tangent function. The single fairness sensitivity parameter (lambda) scales this adjustment, while the baseline parameter (mu) shifts overall utility.",20250128_154445,,,,,
17,,"Let 
  r = combined_earning × (token_self / (token_self + token_opp))
  diff = r − split_self
Then the utility is given by:
  U = mu + split_self − lambda × ln(1 + exp(diff))","A smooth fairness-sensitive utility model that computes a fairness benchmark from token contributions and combined earnings. The model adds the offered monetary share and a baseline bias while subtracting a penalty determined by a softplus transformation of the difference between the fair share and the offer. This yields a utility value in which only offers below the fairness benchmark are penalized, with two learnable parameters (mu and lambda) governing baseline preference and inequity sensitivity.",20250128_154445,,,,,
19,,"Let r = combined_earning × (token_self / (token_self + token_opp))
Let S = 1 / (1 + exp(–(r – split_self)))
Then, the utility is defined as:
 U = mu + split_self – lambda × r × (S – 0.5)","A utility model for responders where the fairness benchmark (r) is computed from token contributions to combined earnings. The model adds the offered monetary share to a baseline bias (mu) and adjusts this value by a fairness penalty determined through a sigmoid transformation of the offer's deviation from the benchmark, scaled by r and a sensitivity parameter (lambda). This approach yields a smooth, nonlinear adjustment for fairness deviations.",20250128_154445,,,,,
