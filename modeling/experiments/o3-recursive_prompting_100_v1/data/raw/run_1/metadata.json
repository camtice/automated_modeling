{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: Be VERY considerate of high covariance between learnable parameters as the model. Have an appropriate number or learnable parameters, with a slight bias for less learnable paramters.\n\nFirst, reason step by step about:\n* The key psychological mechanisms involved\n* How these mechanisms interact\n* What mathematical form could capture these interactions\n* What parameters would be needed to calibrate the model\n* How the model variables map to observable behavior\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Let's break down the requirements and think through the possible model structure.\n\nFirst, the target variable is \"accept\", which is binary (1 or 0). So, the model should output a probability of accepting, which can then be converted to a binary decision via a logistic function. The utility function should be designed such that when the utility is less than 0, the probability of acceptance is less than 50%, and vice versa.\n\nKey variables provided in the dataset that are relevant for the responder's decision:\n\n- token_self: Participant's contribution (tokens found by themselves)\n- token_opp: Opponent's contribution (tokens found by the opponent)\n- split_self: Proposed share for the participant in \u00a3\n- split_opp: Proposed share for the opponent in \u00a3\n- combined_earning: Total money earned (each token is \u00a33, so combined_earning = 3*(token_self + token_opp))\n- trial_type: Whether contributions are equal, opponent more, or participant more\n- trial_role: Only consider when trial_role is 1 (responder)\n\nThe model needs to predict the responder's decision based on these variables. Let's think about the psychological factors that might influence their decision. \n\n1. **Fairness Perception**: The responder might compare their proposed share (split_self) to what they consider a fair share. The fair share could depend on their own contribution (token_self) and the opponent's contribution (token_opp). For example, if the participant contributed more tokens, they might expect a larger share.\n\n2. **Inequity Aversion**: The responder might dislike unequal outcomes. This can be modeled by comparing the split_self and split_opp. If the split is perceived as unfair, the responder might be more likely to reject.\n\n3. **Sensitivity to Absolute and Relative Gains**: The responder's utility could depend on both the absolute amount they receive (split_self) and the relative difference between their share and the opponent's. \n\n4. **Reference Points**: The fair division might be based on the proportion of tokens each contributed. For example, the fair split could be (token_self / (token_self + token_opp)) * combined_earning. If the proposed split_self is below this, the responder might feel it's unfair.\n\nLet's formalize this. The fair share based on contributions would be:\n\nfair_self = (token_self / (token_self + token_opp)) * combined_earning\n\nBut combined_earning is 3*(token_self + token_opp), so substituting:\n\nfair_self = (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3*token_self\n\nWait, that's interesting. Because combined_earning is 3*(token_self + token_opp), the fair_self based on contribution would be 3*token_self. Similarly, the opponent's fair share would be 3*token_opp. So, the fair split would be split_self = 3*token_self and split_opp = 3*token_opp. However, the actual proposed split_self and split_opp may differ. \n\nThe responder's utility could then depend on the difference between the proposed split_self and their fair share (3*token_self). Additionally, they might be sensitive to the difference between their split and the opponent's split (split_opp - split_self), which represents inequity.\n\nSo, the utility function could have two components:\n\n1. **Material Gain**: The utility from the proposed split_self. This could be linear, such as \u03b1 * split_self, where \u03b1 is a parameter.\n\n2. **Inequity Aversion**: The disutility from receiving less than the fair share or from the opponent getting more. This could be modeled as \u03b2 * (split_self - 3*token_self) - \u03b3 * (split_opp - split_self). Here, \u03b2 captures sensitivity to deviation from the fair share, and \u03b3 captures aversion to inequity where the opponent gets more.\n\nAlternatively, perhaps the inequity term should be the difference between the proposed split and the fair split. For example, (split_self - fair_self) and (split_opp - fair_opp). But since split_self + split_opp = combined_earning, which is fixed, maybe it's better to model the difference between the proposed split and the fair split for the self.\n\nAnother angle: The responder's utility could be a function of their own monetary gain (split_self) minus a term that penalizes deviations from the fair split. For example:\n\nUtility = split_self - \u03b8 * |split_self - fair_self|\n\nHere, \u03b8 is a learnable parameter representing the sensitivity to unfairness. If the proposed split is exactly fair, the utility is just split_self. If it's unfair, the utility decreases proportionally to the deviation.\n\nBut this might not capture the direction of the unfairness. For example, if the split is more than fair, the responder might not mind, but if it's less, they might be upset. So maybe a one-sided penalty. Alternatively, the model could have separate parameters for advantageous and disadvantageous inequity, similar to Fehr & Schmidt's model.\n\nFehr & Schmidt's model of inequity aversion has:\n\nUtility = own payoff - \u03b1 * max(others' payoff - own payoff, 0) - \u03b2 * max(own payoff - others' payoff, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequity (others get more), and \u03b2 is aversion to advantageous inequity (self gets more). But in this task, the responder is deciding to accept or reject a split where they get split_self and the proposer (opponent) gets split_opp. So, if the responder accepts, their payoff is split_self, and the proposer's is split_opp. If they reject, both get zero (assuming the task is such that rejection leads to no payout).\n\nSo, applying Fehr & Schmidt's model, the utility of accepting would be:\n\nU_accept = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nBut the utility of rejecting is U_reject = 0 - \u03b1 * max(0 - 0, 0) - \u03b2 * max(0 - 0, 0) = 0.\n\nSo, the responder would accept if U_accept > U_reject, i.e., if split_self - \u03b1*(split_opp - split_self) (if split_opp > split_self) is greater than 0.\n\nBut in this case, the model would have parameters \u03b1 and \u03b2. However, in the current dataset, the responder's decision is to accept or reject based on the proposed split. So, integrating this with the contribution-based fairness.\n\nAlternatively, the fair split could be considered as a reference point. The responder's utility could be based on how much they get compared to their fair share (3*token_self) and how much the opponent gets compared to their fair share (3*token_opp). \n\nSo, the utility might be:\n\nU = (split_self - reference_self) + \u03b4 * (split_opp - reference_opp)\n\nWhere reference_self = 3*token_self and reference_opp = 3*token_opp. \u03b4 could be a parameter indicating how much the responder cares about the opponent's deviation from their fair share. If \u03b4 is negative, the responder dislikes the opponent getting more than their fair share.\n\nBut this might not capture the inequity aversion directly. Alternatively, the utility could be:\n\nU = split_self - \u03bb * max((split_opp - 3*token_opp) - (split_self - 3*token_self), 0)\n\nThis would penalize situations where the opponent's gain relative to their fair share exceeds the responder's. But this is getting complicated.\n\nAlternatively, combining both the Fehr-Schmidt inequity aversion and the contribution-based fairness. Let's think of the utility as:\n\nUtility = split_self + \u03b1 * (split_self - 3*token_self) - \u03b2 * max(split_opp - split_self, 0) - \u03b3 * max(split_self - split_opp, 0)\n\nWait, but split_self - 3*token_self is the difference between the proposed share and the fair share. If \u03b1 is positive, the responder values getting more than their fair share, but if negative, they feel guilty for getting more. Alternatively, maybe the responder has a linear utility based on their own gain and the difference from the fair share.\n\nAlternatively, the utility could be:\n\nUtility = \u03b8 * split_self + (1 - \u03b8) * (split_self - 3*token_self) - \u03b3 * (split_opp - split_self)\n\nHere, \u03b8 is a weight between absolute gain and deviation from fair share. But this might not be the best approach.\n\nAnother angle: The responder's decision is influenced by two factors: 1) how much they get compared to what they contributed (fairness based on input), and 2) how the split compares between themselves and the opponent (inequity aversion).\n\nLet's model the utility as:\n\nU = (split_self - reference_self) - \u03b7 * (split_opp - split_self)\n\nWhere reference_self is 3*token_self (their contribution's value). The first term captures the monetary gain relative to their contribution, and the second term captures inequity aversion. \u03b7 is a learnable parameter that determines how much the responder dislikes the opponent getting more than them.\n\nIf split_self is less than reference_self, the first term is negative, reducing utility. The second term is negative if split_opp > split_self, further reducing utility. So, the responder would be less likely to accept if they're getting less than their fair share and/or the opponent is getting more.\n\nAlternatively, the model could use a non-linear function, but given the need for parameter recovery and simplicity, a linear model might be better.\n\nSo, the utility function could be:\n\nU = \u03b1 * (split_self - 3*token_self) - \u03b2 * (split_opp - split_self)\n\nHere, \u03b1 represents the sensitivity to deviation from the fair share (3*token_self), and \u03b2 represents the aversion to the opponent receiving more. Both \u03b1 and \u03b2 are learnable parameters. If U > 0, the responder is more likely to accept.\n\nBut wait, the problem states that the utility variable may be unbounded, but the probability of acceptance is determined by a logistic function with temperature 1. So, the probability of acceptance is 1 / (1 + exp(-U)). Thus, when U > 0, the probability is greater than 0.5, and vice versa.\n\nThis model has two learnable parameters: \u03b1 and \u03b2. However, we need to ensure that these parameters are bounded to aid parameter recovery. Let's set reasonable bounds. For example, \u03b1 could be constrained to be positive, as getting more than the fair share (split_self > 3*token_self) should increase utility. Similarly, \u03b2 should be positive, as a higher opponent's share (split_opp - split_self) would decrease utility. So, \u03b1 and \u03b2 are both positive parameters.\n\nBut perhaps the model should allow for different signs. For example, if \u03b1 is negative, that would imply that getting more than the fair share reduces utility (unlikely, but possible). However, given the task, it's more plausible that participants prefer higher splits for themselves. So, \u03b1 should be positive, and \u03b2 should be positive (since a higher opponent share is disliked).\n\nThus, the model would be:\n\nU = \u03b1*(split_self - 3*token_self) - \u03b2*(split_opp - split_self)\n\nBut we can also consider that the split_opp is determined by the combined_earning minus split_self. Since combined_earning is 3*(token_self + token_opp), split_opp = combined_earning - split_self. So, substituting:\n\nsplit_opp - split_self = (3*(token_self + token_opp) - split_self) - split_self = 3*(token_self + token_opp) - 2*split_self\n\nBut maybe it's better to keep it as split_opp - split_self for clarity.\n\nAlternatively, the model could include a term for the difference between the proposed split and the equal split. For example, if the combined_earning is C, an equal split would be C/2 for each. The responder might compare their split to C/2. But this might not account for their contribution.\n\nAlternatively, combining both contribution-based fairness and equal split fairness. For example, the reference point could be a weighted average between the contribution-based fair share and the equal split. But this would add more parameters, which might complicate the model.\n\nGiven the need to keep the number of learnable parameters low (to aid parameter recovery), let's stick with two parameters: \u03b1 and \u03b2.\n\nAnother consideration: The variable trial_type indicates whether the contribution was equal, opponent more, or participant more. This might influence the responder's perception of fairness. However, since trial_type is derived from token_self and token_opp, which are already in the model, including trial_type might be redundant. The model can capture the effect through token_self and token_opp.\n\nWait, trial_type is 1 (equal), 2 (opponent more), 3 (participant more). This is determined by comparing token_self and token_opp. So, including trial_type might not add new information beyond what's already in token_self and token_opp. Therefore, the model can rely on token_self and token_opp directly.\n\nPutting it all together, the utility function would be:\n\nU = \u03b1*(split_self - 3*token_self) - \u03b2*(split_opp - split_self)\n\nHere, \u03b1 and \u03b2 are learnable parameters with positive bounds. The first term represents the utility from receiving more than one's fair share (based on their own contribution), and the second term represents the disutility from the opponent receiving more than the participant.\n\nThe probability of accepting is then logistic(U) = 1 / (1 + exp(-U)). If U > 0, the probability is >0.5, leading to a higher chance of acceptance.\n\nNow, checking the requirements:\n\n1. Predict responder behavior (accept) using the specified variables. The model uses split_self, split_opp, token_self, which are all in the dataset. The combined_earning is 3*(token_self + token_opp), but split_opp can be derived as combined_earning - split_self. However, split_opp is already a variable in the dataset, so we can use it directly.\n\n2. The model uses only variables present in the dataset. Yes: split_self, split_opp, token_self.\n\n3. The utility is unbounded, and the probability is determined via logistic function. Correct.\n\n4. Learnable parameters \u03b1 and \u03b2 need to have finite bounds. Let's set \u03b1 and \u03b2 to be positive, with a reasonable upper bound. For example, \u03b1 could be between 0 and 5, and \u03b2 between 0 and 5. These bounds are generous but finite.\n\nWait, but the problem states that the bounds should be generous but finite. So, perhaps set \u03b1 and \u03b2 to have a min of 0 and max of, say, 10. This allows for a wide range of sensitivities.\n\nAnother consideration: Should the model include an intercept term? For example, U = \u03b1*(split_self - fair_self) - \u03b2*(split_opp - split_self) + \u03b3. But the problem states that the utility should predict less than 50% probability when U < 0. Adding an intercept could shift the baseline. However, the current model without an intercept assumes that when split_self equals fair_self and split_opp equals split_self (i.e., equal split), U is zero. In that case, the probability is 0.5. But maybe participants have a general tendency to accept or reject, which could be captured by an intercept. However, adding another parameter (\u03b3) would increase model complexity. Since the priority is parameter recovery, perhaps it's better to keep the model simple with two parameters. Alternatively, include the intercept as a third parameter.\n\nBut the problem says to have a slight bias towards fewer parameters. So, maybe start with two parameters. However, without an intercept, the model assumes that when U=0, the probability is 0.5. If in reality, participants have a baseline tendency to accept or reject even when U=0, the model would miss that. For example, if participants are generally more accepting, an intercept could capture that. But given the task's structure, perhaps the intercept is not necessary. Let's proceed without it for now, but this is a point to consider.\n\nAnother possible variable: The ratio of split_self to the fair_self. For example, (split_self / fair_self) - 1. But this would complicate the model and introduce non-linearity. However, the current linear model is simpler and easier for parameter recovery.\n\nTesting the model with example values:\n\nSuppose token_self = 5, token_opp =5 (trial_type=1). Combined_earning = 3*(5+5)=30. Suppose the split is split_self=15, split_opp=15. Then:\n\nU = \u03b1*(15 - 15) - \u03b2*(15 -15) = 0. Probability of acceptance is 0.5.\n\nIf the split is split_self=10, split_opp=20:\n\nU = \u03b1*(10-15) - \u03b2*(20-10) = -5\u03b1 -10\u03b2. If \u03b1 and \u03b2 are positive, U is negative, leading to rejection.\n\nIf split_self=20, split_opp=10:\n\nU = \u03b1*(20-15) - \u03b2*(10-20) =5\u03b1 +10\u03b2. Positive, leading to acceptance.\n\nThis makes sense: getting more than fair share and opponent getting less increases utility.\n\nAnother example: token_self=8, token_opp=2 (trial_type=3). Fair_self=24. Combined_earning=30. If split_self=20, split_opp=10:\n\nU = \u03b1*(20-24) - \u03b2*(10-20) = -4\u03b1 +10\u03b2. Depending on \u03b1 and \u03b2, this could be positive or negative. If \u03b2 is large enough, the positive term from the opponent's lower share could offset the negative from own share being below fair.\n\nBut in reality, if the participant contributed more, they might expect a higher split. If the proposed split is 20 (which is less than their fair share of 24), but the opponent is getting 10 (which is more than their fair share of 6), the responder might still reject because they're getting less than their fair share, even though the opponent's split is higher than their fair share. The model captures this through the -4\u03b1 (negative) and +10\u03b2 (positive). If \u03b2 is large, the responder cares more about the opponent's share relative to their own, leading to acceptance despite being below fair share. But if \u03b1 is larger, the responder cares more about their own fair share and rejects.\n\nThis seems plausible. The parameters \u03b1 and \u03b2 would capture individual differences in sensitivity to these factors.\n\nNow, considering parameter bounds. Let's set \u03b1 and \u03b2 to be non-negative, with a reasonable upper limit. For example:\n\n\u03b1: min=0, max=10 (inclusive)\n\u03b2: min=0, max=10 (inclusive)\n\nThese bounds are generous enough to cover plausible sensitivities. For instance, if \u03b1 is 0, the responder doesn't care about their own fair share, only the difference between split_opp and split_self. If \u03b2 is 0, they don't care about the opponent's share relative to their own.\n\nNow, the variables in the model:\n\n- split_self: from data\n- split_opp: from data\n- token_self: from data\n- \u03b1: learnable parameter, [0,10]\n- \u03b2: learnable parameter, [0,10]\n\nThe utility U is calculated as \u03b1*(split_self - 3*token_self) - \u03b2*(split_opp - split_self)\n\nThe target variable is 'accept', which is binary. The model's utility U is passed through a logistic function to get the probability of acceptance.\n\nNow, checking if all variables in the model are from the dataset or are learnable parameters. Yes: split_self, split_opp, token_self are data variables. \u03b1 and \u03b2 are learnable.\n\nThis model meets the requirements: predicts responder behavior, uses the given variables, has learnable parameters with finite bounds, and the utility is unbounded. The number of parameters is two, which is manageable for parameter recovery.\n\nPotential issues: \n\n- The model assumes linear effects of the terms. Maybe non-linearities exist, but with limited data (36 trials per participant), a linear model is more practical.\n\n- The fair share is calculated as 3*token_self, which is fixed based on contribution. But perhaps participants have different notions of fairness, such as equal split regardless of contribution. However, the problem statement mentions that two equal split_self values don't represent the same fairness because combined_earning varies. So, the model should account for the contribution-based fairness, which this model does.\n\nAnother possible consideration: The participant's own contribution (token_self) might influence their fairness perception. For example, if they contributed more, they might expect a higher split. The model captures this through the (split_self - 3*token_self) term.\n\nIn summary, the proposed model uses two learnable parameters to capture sensitivity to deviation from contribution-based fairness and inequity aversion. The utility function is linear in these parameters, and the probability of acceptance is determined via logistic function.\n</think>\n\n<MODEL>\nU = alpha * (split_self - 3 * token_self) - beta * (split_opp - split_self)\n</MODEL>\n\n<target_variable>accept</target_variable>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity aversion parameter (sensitivity to opponent receiving more)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for the opponent in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model capturing responder decisions through two key factors: (1) deviation from contribution-based fairness (split_self vs. 3*token_self) and (2) inequity aversion (split_opp - split_self). The model uses two learnable parameters (alpha, beta) with bounded ranges [0,10] to balance parsimony and identifiability. Utility is converted to acceptance probability via logistic function, where U < 0 corresponds to <50% acceptance likelihood.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = alpha * (split_self - 3 * token_self) - beta * (split_opp - split_self)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity aversion parameter (sensitivity to opponent receiving more)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for the opponent in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        U = alpha * (split_self - 3 * token_self) - beta * (split_opp - split_self)\n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 1, \"group\": \"group1\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 5, \"token_self\": 5, \"combined_earning\": 30, \"split_opp\": 15.0, \"split_self\": 15.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 15.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 15.0}, {\"ID\": 2, \"group\": \"group1\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 5, \"token_self\": 5, \"combined_earning\": 30, \"split_opp\": 20.0, \"split_self\": 10.0, \"splitperc_opp\": 67, \"splitperc_self\": 33, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 33.0, \"proposed_amount\": 10.0}, {\"ID\": 3, \"group\": \"group1\", \"trial_number\": 3, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 5, \"token_self\": 5, \"combined_earning\": 30, \"split_opp\": 10.0, \"split_self\": 20.0, \"splitperc_opp\": 33, \"splitperc_self\": 67, \"accept\": 1.0, \"accepted_amount\": 20.0, \"accepted_perc\": 67.0, \"proposed_perc\": 67.0, \"proposed_amount\": 20.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1744015330.1770701
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair share (3 * token_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Inequity aversion parameter (sensitivity to opponent receiving more)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = alpha * (split_self - 3 * token_self) - beta * (split_opp - split_self)",
  "variable_descriptions": {
    "alpha": {
      "description": "Sensitivity to deviation from contribution-based fair share (3 * token_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Inequity aversion parameter (sensitivity to opponent receiving more)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for the participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_opp": {
      "description": "Proposed share for the opponent in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by the participant",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A utility model capturing responder decisions through two key factors: (1) deviation from contribution-based fairness (split_self vs. 3*token_self) and (2) inequity aversion (split_opp - split_self). The model uses two learnable parameters (alpha, beta) with bounded ranges [0,10] to balance parsimony and identifiability. Utility is converted to acceptance probability via logistic function, where U < 0 corresponds to <50% acceptance likelihood.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this problem step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal in the given experiment. The model needs to focus on responder behavior, use the provided variables appropriately, and ensure parameter recovery is feasible. Let's break down the requirements and think through the possible model structure.\n\nFirst, the target variable is \"accept\", which is binary (1 or 0). So, the model should output a probability of accepting, which can then be converted to a binary decision via a logistic function. The utility function should be designed such that when the utility is less than 0, the probability of acceptance is less than 50%, and vice versa.\n\nKey variables provided in the dataset that are relevant for the responder's decision:\n\n- token_self: Participant's contribution (tokens found by themselves)\n- token_opp: Opponent's contribution (tokens found by the opponent)\n- split_self: Proposed share for the participant in \u00a3\n- split_opp: Proposed share for the opponent in \u00a3\n- combined_earning: Total money earned (each token is \u00a33, so combined_earning = 3*(token_self + token_opp))\n- trial_type: Whether contributions are equal, opponent more, or participant more\n- trial_role: Only consider when trial_role is 1 (responder)\n\nThe model needs to predict the responder's decision based on these variables. Let's think about the psychological factors that might influence their decision. \n\n1. **Fairness Perception**: The responder might compare their proposed share (split_self) to what they consider a fair share. The fair share could depend on their own contribution (token_self) and the opponent's contribution (token_opp). For example, if the participant contributed more tokens, they might expect a larger share.\n\n2. **Inequity Aversion**: The responder might dislike unequal outcomes. This can be modeled by comparing the split_self and split_opp. If the split is perceived as unfair, the responder might be more likely to reject.\n\n3. **Sensitivity to Absolute and Relative Gains**: The responder's utility could depend on both the absolute amount they receive (split_self) and the relative difference between their share and the opponent's. \n\n4. **Reference Points**: The fair division might be based on the proportion of tokens each contributed. For example, the fair split could be (token_self / (token_self + token_opp)) * combined_earning. If the proposed split_self is below this, the responder might feel it's unfair.\n\nLet's formalize this. The fair share based on contributions would be:\n\nfair_self = (token_self / (token_self + token_opp)) * combined_earning\n\nBut combined_earning is 3*(token_self + token_opp), so substituting:\n\nfair_self = (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) ) = 3*token_self\n\nWait, that's interesting. Because combined_earning is 3*(token_self + token_opp), the fair_self based on contribution would be 3*token_self. Similarly, the opponent's fair share would be 3*token_opp. So, the fair split would be split_self = 3*token_self and split_opp = 3*token_opp. However, the actual proposed split_self and split_opp may differ. \n\nThe responder's utility could then depend on the difference between the proposed split_self and their fair share (3*token_self). Additionally, they might be sensitive to the difference between their split and the opponent's split (split_opp - split_self), which represents inequity.\n\nSo, the utility function could have two components:\n\n1. **Material Gain**: The utility from the proposed split_self. This could be linear, such as \u03b1 * split_self, where \u03b1 is a parameter.\n\n2. **Inequity Aversion**: The disutility from receiving less than the fair share or from the opponent getting more. This could be modeled as \u03b2 * (split_self - 3*token_self) - \u03b3 * (split_opp - split_self). Here, \u03b2 captures sensitivity to deviation from the fair share, and \u03b3 captures aversion to inequity where the opponent gets more.\n\nAlternatively, perhaps the inequity term should be the difference between the proposed split and the fair split. For example, (split_self - fair_self) and (split_opp - fair_opp). But since split_self + split_opp = combined_earning, which is fixed, maybe it's better to model the difference between the proposed split and the fair split for the self.\n\nAnother angle: The responder's utility could be a function of their own monetary gain (split_self) minus a term that penalizes deviations from the fair split. For example:\n\nUtility = split_self - \u03b8 * |split_self - fair_self|\n\nHere, \u03b8 is a learnable parameter representing the sensitivity to unfairness. If the proposed split is exactly fair, the utility is just split_self. If it's unfair, the utility decreases proportionally to the deviation.\n\nBut this might not capture the direction of the unfairness. For example, if the split is more than fair, the responder might not mind, but if it's less, they might be upset. So maybe a one-sided penalty. Alternatively, the model could have separate parameters for advantageous and disadvantageous inequity, similar to Fehr & Schmidt's model.\n\nFehr & Schmidt's model of inequity aversion has:\n\nUtility = own payoff - \u03b1 * max(others' payoff - own payoff, 0) - \u03b2 * max(own payoff - others' payoff, 0)\n\nHere, \u03b1 is the aversion to disadvantageous inequity (others get more), and \u03b2 is aversion to advantageous inequity (self gets more). But in this task, the responder is deciding to accept or reject a split where they get split_self and the proposer (opponent) gets split_opp. So, if the responder accepts, their payoff is split_self, and the proposer's is split_opp. If they reject, both get zero (assuming the task is such that rejection leads to no payout).\n\nSo, applying Fehr & Schmidt's model, the utility of accepting would be:\n\nU_accept = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0)\n\nBut the utility of rejecting is U_reject = 0 - \u03b1 * max(0 - 0, 0) - \u03b2 * max(0 - 0, 0) = 0.\n\nSo, the responder would accept if U_accept > U_reject, i.e., if split_self - \u03b1*(split_opp - split_self) (if split_opp > split_self) is greater than 0.\n\nBut in this case, the model would have parameters \u03b1 and \u03b2. However, in the current dataset, the responder's decision is to accept or reject based on the proposed split. So, integrating this with the contribution-based fairness.\n\nAlternatively, the fair split could be considered as a reference point. The responder's utility could be based on how much they get compared to their fair share (3*token_self) and how much the opponent gets compared to their fair share (3*token_opp). \n\nSo, the utility might be:\n\nU = (split_self - reference_self) + \u03b4 * (split_opp - reference_opp)\n\nWhere reference_self = 3*token_self and reference_opp = 3*token_opp. \u03b4 could be a parameter indicating how much the responder cares about the opponent's deviation from their fair share. If \u03b4 is negative, the responder dislikes the opponent getting more than their fair share.\n\nBut this might not capture the inequity aversion directly. Alternatively, the utility could be:\n\nU = split_self - \u03bb * max((split_opp - 3*token_opp) - (split_self - 3*token_self), 0)\n\nThis would penalize situations where the opponent's gain relative to their fair share exceeds the responder's. But this is getting complicated.\n\nAlternatively, combining both the Fehr-Schmidt inequity aversion and the contribution-based fairness. Let's think of the utility as:\n\nUtility = split_self + \u03b1 * (split_self - 3*token_self) - \u03b2 * max(split_opp - split_self, 0) - \u03b3 * max(split_self - split_opp, 0)\n\nWait, but split_self - 3*token_self is the difference between the proposed share and the fair share. If \u03b1 is positive, the responder values getting more than their fair share, but if negative, they feel guilty for getting more. Alternatively, maybe the responder has a linear utility based on their own gain and the difference from the fair share.\n\nAlternatively, the utility could be:\n\nUtility = \u03b8 * split_self + (1 - \u03b8) * (split_self - 3*token_self) - \u03b3 * (split_opp - split_self)\n\nHere, \u03b8 is a weight between absolute gain and deviation from fair share. But this might not be the best approach.\n\nAnother angle: The responder's decision is influenced by two factors: 1) how much they get compared to what they contributed (fairness based on input), and 2) how the split compares between themselves and the opponent (inequity aversion).\n\nLet's model the utility as:\n\nU = (split_self - reference_self) - \u03b7 * (split_opp - split_self)\n\nWhere reference_self is 3*token_self (their contribution's value). The first term captures the monetary gain relative to their contribution, and the second term captures inequity aversion. \u03b7 is a learnable parameter that determines how much the responder dislikes the opponent getting more than them.\n\nIf split_self is less than reference_self, the first term is negative, reducing utility. The second term is negative if split_opp > split_self, further reducing utility. So, the responder would be less likely to accept if they're getting less than their fair share and/or the opponent is getting more.\n\nAlternatively, the model could use a non-linear function, but given the need for parameter recovery and simplicity, a linear model might be better.\n\nSo, the utility function could be:\n\nU = \u03b1 * (split_self - 3*token_self) - \u03b2 * (split_opp - split_self)\n\nHere, \u03b1 represents the sensitivity to deviation from the fair share (3*token_self), and \u03b2 represents the aversion to the opponent receiving more. Both \u03b1 and \u03b2 are learnable parameters. If U > 0, the responder is more likely to accept.\n\nBut wait, the problem states that the utility variable may be unbounded, but the probability of acceptance is determined by a logistic function with temperature 1. So, the probability of acceptance is 1 / (1 + exp(-U)). Thus, when U > 0, the probability is greater than 0.5, and vice versa.\n\nThis model has two learnable parameters: \u03b1 and \u03b2. However, we need to ensure that these parameters are bounded to aid parameter recovery. Let's set reasonable bounds. For example, \u03b1 could be constrained to be positive, as getting more than the fair share (split_self > 3*token_self) should increase utility. Similarly, \u03b2 should be positive, as a higher opponent's share (split_opp - split_self) would decrease utility. So, \u03b1 and \u03b2 are both positive parameters.\n\nBut perhaps the model should allow for different signs. For example, if \u03b1 is negative, that would imply that getting more than the fair share reduces utility (unlikely, but possible). However, given the task, it's more plausible that participants prefer higher splits for themselves. So, \u03b1 should be positive, and \u03b2 should be positive (since a higher opponent share is disliked).\n\nThus, the model would be:\n\nU = \u03b1*(split_self - 3*token_self) - \u03b2*(split_opp - split_self)\n\nBut we can also consider that the split_opp is determined by the combined_earning minus split_self. Since combined_earning is 3*(token_self + token_opp), split_opp = combined_earning - split_self. So, substituting:\n\nsplit_opp - split_self = (3*(token_self + token_opp) - split_self) - split_self = 3*(token_self + token_opp) - 2*split_self\n\nBut maybe it's better to keep it as split_opp - split_self for clarity.\n\nAlternatively, the model could include a term for the difference between the proposed split and the equal split. For example, if the combined_earning is C, an equal split would be C/2 for each. The responder might compare their split to C/2. But this might not account for their contribution.\n\nAlternatively, combining both contribution-based fairness and equal split fairness. For example, the reference point could be a weighted average between the contribution-based fair share and the equal split. But this would add more parameters, which might complicate the model.\n\nGiven the need to keep the number of learnable parameters low (to aid parameter recovery), let's stick with two parameters: \u03b1 and \u03b2.\n\nAnother consideration: The variable trial_type indicates whether the contribution was equal, opponent more, or participant more. This might influence the responder's perception of fairness. However, since trial_type is derived from token_self and token_opp, which are already in the model, including trial_type might be redundant. The model can capture the effect through token_self and token_opp.\n\nWait, trial_type is 1 (equal), 2 (opponent more), 3 (participant more). This is determined by comparing token_self and token_opp. So, including trial_type might not add new information beyond what's already in token_self and token_opp. Therefore, the model can rely on token_self and token_opp directly.\n\nPutting it all together, the utility function would be:\n\nU = \u03b1*(split_self - 3*token_self) - \u03b2*(split_opp - split_self)\n\nHere, \u03b1 and \u03b2 are learnable parameters with positive bounds. The first term represents the utility from receiving more than one's fair share (based on their own contribution), and the second term represents the disutility from the opponent receiving more than the participant.\n\nThe probability of accepting is then logistic(U) = 1 / (1 + exp(-U)). If U > 0, the probability is >0.5, leading to a higher chance of acceptance.\n\nNow, checking the requirements:\n\n1. Predict responder behavior (accept) using the specified variables. The model uses split_self, split_opp, token_self, which are all in the dataset. The combined_earning is 3*(token_self + token_opp), but split_opp can be derived as combined_earning - split_self. However, split_opp is already a variable in the dataset, so we can use it directly.\n\n2. The model uses only variables present in the dataset. Yes: split_self, split_opp, token_self.\n\n3. The utility is unbounded, and the probability is determined via logistic function. Correct.\n\n4. Learnable parameters \u03b1 and \u03b2 need to have finite bounds. Let's set \u03b1 and \u03b2 to be positive, with a reasonable upper bound. For example, \u03b1 could be between 0 and 5, and \u03b2 between 0 and 5. These bounds are generous but finite.\n\nWait, but the problem states that the bounds should be generous but finite. So, perhaps set \u03b1 and \u03b2 to have a min of 0 and max of, say, 10. This allows for a wide range of sensitivities.\n\nAnother consideration: Should the model include an intercept term? For example, U = \u03b1*(split_self - fair_self) - \u03b2*(split_opp - split_self) + \u03b3. But the problem states that the utility should predict less than 50% probability when U < 0. Adding an intercept could shift the baseline. However, the current model without an intercept assumes that when split_self equals fair_self and split_opp equals split_self (i.e., equal split), U is zero. In that case, the probability is 0.5. But maybe participants have a general tendency to accept or reject, which could be captured by an intercept. However, adding another parameter (\u03b3) would increase model complexity. Since the priority is parameter recovery, perhaps it's better to keep the model simple with two parameters. Alternatively, include the intercept as a third parameter.\n\nBut the problem says to have a slight bias towards fewer parameters. So, maybe start with two parameters. However, without an intercept, the model assumes that when U=0, the probability is 0.5. If in reality, participants have a baseline tendency to accept or reject even when U=0, the model would miss that. For example, if participants are generally more accepting, an intercept could capture that. But given the task's structure, perhaps the intercept is not necessary. Let's proceed without it for now, but this is a point to consider.\n\nAnother possible variable: The ratio of split_self to the fair_self. For example, (split_self / fair_self) - 1. But this would complicate the model and introduce non-linearity. However, the current linear model is simpler and easier for parameter recovery.\n\nTesting the model with example values:\n\nSuppose token_self = 5, token_opp =5 (trial_type=1). Combined_earning = 3*(5+5)=30. Suppose the split is split_self=15, split_opp=15. Then:\n\nU = \u03b1*(15 - 15) - \u03b2*(15 -15) = 0. Probability of acceptance is 0.5.\n\nIf the split is split_self=10, split_opp=20:\n\nU = \u03b1*(10-15) - \u03b2*(20-10) = -5\u03b1 -10\u03b2. If \u03b1 and \u03b2 are positive, U is negative, leading to rejection.\n\nIf split_self=20, split_opp=10:\n\nU = \u03b1*(20-15) - \u03b2*(10-20) =5\u03b1 +10\u03b2. Positive, leading to acceptance.\n\nThis makes sense: getting more than fair share and opponent getting less increases utility.\n\nAnother example: token_self=8, token_opp=2 (trial_type=3). Fair_self=24. Combined_earning=30. If split_self=20, split_opp=10:\n\nU = \u03b1*(20-24) - \u03b2*(10-20) = -4\u03b1 +10\u03b2. Depending on \u03b1 and \u03b2, this could be positive or negative. If \u03b2 is large enough, the positive term from the opponent's lower share could offset the negative from own share being below fair.\n\nBut in reality, if the participant contributed more, they might expect a higher split. If the proposed split is 20 (which is less than their fair share of 24), but the opponent is getting 10 (which is more than their fair share of 6), the responder might still reject because they're getting less than their fair share, even though the opponent's split is higher than their fair share. The model captures this through the -4\u03b1 (negative) and +10\u03b2 (positive). If \u03b2 is large, the responder cares more about the opponent's share relative to their own, leading to acceptance despite being below fair share. But if \u03b1 is larger, the responder cares more about their own fair share and rejects.\n\nThis seems plausible. The parameters \u03b1 and \u03b2 would capture individual differences in sensitivity to these factors.\n\nNow, considering parameter bounds. Let's set \u03b1 and \u03b2 to be non-negative, with a reasonable upper limit. For example:\n\n\u03b1: min=0, max=10 (inclusive)\n\u03b2: min=0, max=10 (inclusive)\n\nThese bounds are generous enough to cover plausible sensitivities. For instance, if \u03b1 is 0, the responder doesn't care about their own fair share, only the difference between split_opp and split_self. If \u03b2 is 0, they don't care about the opponent's share relative to their own.\n\nNow, the variables in the model:\n\n- split_self: from data\n- split_opp: from data\n- token_self: from data\n- \u03b1: learnable parameter, [0,10]\n- \u03b2: learnable parameter, [0,10]\n\nThe utility U is calculated as \u03b1*(split_self - 3*token_self) - \u03b2*(split_opp - split_self)\n\nThe target variable is 'accept', which is binary. The model's utility U is passed through a logistic function to get the probability of acceptance.\n\nNow, checking if all variables in the model are from the dataset or are learnable parameters. Yes: split_self, split_opp, token_self are data variables. \u03b1 and \u03b2 are learnable.\n\nThis model meets the requirements: predicts responder behavior, uses the given variables, has learnable parameters with finite bounds, and the utility is unbounded. The number of parameters is two, which is manageable for parameter recovery.\n\nPotential issues: \n\n- The model assumes linear effects of the terms. Maybe non-linearities exist, but with limited data (36 trials per participant), a linear model is more practical.\n\n- The fair share is calculated as 3*token_self, which is fixed based on contribution. But perhaps participants have different notions of fairness, such as equal split regardless of contribution. However, the problem statement mentions that two equal split_self values don't represent the same fairness because combined_earning varies. So, the model should account for the contribution-based fairness, which this model does.\n\nAnother possible consideration: The participant's own contribution (token_self) might influence their fairness perception. For example, if they contributed more, they might expect a higher split. The model captures this through the (split_self - 3*token_self) term.\n\nIn summary, the proposed model uses two learnable parameters to capture sensitivity to deviation from contribution-based fairness and inequity aversion. The utility function is linear in these parameters, and the probability of acceptance is determined via logistic function.\n</think>\n\n<MODEL>\nU = alpha * (split_self - 3 * token_self) - beta * (split_opp - split_self)\n</MODEL>\n\n<target_variable>accept</target_variable>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"alpha\": {\n      \"description\": \"Sensitivity to deviation from contribution-based fair share (3 * token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity aversion parameter (sensitivity to opponent receiving more)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for the opponent in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nA utility model capturing responder decisions through two key factors: (1) deviation from contribution-based fairness (split_self vs. 3*token_self) and (2) inequity aversion (split_opp - split_self). The model uses two learnable parameters (alpha, beta) with bounded ranges [0,10] to balance parsimony and identifiability. Utility is converted to acceptance probability via logistic function, where U < 0 corresponds to <50% acceptance likelihood.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = alpha * (split_self - 3 * token_self) - beta * (split_opp - split_self)\nSummary: A utility model capturing responder decisions through two key factors: (1) deviation from contribution-based fairness (split_self vs. 3*token_self) and (2) inequity aversion (split_opp - split_self). The model uses two learnable parameters (alpha, beta) with bounded ranges [0,10] to balance parsimony and identifiability. Utility is converted to acceptance probability via logistic function, where U < 0 corresponds to <50% acceptance likelihood.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        token_self = trial.get(\"token_self\", 0)\n\n        U = alpha * (split_self - 3 * token_self) - beta * (split_opp - split_self)\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.594017094017094,
  "fitting_results": [
    {
      "alpha": 0.0,
      "beta": 9.835067947379711,
      "success": true,
      "log_likelihood": -2.0794490678058852,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.26270074480603406,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -22.431437028314487,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.29298201426897086,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -21.931647438504214,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.37714314429359225,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -20.480649975832563,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.49879723618769006,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -18.394471710884847,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11434269827980549,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.402921678448635,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.09604866349621455,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.56066557338446,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5469996321176775,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -17.61093021952341,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4811860603059997,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -18.68842373446745,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5271787986249522,
      "beta": 0.5849637287904198,
      "success": true,
      "log_likelihood": -7.0868382805803,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5418784326757978,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -17.692595156883083,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.2222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.23439214412165071,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -22.878551886188898,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3333333333333333,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.23318550269743318,
      "success": true,
      "log_likelihood": -19.46281133507771,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.1803494698113672,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -23.654360845304385,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3739160735974874,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -20.536979058437524,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.14455855516024851,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.092498574860876,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12207173216537937,
      "beta": 0.048042471857083276,
      "success": true,
      "log_likelihood": -23.28917260512616,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1510647357002561,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -10.884737750887144,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -2.0794477125089634,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.18093929412342344,
      "success": true,
      "log_likelihood": -21.193297584478792,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3045180523416966,
      "beta": 0.07100334895580479,
      "success": true,
      "log_likelihood": -19.55055000698874,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 5.51364761135529e-09,
      "beta": 0.006085968921280281,
      "success": true,
      "log_likelihood": -24.947821744900647,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.039571023693241066,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.8851449542732,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.32565893034258603,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -21.375266484961852,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.048246172654059526,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.852215955119096,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.13688069597489413,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.176925959469553,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9152648741183258,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -12.878204230015172,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.090452892532937,
      "success": true,
      "log_likelihood": -23.827806264171215,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11991256250204418,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.350215760850975,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12567406171547452,
      "beta": 0.06427941826053628,
      "success": true,
      "log_likelihood": -22.79357428588751,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.27042287437485524,
      "beta": 0.02905553681972493,
      "success": true,
      "log_likelihood": -21.45567403313482,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5793608006738586,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -17.104286910570888,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.021356152884035872,
      "success": true,
      "log_likelihood": -24.886121287183997,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.7225064408960383,
      "success": true,
      "log_likelihood": -8.379935298534733,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.08349317550920295,
      "success": true,
      "log_likelihood": -23.984321202722928,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.003042456676090516,
      "success": true,
      "log_likelihood": -24.951929424077832,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.39033479669061955,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -20.25042968954545,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8177290469433801,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -13.915564891139644,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8634560774737048,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -13.411480246838856,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 1.0186731793777692,
      "success": true,
      "log_likelihood": -5.587887856561962,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06046593181752666,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.79515053286436,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.32002493525770853,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -21.472117164157915,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 9.900519031298703,
      "success": true,
      "log_likelihood": -2.079448497551796,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.11988235006738787,
      "success": true,
      "log_likelihood": -23.072377984641275,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5550892871020859,
      "beta": 0.22703787602180173,
      "success": true,
      "log_likelihood": -12.047299707829804,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11836115907186052,
      "beta": 0.028215248301929393,
      "success": true,
      "log_likelihood": -23.81898121044717,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.05903832019723478,
      "success": true,
      "log_likelihood": -24.45385161501469,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -1.490098315440448,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.8223485310305775,
      "success": true,
      "log_likelihood": -7.225706986430399,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.21206735553602696,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -23.213262139588593,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.061203238183384884,
      "success": true,
      "log_likelihood": -24.41777977982313,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.04650793504650443,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.85932249351738,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.874636992593871,
      "success": true,
      "log_likelihood": -6.717079523559374,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0005982514315419429,
      "beta": 9.540425763136275,
      "success": true,
      "log_likelihood": -2.0794524704132007,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.05580923883049667,
      "success": true,
      "log_likelihood": -24.505530457543145,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5555555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.7225078675704333,
      "success": true,
      "log_likelihood": -8.379935298561302,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.22984096224557163,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -22.948185745380904,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5277777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8807602781938765,
      "beta": 0.8361963608985605,
      "success": true,
      "log_likelihood": -4.334243059690534,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.4166666666666667,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7673367311321708,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -14.509544313589851,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.3611111111111111,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.11988427237887894,
      "success": true,
      "log_likelihood": -23.072377985179124,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.4444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -24.95329849295801,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.6312450476145642,
        "p": 0.050309708707757105
      },
      "beta": {
        "r": 0.7292165878400442,
        "p": 0.016707453353778937
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 47.91519704762368,
  "bic_results": {
    "average_bic": 47.91519704762368,
    "individual_bics": [
      11.32593601252399,
      52.02991193354119,
      51.030332753920646,
      48.128337828577344,
      43.95598129868191,
      55.97288123380949,
      57.073634862828236,
      56.288369023681135,
      57.073634862828236,
      42.38889831595904,
      44.543885345847116,
      21.34071443807282,
      42.552228190678385,
      57.073634862828236,
      57.073634862828236,
      57.073634862828236,
      57.073634862828236,
      57.073634862828236,
      57.073634862828236,
      57.073634862828236,
      52.924141649290014,
      57.073634862828236,
      46.09266054706764,
      54.47575956752099,
      48.240995993787266,
      55.35203502663397,
      53.74538308716454,
      28.936513378686506,
      57.073634862828236,
      57.073634862828236,
      57.073634862828236,
      11.325933301930146,
      49.5536330458698,
      46.2681378908897,
      57.073634862828236,
      57.06268136671351,
      56.93732778545862,
      49.91757084683592,
      56.87146978715041,
      57.073634862828236,
      55.520889795851325,
      32.92344633694256,
      57.073634862828236,
      54.82265040525465,
      55.86746939861417,
      57.073634862828236,
      52.754186448687236,
      50.07838594318186,
      41.375611698053994,
      56.93928045128021,
      57.073634862828236,
      23.926908473981683,
      55.135680282358074,
      57.07089672506788,
      47.667897256003116,
      34.99816765919151,
      33.98999837058993,
      57.073634862828236,
      18.342813590036144,
      57.073634862828236,
      56.757338942640935,
      50.11127220522805,
      11.32593487201581,
      53.31179384619477,
      31.261637292571827,
      54.80500029780656,
      57.073634862828236,
      57.073634862828236,
      56.074741106941595,
      57.073634862828236,
      10.147234507793115,
      21.618451849773017,
      53.593562156089405,
      57.073634862828236,
      56.00259743655848,
      56.885682863946975,
      57.073634862828236,
      20.601196924030965,
      57.073634862828236,
      11.325942817738621,
      56.17809879199851,
      57.073634862828236,
      23.926908474034825,
      53.063409367674026,
      57.073634862828236,
      15.835523996293288,
      57.073634862828236,
      36.18612650409192,
      57.073634862828236,
      53.311793847270465,
      57.073634862828236
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.631\n- beta: r = 0.729",
  "previous_models": [
    "Specification: U = alpha * (split_self - 3 * token_self) - beta * (split_opp - split_self)\nSummary: A utility model capturing responder decisions through two key factors: (1) deviation from contribution-based fairness (split_self vs. 3*token_self) and (2) inequity aversion (split_opp - split_self). The model uses two learnable parameters (alpha, beta) with bounded ranges [0,10] to balance parsimony and identifiability. Utility is converted to acceptance probability via logistic function, where U < 0 corresponds to <50% acceptance likelihood.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 47.91519704762368\nOverall Accuracy: 0.5940\n\nParameter Recovery:\n- alpha: r = 0.631\n- beta: r = 0.729"
  ]
}