{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\nOkay, let's look at the previous results. The second model had a BIC of 42.83, which is better than the first, but the parameter recovery for alpha dropped to 0.3, which is pretty low. Beta's recovery was 0.618. The accuracy improved to 67.3%, which is decent but there's room for improvement.\n\nThe main issues here are the low recovery for alpha and the potential for improving accuracy. The previous instructions suggested adding non-linear terms, hybrid fairness norms, and loss aversion. But the model from run 2 still used a linear combination. Maybe the model needs more nuanced components.\n\nThe parameter recovery for alpha being low suggests that the model isn't capturing the true drivers of fairness sensitivity. Perhaps the alpha term is conflated with other factors. For example, if participants have different fairness reference points depending on trial_type (equal, opponent more, participant more), the model might need to account for that. \n\nAlso, the previous model used split_opp - split_self for inequity, but maybe participants react differently when they're getting more vs. less than the opponent. Splitting beta into two parameters for advantageous and disadvantageous inequity could help. That way, beta_adv applies when split_self > split_opp, and beta_dis when split_opp > split_self. This could improve parameter recovery by separating the effects.\n\nAnother angle is the loss aversion idea. Participants might be more sensitive to receiving less than their fair share (3*token_self) compared to getting more. Adding a loss aversion parameter that amplifies negative deviations (split_self < 3*token_self) could capture this. For example, using a term like gamma * max(0, 3*token_self - split_self) with gamma as a learnable parameter. This would make the model non-linear and might better fit the data.\n\nAdditionally, the model could consider the total combined earning. For instance, the utility might depend on the proportion of the total that split_self represents, not just the absolute value. Or maybe participants compare their split to both their contribution and the equal split. Introducing a parameter that weights between contribution-based and equal-split fairness could help. For example, U = alpha * [w*(split_self - 3*token_self) + (1-w)*(split_self - combined_earning/2)] + beta terms. Here, w is a weight between 0 and 1, learnable.\n\nTo address parameter identifiability, the model should ensure that terms are orthogonal. For example, if the fairness term and inequity term are correlated, the parameters might be harder to recover. Using interaction terms or non-overlapping components could reduce covariance.\n\nThe instructions should encourage testing these ideas: asymmetric inequity parameters, loss aversion, hybrid fairness norms, and interaction with trial_type. Also, suggest keeping the number of parameters to 2-3 to balance complexity. Maybe allow for non-linear transformations like sigmoidal functions to model thresholds in decision-making.\n\nEmphasize improving parameter recovery for alpha by making its term more distinct. For example, if the fairness component is split into loss and gain relative to the fair share, each with their own parameters, that could isolate alpha's effect better.\n\nFinally, stress the importance of balancing model complexity with identifiability. Even if adding a third parameter, ensure it's justified by a clear psychological mechanism and improves fit significantly.\n</think>\n\n**Instructions for Run 3/200:**  \nDesign a model that implements *asymmetric value encoding* with these priorities:  \n1. **Dynamic Fairness Weighting**  \n   - Introduce a parameter (\u03b3) controlling the relative importance of contribution-based fairness vs. equal split norms  \n   - Example: `fairness_term = \u03b3*(split_self - 3*token_self) + (1-\u03b3)*(split_self - combined_earning/2)`  \n\n2. **Non-Linear Disadvantage Coding**  \n   - Apply separate parameters for:  \n     a) Absolute deficit from fair share: `max(0, 3*token_self - split_self)`  \n     b) Relative inequity penalty when `split_opp > split_self`  \n\n3. **Contextual Modulation**  \n   - Let trial_type (1=equal, 2=opponent more, 3=self more tokens) gate fairness expectations through multiplicative terms  \n\n4. **Identifiability Safeguards**  \n   - Use piecewise functions to isolate parameter effects (e.g., \u03b1 only activates when split_self < fair share)  \n   - Enforce parameter orthogonality through term scaling (e.g., divide fairness deviation by token_self)  \n\n*Radical Suggestion:* Test a prospect theory-inspired model where utility depends on *changes* from expected fairness rather than absolute values, with separate loss/gain sensitivities.  \n\n**Constraints:**  \n- Maximum 3 parameters  \n- All parameters must have distinct psychological interpretations  \n- BIC target <40 through strategic non-linear compression of effects",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: <think>\nOkay, let's look at the previous results. The second model had a BIC of 42.83, which is better than the first, but the parameter recovery for alpha dropped to 0.3, which is pretty low. Beta's recovery was 0.618. The accuracy improved to 67.3%, which is decent but there's room for improvement.\n\nThe main issues here are the low recovery for alpha and the potential for improving accuracy. The previous instructions suggested adding non-linear terms, hybrid fairness norms, and loss aversion. But the model from run 2 still used a linear combination. Maybe the model needs more nuanced components.\n\nThe parameter recovery for alpha being low suggests that the model isn't capturing the true drivers of fairness sensitivity. Perhaps the alpha term is conflated with other factors. For example, if participants have different fairness reference points depending on trial_type (equal, opponent more, participant more), the model might need to account for that. \n\nAlso, the previous model used split_opp - split_self for inequity, but maybe participants react differently when they're getting more vs. less than the opponent. Splitting beta into two parameters for advantageous and disadvantageous inequity could help. That way, beta_adv applies when split_self > split_opp, and beta_dis when split_opp > split_self. This could improve parameter recovery by separating the effects.\n\nAnother angle is the loss aversion idea. Participants might be more sensitive to receiving less than their fair share (3*token_self) compared to getting more. Adding a loss aversion parameter that amplifies negative deviations (split_self < 3*token_self) could capture this. For example, using a term like gamma * max(0, 3*token_self - split_self) with gamma as a learnable parameter. This would make the model non-linear and might better fit the data.\n\nAdditionally, the model could consider the total combined earning. For instance, the utility might depend on the proportion of the total that split_self represents, not just the absolute value. Or maybe participants compare their split to both their contribution and the equal split. Introducing a parameter that weights between contribution-based and equal-split fairness could help. For example, U = alpha * [w*(split_self - 3*token_self) + (1-w)*(split_self - combined_earning/2)] + beta terms. Here, w is a weight between 0 and 1, learnable.\n\nTo address parameter identifiability, the model should ensure that terms are orthogonal. For example, if the fairness term and inequity term are correlated, the parameters might be harder to recover. Using interaction terms or non-overlapping components could reduce covariance.\n\nThe instructions should encourage testing these ideas: asymmetric inequity parameters, loss aversion, hybrid fairness norms, and interaction with trial_type. Also, suggest keeping the number of parameters to 2-3 to balance complexity. Maybe allow for non-linear transformations like sigmoidal functions to model thresholds in decision-making.\n\nEmphasize improving parameter recovery for alpha by making its term more distinct. For example, if the fairness component is split into loss and gain relative to the fair share, each with their own parameters, that could isolate alpha's effect better.\n\nFinally, stress the importance of balancing model complexity with identifiability. Even if adding a third parameter, ensure it's justified by a clear psychological mechanism and improves fit significantly.\n</think>\n\n**Instructions for Run 3/200:**  \nDesign a model that implements *asymmetric value encoding* with these priorities:  \n1. **Dynamic Fairness Weighting**  \n   - Introduce a parameter (\u03b3) controlling the relative importance of contribution-based fairness vs. equal split norms  \n   - Example: `fairness_term = \u03b3*(split_self - 3*token_self) + (1-\u03b3)*(split_self - combined_earning/2)`  \n\n2. **Non-Linear Disadvantage Coding**  \n   - Apply separate parameters for:  \n     a) Absolute deficit from fair share: `max(0, 3*token_self - split_self)`  \n     b) Relative inequity penalty when `split_opp > split_self`  \n\n3. **Contextual Modulation**  \n   - Let trial_type (1=equal, 2=opponent more, 3=self more tokens) gate fairness expectations through multiplicative terms  \n\n4. **Identifiability Safeguards**  \n   - Use piecewise functions to isolate parameter effects (e.g., \u03b1 only activates when split_self < fair share)  \n   - Enforce parameter orthogonality through term scaling (e.g., divide fairness deviation by token_self)  \n\n*Radical Suggestion:* Test a prospect theory-inspired model where utility depends on *changes* from expected fairness rather than absolute values, with separate loss/gain sensitivities.  \n\n**Constraints:**  \n- Maximum 3 parameters  \n- All parameters must have distinct psychological interpretations  \n- BIC target <40 through strategic non-linear compression of effects\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant accepts or rejects a proposal, focusing on responder behavior. The model needs to use the given variables appropriately, pass parameter recovery tests, and balance accuracy with parameter identifiability.\n\nFirst, I need to identify the key variables that influence the responder's decision. The target variable is \"accept\", which is binary. The main factors likely involve fairness considerations based on the splits and contributions (token_self and token_opp). \n\nThe problem mentions that two equal split_self values don't represent the same fairness because combined_earning varies. So, the model should account for both the absolute split and the context of total earnings. \n\nI recall that in ultimatum game models, people often reject unfair offers even if it means getting nothing, suggesting a sensitivity to inequity. So, incorporating terms that capture inequity aversion could be useful. \n\nLet me think about utility functions. The utility of accepting might depend on the proposed share (split_self) compared to some fairness benchmark. A common approach is to compare the split to what's considered fair, which could be based on the participant's contribution (token_self * \u00a33) or an equal split (combined_earning / 2). \n\nMaybe the utility function should have two components: one for the monetary gain and another for fairness. For example, the utility could be the proposed amount minus a fairness term. Alternatively, the fairness term could be a function of the difference between the proposed split and the fair split.\n\nAnother consideration is the role of trial_type, which indicates whether contributions are equal, opponent more, or participant more. This might modulate how fairness is perceived. For instance, if the participant contributed more, they might expect a larger share, so deviations from that could be more impactful.\n\nBut the model needs to be applicable using the dataset variables. Let's list the relevant variables for the responder role:\n\n- token_self: participant's contribution\n- token_opp: opponent's contribution\n- combined_earning: total money (since each token is \u00a33, this is 3*(token_self + token_opp))\n- split_self: proposed share for participant\n- split_opp: proposed share for opponent\n- trial_type: 1=equal, 2=opponent more, 3=participant more\n\nThe fairness benchmark could be calculated in different ways. For example, the fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. Alternatively, an equal split would be combined_earning / 2. The model might weigh these two fairness norms.\n\nLet me consider a utility function that combines the monetary value of the offer and a term representing inequity aversion. The inequity term could be the difference between the proposed split and the fair share, multiplied by a sensitivity parameter. \n\nFor example:\n\nUtility = split_self - \u03b1 * |split_self - fair_share|\n\nHere, \u03b1 is a learnable parameter representing sensitivity to deviations from fairness. The fair_share could be a weighted average between contribution-based and equal split, but to keep it simple initially, maybe use contribution-based fair share: fair_share = 3 * token_self. Wait, but combined_earning is 3*(token_self + token_opp), so the contribution-based fair share would be (token_self / (token_self + token_opp)) * combined_earning. But that's equivalent to 3*token_self, since combined_earning is 3*(token_self + token_opp). So, split_self is compared to 3*token_self. If split_self is less than 3*token_self, that's unfair to the participant. The difference could be a negative term in utility.\n\nAlternatively, the model could use both advantageous and disadvantageous inequity. For example, if split_self is less than the fair share (disadvantageous), the utility is reduced, but if it's more (advantageous), maybe it's not penalized. Or both could be considered with different parameters.\n\nAnother approach is to model the utility as the proposed split minus a term that penalizes unfairness. The unfairness could be the difference between the opponent's split and the participant's split, but scaled by some parameter.\n\nWait, the variable splitperc_self is the percentage proposed to the participant. But the problem states that split_self is in \u00a3. So, perhaps the utility depends on both the absolute amount and the percentage relative to the total.\n\nBut the model needs to predict the binary accept/reject decision. The utility would be passed through a logistic function to get the probability of accepting. So, the utility function should be designed such that when utility is positive, the probability of accepting is higher.\n\nLet me formalize this. The probability of accepting is 1 / (1 + exp(-Utility)). So, the model needs to define Utility in terms of the variables and parameters.\n\nPossible variables to include:\n\n- split_self: the monetary amount offered to the participant.\n- split_opp: the opponent's share.\n- token_self: participant's contribution.\n- token_opp: opponent's contribution.\n- combined_earning: total money.\n\nThe fairness could be calculated as 3*token_self (since each token is \u00a33). The offered amount split_self minus this fair share would indicate whether the offer is fair. For example, if split_self < 3*token_self, the offer is below the participant's contribution, which might be perceived as unfair.\n\nAlternatively, the fair share could be the equal split (combined_earning / 2). The model might compare the offered split to both benchmarks. But that could complicate the model. Maybe a parameter that weights between contribution-based and equal-split fairness.\n\nBut to keep the model simple and ensure parameter recovery, perhaps start with a single fairness benchmark. Let's choose contribution-based fairness, as token_self is a key variable.\n\nSo, the utility could be:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) + \u03b2 * (split_opp - split_self)\n\nHere, \u03b1 is the sensitivity to being offered less than their contribution (disadvantageous inequity), and \u03b2 is the sensitivity to the difference between opponent's share and their own (inequity aversion). But this might have two parameters, \u03b1 and \u03b2.\n\nAlternatively, maybe combine these terms. For example, the inequity term could be the difference between the fair share and the offered split, scaled by \u03b1. If the offered split is less than the fair share, this term is positive, reducing utility.\n\nWait, let's think again. If the participant's fair share is 3*token_self, then the difference is (3*token_self - split_self). If this is positive, the offer is unfair (participant gets less than their contribution). So, the utility could decrease by \u03b1*(3*token_self - split_self) when split_self < 3*token_self. For cases where split_self is more than the fair share, maybe no penalty, or a different parameter.\n\nAlternatively, use a linear penalty for any deviation from the fair share, but that might not capture the asymmetry where getting less is worse than getting more.\n\nAnother approach is to model the utility as:\n\nUtility = split_self + \u03b1 * (split_self - 3*token_self) + \u03b2 * (split_self - split_opp)\n\nBut this may not capture the right direction. If split_self is less than 3*token_self, the term \u03b1*(split_self - 3*token_self) would be negative, reducing utility. So \u03b1 should be positive to penalize lower offers. Wait, no: if \u03b1 is positive, then when split_self is less than fair share, the term is negative, reducing utility. So higher \u03b1 means more sensitivity to unfairness.\n\nAlternatively, perhaps:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * max(0, split_self - split_opp)\n\nWait, but split_opp is the opponent's proposed share. If split_self is less than split_opp, that's another form of inequity. So the participant might dislike when the opponent gets more. So the term (split_opp - split_self) could be a measure of inequity, and the participant's utility decreases as this difference increases.\n\nBut combining these terms requires careful consideration. Let's try to outline possible components:\n\n1. Monetary value: split_self (higher is better)\n2. Disadvantageous inequity relative to contribution: max(0, 3*token_self - split_self) (penalty)\n3. Inequity relative to opponent's share: (split_opp - split_self) (if positive, opponent gets more, which is bad)\n\nSo the utility could be:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * (split_opp - split_self)\n\nBut this might have two parameters, \u03b1 and \u03b2. However, the problem states that the model should have learnable parameters with finite bounds. Let's see.\n\nAlternatively, combine the two inequity terms into one. For example, the total unfairness is the sum of the contribution-based unfairness and the split-based unfairness. But that would still require two parameters.\n\nAnother angle: since combined_earning is 3*(token_self + token_opp), the fair share based on equal split would be combined_earning / 2. The participant might compare their split to both their contribution and the equal split. Maybe a parameter \u03b3 weights between these two fairness norms:\n\nfairness_benchmark = \u03b3*(3*token_self) + (1-\u03b3)*(combined_earning / 2)\n\nThen, the difference between split_self and this benchmark is the unfairness. So:\n\nUtility = split_self - \u03b1 * (fairness_benchmark - split_self)\n\nBut this introduces \u03b3 as a learnable parameter, making it three parameters (\u03b1, \u03b3, and maybe \u03b2 for another term). However, the problem allows for multiple parameters as long as they are within bounds.\n\nBut the initial instruction says to prioritize parameter recovery and accuracy. Let's aim for a model with 2-3 parameters.\n\nAlternatively, use a single parameter for inequity aversion that combines both contribution and opponent's share. For example:\n\nUtility = split_self - \u03b1 * ( (3*token_self - split_self) + (split_opp - split_self) )\n\nThis combines the two sources of inequity into a single term scaled by \u03b1. But this assumes that both types of inequity have the same sensitivity, which might not be the case.\n\nAlternatively, have separate parameters for each type:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * max(0, split_opp - split_self)\n\nHere, \u03b1 penalizes getting less than contribution, \u03b2 penalizes when the opponent gets more. This model has two parameters. \n\nBut maybe participants are also sensitive to getting more than the opponent, but that's less likely. Alternatively, the second term could be the absolute difference, but that would require a different formulation.\n\nAnother consideration is the trial_type. For example, in trials where the participant contributed more (trial_type=3), they might have higher expectations, so the sensitivity to unfairness could be different. But including trial_type would require interaction terms, complicating the model. Since the model needs to use the provided variables, but not necessarily all, perhaps trial_type can be omitted if it's not critical.\n\nAlternatively, the model could include trial_type as a modulator of the fairness benchmark. For instance, in trial_type=2 (opponent more), the participant might expect a different fair share. But this would add complexity.\n\nGiven the need for simplicity and parameter recovery, perhaps start with a model that uses split_self, token_self, and split_opp, with two parameters: one for contribution-based inequity and one for split-based inequity.\n\nSo, the utility function could be:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * (split_opp - split_self)\n\nHere, \u03b1 is the sensitivity to being below one's contribution, and \u03b2 is the sensitivity to the opponent getting more. Both \u03b1 and \u03b2 are learnable parameters with bounds, say [0, 5] to allow for positive sensitivity.\n\nThe probability of accepting is then logistic(Utility). If Utility > 0, the probability is greater than 0.5.\n\nBut wait, the problem states that the utility should predict the probability such that less than 0 utility corresponds to less than 50% chance. So the logistic function with temperature 1 is used: P(accept) = 1 / (1 + exp(-Utility)).\n\nThis model uses split_self, token_self, split_opp, and combined_earning (since 3*token_self is part of the calculation, but combined_earning is 3*(token_self + token_opp), which might not be directly used here).\n\nVariables in the dataset needed are split_self, token_self, split_opp. All are available.\n\nNow, the parameters are \u03b1 and \u03b2, both constrained to be non-negative (since higher values mean more penalty for inequity). Their bounds could be [0, 10] or similar.\n\nAnother thought: maybe the participant's perception of fairness is influenced by their contribution relative to the opponent's. For example, if they contributed more, they might expect a larger share. The term (token_self - token_opp) could modulate the fairness benchmark. But this complicates the model.\n\nAlternatively, the fairness benchmark could be a weighted average between contribution-based and equal split. For example:\n\nfair_share = \u03b3*(3*token_self) + (1-\u03b3)*(combined_earning / 2)\n\nThen, the utility is:\n\nUtility = split_self - \u03b1 * (fair_share - split_self)\n\nHere, \u03b3 is a learnable parameter between 0 and 1, representing the weight on contribution-based fairness vs. equal split. \u03b1 is the sensitivity to deviations from this fair share. This introduces two parameters: \u03b1 and \u03b3.\n\nBut this might capture different fairness norms. For example, \u03b3=1 means pure contribution-based, \u03b3=0 means pure equal split.\n\nThis model could be:\n\nUtility = split_self - \u03b1 * ( \u03b3*3*token_self + (1-\u03b3)*(combined_earning/2) - split_self )\n\nSimplifying:\n\nUtility = split_self + \u03b1*split_self - \u03b1*( \u03b3*3*token_self + (1-\u03b3)*(combined_earning/2) )\n\nUtility = split_self*(1 + \u03b1) - \u03b1*( \u03b3*3*token_self + (1-\u03b3)*( (3*(token_self + token_opp))/2 ) )\n\nBut this might complicate the interpretation. Alternatively, keep it as:\n\nUtility = split_self - \u03b1*( fair_share - split_self )\n\nWhich can be rewritten as:\n\nUtility = split_self*(1 + \u03b1) - \u03b1*fair_share\n\nBut the exact formulation needs to be clear.\n\nAlternatively, the utility could be the difference between the offered split and the fair share, scaled by \u03b1:\n\nUtility = split_self - \u03b1*( fair_share - split_self )\n\nThis becomes:\n\nUtility = split_self*(1 + \u03b1) - \u03b1*fair_share\n\nIf fair_share is higher than split_self, this term reduces utility.\n\nBut this model has two parameters: \u03b1 (sensitivity) and \u03b3 (fairness norm weight). However, \u03b3 must be between 0 and 1, and \u03b1 between 0 and some upper bound.\n\nThis might capture different fairness considerations. However, with two parameters, it's manageable for parameter recovery.\n\nComparing the two models:\n\nModel 1: Two parameters (\u03b1, \u03b2) for contribution and split-based inequity.\n\nModel 2: Two parameters (\u03b1, \u03b3) for sensitivity and fairness norm weighting.\n\nWhich is better? Model 1 is simpler but might miss the interplay between contribution and equal split. Model 2 captures that but might be harder to recover parameters if \u03b3 and \u03b1 are correlated.\n\nGiven the task mentions that \"two equal split_self variables do not represent the same 'fairness'\", the model should account for the context of combined_earning. Model 2's fair_share incorporates combined_earning via the equal split term, which might better capture the varying contexts.\n\nSo, proceeding with Model 2:\n\nUtility = split_self - \u03b1*( fair_share - split_self )\n\nWhere fair_share = \u03b3*(3*token_self) + (1-\u03b3)*(combined_earning / 2)\n\nBut combined_earning is 3*(token_self + token_opp), so combined_earning / 2 = (3/2)*(token_self + token_opp)\n\nThus, fair_share = \u03b3*3*token_self + (1-\u03b3)*(3/2)*(token_self + token_opp)\n\nThis can be simplified, but the model would use the variables as given.\n\nThe parameters are \u03b1 (sensitivity to deviation from fair_share) and \u03b3 (weight between contribution and equal split). Both need bounds: \u03b1 \u2265 0, \u03b3 \u2208 [0,1].\n\nAlternatively, to avoid overcomplicating, perhaps use a single fairness benchmark. For example, assume participants compare their split to their contribution (3*token_self) and the model penalizes deviations below that. So:\n\nUtility = split_self - \u03b1*max(0, 3*token_self - split_self)\n\nThis is a simpler model with one parameter \u03b1. But maybe this misses the effect of the opponent's share or the total combined earning.\n\nAlternatively, include a term that accounts for the proportion of the total that the participant receives. For example, split_self / combined_earning compared to token_self / (token_self + token_opp). The difference between these proportions could be a measure of fairness.\n\nBut that might require more parameters. For example:\n\nfair_proportion = token_self / (token_self + token_opp)\nactual_proportion = split_self / combined_earning\nUtility = split_self - \u03b1*(fair_proportion - actual_proportion)*combined_earning\n\nBut this complicates the model. However, it directly compares the expected proportion based on contribution to the actual received proportion.\n\nAlternatively, the utility could be:\n\nUtility = split_self + \u03b1*(split_self - 3*token_self) + \u03b2*(split_self - (combined_earning / 2))\n\nHere, \u03b1 and \u03b2 are parameters capturing sensitivity to deviation from contribution-based and equal-split fairness, respectively. If split_self is higher than 3*token_self, the \u03b1 term is positive, increasing utility. If it's lower, the term is negative. Similarly for the equal split.\n\nBut this model has two parameters and might capture both types of fairness considerations. However, the signs of \u03b1 and \u03b2 would determine whether deviations are preferred or penalized. For example, if \u03b1 is positive, getting more than contribution-based fair share increases utility, which might not be the case\u2014participants might not penalize getting more, but the model allows for that.\n\nAlternatively, use max functions to only penalize disadvantageous deviations:\n\nUtility = split_self - \u03b1*max(0, 3*token_self - split_self) - \u03b2*max(0, combined_earning/2 - split_self)\n\nHere, \u03b1 penalizes being below contribution-based fair share, \u03b2 penalizes being below equal split. This model has two parameters. But participants might have different sensitivities to these two types of unfairness.\n\nThis could be a viable model. For example, a participant might care more about receiving less than their contribution (\u03b1) or less than half (\u03b2). The parameters \u03b1 and \u03b2 would capture these sensitivities.\n\nIncluding both terms allows the model to account for both fairness norms. However, it's possible that these terms are correlated, which could affect parameter recovery. But given the task's emphasis on parameter recovery, this model might still be acceptable if the parameters can be identified.\n\nAnother consideration is the opponent's share. If the opponent's split (split_opp) is very high compared to the participant's, that might influence rejection. So, including a term comparing split_self to split_opp could help. For example:\n\nUtility = split_self - \u03b1*max(0, 3*token_self - split_self) - \u03b2*max(0, split_opp - split_self)\n\nThis model penalizes the participant for receiving less than their contribution and less than the opponent. This has two parameters, \u03b1 and \u03b2.\n\nBut split_opp is part of the data, so it's allowed. However, split_opp + split_self = combined_earning, so there's a dependency. But including split_opp directly might capture the relative difference better.\n\nIn this case, the model would have:\n\nUtility = split_self - \u03b1*(3*token_self - split_self if split_self < 3*token_self else 0) - \u03b2*(split_opp - split_self if split_opp > split_self else 0)\n\nThis way, both disadvantageous inequity relative to contribution and relative to the opponent's share are penalized.\n\nThis model uses two parameters, \u03b1 and \u03b2, both non-negative. The utility increases with higher split_self, decreases if split_self is below contribution or below opponent's share.\n\nThis seems plausible. Participants might reject offers that are below their contribution or where the opponent is taking a larger share, even if the absolute amount is acceptable.\n\nNow, considering parameter bounds. For \u03b1 and \u03b2, setting a range of [0, 10] with inclusive min and max. These are generous bounds that allow for strong sensitivities.\n\nThe variables used are split_self, token_self, split_opp. All are present in the dataset.\n\nThe mathematical model would be:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * max(0, split_opp - split_self)\n\nThis utility is then passed through a logistic function to determine the probability of accepting.\n\nThis model has two learnable parameters, \u03b1 and \u03b2, with bounds [0, 10]. It uses the variables split_self, token_self, and split_opp from the dataset.\n\nAnother check: when split_self is greater than or equal to 3*token_self and split_opp is less than or equal to split_self, the penalty terms are zero, so Utility = split_self, leading to higher acceptance likelihood.\n\nIf split_self is below contribution or the opponent's share is higher, the utility decreases, making rejection more likely.\n\nThis model should be able to capture the key factors influencing responder decisions, uses the necessary variables, and has parameters that can be recovered with appropriate bounds.\n</think>\n\n<MODEL>\nUtility = split_self - alpha * max(0, 3*token_self - split_self) - beta * max(0, split_opp - split_self)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"Utility\": {\n      \"description\": \"Net utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to receiving less than contribution-based fair share (3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequity relative to opponent's share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for opponent in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-reference utility model evaluating offers against both contribution-based fairness (3*token_self) and relative opponent shares. Penalizes offers below the participant's contribution value (alpha) and offers where the opponent receives more (beta). Parameters capture distinct sensitivities to different fairness violations while maintaining orthogonal, recoverable effects.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                Utility = split_self - alpha * max(0, 3*token_self - split_self) - beta * max(0, split_opp - split_self)\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"Utility\": {\n      \"description\": \"Net utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to receiving less than contribution-based fair share (3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequity relative to opponent's share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for opponent in \\u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n\n        utility = split_self - alpha * max(0, 3*token_self - split_self) - beta * max(0, split_opp - split_self)\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1744015522.993595
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Sensitivity to receiving less than contribution-based fair share (3*token_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to disadvantageous inequity relative to opponent's share",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "Utility = split_self - alpha * max(0, 3*token_self - split_self) - beta * max(0, split_opp - split_self)",
  "variable_descriptions": {
    "Utility": {
      "description": "Net utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha": {
      "description": "Sensitivity to receiving less than contribution-based fair share (3*token_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Sensitivity to disadvantageous inequity relative to opponent's share",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for participant in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Participant's token contribution",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_opp": {
      "description": "Proposed share for opponent in \u00a3",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A dual-reference utility model evaluating offers against both contribution-based fairness (3*token_self) and relative opponent shares. Penalizes offers below the participant's contribution value (alpha) and offers where the opponent receives more (beta). Parameters capture distinct sensitivities to different fairness violations while maintaining orthogonal, recoverable effects.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant accepts or rejects a proposal, focusing on responder behavior. The model needs to use the given variables appropriately, pass parameter recovery tests, and balance accuracy with parameter identifiability.\n\nFirst, I need to identify the key variables that influence the responder's decision. The target variable is \"accept\", which is binary. The main factors likely involve fairness considerations based on the splits and contributions (token_self and token_opp). \n\nThe problem mentions that two equal split_self values don't represent the same fairness because combined_earning varies. So, the model should account for both the absolute split and the context of total earnings. \n\nI recall that in ultimatum game models, people often reject unfair offers even if it means getting nothing, suggesting a sensitivity to inequity. So, incorporating terms that capture inequity aversion could be useful. \n\nLet me think about utility functions. The utility of accepting might depend on the proposed share (split_self) compared to some fairness benchmark. A common approach is to compare the split to what's considered fair, which could be based on the participant's contribution (token_self * \u00a33) or an equal split (combined_earning / 2). \n\nMaybe the utility function should have two components: one for the monetary gain and another for fairness. For example, the utility could be the proposed amount minus a fairness term. Alternatively, the fairness term could be a function of the difference between the proposed split and the fair split.\n\nAnother consideration is the role of trial_type, which indicates whether contributions are equal, opponent more, or participant more. This might modulate how fairness is perceived. For instance, if the participant contributed more, they might expect a larger share, so deviations from that could be more impactful.\n\nBut the model needs to be applicable using the dataset variables. Let's list the relevant variables for the responder role:\n\n- token_self: participant's contribution\n- token_opp: opponent's contribution\n- combined_earning: total money (since each token is \u00a33, this is 3*(token_self + token_opp))\n- split_self: proposed share for participant\n- split_opp: proposed share for opponent\n- trial_type: 1=equal, 2=opponent more, 3=participant more\n\nThe fairness benchmark could be calculated in different ways. For example, the fair share based on contribution would be (token_self / (token_self + token_opp)) * combined_earning. Alternatively, an equal split would be combined_earning / 2. The model might weigh these two fairness norms.\n\nLet me consider a utility function that combines the monetary value of the offer and a term representing inequity aversion. The inequity term could be the difference between the proposed split and the fair share, multiplied by a sensitivity parameter. \n\nFor example:\n\nUtility = split_self - \u03b1 * |split_self - fair_share|\n\nHere, \u03b1 is a learnable parameter representing sensitivity to deviations from fairness. The fair_share could be a weighted average between contribution-based and equal split, but to keep it simple initially, maybe use contribution-based fair share: fair_share = 3 * token_self. Wait, but combined_earning is 3*(token_self + token_opp), so the contribution-based fair share would be (token_self / (token_self + token_opp)) * combined_earning. But that's equivalent to 3*token_self, since combined_earning is 3*(token_self + token_opp). So, split_self is compared to 3*token_self. If split_self is less than 3*token_self, that's unfair to the participant. The difference could be a negative term in utility.\n\nAlternatively, the model could use both advantageous and disadvantageous inequity. For example, if split_self is less than the fair share (disadvantageous), the utility is reduced, but if it's more (advantageous), maybe it's not penalized. Or both could be considered with different parameters.\n\nAnother approach is to model the utility as the proposed split minus a term that penalizes unfairness. The unfairness could be the difference between the opponent's split and the participant's split, but scaled by some parameter.\n\nWait, the variable splitperc_self is the percentage proposed to the participant. But the problem states that split_self is in \u00a3. So, perhaps the utility depends on both the absolute amount and the percentage relative to the total.\n\nBut the model needs to predict the binary accept/reject decision. The utility would be passed through a logistic function to get the probability of accepting. So, the utility function should be designed such that when utility is positive, the probability of accepting is higher.\n\nLet me formalize this. The probability of accepting is 1 / (1 + exp(-Utility)). So, the model needs to define Utility in terms of the variables and parameters.\n\nPossible variables to include:\n\n- split_self: the monetary amount offered to the participant.\n- split_opp: the opponent's share.\n- token_self: participant's contribution.\n- token_opp: opponent's contribution.\n- combined_earning: total money.\n\nThe fairness could be calculated as 3*token_self (since each token is \u00a33). The offered amount split_self minus this fair share would indicate whether the offer is fair. For example, if split_self < 3*token_self, the offer is below the participant's contribution, which might be perceived as unfair.\n\nAlternatively, the fair share could be the equal split (combined_earning / 2). The model might compare the offered split to both benchmarks. But that could complicate the model. Maybe a parameter that weights between contribution-based and equal-split fairness.\n\nBut to keep the model simple and ensure parameter recovery, perhaps start with a single fairness benchmark. Let's choose contribution-based fairness, as token_self is a key variable.\n\nSo, the utility could be:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) + \u03b2 * (split_opp - split_self)\n\nHere, \u03b1 is the sensitivity to being offered less than their contribution (disadvantageous inequity), and \u03b2 is the sensitivity to the difference between opponent's share and their own (inequity aversion). But this might have two parameters, \u03b1 and \u03b2.\n\nAlternatively, maybe combine these terms. For example, the inequity term could be the difference between the fair share and the offered split, scaled by \u03b1. If the offered split is less than the fair share, this term is positive, reducing utility.\n\nWait, let's think again. If the participant's fair share is 3*token_self, then the difference is (3*token_self - split_self). If this is positive, the offer is unfair (participant gets less than their contribution). So, the utility could decrease by \u03b1*(3*token_self - split_self) when split_self < 3*token_self. For cases where split_self is more than the fair share, maybe no penalty, or a different parameter.\n\nAlternatively, use a linear penalty for any deviation from the fair share, but that might not capture the asymmetry where getting less is worse than getting more.\n\nAnother approach is to model the utility as:\n\nUtility = split_self + \u03b1 * (split_self - 3*token_self) + \u03b2 * (split_self - split_opp)\n\nBut this may not capture the right direction. If split_self is less than 3*token_self, the term \u03b1*(split_self - 3*token_self) would be negative, reducing utility. So \u03b1 should be positive to penalize lower offers. Wait, no: if \u03b1 is positive, then when split_self is less than fair share, the term is negative, reducing utility. So higher \u03b1 means more sensitivity to unfairness.\n\nAlternatively, perhaps:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * max(0, split_self - split_opp)\n\nWait, but split_opp is the opponent's proposed share. If split_self is less than split_opp, that's another form of inequity. So the participant might dislike when the opponent gets more. So the term (split_opp - split_self) could be a measure of inequity, and the participant's utility decreases as this difference increases.\n\nBut combining these terms requires careful consideration. Let's try to outline possible components:\n\n1. Monetary value: split_self (higher is better)\n2. Disadvantageous inequity relative to contribution: max(0, 3*token_self - split_self) (penalty)\n3. Inequity relative to opponent's share: (split_opp - split_self) (if positive, opponent gets more, which is bad)\n\nSo the utility could be:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * (split_opp - split_self)\n\nBut this might have two parameters, \u03b1 and \u03b2. However, the problem states that the model should have learnable parameters with finite bounds. Let's see.\n\nAlternatively, combine the two inequity terms into one. For example, the total unfairness is the sum of the contribution-based unfairness and the split-based unfairness. But that would still require two parameters.\n\nAnother angle: since combined_earning is 3*(token_self + token_opp), the fair share based on equal split would be combined_earning / 2. The participant might compare their split to both their contribution and the equal split. Maybe a parameter \u03b3 weights between these two fairness norms:\n\nfairness_benchmark = \u03b3*(3*token_self) + (1-\u03b3)*(combined_earning / 2)\n\nThen, the difference between split_self and this benchmark is the unfairness. So:\n\nUtility = split_self - \u03b1 * (fairness_benchmark - split_self)\n\nBut this introduces \u03b3 as a learnable parameter, making it three parameters (\u03b1, \u03b3, and maybe \u03b2 for another term). However, the problem allows for multiple parameters as long as they are within bounds.\n\nBut the initial instruction says to prioritize parameter recovery and accuracy. Let's aim for a model with 2-3 parameters.\n\nAlternatively, use a single parameter for inequity aversion that combines both contribution and opponent's share. For example:\n\nUtility = split_self - \u03b1 * ( (3*token_self - split_self) + (split_opp - split_self) )\n\nThis combines the two sources of inequity into a single term scaled by \u03b1. But this assumes that both types of inequity have the same sensitivity, which might not be the case.\n\nAlternatively, have separate parameters for each type:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * max(0, split_opp - split_self)\n\nHere, \u03b1 penalizes getting less than contribution, \u03b2 penalizes when the opponent gets more. This model has two parameters. \n\nBut maybe participants are also sensitive to getting more than the opponent, but that's less likely. Alternatively, the second term could be the absolute difference, but that would require a different formulation.\n\nAnother consideration is the trial_type. For example, in trials where the participant contributed more (trial_type=3), they might have higher expectations, so the sensitivity to unfairness could be different. But including trial_type would require interaction terms, complicating the model. Since the model needs to use the provided variables, but not necessarily all, perhaps trial_type can be omitted if it's not critical.\n\nAlternatively, the model could include trial_type as a modulator of the fairness benchmark. For instance, in trial_type=2 (opponent more), the participant might expect a different fair share. But this would add complexity.\n\nGiven the need for simplicity and parameter recovery, perhaps start with a model that uses split_self, token_self, and split_opp, with two parameters: one for contribution-based inequity and one for split-based inequity.\n\nSo, the utility function could be:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * (split_opp - split_self)\n\nHere, \u03b1 is the sensitivity to being below one's contribution, and \u03b2 is the sensitivity to the opponent getting more. Both \u03b1 and \u03b2 are learnable parameters with bounds, say [0, 5] to allow for positive sensitivity.\n\nThe probability of accepting is then logistic(Utility). If Utility > 0, the probability is greater than 0.5.\n\nBut wait, the problem states that the utility should predict the probability such that less than 0 utility corresponds to less than 50% chance. So the logistic function with temperature 1 is used: P(accept) = 1 / (1 + exp(-Utility)).\n\nThis model uses split_self, token_self, split_opp, and combined_earning (since 3*token_self is part of the calculation, but combined_earning is 3*(token_self + token_opp), which might not be directly used here).\n\nVariables in the dataset needed are split_self, token_self, split_opp. All are available.\n\nNow, the parameters are \u03b1 and \u03b2, both constrained to be non-negative (since higher values mean more penalty for inequity). Their bounds could be [0, 10] or similar.\n\nAnother thought: maybe the participant's perception of fairness is influenced by their contribution relative to the opponent's. For example, if they contributed more, they might expect a larger share. The term (token_self - token_opp) could modulate the fairness benchmark. But this complicates the model.\n\nAlternatively, the fairness benchmark could be a weighted average between contribution-based and equal split. For example:\n\nfair_share = \u03b3*(3*token_self) + (1-\u03b3)*(combined_earning / 2)\n\nThen, the utility is:\n\nUtility = split_self - \u03b1 * (fair_share - split_self)\n\nHere, \u03b3 is a learnable parameter between 0 and 1, representing the weight on contribution-based fairness vs. equal split. \u03b1 is the sensitivity to deviations from this fair share. This introduces two parameters: \u03b1 and \u03b3.\n\nBut this might capture different fairness norms. For example, \u03b3=1 means pure contribution-based, \u03b3=0 means pure equal split.\n\nThis model could be:\n\nUtility = split_self - \u03b1 * ( \u03b3*3*token_self + (1-\u03b3)*(combined_earning/2) - split_self )\n\nSimplifying:\n\nUtility = split_self + \u03b1*split_self - \u03b1*( \u03b3*3*token_self + (1-\u03b3)*(combined_earning/2) )\n\nUtility = split_self*(1 + \u03b1) - \u03b1*( \u03b3*3*token_self + (1-\u03b3)*( (3*(token_self + token_opp))/2 ) )\n\nBut this might complicate the interpretation. Alternatively, keep it as:\n\nUtility = split_self - \u03b1*( fair_share - split_self )\n\nWhich can be rewritten as:\n\nUtility = split_self*(1 + \u03b1) - \u03b1*fair_share\n\nBut the exact formulation needs to be clear.\n\nAlternatively, the utility could be the difference between the offered split and the fair share, scaled by \u03b1:\n\nUtility = split_self - \u03b1*( fair_share - split_self )\n\nThis becomes:\n\nUtility = split_self*(1 + \u03b1) - \u03b1*fair_share\n\nIf fair_share is higher than split_self, this term reduces utility.\n\nBut this model has two parameters: \u03b1 (sensitivity) and \u03b3 (fairness norm weight). However, \u03b3 must be between 0 and 1, and \u03b1 between 0 and some upper bound.\n\nThis might capture different fairness considerations. However, with two parameters, it's manageable for parameter recovery.\n\nComparing the two models:\n\nModel 1: Two parameters (\u03b1, \u03b2) for contribution and split-based inequity.\n\nModel 2: Two parameters (\u03b1, \u03b3) for sensitivity and fairness norm weighting.\n\nWhich is better? Model 1 is simpler but might miss the interplay between contribution and equal split. Model 2 captures that but might be harder to recover parameters if \u03b3 and \u03b1 are correlated.\n\nGiven the task mentions that \"two equal split_self variables do not represent the same 'fairness'\", the model should account for the context of combined_earning. Model 2's fair_share incorporates combined_earning via the equal split term, which might better capture the varying contexts.\n\nSo, proceeding with Model 2:\n\nUtility = split_self - \u03b1*( fair_share - split_self )\n\nWhere fair_share = \u03b3*(3*token_self) + (1-\u03b3)*(combined_earning / 2)\n\nBut combined_earning is 3*(token_self + token_opp), so combined_earning / 2 = (3/2)*(token_self + token_opp)\n\nThus, fair_share = \u03b3*3*token_self + (1-\u03b3)*(3/2)*(token_self + token_opp)\n\nThis can be simplified, but the model would use the variables as given.\n\nThe parameters are \u03b1 (sensitivity to deviation from fair_share) and \u03b3 (weight between contribution and equal split). Both need bounds: \u03b1 \u2265 0, \u03b3 \u2208 [0,1].\n\nAlternatively, to avoid overcomplicating, perhaps use a single fairness benchmark. For example, assume participants compare their split to their contribution (3*token_self) and the model penalizes deviations below that. So:\n\nUtility = split_self - \u03b1*max(0, 3*token_self - split_self)\n\nThis is a simpler model with one parameter \u03b1. But maybe this misses the effect of the opponent's share or the total combined earning.\n\nAlternatively, include a term that accounts for the proportion of the total that the participant receives. For example, split_self / combined_earning compared to token_self / (token_self + token_opp). The difference between these proportions could be a measure of fairness.\n\nBut that might require more parameters. For example:\n\nfair_proportion = token_self / (token_self + token_opp)\nactual_proportion = split_self / combined_earning\nUtility = split_self - \u03b1*(fair_proportion - actual_proportion)*combined_earning\n\nBut this complicates the model. However, it directly compares the expected proportion based on contribution to the actual received proportion.\n\nAlternatively, the utility could be:\n\nUtility = split_self + \u03b1*(split_self - 3*token_self) + \u03b2*(split_self - (combined_earning / 2))\n\nHere, \u03b1 and \u03b2 are parameters capturing sensitivity to deviation from contribution-based and equal-split fairness, respectively. If split_self is higher than 3*token_self, the \u03b1 term is positive, increasing utility. If it's lower, the term is negative. Similarly for the equal split.\n\nBut this model has two parameters and might capture both types of fairness considerations. However, the signs of \u03b1 and \u03b2 would determine whether deviations are preferred or penalized. For example, if \u03b1 is positive, getting more than contribution-based fair share increases utility, which might not be the case\u2014participants might not penalize getting more, but the model allows for that.\n\nAlternatively, use max functions to only penalize disadvantageous deviations:\n\nUtility = split_self - \u03b1*max(0, 3*token_self - split_self) - \u03b2*max(0, combined_earning/2 - split_self)\n\nHere, \u03b1 penalizes being below contribution-based fair share, \u03b2 penalizes being below equal split. This model has two parameters. But participants might have different sensitivities to these two types of unfairness.\n\nThis could be a viable model. For example, a participant might care more about receiving less than their contribution (\u03b1) or less than half (\u03b2). The parameters \u03b1 and \u03b2 would capture these sensitivities.\n\nIncluding both terms allows the model to account for both fairness norms. However, it's possible that these terms are correlated, which could affect parameter recovery. But given the task's emphasis on parameter recovery, this model might still be acceptable if the parameters can be identified.\n\nAnother consideration is the opponent's share. If the opponent's split (split_opp) is very high compared to the participant's, that might influence rejection. So, including a term comparing split_self to split_opp could help. For example:\n\nUtility = split_self - \u03b1*max(0, 3*token_self - split_self) - \u03b2*max(0, split_opp - split_self)\n\nThis model penalizes the participant for receiving less than their contribution and less than the opponent. This has two parameters, \u03b1 and \u03b2.\n\nBut split_opp is part of the data, so it's allowed. However, split_opp + split_self = combined_earning, so there's a dependency. But including split_opp directly might capture the relative difference better.\n\nIn this case, the model would have:\n\nUtility = split_self - \u03b1*(3*token_self - split_self if split_self < 3*token_self else 0) - \u03b2*(split_opp - split_self if split_opp > split_self else 0)\n\nThis way, both disadvantageous inequity relative to contribution and relative to the opponent's share are penalized.\n\nThis model uses two parameters, \u03b1 and \u03b2, both non-negative. The utility increases with higher split_self, decreases if split_self is below contribution or below opponent's share.\n\nThis seems plausible. Participants might reject offers that are below their contribution or where the opponent is taking a larger share, even if the absolute amount is acceptable.\n\nNow, considering parameter bounds. For \u03b1 and \u03b2, setting a range of [0, 10] with inclusive min and max. These are generous bounds that allow for strong sensitivities.\n\nThe variables used are split_self, token_self, split_opp. All are present in the dataset.\n\nThe mathematical model would be:\n\nUtility = split_self - \u03b1 * max(0, 3*token_self - split_self) - \u03b2 * max(0, split_opp - split_self)\n\nThis utility is then passed through a logistic function to determine the probability of accepting.\n\nThis model has two learnable parameters, \u03b1 and \u03b2, with bounds [0, 10]. It uses the variables split_self, token_self, and split_opp from the dataset.\n\nAnother check: when split_self is greater than or equal to 3*token_self and split_opp is less than or equal to split_self, the penalty terms are zero, so Utility = split_self, leading to higher acceptance likelihood.\n\nIf split_self is below contribution or the opponent's share is higher, the utility decreases, making rejection more likely.\n\nThis model should be able to capture the key factors influencing responder decisions, uses the necessary variables, and has parameters that can be recovered with appropriate bounds.\n</think>\n\n<MODEL>\nUtility = split_self - alpha * max(0, 3*token_self - split_self) - beta * max(0, split_opp - split_self)\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"Utility\": {\n      \"description\": \"Net utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Sensitivity to receiving less than contribution-based fair share (3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to disadvantageous inequity relative to opponent's share\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for opponent in \u00a3\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-reference utility model evaluating offers against both contribution-based fairness (3*token_self) and relative opponent shares. Penalizes offers below the participant's contribution value (alpha) and offers where the opponent receives more (beta). Parameters capture distinct sensitivities to different fairness violations while maintaining orthogonal, recoverable effects.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: Utility = split_self - alpha * max(0, 3*token_self - split_self) - beta * max(0, split_opp - split_self)\nSummary: A dual-reference utility model evaluating offers against both contribution-based fairness (3*token_self) and relative opponent shares. Penalizes offers below the participant's contribution value (alpha) and offers where the opponent receives more (beta). Parameters capture distinct sensitivities to different fairness violations while maintaining orthogonal, recoverable effects.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n\n        utility = split_self - alpha * max(0, 3*token_self - split_self) - beta * max(0, split_opp - split_self)\n        model_predictions.append(utility)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8159340659340659,
  "fitting_results": [
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.022287734399213947,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.058165608789356,
      "beta": 0.19487472284197685,
      "success": true,
      "log_likelihood": -17.228732205544883,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.4592871266273497,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -10.228883640726515,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1292136729568445,
      "beta": 0.3068103916193897,
      "success": true,
      "log_likelihood": -13.311238099598691,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9111395707233545,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -9.848709686242294,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7216503017729656,
      "beta": 0.1934316526833398,
      "success": true,
      "log_likelihood": -10.818081722300912,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.3703753451934495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9728190679539933,
      "beta": 0.024475082150782295,
      "success": true,
      "log_likelihood": -11.245424621551042,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.370375345193449,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8964105368953308,
      "beta": 0.07068442322174513,
      "success": true,
      "log_likelihood": -11.365271177757508,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8719669952721836,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -5.583534937093932,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2669722635095118,
      "beta": 1.8379577450002094,
      "success": true,
      "log_likelihood": -6.797422531513243,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8896514582754615,
      "beta": 0.06791503292913725,
      "success": true,
      "log_likelihood": -8.82001360372863,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.370375345193449,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2875424116747833,
      "beta": 0.05496974129698469,
      "success": true,
      "log_likelihood": -6.189259323420028,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3621358903704304,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.656146828788505,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.3703753451934495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.328278373129843,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -15.841966351222442,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.43351620540459684,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -8.623736865237996,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 1.0653972583727382,
      "success": true,
      "log_likelihood": -11.8003810282787,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5008804908625671,
      "beta": 0.41096590709196656,
      "success": true,
      "log_likelihood": -16.631973131692483,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6367998956143686,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -14.538881710394902,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1240948398330959,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -15.182962254640646,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2532762154253351,
      "beta": 0.593763505846406,
      "success": true,
      "log_likelihood": -8.202061704856977,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.190889904306372,
      "beta": 0.21408888335200935,
      "success": true,
      "log_likelihood": -3.987596376344774,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.33506938637315076,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -9.476787731626334,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6035295622958764,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -16.64351367169363,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.022287734399213947,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.9178921134682797,
      "success": true,
      "log_likelihood": -15.309030765117114,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5775711452533754,
      "beta": 0.7349508655946208,
      "success": true,
      "log_likelihood": -13.167277670077478,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.370375345193449,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.4881404512224026,
      "success": true,
      "log_likelihood": -10.760896916067477,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8928408447801396,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -8.625441841616412,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.531309189210426,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -14.738122138847672,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9111701947705447,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -19.198954265692876,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.10231686062281951,
      "success": true,
      "log_likelihood": -9.47000891494885,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7305029002755707,
      "beta": 0.21756622530768713,
      "success": true,
      "log_likelihood": -18.971727886236227,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6971073436916018,
      "beta": 0.6954164240641115,
      "success": true,
      "log_likelihood": -15.54850604135938,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.770375323053267,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.674151404825744,
      "success": true,
      "log_likelihood": -12.266957306479803,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7236677336396297,
      "beta": 0.19944521988692032,
      "success": true,
      "log_likelihood": -12.866095612377531,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.35429051707261494,
      "success": true,
      "log_likelihood": -15.08567258649757,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.25953903750500623,
      "beta": 0.6374194919494419,
      "success": true,
      "log_likelihood": -11.979792517514616,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5522553695775855,
      "beta": 0.586552115417462,
      "success": true,
      "log_likelihood": -21.416774921322954,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0936835718129134,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.652269950459583,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.5190827352647046,
      "success": true,
      "log_likelihood": -31.74512404809155,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2278208709819071,
      "beta": 0.2939949894187323,
      "success": true,
      "log_likelihood": -17.753829374517128,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.184030496729548,
      "success": true,
      "log_likelihood": -8.046709069392906,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.6572714684304949,
      "success": true,
      "log_likelihood": -15.865141027920233,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.4820953392112648,
      "success": true,
      "log_likelihood": -23.64749981027576,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6725989018804512,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -27.547954303276025,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.680168254994201,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -5.080842705206226,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6155175878070989,
      "beta": 0.6789193889911376,
      "success": true,
      "log_likelihood": -8.966250635601918,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.010187992693023855,
      "success": true,
      "log_likelihood": -16.01530187707498,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.11664427558582785,
      "beta": 2.609685231195979,
      "success": true,
      "log_likelihood": -2.978600227710203,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.241706278220731,
      "success": true,
      "log_likelihood": -13.422415552944202,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.22755997235143094,
      "beta": 0.41413686973818875,
      "success": true,
      "log_likelihood": -21.352966483784957,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3661098550376898,
      "beta": 0.08916167469919949,
      "success": true,
      "log_likelihood": -15.422272803220364,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.022287734399213943,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.7489890782913035,
      "success": true,
      "log_likelihood": -21.551244621449538,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9713315049324337,
      "beta": 1.1942809276164361,
      "success": true,
      "log_likelihood": -5.677943400961094,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.24524371326084804,
      "beta": 0.5439568728691764,
      "success": true,
      "log_likelihood": -11.64401037821869,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.11402090051339744,
      "success": true,
      "log_likelihood": -8.375252437530017,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3890010252389262,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.799111117732592,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.6002865073868745,
      "success": true,
      "log_likelihood": -12.811189253372596,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12228585221238357,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -12.316454827603591,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 0.8376593432914206,
      "success": true,
      "log_likelihood": -0.24407541504365637,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 2.3626208258049757,
      "success": true,
      "log_likelihood": -3.89214684290049,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7689838189852812,
      "beta": 0.2963337392832061,
      "success": true,
      "log_likelihood": -15.424430812123342,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.2667781152742782,
      "success": true,
      "log_likelihood": -14.75467587260195,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.23627358587617325,
      "beta": 0.487807909941372,
      "success": true,
      "log_likelihood": -29.979664948281034,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.22638447486248012,
      "beta": 0.3977592532395828,
      "success": true,
      "log_likelihood": -11.799523632586132,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08489558405704245,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.806716493567825,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6387642803380895,
      "beta": 1.9221331510839057,
      "success": true,
      "log_likelihood": -17.511997855884353,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.23065558993546068,
      "beta": 0.2679830246178756,
      "success": true,
      "log_likelihood": -30.246599068742057,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -0.022287734399213943,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.5930262785305748,
      "success": true,
      "log_likelihood": -28.79817196933009,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.04045822800978617,
      "success": true,
      "log_likelihood": -12.780323709787435,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0284531010810336,
      "beta": 1.7392626275437713,
      "success": true,
      "log_likelihood": -11.722269345189918,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5214386375524177,
      "beta": 0.4695402098952899,
      "success": true,
      "log_likelihood": -8.386405415721955,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.46740268810139374,
      "beta": 0.17284315184110544,
      "success": true,
      "log_likelihood": -36.73333952278811,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7634994846502359,
      "beta": 2.349934957848758,
      "success": true,
      "log_likelihood": -1.789396028609059,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7176213967040395,
      "beta": 0.04339031486004511,
      "success": true,
      "log_likelihood": -19.617582667283067,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.4658750270051923,
      "beta": 0.19710079619093088,
      "success": true,
      "log_likelihood": -21.55183748273503,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12228604230638569,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -6.3164548554793125,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.7489894468569378,
      "success": true,
      "log_likelihood": -9.551244804314495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.36909747452088554,
      "success": true,
      "log_likelihood": -25.017621470941343,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.8212858869976349,
        "p": 0.0035752198264540954
      },
      "beta": {
        "r": 0.5234533684734824,
        "p": 0.12047868392716007
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 31.0399522741882,
  "bic_results": {
    "average_bic": 31.0399522741882,
    "individual_bics": [
      7.2116133457106475,
      41.624502288001985,
      27.62480515836525,
      33.789514076109604,
      26.864457249396807,
      28.803201321514045,
      11.907788567299118,
      29.6578871200143,
      11.907788567299118,
      29.897580232427238,
      18.334107751100085,
      20.761882939938708,
      24.807065084369476,
      11.907788567299118,
      11.90778856729912,
      11.90778856729912,
      11.90778856729912,
      19.545556523752275,
      22.47933153448923,
      11.907788567299118,
      38.8509705793571,
      24.414511607388214,
      30.767799933469618,
      40.430984140297184,
      36.244801297702026,
      37.53296238619351,
      23.57116128662617,
      15.142230629601768,
      26.120613340164887,
      40.454065220299476,
      11.90778856729912,
      7.2116133457106475,
      37.78509940714645,
      33.501593217067175,
      11.907788567299118,
      28.68883170904717,
      24.417921560145047,
      36.64328215460756,
      45.56494640829797,
      26.107055706809923,
      45.11049364938467,
      38.26404995963098,
      22.707788523018756,
      31.700952489871824,
      32.89922910166728,
      37.33838304990736,
      31.126622911941453,
      50.000587719558126,
      30.471577777831385,
      70.65728597309533,
      42.674696625946474,
      23.260456015698033,
      38.89731993275269,
      54.46203749746374,
      62.26294648346427,
      17.32872328732467,
      25.099539148116058,
      39.197641631062176,
      13.124238332332625,
      34.01186898280062,
      49.87297084448213,
      38.01158348335295,
      7.2116133457106475,
      50.26952711981129,
      18.522924678834407,
      30.4550586333496,
      23.917542751972256,
      30.765260112377405,
      32.78941638365741,
      31.799947532119404,
      7.655188706999533,
      14.9513315627132,
      38.015899501158906,
      36.67638962211612,
      67.12636777347429,
      30.766085142084485,
      22.78047086404787,
      42.191033588680924,
      67.66023601439633,
      7.2116133457106475,
      64.7633818155724,
      32.72768529648709,
      30.611576567292055,
      23.93984870835613,
      80.63371692248843,
      10.745829934130338,
      46.40220321147835,
      50.27071284238228,
      19.799947587870847,
      26.26952748554121,
      57.202280818794904
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.821\n- beta: r = 0.523",
  "previous_models": [
    "Specification: Utility = split_self - alpha * max(0, 3*token_self - split_self) - beta * max(0, split_opp - split_self)\nSummary: A dual-reference utility model evaluating offers against both contribution-based fairness (3*token_self) and relative opponent shares. Penalizes offers below the participant's contribution value (alpha) and offers where the opponent receives more (beta). Parameters capture distinct sensitivities to different fairness violations while maintaining orthogonal, recoverable effects.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 31.0399522741882\nOverall Accuracy: 0.8159\n\nParameter Recovery:\n- alpha: r = 0.821\n- beta: r = 0.523"
  ]
}