{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\nOkay, let's look at the previous results. The model had a BIC of 42.36, which is okay but not great. The accuracy was 70.1%, which is decent but could be better. The parameter recovery for alpha_gain was low (0.272), alpha_loss was better (0.78), and beta was moderate (0.693). So the main issues are improving alpha_gain's recovery and lowering BIC.\n\nThe previous model used separate gain and loss parameters (alpha_gain and alpha_loss) and a beta for inequity. The low recovery for alpha_gain suggests that the gain term isn't capturing enough variance or is collinear with other terms. Maybe the gain term isn't as influential as the loss term, leading to poor identifiability.\n\nTo improve, perhaps simplify the model by combining gain and loss into a single parameter with a loss aversion ratio. For example, use a single alpha for the difference (split_self - reference) and a lambda parameter that scales the loss term. This reduces the number of parameters, which can lower BIC and improve identifiability.\n\nAlso, the beta term's recovery was moderate. The previous model included trial_type modulation, but maybe the way it's incorporated isn't optimal. Instead of a multiplicative factor, maybe use trial_type to adjust the reference point or the inequity term differently. For example, in trials where the participant contributed more (trial_type=3), the inequity aversion could be more sensitive.\n\nAnother idea: Introduce a non-linear transformation for the loss term, like a power function with a fixed exponent (e.g., squared term) to capture diminishing sensitivity. This could help with parameter recovery by making the loss term more distinct.\n\nAdditionally, the model normalized the inequity term by combined_earning/10. Maybe scaling it differently, like by the square root of combined_earning, could better capture the context without overcomplicating the model.\n\nAlso, considering that the reference point is fixed at 3*token_self, perhaps include an interaction term between split_self and token_self to account for proportional fairness. For example, (split_self / (3*token_self)) - 1, which measures the percentage deviation from the fair share.\n\nBut to keep the model parsimonious, maybe focus on refining the existing components. Let's outline the new instructions:\n\n1. **Unified Loss Aversion Parameter**: Replace alpha_gain and alpha_loss with a single alpha (fairness sensitivity) and lambda (loss aversion multiplier). This reduces parameters and enforces lambda > 1, aligning with prospect theory.\n\n2. **Dynamic Inequity Modulation**: Use trial_type to adjust the inequity term. For trial_type=3 (participant contributed more), apply a multiplier to beta to increase sensitivity to unfair splits.\n\n3. **Non-Linear Loss Scaling**: Apply a convex transformation (e.g., squared term) to the loss component to enhance identifiability of lambda.\n\n4. **Proportional Fairness Adjustment**: Scale the gain/loss terms by the reference point to account for proportional deviations, improving sensitivity across different token amounts.\n\n5. **Parameter Bounds**: Set stricter bounds based on behavioral priors\u2014lambda between [1, 5], alpha [0, 3], beta [0, 10]\u2014to guide estimation and improve recovery.\n\nBy simplifying the parameters, introducing non-linear loss, and dynamically adjusting inequity based on trial_type, the model should better capture decision processes, leading to lower BIC, higher accuracy, and improved parameter recovery.\n</think>\n\n**Instructions for Run 31/200:**  \nDesign a *neurocognitive fairness model* with these innovations:  \n\n1. **Prospect-Theoretic Fusion**  \n   - Replace gain/loss separation with *unified fairness sensitivity*:  \n     ```  \n     U = \u03b1*(split_self - 3*token_self) - \u03bb*[max(3*token_self - split_self, 0)]^1.5  \n     ```  \n     Where \u03bb >1 enforces loss aversion (\u03bb \u2208 [1,5]) and \u03b1 \u2208 [0,3]  \n\n2. **Context-Aware Inequity Penalty**  \n   - Implement *trial-type dependent normalization*:  \n     ```  \n     inequity_penalty = \u03b2 * (split_opp - split_self) / (3*token_self if trial_type==3 else combined_earning)  \n     ```  \n     Penalize opponent advantage relative to contribution when participant contributed more  \n\n3. **Dynamic Reference Scaling**  \n   - Convert absolute splits to *proportional fairness deviations*:  \n     ```  \n     gain_term = split_self / (3*token_self + \u03b5) - 1  \n     ```  \n     (\u03b5=0.01 prevents division by zero)  \n\n**Orthogonality Constraints:**  \n- Fix loss exponent to 1.5 (diminishing sensitivity) while keeping gain linear  \n- Confine \u03b1 to [0,3], \u03bb to [1,5], \u03b2 to [0,10] using prospect theory meta-analysis priors  \n\n**Success Targets:**  \n- Achieve \u03b2 recovery r >0.8 via context-aware normalization  \n- Drive \u03bb recovery r >0.9 through convex loss architecture  \n- Target BIC <35 via fused parameter architecture",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: <think>\nOkay, let's look at the previous results. The model had a BIC of 42.36, which is okay but not great. The accuracy was 70.1%, which is decent but could be better. The parameter recovery for alpha_gain was low (0.272), alpha_loss was better (0.78), and beta was moderate (0.693). So the main issues are improving alpha_gain's recovery and lowering BIC.\n\nThe previous model used separate gain and loss parameters (alpha_gain and alpha_loss) and a beta for inequity. The low recovery for alpha_gain suggests that the gain term isn't capturing enough variance or is collinear with other terms. Maybe the gain term isn't as influential as the loss term, leading to poor identifiability.\n\nTo improve, perhaps simplify the model by combining gain and loss into a single parameter with a loss aversion ratio. For example, use a single alpha for the difference (split_self - reference) and a lambda parameter that scales the loss term. This reduces the number of parameters, which can lower BIC and improve identifiability.\n\nAlso, the beta term's recovery was moderate. The previous model included trial_type modulation, but maybe the way it's incorporated isn't optimal. Instead of a multiplicative factor, maybe use trial_type to adjust the reference point or the inequity term differently. For example, in trials where the participant contributed more (trial_type=3), the inequity aversion could be more sensitive.\n\nAnother idea: Introduce a non-linear transformation for the loss term, like a power function with a fixed exponent (e.g., squared term) to capture diminishing sensitivity. This could help with parameter recovery by making the loss term more distinct.\n\nAdditionally, the model normalized the inequity term by combined_earning/10. Maybe scaling it differently, like by the square root of combined_earning, could better capture the context without overcomplicating the model.\n\nAlso, considering that the reference point is fixed at 3*token_self, perhaps include an interaction term between split_self and token_self to account for proportional fairness. For example, (split_self / (3*token_self)) - 1, which measures the percentage deviation from the fair share.\n\nBut to keep the model parsimonious, maybe focus on refining the existing components. Let's outline the new instructions:\n\n1. **Unified Loss Aversion Parameter**: Replace alpha_gain and alpha_loss with a single alpha (fairness sensitivity) and lambda (loss aversion multiplier). This reduces parameters and enforces lambda > 1, aligning with prospect theory.\n\n2. **Dynamic Inequity Modulation**: Use trial_type to adjust the inequity term. For trial_type=3 (participant contributed more), apply a multiplier to beta to increase sensitivity to unfair splits.\n\n3. **Non-Linear Loss Scaling**: Apply a convex transformation (e.g., squared term) to the loss component to enhance identifiability of lambda.\n\n4. **Proportional Fairness Adjustment**: Scale the gain/loss terms by the reference point to account for proportional deviations, improving sensitivity across different token amounts.\n\n5. **Parameter Bounds**: Set stricter bounds based on behavioral priors\u2014lambda between [1, 5], alpha [0, 3], beta [0, 10]\u2014to guide estimation and improve recovery.\n\nBy simplifying the parameters, introducing non-linear loss, and dynamically adjusting inequity based on trial_type, the model should better capture decision processes, leading to lower BIC, higher accuracy, and improved parameter recovery.\n</think>\n\n**Instructions for Run 31/200:**  \nDesign a *neurocognitive fairness model* with these innovations:  \n\n1. **Prospect-Theoretic Fusion**  \n   - Replace gain/loss separation with *unified fairness sensitivity*:  \n     ```  \n     U = \u03b1*(split_self - 3*token_self) - \u03bb*[max(3*token_self - split_self, 0)]^1.5  \n     ```  \n     Where \u03bb >1 enforces loss aversion (\u03bb \u2208 [1,5]) and \u03b1 \u2208 [0,3]  \n\n2. **Context-Aware Inequity Penalty**  \n   - Implement *trial-type dependent normalization*:  \n     ```  \n     inequity_penalty = \u03b2 * (split_opp - split_self) / (3*token_self if trial_type==3 else combined_earning)  \n     ```  \n     Penalize opponent advantage relative to contribution when participant contributed more  \n\n3. **Dynamic Reference Scaling**  \n   - Convert absolute splits to *proportional fairness deviations*:  \n     ```  \n     gain_term = split_self / (3*token_self + \u03b5) - 1  \n     ```  \n     (\u03b5=0.01 prevents division by zero)  \n\n**Orthogonality Constraints:**  \n- Fix loss exponent to 1.5 (diminishing sensitivity) while keeping gain linear  \n- Confine \u03b1 to [0,3], \u03bb to [1,5], \u03b2 to [0,10] using prospect theory meta-analysis priors  \n\n**Success Targets:**  \n- Achieve \u03b2 recovery r >0.8 via context-aware normalization  \n- Drive \u03bb recovery r >0.9 through convex loss architecture  \n- Target BIC <35 via fused parameter architecture\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant accepts or rejects a proposal in the responder role. The model needs to focus on utility calculation, which then translates into the probability of acceptance via a logistic function.\n\nFirst, I need to identify the key variables that influence the responder's decision. From the dataset, the important variables related to the proposal are split_self (the amount offered to the participant), split_opp (the opponent's share), token_self, and token_opp. The combined_earning is also crucial because it represents the total pot, which affects the fairness perception.\n\nThe model should incorporate both the participant's gain and fairness considerations. A common approach is to use a utility function that combines the participant's monetary gain with a term that accounts for inequity aversion. Inequity aversion suggests that people dislike unequal outcomes, even if it means sacrificing some personal gain.\n\nSo, the utility (U) could be modeled as the offered amount (split_self) minus some measure of inequity. The inequity term might depend on the difference between the participant's share and the opponent's share, adjusted by their respective contributions (tokens found). \n\nBut wait, the problem states that two equal split_self values don't represent the same fairness because the combined earnings vary. This means the fairness should be relative to the contributions. The participant's contribution is token_self, each worth \u00a33, so their fair share would be 3 * token_self. Similarly, the opponent's contribution is 3 * token_opp. The total combined_earning is 3*(token_self + token_opp). \n\nTherefore, a fair split would be proportional to their contributions. If the participant's share (split_self) is less than their fair share (3*token_self), they might perceive it as unfair. Similarly, if the opponent is getting more than their contribution warrants, that could also affect the utility.\n\nLet me structure the utility function. The monetary component is straightforward: split_self. The fairness component needs to compare the proposed split to the fair split based on contributions. \n\nFor inequity aversion, perhaps we can model it as a dislike for situations where the participant's share is less than their fair contribution (split_self < 3*token_self) and also when the opponent's share is more than their contribution (split_opp > 3*token_opp). But since split_opp is the opponent's proposed share, and split_self is the participant's, maybe the difference between split_self and the fair share is key.\n\nAlternatively, the model could consider the difference between the participant's share and their fair share, and the opponent's share minus their fair share. But since split_opp + split_self = combined_earning, maybe we can express everything in terms of split_self.\n\nWait, the fair split based on contributions would be: participant's fair share = (3*token_self / combined_earning) * combined_earning = 3*token_self. Similarly for the opponent. So if split_self is less than 3*token_self, the participant is getting less than their contribution, which is unfair. The utility loss here could be modeled as a parameter multiplied by the difference (3*token_self - split_self) if it's positive (a loss), and similarly for gains if split_self is more than fair.\n\nBut the problem states that the utility should be such that less than 0 utility corresponds to less than 50% likelihood of acceptance. So the utility function's sign directly affects the probability.\n\nLet me think of a prospect theory-inspired approach where losses (getting less than fair) are weighted more heavily than gains. So, the utility could be:\n\nU = split_self - reference_point + \u03b1 * max(reference_point - split_self, 0) + \u03b2 * (split_opp - reference_opp)\n\nBut reference_point here is 3*token_self, and reference_opp is 3*token_opp. However, since split_opp = combined_earning - split_self, maybe we can express it in terms of split_self.\n\nAlternatively, the inequity could be modeled as the difference between split_self and the fair share, and the difference between split_opp and the opponent's fair share. But since the participant might care about both, perhaps the utility includes terms for both under-receiving and over-receiving.\n\nBut to simplify, maybe the key term is the participant's gain compared to their fair share, and the difference between the proposed split and the fair split. Let's try:\n\nU = split_self + \u03b1 * (split_self - 3*token_self) - \u03b2 * |split_self - 3*token_self|\n\nWait, that might not capture the direction. Alternatively, separate parameters for gains and losses relative to the fair share:\n\nU = split_self + \u03b1_gain * max(split_self - 3*token_self, 0) - \u03b1_loss * max(3*token_self - split_self, 0)\n\nHere, \u03b1_gain is the sensitivity to getting more than fair, and \u03b1_loss is sensitivity to getting less. But since people are loss-averse, \u03b1_loss would be larger than \u03b1_gain. However, the model needs learnable parameters with bounds. So \u03b1_loss and \u03b1_gain could be separate parameters, both positive.\n\nAdditionally, the participant might care about the opponent's share relative to their contribution. So another term could be the difference between split_opp and 3*token_opp. But since split_opp = combined_earning - split_self, and 3*token_opp = combined_earning - 3*token_self (because combined_earning is 3*(token_self + token_opp)), then split_opp - 3*token_opp = (combined_earning - split_self) - (combined_earning - 3*token_self) = 3*token_self - split_self. So that term is the same as the participant's loss. Therefore, maybe the opponent's share relative to their contribution is already captured in the participant's loss term.\n\nAlternatively, the participant might compare their share to the opponent's share in absolute terms, leading to inequity aversion. For example, Fehr & Schmidt's model: U = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0). But in this case, the reference is the opponent's share, not their contribution.\n\nBut the problem states that the fairness isn't just about equal splits but relative to contributions. So the Fehr & Schmidt model might not directly apply here. We need to adjust for the contributions.\n\nSo perhaps the utility function should compare the proposed split to the fair split based on contributions, and also consider the difference between the participant's and opponent's shares.\n\nLet me try combining these ideas:\n\nU = split_self + \u03b1 * (split_self - 3*token_self) - \u03b2 * (split_opp - 3*token_opp)\n\nBut split_opp is combined_earning - split_self, and 3*token_opp is combined_earning - 3*token_self. So substituting:\n\nsplit_opp - 3*token_opp = (combined_earning - split_self) - (combined_earning - 3*token_self) = 3*token_self - split_self\n\nSo the term becomes -\u03b2*(3*token_self - split_self). Therefore, the utility simplifies to:\n\nU = split_self + \u03b1*(split_self - 3*token_self) - \u03b2*(3*token_self - split_self)\n\nWhich can be rewritten as:\n\nU = split_self + (\u03b1 + \u03b2)*(split_self - 3*token_self)\n\nBut this seems to combine \u03b1 and \u03b2 into a single coefficient, which might not be ideal. Alternatively, maybe the model should have separate terms for over and under the fair share.\n\nAlternatively, let's model the utility as:\n\nU = split_self - \u03b3 * max(3*token_self - split_self, 0) - \u03b4 * max(split_self - 3*token_self, 0) + \u03b8 * (split_opp - 3*token_opp)\n\nBut this might be getting too complex. Let's think about the key factors:\n\n1. The monetary value of the offer (split_self).\n2. The disutility from receiving less than their fair share (3*token_self - split_self if positive).\n3. The disutility from the opponent receiving more than their fair share (split_opp - 3*token_opp if positive).\n\nBut split_opp - 3*token_opp = (combined_earning - split_self) - (combined_earning - 3*token_self) = 3*token_self - split_self. So this term is the same as the participant's under-receiving. Therefore, including both might be redundant.\n\nThus, perhaps the key term is the participant's under-receiving relative to their contribution. So the utility could be:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * max(split_opp - 3*token_opp, 0)\n\nBut as established earlier, split_opp - 3*token_opp = 3*token_self - split_self. So this becomes:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * max(3*token_self - split_self, 0)\n\nWhich simplifies to:\n\nU = split_self - (\u03b1 + \u03b2) * max(3*token_self - split_self, 0)\n\nThis suggests that the parameters \u03b1 and \u03b2 are not identifiable separately. Therefore, perhaps combining them into a single parameter.\n\nAlternatively, maybe the participant cares about both their own under-receiving and the opponent's over-receiving, even if they are mathematically related. But in this case, they are the same, so including both would lead to collinearity.\n\nTherefore, perhaps the model should focus on the participant's under-receiving relative to their contribution and the difference in shares.\n\nAlternatively, consider the ratio of the participant's share to their fair share. For example, if split_self / (3*token_self) is less than 1, that's a loss. But using ratios might complicate the model, especially when token_self is zero (though the dataset probably doesn't have that as each participant contributes some tokens).\n\nAlternatively, use a non-linear function, but the problem states that the utility should be such that U < 0 corresponds to less than 50% acceptance. So keeping it linear might be better for interpretability.\n\nLet me try this approach:\n\nThe reference point is the fair share: 3*token_self. The utility gain or loss is split_self - 3*token_self. Then, the utility function could have different coefficients for gains and losses (prospect theory-like).\n\nSo:\n\nU = split_self + \u03b1_gain * max(split_self - 3*token_self, 0) - \u03b1_loss * max(3*token_self - split_self, 0)\n\nHere, \u03b1_gain and \u03b1_loss are parameters. If the participant is loss-averse, \u03b1_loss > \u03b1_gain. But since the model needs to predict acceptance, which is a binary variable, the utility U is passed through a logistic function to get the probability of acceptance.\n\nAdditionally, the participant might also care about the opponent's share relative to their own. For example, if the opponent is getting a much higher share, even if the participant is getting their fair share, they might reject out of fairness. So perhaps adding a term that accounts for the difference in shares:\n\nU = split_self + \u03b1_gain * max(split_self - 3*token_self, 0) - \u03b1_loss * max(3*token_self - split_self, 0) - \u03b2 * (split_opp - split_self)\n\nBut split_opp is combined_earning - split_self, so split_opp - split_self = combined_earning - 2*split_self. Alternatively, the difference could be normalized by combined_earning to make it a proportion.\n\nAlternatively, the inequity term could be (split_opp / (3*token_opp)) - (split_self / (3*token_self)), representing the relative fairness. But this might complicate the model.\n\nGiven the complexity, perhaps the initial model should focus on the participant's gain/loss relative to their fair share and an inequity term based on the difference between the proposed split and the fair split.\n\nPutting it all together:\n\nU = split_self - \u03b1_loss * max(3*token_self - split_self, 0) + \u03b1_gain * max(split_self - 3*token_self, 0) - \u03b2 * ( (split_opp - 3*token_opp) - (split_self - 3*token_self) )\n\nBut simplifying this, since split_opp - 3*token_opp = (combined_earning - split_self) - (combined_earning - 3*token_self) = 3*token_self - split_self. So the term becomes:\n\n- \u03b2 * ( (3*token_self - split_self) - (split_self - 3*token_self) ) = -\u03b2 * (2*(3*token_self - split_self)) if split_self < 3*token_self.\n\nThis seems redundant. Maybe the inequity term should be the absolute difference between the participant's and opponent's shares relative to their contributions.\n\nAlternatively, the model could include a term that captures the difference between the participant's share deviation and the opponent's share deviation:\n\nsplit_self - 3*token_self vs. split_opp - 3*token_opp. The participant might dislike if the opponent's deviation is better than theirs. So, the term could be max( (split_opp - 3*token_opp) - (split_self - 3*token_self), 0 ), which is the excess of the opponent's gain over the participant's. But this might be too complex.\n\nAlternatively, use the difference in deviations: (split_self - 3*token_self) - (split_opp - 3*token_opp). If this is negative, the participant is worse off relative to their contribution compared to the opponent. So the utility could include a term like -\u03b2 * max( (split_opp - 3*token_opp) - (split_self - 3*token_self), 0 ), which penalizes when the opponent's gain over their contribution is more than the participant's.\n\nBut this is getting complicated. Let's simplify. The key factors are:\n\n1. The monetary value of the offer (split_self).\n2. The loss/gain relative to the fair share (3*token_self).\n3. The inequity between the participant's and opponent's shares relative to their contributions.\n\nA possible model is:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * max( (split_opp - 3*token_opp) - (split_self - 3*token_self), 0 )\n\nHere, \u03b1 captures the sensitivity to getting less than the fair share, and \u03b2 captures the sensitivity to the opponent having a better relative gain. \n\nBut split_opp - 3*token_opp = 3*token_self - split_self, as before. So the term inside the max becomes (3*token_self - split_self) - (split_self - 3*token_self) if split_self > 3*token_self. Wait, no. Let's compute:\n\nsplit_opp - 3*token_opp = (combined_earning - split_self) - (3*(token_self + token_opp) - 3*token_self) ) = (combined_earning - split_self) - 3*token_opp = 3*token_self + 3*token_opp - split_self - 3*token_opp = 3*token_self - split_self.\n\nSo split_opp - 3*token_opp = 3*token_self - split_self.\n\nSimilarly, split_self - 3*token_self is the participant's gain over their fair share.\n\nSo the term (split_opp - 3*token_opp) - (split_self - 3*token_self) becomes (3*token_self - split_self) - (split_self - 3*token_self) = 6*token_self - 2*split_self.\n\nThis seems messy. Maybe this approach isn't the best.\n\nAlternative approach: Use the difference between the participant's share ratio and the opponent's share ratio. The fair share ratio for the participant is (3*token_self)/combined_earning, and similarly for the opponent. The proposed ratios are split_self/combined_earning and split_opp/combined_earning. The difference between these ratios could be a measure of inequity.\n\nSo, the participant's ratio minus their fair ratio: (split_self / combined_earning) - (3*token_self / combined_earning) = (split_self - 3*token_self)/combined_earning.\n\nSimilarly for the opponent: (split_opp / combined_earning) - (3*token_opp / combined_earning) = (split_opp - 3*token_opp)/combined_earning = (3*token_self - split_self)/combined_earning.\n\nThe difference between these two ratios is [(split_self - 3*token_self) - (3*token_self - split_self)] / combined_earning = [2*(split_self - 3*token_self)] / combined_earning.\n\nBut this might not capture the intended inequity. Alternatively, the sum of the squared differences of the ratios could be a term, but that complicates the model.\n\nPerhaps the simplest way is to model the utility as:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * (split_opp - split_self)\n\nBut split_opp - split_self = combined_earning - 2*split_self. This term represents how much more the opponent is getting compared to the participant. If the participant dislikes inequity, they would penalize this difference. However, this doesn't account for contributions. For example, if the opponent contributed more, maybe the participant is okay with them getting more.\n\nBut the trial_type variable indicates whether the split is equal, opponent more, or participant more. This could modulate the \u03b2 parameter. For instance, in trials where the participant contributed more (trial_type=3), they might be more sensitive to receiving less than their fair share.\n\nHowever, the model should use the variables provided, and trial_type is available. But the problem states that the model should predict responder behavior, and trial_role is 1 for responder. So perhaps trial_type can be used to adjust the parameters.\n\nBut to keep the model applicable using the provided variables, maybe incorporate trial_type into the utility function. For example, have different \u03b1 or \u03b2 parameters depending on trial_type. However, this would require interaction terms, increasing the number of parameters.\n\nAlternatively, since trial_type categorizes the contribution comparison, it could influence the reference point or the weight of the inequity term.\n\nBut given the complexity, perhaps the initial model should focus on the key variables: split_self, token_self, and the difference from the fair share, along with an inequity term.\n\nPutting this together, here's a possible model:\n\nUtility U is calculated as:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * (split_opp - 3*token_opp)\n\nBut split_opp - 3*token_opp = 3*token_self - split_self, so:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * (3*token_self - split_self)\n\nThis can be rewritten as:\n\nU = split_self - (\u03b1 + \u03b2) * max(3*token_self - split_self, 0) - \u03b2 * (3*token_self - split_self - max(3*token_self - split_self, 0))\n\nBut this seems redundant. Alternatively, if split_self < 3*token_self, then max(...) is 3*token_self - split_self, and the term becomes:\n\nU = split_self - (\u03b1 + \u03b2) * (3*token_self - split_self)\n\nIf split_self >= 3*token_self, then max(...) is 0, and the term is:\n\nU = split_self - \u03b2 * (3*token_self - split_self)\n\nBut this would mean that even when the participant gets more than their fair share, they still have a penalty term, which doesn't make sense. So perhaps the \u03b2 term should only apply when the opponent is getting more than their fair share, which is when split_opp > 3*token_opp. But split_opp > 3*token_opp implies split_self < 3*token_self, since split_opp = combined_earning - split_self and 3*token_opp = combined_earning - 3*token_self. Therefore, split_opp > 3*token_opp is equivalent to split_self < 3*token_self.\n\nThus, the term split_opp - 3*token_opp is equal to 3*token_self - split_self, which is positive only when split_self < 3*token_self. Therefore, the \u03b2 term is only active in that case. So the model can be simplified to:\n\nU = split_self - \u03b1 * (3*token_self - split_self) - \u03b2 * (3*token_self - split_self)\n\nWhen split_self < 3*token_self:\n\nU = split_self - (\u03b1 + \u03b2) * (3*token_self - split_self)\n\nWhen split_self >= 3*token_self:\n\nU = split_self\n\nBut this assumes that when the participant gets their fair share or more, there's no inequity penalty. However, the participant might still care if the opponent is getting less than their fair share, but that's when split_self > 3*token_self, which means split_opp = combined_earning - split_self < 3*token_opp. In that case, the opponent is getting less than their fair share, but the participant might not care as much. Alternatively, they might feel guilty and have a negative utility from that. But this complicates the model further.\n\nGiven the problem constraints, perhaps the model should focus on the participant's loss relative to their fair share and the overall inequity. Let's proceed with:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * (split_opp - split_self) / combined_earning\n\nHere, the second term is the loss from receiving less than the fair share, and the third term is the inequity between the opponent's and participant's shares, normalized by the total combined earning to account for different pot sizes. The division by combined_earning makes the inequity term a proportion, which addresses the issue that the same split difference represents different fairness levels depending on the total pot.\n\nThis model uses split_self, token_self, split_opp, and combined_earning. The parameters \u03b1 and \u03b2 are learnable, with \u03b1 representing the sensitivity to receiving less than the fair share, and \u03b2 representing the sensitivity to inequity (how much more the opponent is getting relative to the participant).\n\nThe utility U is then passed through a logistic function to determine the probability of acceptance: P(accept) = 1 / (1 + exp(-U)). If U < 0, the probability is less than 0.5, which aligns with the requirement.\n\nNow, specifying the variables:\n\n- \u03b1 (alpha_loss): Sensitivity to loss (receiving less than fair share). Bounded between 0 and a large value, say 10.\n- \u03b2 (beta_inequity): Sensitivity to inequity (opponent's share minus participant's share, normalized). Bounded between 0 and 10.\n- split_self: From data.\n- token_self: From data.\n- split_opp: From data.\n- combined_earning: From data.\n\nThe model formula in LaTeX:\n\nU = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}\n\nBut since split_opp = combined_earning - split_self, this can be simplified:\n\nsplit_opp - split_self = combined_earning - 2*split_self\n\nSo the term becomes (combined_earning - 2*split_self)/combined_earning = 1 - 2*(split_self / combined_earning)\n\nBut perhaps keeping it as (split_opp - split_self)/combined_earning is clearer.\n\nNow, checking the variables in the dataset:\n\nThe variables used are split_self, token_self, split_opp, combined_earning. All are present in the dataset.\n\nThe target variable is accept, which is binary. The model's U will be used in a logistic regression to predict accept.\n\nFor parameter bounds, \u03b1 and \u03b2 should be positive as they represent disutility. So \u03b1 \u2208 [0, 10], \u03b2 \u2208 [0, 10], for example.\n\nThus, the final model is:\n\n<MODEL>\nU = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}\n</MODEL>\n\nVariables:\n\n- \u03b1 (alpha): sensitivity to loss, learnable, range [0, 10]\n- \u03b2 (beta): sensitivity to inequity, learnable, range [0, 10]\n- split_self: from data\n- token_self: from data\n- split_opp: from data\n- combined_earning: from data\n\nThe variable descriptions in JSON format would include these variables, specifying their ranges, learnability, and sources.\n</think>\n\n<MODEL>\nU = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Loss sensitivity parameter for receiving less than fair share (3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity aversion parameter for opponent's share advantage normalized by total pot\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant (fair share = 3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for opponent in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total pot size in pounds (3*(token_self + token_opp))\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-component utility model combining loss aversion relative to contribution-based fairness and normalized inequity aversion. The utility function subtracts 1) a loss term scaled by \u03b1 when receiving less than 3*token_self and 2) a normalized opponent advantage term scaled by \u03b2. Parameters are constrained to positive values to enforce theoretically consistent directionality.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['alpha', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Loss sensitivity parameter for receiving less than fair share (3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity aversion parameter for opponent's share advantage normalized by total pot\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant (fair share = 3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for opponent in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total pot size in pounds (3*(token_self + token_opp))\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n\n        loss = max(3 * token_self - split_self, 0)\n        inequity = (split_opp - split_self) / combined_earning if combined_earning > 0 else 0\n\n        U = split_self - alpha * loss - beta * inequity\n        model_predictions.append(U)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n",
      "timestamp": 1744017266.567432
    }
  ],
  "learnable_parameters": {
    "alpha": {
      "description": "Loss sensitivity parameter for receiving less than fair share (3*token_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Inequity aversion parameter for opponent's share advantage normalized by total pot",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}",
  "variable_descriptions": {
    "U": {
      "description": "Utility of accepting the proposal",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "calculated"
    },
    "alpha": {
      "description": "Loss sensitivity parameter for receiving less than fair share (3*token_self)",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Inequity aversion parameter for opponent's share advantage normalized by total pot",
      "range": {
        "min": 0,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Proposed share for participant in pounds",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Number of tokens found by participant (fair share = 3*token_self)",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_opp": {
      "description": "Proposed share for opponent in pounds",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": false,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "combined_earning": {
      "description": "Total pot size in pounds (3*(token_self + token_opp))",
      "range": {
        "min": 0,
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": false
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A dual-component utility model combining loss aversion relative to contribution-based fairness and normalized inequity aversion. The utility function subtracts 1) a loss term scaled by \u03b1 when receiving less than 3*token_self and 2) a normalized opponent advantage term scaled by \u03b2. Parameters are constrained to positive values to enforce theoretically consistent directionality.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant accepts or rejects a proposal in the responder role. The model needs to focus on utility calculation, which then translates into the probability of acceptance via a logistic function.\n\nFirst, I need to identify the key variables that influence the responder's decision. From the dataset, the important variables related to the proposal are split_self (the amount offered to the participant), split_opp (the opponent's share), token_self, and token_opp. The combined_earning is also crucial because it represents the total pot, which affects the fairness perception.\n\nThe model should incorporate both the participant's gain and fairness considerations. A common approach is to use a utility function that combines the participant's monetary gain with a term that accounts for inequity aversion. Inequity aversion suggests that people dislike unequal outcomes, even if it means sacrificing some personal gain.\n\nSo, the utility (U) could be modeled as the offered amount (split_self) minus some measure of inequity. The inequity term might depend on the difference between the participant's share and the opponent's share, adjusted by their respective contributions (tokens found). \n\nBut wait, the problem states that two equal split_self values don't represent the same fairness because the combined earnings vary. This means the fairness should be relative to the contributions. The participant's contribution is token_self, each worth \u00a33, so their fair share would be 3 * token_self. Similarly, the opponent's contribution is 3 * token_opp. The total combined_earning is 3*(token_self + token_opp). \n\nTherefore, a fair split would be proportional to their contributions. If the participant's share (split_self) is less than their fair share (3*token_self), they might perceive it as unfair. Similarly, if the opponent is getting more than their contribution warrants, that could also affect the utility.\n\nLet me structure the utility function. The monetary component is straightforward: split_self. The fairness component needs to compare the proposed split to the fair split based on contributions. \n\nFor inequity aversion, perhaps we can model it as a dislike for situations where the participant's share is less than their fair contribution (split_self < 3*token_self) and also when the opponent's share is more than their contribution (split_opp > 3*token_opp). But since split_opp is the opponent's proposed share, and split_self is the participant's, maybe the difference between split_self and the fair share is key.\n\nAlternatively, the model could consider the difference between the participant's share and their fair share, and the opponent's share minus their fair share. But since split_opp + split_self = combined_earning, maybe we can express everything in terms of split_self.\n\nWait, the fair split based on contributions would be: participant's fair share = (3*token_self / combined_earning) * combined_earning = 3*token_self. Similarly for the opponent. So if split_self is less than 3*token_self, the participant is getting less than their contribution, which is unfair. The utility loss here could be modeled as a parameter multiplied by the difference (3*token_self - split_self) if it's positive (a loss), and similarly for gains if split_self is more than fair.\n\nBut the problem states that the utility should be such that less than 0 utility corresponds to less than 50% likelihood of acceptance. So the utility function's sign directly affects the probability.\n\nLet me think of a prospect theory-inspired approach where losses (getting less than fair) are weighted more heavily than gains. So, the utility could be:\n\nU = split_self - reference_point + \u03b1 * max(reference_point - split_self, 0) + \u03b2 * (split_opp - reference_opp)\n\nBut reference_point here is 3*token_self, and reference_opp is 3*token_opp. However, since split_opp = combined_earning - split_self, maybe we can express it in terms of split_self.\n\nAlternatively, the inequity could be modeled as the difference between split_self and the fair share, and the difference between split_opp and the opponent's fair share. But since the participant might care about both, perhaps the utility includes terms for both under-receiving and over-receiving.\n\nBut to simplify, maybe the key term is the participant's gain compared to their fair share, and the difference between the proposed split and the fair split. Let's try:\n\nU = split_self + \u03b1 * (split_self - 3*token_self) - \u03b2 * |split_self - 3*token_self|\n\nWait, that might not capture the direction. Alternatively, separate parameters for gains and losses relative to the fair share:\n\nU = split_self + \u03b1_gain * max(split_self - 3*token_self, 0) - \u03b1_loss * max(3*token_self - split_self, 0)\n\nHere, \u03b1_gain is the sensitivity to getting more than fair, and \u03b1_loss is sensitivity to getting less. But since people are loss-averse, \u03b1_loss would be larger than \u03b1_gain. However, the model needs learnable parameters with bounds. So \u03b1_loss and \u03b1_gain could be separate parameters, both positive.\n\nAdditionally, the participant might care about the opponent's share relative to their contribution. So another term could be the difference between split_opp and 3*token_opp. But since split_opp = combined_earning - split_self, and 3*token_opp = combined_earning - 3*token_self (because combined_earning is 3*(token_self + token_opp)), then split_opp - 3*token_opp = (combined_earning - split_self) - (combined_earning - 3*token_self) = 3*token_self - split_self. So that term is the same as the participant's loss. Therefore, maybe the opponent's share relative to their contribution is already captured in the participant's loss term.\n\nAlternatively, the participant might compare their share to the opponent's share in absolute terms, leading to inequity aversion. For example, Fehr & Schmidt's model: U = split_self - \u03b1 * max(split_opp - split_self, 0) - \u03b2 * max(split_self - split_opp, 0). But in this case, the reference is the opponent's share, not their contribution.\n\nBut the problem states that the fairness isn't just about equal splits but relative to contributions. So the Fehr & Schmidt model might not directly apply here. We need to adjust for the contributions.\n\nSo perhaps the utility function should compare the proposed split to the fair split based on contributions, and also consider the difference between the participant's and opponent's shares.\n\nLet me try combining these ideas:\n\nU = split_self + \u03b1 * (split_self - 3*token_self) - \u03b2 * (split_opp - 3*token_opp)\n\nBut split_opp is combined_earning - split_self, and 3*token_opp is combined_earning - 3*token_self. So substituting:\n\nsplit_opp - 3*token_opp = (combined_earning - split_self) - (combined_earning - 3*token_self) = 3*token_self - split_self\n\nSo the term becomes -\u03b2*(3*token_self - split_self). Therefore, the utility simplifies to:\n\nU = split_self + \u03b1*(split_self - 3*token_self) - \u03b2*(3*token_self - split_self)\n\nWhich can be rewritten as:\n\nU = split_self + (\u03b1 + \u03b2)*(split_self - 3*token_self)\n\nBut this seems to combine \u03b1 and \u03b2 into a single coefficient, which might not be ideal. Alternatively, maybe the model should have separate terms for over and under the fair share.\n\nAlternatively, let's model the utility as:\n\nU = split_self - \u03b3 * max(3*token_self - split_self, 0) - \u03b4 * max(split_self - 3*token_self, 0) + \u03b8 * (split_opp - 3*token_opp)\n\nBut this might be getting too complex. Let's think about the key factors:\n\n1. The monetary value of the offer (split_self).\n2. The disutility from receiving less than their fair share (3*token_self - split_self if positive).\n3. The disutility from the opponent receiving more than their fair share (split_opp - 3*token_opp if positive).\n\nBut split_opp - 3*token_opp = (combined_earning - split_self) - (combined_earning - 3*token_self) = 3*token_self - split_self. So this term is the same as the participant's under-receiving. Therefore, including both might be redundant.\n\nThus, perhaps the key term is the participant's under-receiving relative to their contribution. So the utility could be:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * max(split_opp - 3*token_opp, 0)\n\nBut as established earlier, split_opp - 3*token_opp = 3*token_self - split_self. So this becomes:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * max(3*token_self - split_self, 0)\n\nWhich simplifies to:\n\nU = split_self - (\u03b1 + \u03b2) * max(3*token_self - split_self, 0)\n\nThis suggests that the parameters \u03b1 and \u03b2 are not identifiable separately. Therefore, perhaps combining them into a single parameter.\n\nAlternatively, maybe the participant cares about both their own under-receiving and the opponent's over-receiving, even if they are mathematically related. But in this case, they are the same, so including both would lead to collinearity.\n\nTherefore, perhaps the model should focus on the participant's under-receiving relative to their contribution and the difference in shares.\n\nAlternatively, consider the ratio of the participant's share to their fair share. For example, if split_self / (3*token_self) is less than 1, that's a loss. But using ratios might complicate the model, especially when token_self is zero (though the dataset probably doesn't have that as each participant contributes some tokens).\n\nAlternatively, use a non-linear function, but the problem states that the utility should be such that U < 0 corresponds to less than 50% acceptance. So keeping it linear might be better for interpretability.\n\nLet me try this approach:\n\nThe reference point is the fair share: 3*token_self. The utility gain or loss is split_self - 3*token_self. Then, the utility function could have different coefficients for gains and losses (prospect theory-like).\n\nSo:\n\nU = split_self + \u03b1_gain * max(split_self - 3*token_self, 0) - \u03b1_loss * max(3*token_self - split_self, 0)\n\nHere, \u03b1_gain and \u03b1_loss are parameters. If the participant is loss-averse, \u03b1_loss > \u03b1_gain. But since the model needs to predict acceptance, which is a binary variable, the utility U is passed through a logistic function to get the probability of acceptance.\n\nAdditionally, the participant might also care about the opponent's share relative to their own. For example, if the opponent is getting a much higher share, even if the participant is getting their fair share, they might reject out of fairness. So perhaps adding a term that accounts for the difference in shares:\n\nU = split_self + \u03b1_gain * max(split_self - 3*token_self, 0) - \u03b1_loss * max(3*token_self - split_self, 0) - \u03b2 * (split_opp - split_self)\n\nBut split_opp is combined_earning - split_self, so split_opp - split_self = combined_earning - 2*split_self. Alternatively, the difference could be normalized by combined_earning to make it a proportion.\n\nAlternatively, the inequity term could be (split_opp / (3*token_opp)) - (split_self / (3*token_self)), representing the relative fairness. But this might complicate the model.\n\nGiven the complexity, perhaps the initial model should focus on the participant's gain/loss relative to their fair share and an inequity term based on the difference between the proposed split and the fair split.\n\nPutting it all together:\n\nU = split_self - \u03b1_loss * max(3*token_self - split_self, 0) + \u03b1_gain * max(split_self - 3*token_self, 0) - \u03b2 * ( (split_opp - 3*token_opp) - (split_self - 3*token_self) )\n\nBut simplifying this, since split_opp - 3*token_opp = (combined_earning - split_self) - (combined_earning - 3*token_self) = 3*token_self - split_self. So the term becomes:\n\n- \u03b2 * ( (3*token_self - split_self) - (split_self - 3*token_self) ) = -\u03b2 * (2*(3*token_self - split_self)) if split_self < 3*token_self.\n\nThis seems redundant. Maybe the inequity term should be the absolute difference between the participant's and opponent's shares relative to their contributions.\n\nAlternatively, the model could include a term that captures the difference between the participant's share deviation and the opponent's share deviation:\n\nsplit_self - 3*token_self vs. split_opp - 3*token_opp. The participant might dislike if the opponent's deviation is better than theirs. So, the term could be max( (split_opp - 3*token_opp) - (split_self - 3*token_self), 0 ), which is the excess of the opponent's gain over the participant's. But this might be too complex.\n\nAlternatively, use the difference in deviations: (split_self - 3*token_self) - (split_opp - 3*token_opp). If this is negative, the participant is worse off relative to their contribution compared to the opponent. So the utility could include a term like -\u03b2 * max( (split_opp - 3*token_opp) - (split_self - 3*token_self), 0 ), which penalizes when the opponent's gain over their contribution is more than the participant's.\n\nBut this is getting complicated. Let's simplify. The key factors are:\n\n1. The monetary value of the offer (split_self).\n2. The loss/gain relative to the fair share (3*token_self).\n3. The inequity between the participant's and opponent's shares relative to their contributions.\n\nA possible model is:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * max( (split_opp - 3*token_opp) - (split_self - 3*token_self), 0 )\n\nHere, \u03b1 captures the sensitivity to getting less than the fair share, and \u03b2 captures the sensitivity to the opponent having a better relative gain. \n\nBut split_opp - 3*token_opp = 3*token_self - split_self, as before. So the term inside the max becomes (3*token_self - split_self) - (split_self - 3*token_self) if split_self > 3*token_self. Wait, no. Let's compute:\n\nsplit_opp - 3*token_opp = (combined_earning - split_self) - (3*(token_self + token_opp) - 3*token_self) ) = (combined_earning - split_self) - 3*token_opp = 3*token_self + 3*token_opp - split_self - 3*token_opp = 3*token_self - split_self.\n\nSo split_opp - 3*token_opp = 3*token_self - split_self.\n\nSimilarly, split_self - 3*token_self is the participant's gain over their fair share.\n\nSo the term (split_opp - 3*token_opp) - (split_self - 3*token_self) becomes (3*token_self - split_self) - (split_self - 3*token_self) = 6*token_self - 2*split_self.\n\nThis seems messy. Maybe this approach isn't the best.\n\nAlternative approach: Use the difference between the participant's share ratio and the opponent's share ratio. The fair share ratio for the participant is (3*token_self)/combined_earning, and similarly for the opponent. The proposed ratios are split_self/combined_earning and split_opp/combined_earning. The difference between these ratios could be a measure of inequity.\n\nSo, the participant's ratio minus their fair ratio: (split_self / combined_earning) - (3*token_self / combined_earning) = (split_self - 3*token_self)/combined_earning.\n\nSimilarly for the opponent: (split_opp / combined_earning) - (3*token_opp / combined_earning) = (split_opp - 3*token_opp)/combined_earning = (3*token_self - split_self)/combined_earning.\n\nThe difference between these two ratios is [(split_self - 3*token_self) - (3*token_self - split_self)] / combined_earning = [2*(split_self - 3*token_self)] / combined_earning.\n\nBut this might not capture the intended inequity. Alternatively, the sum of the squared differences of the ratios could be a term, but that complicates the model.\n\nPerhaps the simplest way is to model the utility as:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * (split_opp - split_self)\n\nBut split_opp - split_self = combined_earning - 2*split_self. This term represents how much more the opponent is getting compared to the participant. If the participant dislikes inequity, they would penalize this difference. However, this doesn't account for contributions. For example, if the opponent contributed more, maybe the participant is okay with them getting more.\n\nBut the trial_type variable indicates whether the split is equal, opponent more, or participant more. This could modulate the \u03b2 parameter. For instance, in trials where the participant contributed more (trial_type=3), they might be more sensitive to receiving less than their fair share.\n\nHowever, the model should use the variables provided, and trial_type is available. But the problem states that the model should predict responder behavior, and trial_role is 1 for responder. So perhaps trial_type can be used to adjust the parameters.\n\nBut to keep the model applicable using the provided variables, maybe incorporate trial_type into the utility function. For example, have different \u03b1 or \u03b2 parameters depending on trial_type. However, this would require interaction terms, increasing the number of parameters.\n\nAlternatively, since trial_type categorizes the contribution comparison, it could influence the reference point or the weight of the inequity term.\n\nBut given the complexity, perhaps the initial model should focus on the key variables: split_self, token_self, and the difference from the fair share, along with an inequity term.\n\nPutting this together, here's a possible model:\n\nUtility U is calculated as:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * (split_opp - 3*token_opp)\n\nBut split_opp - 3*token_opp = 3*token_self - split_self, so:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * (3*token_self - split_self)\n\nThis can be rewritten as:\n\nU = split_self - (\u03b1 + \u03b2) * max(3*token_self - split_self, 0) - \u03b2 * (3*token_self - split_self - max(3*token_self - split_self, 0))\n\nBut this seems redundant. Alternatively, if split_self < 3*token_self, then max(...) is 3*token_self - split_self, and the term becomes:\n\nU = split_self - (\u03b1 + \u03b2) * (3*token_self - split_self)\n\nIf split_self >= 3*token_self, then max(...) is 0, and the term is:\n\nU = split_self - \u03b2 * (3*token_self - split_self)\n\nBut this would mean that even when the participant gets more than their fair share, they still have a penalty term, which doesn't make sense. So perhaps the \u03b2 term should only apply when the opponent is getting more than their fair share, which is when split_opp > 3*token_opp. But split_opp > 3*token_opp implies split_self < 3*token_self, since split_opp = combined_earning - split_self and 3*token_opp = combined_earning - 3*token_self. Therefore, split_opp > 3*token_opp is equivalent to split_self < 3*token_self.\n\nThus, the term split_opp - 3*token_opp is equal to 3*token_self - split_self, which is positive only when split_self < 3*token_self. Therefore, the \u03b2 term is only active in that case. So the model can be simplified to:\n\nU = split_self - \u03b1 * (3*token_self - split_self) - \u03b2 * (3*token_self - split_self)\n\nWhen split_self < 3*token_self:\n\nU = split_self - (\u03b1 + \u03b2) * (3*token_self - split_self)\n\nWhen split_self >= 3*token_self:\n\nU = split_self\n\nBut this assumes that when the participant gets their fair share or more, there's no inequity penalty. However, the participant might still care if the opponent is getting less than their fair share, but that's when split_self > 3*token_self, which means split_opp = combined_earning - split_self < 3*token_opp. In that case, the opponent is getting less than their fair share, but the participant might not care as much. Alternatively, they might feel guilty and have a negative utility from that. But this complicates the model further.\n\nGiven the problem constraints, perhaps the model should focus on the participant's loss relative to their fair share and the overall inequity. Let's proceed with:\n\nU = split_self - \u03b1 * max(3*token_self - split_self, 0) - \u03b2 * (split_opp - split_self) / combined_earning\n\nHere, the second term is the loss from receiving less than the fair share, and the third term is the inequity between the opponent's and participant's shares, normalized by the total combined earning to account for different pot sizes. The division by combined_earning makes the inequity term a proportion, which addresses the issue that the same split difference represents different fairness levels depending on the total pot.\n\nThis model uses split_self, token_self, split_opp, and combined_earning. The parameters \u03b1 and \u03b2 are learnable, with \u03b1 representing the sensitivity to receiving less than the fair share, and \u03b2 representing the sensitivity to inequity (how much more the opponent is getting relative to the participant).\n\nThe utility U is then passed through a logistic function to determine the probability of acceptance: P(accept) = 1 / (1 + exp(-U)). If U < 0, the probability is less than 0.5, which aligns with the requirement.\n\nNow, specifying the variables:\n\n- \u03b1 (alpha_loss): Sensitivity to loss (receiving less than fair share). Bounded between 0 and a large value, say 10.\n- \u03b2 (beta_inequity): Sensitivity to inequity (opponent's share minus participant's share, normalized). Bounded between 0 and 10.\n- split_self: From data.\n- token_self: From data.\n- split_opp: From data.\n- combined_earning: From data.\n\nThe model formula in LaTeX:\n\nU = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}\n\nBut since split_opp = combined_earning - split_self, this can be simplified:\n\nsplit_opp - split_self = combined_earning - 2*split_self\n\nSo the term becomes (combined_earning - 2*split_self)/combined_earning = 1 - 2*(split_self / combined_earning)\n\nBut perhaps keeping it as (split_opp - split_self)/combined_earning is clearer.\n\nNow, checking the variables in the dataset:\n\nThe variables used are split_self, token_self, split_opp, combined_earning. All are present in the dataset.\n\nThe target variable is accept, which is binary. The model's U will be used in a logistic regression to predict accept.\n\nFor parameter bounds, \u03b1 and \u03b2 should be positive as they represent disutility. So \u03b1 \u2208 [0, 10], \u03b2 \u2208 [0, 10], for example.\n\nThus, the final model is:\n\n<MODEL>\nU = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}\n</MODEL>\n\nVariables:\n\n- \u03b1 (alpha): sensitivity to loss, learnable, range [0, 10]\n- \u03b2 (beta): sensitivity to inequity, learnable, range [0, 10]\n- split_self: from data\n- token_self: from data\n- split_opp: from data\n- combined_earning: from data\n\nThe variable descriptions in JSON format would include these variables, specifying their ranges, learnability, and sources.\n</think>\n\n<MODEL>\nU = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U\": {\n      \"description\": \"Utility of accepting the proposal\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"alpha\": {\n      \"description\": \"Loss sensitivity parameter for receiving less than fair share (3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity aversion parameter for opponent's share advantage normalized by total pot\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for participant in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by participant (fair share = 3*token_self)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for opponent in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": false,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total pot size in pounds (3*(token_self + token_opp))\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": false\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<target_variable>accept</target_variable>\n\n<SUMMARY>\nA dual-component utility model combining loss aversion relative to contribution-based fairness and normalized inequity aversion. The utility function subtracts 1) a loss term scaled by \u03b1 when receiving less than 3*token_self and 2) a normalized opponent advantage term scaled by \u03b2. Parameters are constrained to positive values to enforce theoretically consistent directionality.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}\nSummary: A dual-component utility model combining loss aversion relative to contribution-based fairness and normalized inequity aversion. The utility function subtracts 1) a loss term scaled by \u03b1 when receiving less than 3*token_self and 2) a normalized opponent advantage term scaled by \u03b2. Parameters are constrained to positive values to enforce theoretically consistent directionality.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [],
  "current_retry": 0,
  "simulation_code": "def simulate_model(trial_data, alpha=1, beta=1):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 0)\n\n        loss = max(3 * token_self - split_self, 0)\n        inequity = (split_opp - split_self) / combined_earning if combined_earning > 0 else 0\n\n        U = split_self - alpha * loss - beta * inequity\n        model_predictions.append(U)\n    return model_predictions",
  "total_retries": 0,
  "skipped_participants": [],
  "num_skipped_participants": 0,
  "overall_accuracy": 0.8073870573870574,
  "fitting_results": [
    {
      "alpha": 3.5087333390720516,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -8.588146588327998,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.2488970288933285,
      "beta": 1.0726505725684412,
      "success": true,
      "log_likelihood": -17.479247279294817,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.459287118331834,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -10.228883640726517,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1059689488137612,
      "beta": 4.161666224842006,
      "success": true,
      "log_likelihood": -12.502978211768799,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.911139679581686,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -9.848709686242172,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0558816714445525,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.19818715356074,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.3703753451934495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0150754503467114,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.251236946184806,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.370375345193449,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9954823179706482,
      "beta": 0.16960259236160394,
      "success": true,
      "log_likelihood": -11.400236545945278,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.8719669112401547,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -5.583534937094041,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.9262933933385513,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -8.741500072684111,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0080158202019667,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -8.854760654857694,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.370375345193449,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.2663205784333548,
      "beta": 0.8424793633510929,
      "success": true,
      "log_likelihood": -6.1538579161314715,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3621359657824505,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.656146828788673,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.3703753451934495,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.3282784059067723,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -15.841966351222428,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4335162073249491,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -8.623736865237996,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3509479737798727,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -11.842431670734154,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6346406522727426,
      "beta": 4.2247670790286715,
      "success": true,
      "log_likelihood": -16.514835150789068,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6666666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6367996931617381,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -14.53888171039523,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.036218777433501,
      "beta": 0.6445345573203058,
      "success": true,
      "log_likelihood": -15.142159039328906,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7698824102395766,
      "beta": 3.6263832101114084,
      "success": true,
      "log_likelihood": -10.313826146777807,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.302913915475119,
      "beta": 1.6781796388684034,
      "success": true,
      "log_likelihood": -4.0638044187503795,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.33506938268630065,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -9.476787731626333,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6035295063975329,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -16.643513671693626,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.37037534519345,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.5087330719723644,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -8.588146588327946,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.016204420958262425,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -17.30697481408809,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.0380096434361326,
      "beta": 5.584453963776253,
      "success": true,
      "log_likelihood": -14.7231997983665,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -2.370375345193449,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9722222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 6.1980200488304575,
      "success": true,
      "log_likelihood": -8.908643167884868,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8928407675288527,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -8.625441841616514,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5313101547066161,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -14.73812213883687,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9111701382415641,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -19.19895426569292,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.18165598656422408,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -9.550269052105216,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.1066885629452408,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -19.45218512205894,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9888063744517765,
      "beta": 6.093169164793872,
      "success": true,
      "log_likelihood": -16.212450304340244,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.770375323053267,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.44708384093527226,
      "beta": 5.1319168014838565,
      "success": true,
      "log_likelihood": -14.217814759244687,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.867348673695401,
      "beta": 1.4690257007083543,
      "success": true,
      "log_likelihood": -13.053168472103751,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.19787970043733485,
      "beta": 2.962165713035785,
      "success": true,
      "log_likelihood": -15.483862961405231,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.8962837212097988,
      "beta": 3.233150033998174,
      "success": true,
      "log_likelihood": -14.77900032681725,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6111111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7242070881935805,
      "beta": 6.189855415982733,
      "success": true,
      "log_likelihood": -20.96481375189655,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.0936842874321107,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.652269950462918,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 6.636936672984765,
      "success": true,
      "log_likelihood": -29.592514834530473,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5859412931546876,
      "beta": 1.0425600881924129,
      "success": true,
      "log_likelihood": -18.54580816716569,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.555092497634738,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -15.848031922175286,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.001011401151684428,
      "beta": 8.009215723298588,
      "success": true,
      "log_likelihood": -15.861070816102705,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 5.9876367581480965,
      "success": true,
      "log_likelihood": -22.577116367241718,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.6725974136178094,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -27.54795430330384,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.6801670862182023,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -5.08084270520353,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.9018203090624304,
      "beta": 5.919866848010982,
      "success": true,
      "log_likelihood": -9.579523628825843,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.22631823352343367,
      "success": true,
      "log_likelihood": -16.00253634400988,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.6371062496577937,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -9.441849634587852,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 3.008747368033132,
      "success": true,
      "log_likelihood": -13.085564104916163,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6839316125581568,
      "beta": 1.8475439242841045,
      "success": true,
      "log_likelihood": -22.83134580825831,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6944444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5189371729001044,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -15.495655271331858,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.5087336324548954,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -8.588146588328147,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.06745117496174731,
      "beta": 9.028225088471578,
      "success": true,
      "log_likelihood": -20.01578114433016,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.5550938246212214,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -6.2480319318119815,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5739620248194586,
      "beta": 4.425278903747812,
      "success": true,
      "log_likelihood": -12.63176371924848,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.75,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.20232628480248466,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -8.477425857323844,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.3890010527111754,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -11.799111117732576,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.004300087801850554,
      "beta": 7.582447306445306,
      "success": true,
      "log_likelihood": -11.003881405454973,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12228551312623138,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -12.316454827608268,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 10.0,
      "beta": 8.26406724051947,
      "success": true,
      "log_likelihood": -0.23408368566752763,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 1.0,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 1.7101434950614982,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -12.875638148187745,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9742394959182767,
      "beta": 2.253890912183436,
      "success": true,
      "log_likelihood": -15.778122098415254,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 3.3416600225269852,
      "success": true,
      "log_likelihood": -14.215558395962884,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8611111111111112,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.5597708275100998,
      "beta": 3.757523026633434,
      "success": true,
      "log_likelihood": -30.988087007707126,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.9074830245318165,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -13.485353126362691,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7222222222222222,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.08489467452903376,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -7.806716493600918,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.256544098581945,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -23.4021157669104,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.058271497573741636,
      "beta": 4.575365562311007,
      "success": true,
      "log_likelihood": -28.695128364641494,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.508733127766074,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -8.588146588327952,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8888888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.43086677502969095,
      "beta": 4.250855772438633,
      "success": true,
      "log_likelihood": -30.65261134199011,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 0.2263088475703451,
      "success": true,
      "log_likelihood": -12.852536347101607,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9166666666666666,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.5491563859585926,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -14.480765304362587,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.4375630398398135,
      "beta": 6.669009214284052,
      "success": true,
      "log_likelihood": -6.344372054656104,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.8055555555555556,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.6224822479973793,
      "beta": 1.0145904941333646,
      "success": true,
      "log_likelihood": -36.933866253756186,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 3.5087330188891444,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -6.988146588803061,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.7913325227602154,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -19.635712032261033,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.6388888888888888,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 2.516654601024506,
      "beta": 1.8753542374368526,
      "success": true,
      "log_likelihood": -21.537399136789645,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.7777777777777778,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.12228608859516539,
      "beta": 0.0,
      "success": true,
      "log_likelihood": -6.316454855479342,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.8333333333333334,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.0,
      "beta": 10.0,
      "success": true,
      "log_likelihood": -5.110306503755335,
      "optimization_message": "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL",
      "prediction_type": "utility",
      "accuracy": 0.9444444444444444,
      "participant_id": null,
      "n_trials": 36
    },
    {
      "alpha": 0.26467321715859593,
      "beta": 2.660997047023668,
      "success": true,
      "log_likelihood": -25.7251582033185,
      "optimization_message": "CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH",
      "prediction_type": "utility",
      "accuracy": 0.5833333333333334,
      "participant_id": null,
      "n_trials": 36
    }
  ],
  "parameter_recovery": {
    "correlations": {
      "alpha": {
        "r": 0.9136112584783078,
        "p": 0.00021930993612789075
      },
      "beta": {
        "r": 0.4828517956527937,
        "p": 0.1574831798600958
      }
    },
    "plots_directory": "param_recovery_plots"
  },
  "average_bic": 32.75528246506068,
  "bic_results": {
    "average_bic": 32.75528246506068,
    "individual_bics": [
      24.343331053568214,
      42.12553243550185,
      27.624805158365255,
      32.172994300449815,
      26.864457249396565,
      29.5634121840337,
      11.907788567299118,
      29.669511769281833,
      11.907788567299118,
      29.967510968802777,
      18.3341077511003,
      24.65003802228044,
      24.87655918662761,
      11.907788567299118,
      11.90778856729912,
      11.90778856729912,
      11.90778856729912,
      19.47475370917516,
      22.479331534489567,
      11.907788567299118,
      38.850970579357075,
      24.414511607388214,
      30.851901218380526,
      40.196708178490354,
      36.24480129770268,
      37.45135595557003,
      27.79469017046783,
      15.294646714412979,
      26.120613340164887,
      40.45406522029947,
      11.90778856729912,
      24.343331053568114,
      41.7809875050884,
      36.61343747364522,
      11.907788567299118,
      24.984324212681955,
      24.417921560145246,
      36.64328215458596,
      45.564946408298056,
      26.267575981122654,
      46.071408121030096,
      39.591938485592706,
      22.707788523018756,
      35.602667395401596,
      33.273374821119724,
      38.134763799722684,
      36.72503853054672,
      49.09666538070532,
      30.471577777838057,
      66.35206754597317,
      44.258654211243595,
      38.863101721262794,
      38.88917950911763,
      52.321270611395654,
      62.2629464835199,
      17.328723287319278,
      26.326085134563904,
      39.17211056493198,
      26.050737146087926,
      33.33816608674454,
      52.829729493428836,
      38.15834841957594,
      24.343331053568512,
      47.198600165572536,
      19.663101740536185,
      32.43056531540918,
      24.121889591559906,
      30.76526011237737,
      29.174800687822163,
      31.799947532128755,
      7.635205248247275,
      32.91831417328771,
      38.72328207374273,
      35.598154668837985,
      69.14321189232648,
      34.1377441296376,
      22.780470864114058,
      53.971269410733015,
      64.55729460619521,
      24.34333105356812,
      68.47226056089244,
      32.872110571115435,
      36.128568485637395,
      19.85578198622443,
      81.03477038442459,
      21.14333105451834,
      46.438461941434284,
      50.24183615049151,
      19.799947587870903,
      17.38765088442289,
      58.61735428354922
    ],
    "group_enabled": false,
    "group_bics": null,
    "num_parameters": 2,
    "prediction_type": "utility",
    "bic_formula": "BIC = -2 * log_likelihood + k * log(n_trials)"
  },
  "recovery_summary": "Parameter Recovery:\n- alpha: r = 0.914\n- beta: r = 0.483",
  "previous_models": [
    "Specification: U = \\text{split\\_self} - \\alpha \\cdot \\max(3 \\cdot \\text{token\\_self} - \\text{split\\_self}, 0) - \\beta \\cdot \\frac{\\text{split\\_opp} - \\text{split\\_self}}{\\text{combined\\_earning}}\nSummary: A dual-component utility model combining loss aversion relative to contribution-based fairness and normalized inequity aversion. The utility function subtracts 1) a loss term scaled by \u03b1 when receiving less than 3*token_self and 2) a normalized opponent advantage term scaled by \u03b2. Parameters are constrained to positive values to enforce theoretically consistent directionality.\nTarget Variable: accept\nPrediction Type: utility\nBIC: 32.75528246506068\nOverall Accuracy: 0.8074\n\nParameter Recovery:\n- alpha: r = 0.914\n- beta: r = 0.483"
  ]
}