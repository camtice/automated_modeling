{
  "task_description": "In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "output_description": "Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.",
  "instructions": "<think>\n\nOkay, let's analyze the previous run's results. The model had a BIC of around 47.93, which is higher than the best previous model (31.04), and the accuracy was about 59.9%, which is lower than the third model's 81.6%. The parameter recovery for alpha was very low (r=0.104) and beta was moderate (0.521). This suggests that the model structure from the previous run isn't capturing the underlying decision processes effectively, especially for alpha.\n\nThe previous instructions focused on prospect theory with reference points, non-linear transformations, and trial_type scaling. However, the results didn't improve, indicating that the model might be overcomplicating things or the parameters aren't orthogonal enough. The low alpha recovery suggests that the reference point calculation (gamma) might not be identifiable, or the terms are collinear.\n\nTo improve, the next model should simplify the reference point or make parameters more identifiable. Maybe fixing gamma to a specific value (like 0.5) to reduce parameters. Alternatively, using a different approach to reference points, such as a fixed fair share (3*token_self) instead of a dynamic mix. Also, the non-linear transformations (\u03c3 and \u03c1) might be causing issues if they're fixed but not appropriate for the data. Perhaps removing them or making them learnable with tight bounds could help.\n\nAnother angle: The trial_type scaling for beta (\u03b8) wasn't part of the previous model's parameters, but maybe including it as a learnable parameter could help capture context effects. However, adding parameters increases BIC, so need to balance. Alternatively, using trial_type to modulate the reference point instead of loss aversion.\n\nThe radical constraint in the previous run fixed \u03c3 and \u03c1, but maybe allowing them to vary slightly within a narrow range could capture individual differences without adding too many parameters. Or, using a different form of non-linearity, like logarithmic scaling for gains and losses.\n\nTo improve orthogonality, ensure that each parameter's term is scaled by different variables. For example, scaling the loss term by token_self and the inequity term by combined_earning. This could reduce covariance between parameters.\n\nThe new instructions should encourage a balance between prospect theory elements and simplicity. Maybe a hybrid model that uses a fixed reference point (3*token_self) with loss aversion and gain sensitivity, while incorporating trial_type into the inequity term. Also, simplifying the non-linear transformations to avoid overcomplicating the utility function.\n\nPotential steps:\n1. Use a fixed reference point (3*token_self) to eliminate gamma.\n2. Separate loss and gain sensitivities (lambda and kappa) with fixed exponents (\u03c3=0.88, \u03c1=1.25) to maintain parsimony.\n3. Incorporate trial_type into the inequity term (beta) by scaling it when trial_type=3 (participant contributed more).\n4. Ensure terms are orthogonal by scaling each parameter's component with different variables (e.g., loss term scaled by token_self, inequity by combined_earning).\n\nThis approach reduces the number of parameters (lambda, kappa, beta) while maintaining identifiability. The trial_type scaling on beta could improve its recovery by isolating its effect in specific contexts.\n</think>\n\n**Instructions for Run 5/200:**  \nDesign a *hybrid prospect-theoretic fairness model* with these innovations:  \n\n1. **Simplified Reference Architecture**  \n   - Anchor utility to an *immutable contribution standard*:  \n     `reference = 3*token_self` (eliminate \u03b3 parameter)  \n   - Implement *asymmetric outcome valuation*:  \n     ```  \n     U = \u03ba*(split_self - reference)^+ - \u03bb*(reference - split_self)^+  \n            - \u03b2*(split_opp - split_self)^+/(combined_earning/10)  \n     ```  \n     Where `(x)^+ = max(0,x)`  \n\n2. **Context-Contingent Inequality Penalty**  \n   - Amplify \u03b2 sensitivity in trials where participants contributed more tokens:  \n     ```  \n     \u03b2_effective = \u03b2 * [1 + trial_type*(trial_type==3)]  \n     ```  \n     (Intrinsic 3x \u03b2 weighting when token_self > token_opp)  \n\n3. **Neuroeconomic Compression**  \n   - Apply *fixed behavioral economics priors* to curvature parameters:  \n     - \u03ba gains: Linear (\u03c3=1.0 locked)  \n     - \u03bb losses: Convex (\u03c1=1.33 locked from meta-analysis)  \n\n**Identifiability Safeguards:**  \n- Z-score split_self relative to token_self before exponentiation  \n- Confine \u03bb to [0,8], \u03ba to [0,3] to match prospect theory magnitude ratios  \n\n**Success Metrics:**  \n- Target BIC <25 through \u03ba/\u03bb synergy effects  \n- Achieve \u03b2 recovery r >0.65 via trial_type contrast enhancement  \n- Maintain \u03bb recovery r >0.85 through convex loss architecture",
  "dataset_info": "\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276",
  "complete_model_interaction": [
    {
      "solver": "model_design_solver",
      "input": "Task Description: In the  experiment, each participant is given 36 different proposals to respond to. Each is designated by a trial number in the data set This gives 36 instances for the learnable parameters to be trained. There is also a pretask where the participant finds a number of tokens that correspond to their contribution to the group. This is designated by the variable \"token_self\". The other variable, \"token_opp\", is the number of tokens found the opponent's and represents the opoonents contribution to the shared pot that will be later divided by the opponent.\n\nNote that in each trial there is a variable amount of money in the combined earning, so that two equal split_self variables do not represent the same \"fairness\".\n\nBelow is the description of each variable in the dataset, which will be followed by information about the data type of the variable:\n\ntrial_type = how much was the contribution [1 = equal, 2 = opponent more, 3 = participant more]\ntrial_role = the role of the participant [1 = responder, 2 = proposer]\ntoken_opp = number of tokens found by the opponent\ntoken_self = number of tokens found by the participant\ncombined_earning = amount of money earned in total (note that each token found is \u00a33)\nsplit_opp = the proposed share of the opponent (in \u00a3)\nsplit_self = the proposed share for the participant (in \u00a3)\nsplitperc_opp = same but in %\nsplit_perc_self = same but in %\naccept = whether the participant responded accept (1) or reject (0)\naccepted_amount = if accepted, what is the amount gain (in \u00a3)\naccepted_perc = same as above but in %\nproposed_perc = when participant is the proposer, what is the % proposed to the opponent\n\nDataset:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\nDesired Output Specification: Design a computational model for the described task. Your model should utilyze the given variables appropriately (but not necessarily all unless otherwise specified). \n\nThe model should:\n1. Predict only responder behavior (should target \"accept = whether the participant responded accept (1) or reject (0)\")\nwill have to pass parameter recovery tests. This should be roughly equal in priority to accuracy.\n2. Be applicable to the dataset given within the task description (i.e. the model should be able\nto be run using only the provided dataset structure).\n4. Predict utility, where less than 0 is less than 50% likely.\n\nFor any learnable parameters in your model, you must specify finite numerical bounds (it just can not be infinity) in the variable descriptions. However, this bound should be generous. The utility variable may be unbounded (-inf to inf).\n\nThen, provide your formal mathematical model between <MODEL> tags, followed by variable descriptions between <VARIABLES> tags. Additionally, specify the target variable that your model will predict. This should be one of the variables from the dataset above. Enclose it in <target_variable> tags.\nFor example: <target_variable>accept</target_variable> if your model is predicting whether a participant accepts or rejects a proposal. \n\nUse clear mathematical notation and ensure each variable is fully described. Every variable must be utilized within the mathematical formula provided between <MODEL> and </MODEL>, and only strictly the mathematical formula should be provided between these tags with no filler words or descriptions. For variable descriptions, provide them in a JSON format with the following structure: <VARIABLES> Bounds are by default inclusive. Please spell out the greek variable names instead of using the symbols. Finally, you should provide a short description of the model in the summary tags <SUMMARY> </SUMMARY>. This will be used to compare different computational models of the task; it should be concise and purely descriptive. Do not include generic information about the name of the tasks, since that is implied. \n\n{\n  \"variables\": {\n    \"variable_name\": {\n      \"description\": \"Clear description\",\n      \"range\": {\n        \"min\": value,\n        \"max\": value,\n        \"inclusive_min\": boolean,\n        \"inclusive_max\": boolean\n      },\n      \"distribution\": {\n        \"type\": \"distribution type\",\n        \"parameters\": {}\n      },\n      \"learnable\": boolean,\n      \"source\": \"learnable/data/calculated\"\n    }\n  }\n}\n</VARIABLES>\n\nExample #1: \n<EXAMPLE>\n(Model of a Random Choice Task):\nLet me think through this step by step...\n[your reasoning here]\n\n<MODEL>\nU_i = \u03b2 + \u03b5E + \u03b7N\n</MODEL>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"U_i\": {\n      \"description\": \"Utility of choosing action i\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    },\n    \"beta\": {\n      \"description\": \"Base tendency parameter (inherent preference)\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"epsilon\": {\n      \"description\": \"Environmental sensitivity parameter\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"E\": {\n      \"description\": \"Environmental cue value\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\"\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"eta\": {\n      \"description\": \"Noise parameter\",\n      \"range\": {\n        \"min\": 0,\n        \"max\": 1,\n        \"exclusive_max\": true\n      },\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"N\": {\n      \"description\": \"Random noise drawn from normal distribution N(0,1)\",\n      \"distribution\": {\n        \"type\": \"normal\",\n        \"mean\": 0,\n        \"std\": 1\n      },\n      \"learnable\": false,\n      \"source\": \"calculated\"\n    }\n  }\n}\n</VARIABLES>\n<SUMMARY>\nA linear utility model combining base preference, environmental influence, and stochastic noise. The model uses two learnable parameters: a base tendency parameter (\u03b2) constrained to [0,1] and an unconstrained environmental sensitivity parameter (\u03b5). Environmental cues and normally distributed noise modify the base utility, allowing for both systematic and random variation in choice behavior.\n</SUMMARY>\n</EXAMPLE>\n\nPlease be sure to provide the mathematical formula for the model between <MODEL> and </MODEL>, and the variable descriptions between <VARIABLES> and </VARIABLES>. No description of the model should be provided within the <MODEL> tags.\n\nThe parameter names you choose in your <VARIABLES> section will be used exactly as specified in subsequent code generation. Please choose clear, non-reserved parameter names that can be used directly in Python code. For example, avoid Python reserved words like 'lambda' and use alternatives like 'learning_rate' or 'lambda_param'.\n\nModel Type: \nYour model should predict the utility of a binary choice. The utility will be converted to a probability using a logistic function with temperature 1, and then used to predict binary decisions.\n\n\nInstructions: <think>\n\nOkay, let's analyze the previous run's results. The model had a BIC of around 47.93, which is higher than the best previous model (31.04), and the accuracy was about 59.9%, which is lower than the third model's 81.6%. The parameter recovery for alpha was very low (r=0.104) and beta was moderate (0.521). This suggests that the model structure from the previous run isn't capturing the underlying decision processes effectively, especially for alpha.\n\nThe previous instructions focused on prospect theory with reference points, non-linear transformations, and trial_type scaling. However, the results didn't improve, indicating that the model might be overcomplicating things or the parameters aren't orthogonal enough. The low alpha recovery suggests that the reference point calculation (gamma) might not be identifiable, or the terms are collinear.\n\nTo improve, the next model should simplify the reference point or make parameters more identifiable. Maybe fixing gamma to a specific value (like 0.5) to reduce parameters. Alternatively, using a different approach to reference points, such as a fixed fair share (3*token_self) instead of a dynamic mix. Also, the non-linear transformations (\u03c3 and \u03c1) might be causing issues if they're fixed but not appropriate for the data. Perhaps removing them or making them learnable with tight bounds could help.\n\nAnother angle: The trial_type scaling for beta (\u03b8) wasn't part of the previous model's parameters, but maybe including it as a learnable parameter could help capture context effects. However, adding parameters increases BIC, so need to balance. Alternatively, using trial_type to modulate the reference point instead of loss aversion.\n\nThe radical constraint in the previous run fixed \u03c3 and \u03c1, but maybe allowing them to vary slightly within a narrow range could capture individual differences without adding too many parameters. Or, using a different form of non-linearity, like logarithmic scaling for gains and losses.\n\nTo improve orthogonality, ensure that each parameter's term is scaled by different variables. For example, scaling the loss term by token_self and the inequity term by combined_earning. This could reduce covariance between parameters.\n\nThe new instructions should encourage a balance between prospect theory elements and simplicity. Maybe a hybrid model that uses a fixed reference point (3*token_self) with loss aversion and gain sensitivity, while incorporating trial_type into the inequity term. Also, simplifying the non-linear transformations to avoid overcomplicating the utility function.\n\nPotential steps:\n1. Use a fixed reference point (3*token_self) to eliminate gamma.\n2. Separate loss and gain sensitivities (lambda and kappa) with fixed exponents (\u03c3=0.88, \u03c1=1.25) to maintain parsimony.\n3. Incorporate trial_type into the inequity term (beta) by scaling it when trial_type=3 (participant contributed more).\n4. Ensure terms are orthogonal by scaling each parameter's component with different variables (e.g., loss term scaled by token_self, inequity by combined_earning).\n\nThis approach reduces the number of parameters (lambda, kappa, beta) while maintaining identifiability. The trial_type scaling on beta could improve its recovery by isolating its effect in specific contexts.\n</think>\n\n**Instructions for Run 5/200:**  \nDesign a *hybrid prospect-theoretic fairness model* with these innovations:  \n\n1. **Simplified Reference Architecture**  \n   - Anchor utility to an *immutable contribution standard*:  \n     `reference = 3*token_self` (eliminate \u03b3 parameter)  \n   - Implement *asymmetric outcome valuation*:  \n     ```  \n     U = \u03ba*(split_self - reference)^+ - \u03bb*(reference - split_self)^+  \n            - \u03b2*(split_opp - split_self)^+/(combined_earning/10)  \n     ```  \n     Where `(x)^+ = max(0,x)`  \n\n2. **Context-Contingent Inequality Penalty**  \n   - Amplify \u03b2 sensitivity in trials where participants contributed more tokens:  \n     ```  \n     \u03b2_effective = \u03b2 * [1 + trial_type*(trial_type==3)]  \n     ```  \n     (Intrinsic 3x \u03b2 weighting when token_self > token_opp)  \n\n3. **Neuroeconomic Compression**  \n   - Apply *fixed behavioral economics priors* to curvature parameters:  \n     - \u03ba gains: Linear (\u03c3=1.0 locked)  \n     - \u03bb losses: Convex (\u03c1=1.33 locked from meta-analysis)  \n\n**Identifiability Safeguards:**  \n- Z-score split_self relative to token_self before exponentiation  \n- Confine \u03bb to [0,8], \u03ba to [0,3] to match prospect theory magnitude ratios  \n\n**Success Metrics:**  \n- Target BIC <25 through \u03ba/\u03bb synergy effects  \n- Achieve \u03b2 recovery r >0.65 via trial_type contrast enhancement  \n- Maintain \u03bb recovery r >0.85 through convex loss architecture\n\nPrevious Models:Not Provided\n\nPlease think through this step by step, then provide your model specification and variable descriptions.",
      "output": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal. The model needs to focus on responder behavior, use the given variables appropriately, and ensure parameters can be recovered while maintaining accuracy.\n\nFirst, I need to identify which variables are relevant for the responder's decision. The target variable is \"accept\", which is binary (1 or 0). The model should predict the utility of accepting, which will then be converted to a probability via a logistic function.\n\nKey variables from the dataset that might influence the responder's decision:\n- token_self: The participant's contribution (tokens they found).\n- token_opp: The opponent's contribution.\n- split_self: The proposed share for the participant in \u00a3.\n- split_opp: The proposed share for the opponent.\n- combined_earning: Total money in the pot (each token is \u00a33, so this is 3*(token_self + token_opp)).\n- trial_type: Whether contributions are equal, opponent more, or participant more.\n\nThe model should incorporate fairness considerations and the participant's own contribution. Since the utility needs to predict acceptance, the model should compare the proposed split to what's considered fair based on contributions.\n\nI recall that in ultimatum games, people often reject unfair offers even if it means getting nothing, which suggests that fairness (or inequity aversion) plays a role. So, the model might include terms for both the participant's share relative to their contribution and the difference between the shares.\n\nPossible components of utility:\n1. Utility from the participant's share compared to their contribution.\n2. Disutility from inequity (difference between split_self and split_opp).\n3. Sensitivity parameters for these components.\n\nConsidering the parameters need to be learnable with bounds. Let's think about each component:\n\n1. **Contribution-based Fairness**: The participant's expected fair share might be proportional to their contribution. For example, if they contributed token_self tokens out of total tokens (token_self + token_opp), their fair share would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair share could be a term, scaled by a parameter.\n\n2. **Inequity Aversion**: The difference between split_opp and split_self could cause disutility. People might dislike when the opponent gets more than them. This could be modeled as (split_opp - split_self) scaled by another parameter.\n\n3. **Non-linear Transformations**: Maybe the utility isn't linear in these differences. For example, people might be more sensitive to inequity when it's against them. So, perhaps a parameter that scales the inequity term only when split_opp > split_self.\n\n4. **Reference Points**: The participant's contribution (token_self) might serve as a reference point. If split_self is less than what they contributed (in \u00a3 terms, 3*token_self), that could be a loss, and treated differently than gains.\n\nPutting this together, the utility function could have terms for gains/losses relative to contribution and inequity aversion. Let's formalize this.\n\nLet\u2019s define:\n- Reference point: 3 * token_self (the amount the participant contributed in \u00a3).\n- Gain/Loss: split_self - reference. If positive, it's a gain; if negative, a loss.\n- Inequity: split_opp - split_self.\n\nProspect Theory elements: People weigh losses more than gains. So, we can have a loss aversion parameter (\u03bb) that scales the loss term.\n\nInequity aversion can be modeled with a parameter (\u03b2) that scales the difference. However, maybe the inequity effect depends on the total combined earning. For example, a \u00a35 difference in a \u00a310 pot is more significant than in a \u00a3100 pot. So, perhaps normalize the inequity by combined_earning.\n\nAlso, trial_type might affect the sensitivity. For example, if the participant contributed more (trial_type=3), they might be more sensitive to inequity.\n\nBut the model should not use trial_type directly unless specified. Wait, the task says the model should use the variables in the dataset. Since trial_type is available, maybe it can be used to modulate parameters. For example, different \u03b2 for different trial types.\n\nBut to keep it simple initially, let's focus on the core variables.\n\nDraft model:\n\nUtility = [\u03b1 * (split_self - 3*token_self)] + [\u03b2 * (split_opp - split_self)/combined_earning] + [\u03bb * (split_self < 3*token_self) * (split_self - 3*token_self)]\n\nWait, but \u03bb would be for loss aversion. So maybe:\n\nUtility = (split_self - reference) + \u03bb * (split_self < reference) * (split_self - reference) + \u03b2 * (split_opp - split_self)/combined_earning\n\nBut this might not capture the non-linear aspects. Alternatively, separate gain and loss terms:\n\nIf split_self >= reference:\n    gain_utility = (split_self - reference)^\u03c3\nElse:\n    loss_utility = -\u03bb * (reference - split_self)^\u03c1\n\nThen add the inequity term.\n\nBut introducing \u03c3 and \u03c1 as parameters might complicate things. However, the task says parameters need bounds. Maybe fix \u03c3 and \u03c1 to 1 for simplicity, leaving \u03bb and \u03b2 as learnable.\n\nAlternatively, allow \u03c3 and \u03c1 to vary but with bounds (e.g., between 0.5 and 2).\n\nBut the model needs to be parsimonious to pass parameter recovery. Let's try:\n\nUtility = (split_self - reference) + \u03bb * max(reference - split_self, 0) + \u03b2 * (split_opp - split_self)/combined_earning\n\nWait, that might not be right. The loss aversion is typically a multiplier on losses. So:\n\nUtility = (split_self - reference) + \u03bb * (reference - split_self) * I(split_self < reference) + \u03b2 * (split_opp - split_self)/combined_earning\n\nWhere I() is an indicator function. But this can be rewritten as:\n\nIf split_self >= reference:\n    utility_gain = (split_self - reference)\nElse:\n    utility_loss = \u03bb * (reference - split_self)\n\nThen add the inequity term.\n\nBut combining these:\n\nUtility = [ (split_self >= reference) * (split_self - reference) ] + [ (split_self < reference) * \u03bb * (split_self - reference) ] + \u03b2 * (split_opp - split_self)/combined_earning\n\nWait, that might not be correct. If split_self < reference, then (split_self - reference) is negative, so multiplying by \u03bb (which is positive) would make it more negative. But loss aversion suggests that losses are felt more strongly, so perhaps:\n\nUtility = (split_self - reference) + \u03bb * max(reference - split_self, 0) + \u03b2 * (split_opp - split_self)/combined_earning\n\nWait, no. Let's think again. In prospect theory, the value function is:\n\nV(x) = x^\u03c3 if x >=0, -\u03bb*(-x)^\u03c1 if x <0.\n\nSo for the difference (split_self - reference), if positive, it's a gain, else a loss.\n\nSo:\n\ngain_loss_utility = if split_self >= reference: (split_self - reference)^\u03c3 else: -\u03bb * (reference - split_self)^\u03c1\n\nThen add the inequity term.\n\nBut this introduces \u03c3, \u03c1, and \u03bb as parameters. To keep it manageable, maybe set \u03c3=1 and \u03c1=1, simplifying to:\n\ngain_loss_utility = (split_self - reference) - \u03bb * (reference - split_self) * I(split_self < reference)\n\nWhich simplifies to:\n\ngain_loss_utility = (split_self - reference) * [1 + (\u03bb -1) * I(split_self < reference)]\n\nBut this might not capture the true loss aversion. Alternatively, separate the terms:\n\ngain_loss_utility = (split_self - reference) + (\u03bb -1) * max(reference - split_self, 0)\n\nBecause when split_self < reference, the loss is (reference - split_self), and the total utility would be (split_self - reference) + (\u03bb -1)(reference - split_self) = (split_self - reference) + (\u03bb -1)(reference - split_self) = (split_self - reference)(1 - (\u03bb -1)) = (split_self - reference)(2 - \u03bb). Hmm, maybe not the best approach.\n\nAlternatively, model the gain and loss separately:\n\ngain = max(split_self - reference, 0)\nloss = max(reference - split_self, 0)\ngain_loss_utility = gain - \u03bb * loss\n\nThen add the inequity term.\n\nSo:\n\nUtility = (gain - \u03bb * loss) + \u03b2 * (split_opp - split_self)/combined_earning\n\nThis seems better. Here, \u03bb >1 would indicate loss aversion.\n\nAdditionally, the inequity term (split_opp - split_self) could be positive or negative. If the opponent is getting more, that's negative for the participant. But maybe the responder dislikes when the opponent gets more, so the term is (split_opp - split_self), scaled by \u03b2. If \u03b2 is positive, then a higher split_opp than split_self would decrease utility, leading to lower acceptance.\n\nBut perhaps the inequity term should be the absolute difference, or only consider cases where the opponent gets more. Alternatively, split_opp - split_self could be positive (opponent gets more) or negative (participant gets more). The parameter \u03b2 would then capture sensitivity to this difference.\n\nBut to normalize, dividing by combined_earning makes sense, as a \u00a31 difference in a \u00a310 pot is more significant than in a \u00a3100 pot.\n\nPutting it all together:\n\nUtility = [max(split_self - reference, 0) - \u03bb * max(reference - split_self, 0)] + \u03b2 * (split_opp - split_self)/combined_earning\n\nWhere reference = 3 * token_self.\n\nThis model has two learnable parameters: \u03bb (loss aversion) and \u03b2 (inequity aversion). Additionally, maybe a parameter for the gain term, but in the current setup, the gain is linear. If we want to allow diminishing sensitivity to gains, we could add a parameter \u03c3 for the gain exponent:\n\nUtility = [ (max(split_self - reference, 0))^\u03c3 - \u03bb * (max(reference - split_self, 0))^\u03c1 ] + \u03b2 * (split_opp - split_self)/combined_earning\n\nBut this adds \u03c3 and \u03c1 as parameters. However, the task says to keep the model applicable and not too complex. Since parameter recovery is a priority, perhaps keep \u03c3 and \u03c1 fixed (e.g., \u03c3=1, \u03c1=1) to reduce the number of parameters. Alternatively, set \u03c3=0.88 and \u03c1=0.88 as per prospect theory findings, but fix them, leaving \u03bb and \u03b2 as learnable.\n\nBut the user's example included parameters with ranges, so let's proceed with the model that has \u03bb and \u03b2 as learnable, with \u03c3 and \u03c1 fixed.\n\nWait, the task says \"For any learnable parameters in your model, you must specify finite numerical bounds\". So if \u03c3 and \u03c1 are fixed (not learnable), they don't need bounds. But if they are learnable, they do.\n\nTo balance complexity and recovery, perhaps fix \u03c3 and \u03c1 to 1, making the gain and loss terms linear. Then the model has two parameters: \u03bb (loss aversion) and \u03b2 (inequity aversion).\n\nAlternatively, allow \u03c3 and \u03c1 to be learnable but with tight bounds. For example, \u03c3 in [0.5, 1.5], \u03c1 in [0.5, 2]. But this adds more parameters, which might complicate recovery.\n\nGiven the dataset has 36 instances per participant, having 2-3 parameters is manageable.\n\nSo, proceeding with:\n\nUtility = (split_self - reference) - \u03bb*(reference - split_self) * I(split_self < reference) + \u03b2*(split_opp - split_self)/combined_earning\n\nBut when split_self < reference, the term becomes (split_self - reference) - \u03bb*(reference - split_self) = (split_self - reference)(1 + \u03bb). For example, if \u03bb=2, then the loss is weighted 2 times more than the gain.\n\nWait, let's re-express:\n\nIf split_self >= reference:\n    gain = split_self - reference\n    loss = 0\nElse:\n    gain = 0\n    loss = reference - split_self\n\nUtility = gain - \u03bb*loss + \u03b2*(split_opp - split_self)/combined_earning\n\nYes, this makes sense. So the utility is the gain minus lambda times the loss, plus beta times the inequity term.\n\nNow, the probability of accepting is logistic(Utility). The model needs to predict the log odds of accepting, which is the utility.\n\nParameters:\n- lambda (\u03bb): loss aversion factor, must be >=1 since losses loom larger. Bounds [1, 5], for example.\n- beta (\u03b2): sensitivity to inequity. If beta is positive, then when split_opp > split_self (opponent gets more), the utility decreases, making acceptance less likely. Bounds could be [-5, 5], but likely negative since (split_opp - split_self) being positive (opponent gets more) would decrease utility, so beta should be negative to penalize that. Wait, no: if beta is negative, then a positive (split_opp - split_self) would multiply by beta, leading to a negative term, reducing utility. So beta's sign determines the direction of the effect.\n\nAlternatively, take absolute value or square, but perhaps better to let beta be learned as a free parameter. So beta could be negative, indicating that higher opponent shares reduce utility.\n\nBut to avoid confusion, perhaps define the inequity term as (split_self - split_opp), so that higher values (participant gets more) increase utility. Then beta would be positive. Let me think.\n\nCurrent term: \u03b2*(split_opp - split_self)/combined_earning\n\nIf split_opp > split_self, this term is positive. If beta is negative, that would decrease utility, making acceptance less likely. If beta is positive, it would increase utility, which doesn't make sense. So beta should be negative to penalize the participant when the opponent gets more.\n\nBut maybe participants care about fairness in terms of the difference relative to the total. Alternatively, the term could be (split_self - fair_share), where fair_share is based on contributions.\n\nWait, another approach: fair_share = (token_self / (token_self + token_opp)) * combined_earning\n\nThen, the difference between split_self and fair_share could be a term. But this introduces another component.\n\nBut the initial model with gain-loss and inequity terms might be sufficient.\n\nSo, parameters are \u03bb (loss aversion) and \u03b2 (inequity aversion). Both need bounds.\n\nFor \u03bb, since it's loss aversion, typically >=1. So bounds [1, 5].\n\nFor \u03b2, it's the sensitivity to the opponent's share relative to self. If the participant dislikes the opponent getting more, \u03b2 should be negative. So bounds could be [-5, 0], but maybe wider [-10, 10] to allow the model to learn.\n\nWait, but the term is (split_opp - split_self)/combined_earning. If split_opp is larger than split_self, this term is positive. If beta is negative, then a positive term multiplied by beta gives a negative contribution to utility, reducing the chance of acceptance. That makes sense. So beta should be negative. But to allow the model to capture different behaviors, maybe not restrict the sign. For example, if beta is positive, it would imply that the participant prefers when the opponent gets more, which is unlikely. So perhaps constrain beta to be <=0. But the task says bounds should be generous. So maybe set beta's range as [-10, 10], but in practice, we expect it to be negative.\n\nAlternatively, use absolute value of the difference, but that might not capture the direction.\n\nAlternatively, the term could be (split_self - split_opp)/combined_earning, so a positive value means the participant gets more. Then beta positive would mean they prefer that, which is sensible. Let's adjust the model:\n\nUtility = gain - \u03bb*loss + \u03b2*(split_self - split_opp)/combined_earning\n\nNow, if split_self > split_opp, the term is positive, and if beta is positive, it increases utility. This makes more intuitive sense. So beta would be positive, and we can set bounds [0, 10], but maybe [-10, 10] to allow for negative values (though unlikely).\n\nBut this changes the interpretation. The original term was (split_opp - split_self), which is negative when the participant gets more. By flipping it to (split_self - split_opp), it's positive when the participant gets more, and beta positive would mean they like that. So this seems better.\n\nSo adjusting the model:\n\nUtility = [gain - \u03bb*loss] + \u03b2*(split_self - split_opp)/combined_earning\n\nWhere gain = max(split_self - reference, 0), loss = max(reference - split_self, 0)\n\nThis way, a positive beta means the participant prefers getting more than the opponent, which aligns with self-interest. However, if beta is negative, it might indicate a preference for equity, but that's less likely. Alternatively, participants might have inequity aversion, disliking both being ahead and behind. But the model as structured only considers the difference in splits, not the fair share based on contributions.\n\nWait, but the first term (gain - \u03bb*loss) already captures the comparison to their own contribution (reference = 3*token_self). The second term captures the difference in splits. So combining both own contribution fairness and split difference.\n\nBut maybe the split difference should be compared to the fair split based on contributions. For example, the fair split would be (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and the fair share could be a term.\n\nBut that introduces another component. Let's consider that.\n\nFair_share_self = (token_self / (token_self + token_opp)) * combined_earning\n\nThen, split_self - fair_share_self could be a term. If positive, the participant is getting more than fair; if negative, less.\n\nBut this might be redundant with the gain/loss term, which is based on their own contribution (reference = 3*token_self). Because the fair share based on contributions is exactly 3*token_self, since combined_earning is 3*(token_self + token_opp), so:\n\nFair_share_self = 3*token_self\n\nBecause (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) = 3*token_self.\n\nAh, right! So the fair share based on contributions is exactly 3*token_self. Therefore, the gain/loss term (split_self - reference) where reference=3*token_self is exactly the difference between the proposed split and the fair share. So the first term already captures deviation from the contribution-based fair share.\n\nThe second term, (split_self - split_opp)/combined_earning, captures the difference in splits, which might relate to another form of fairness (equality regardless of contributions). But in the task description, it's noted that two equal splits may not be fair if contributions differ. So participants might care about both their share relative to their contribution and the equality of splits.\n\nThus, the model includes both terms: one for contribution-based fairness (gain/loss) and one for split equality (inequity).\n\nSo the final model is:\n\nUtility = (split_self - reference) - \u03bb*max(reference - split_self, 0) + \u03b2*(split_self - split_opp)/combined_earning\n\nBut wait, when split_self >= reference, the first term is (split_self - reference), and the second term is 0. When split_self < reference, the first term is (split_self - reference) - \u03bb*(reference - split_self) = (split_self - reference) * (1 + \u03bb). Which is equivalent to:\n\nIf split_self >= reference:\n    utility_contribution = (split_self - reference)\nElse:\n    utility_contribution = -(1 + \u03bb)*(reference - split_self)\n\nThen add the inequity term.\n\nBut combining it into one expression:\n\nutility_contribution = (split_self - reference) - \u03bb*max(reference - split_self, 0)\n\nWhich simplifies to:\n\nIf split_self >= reference: (split_self - reference)\nElse: (split_self - reference) - \u03bb*(reference - split_self) = (split_self - reference) - \u03bb*(reference - split_self) = -(1 + \u03bb)*(reference - split_self)\n\nSo the contribution term is linear in gains and losses, with loss aversion \u03bb.\n\nThe inequity term is \u03b2*(split_self - split_opp)/combined_earning.\n\nThus, the utility function becomes:\n\nU = (split_self - reference) - \u03bb*max(reference - split_self, 0) + \u03b2*(split_self - split_opp)/combined_earning\n\nNow, the parameters are \u03bb (loss aversion) and \u03b2 (inequity sensitivity). Both need bounds.\n\nFor \u03bb, since it's loss aversion, typically >=1. So set bounds [1, 5], inclusive.\n\nFor \u03b2, it's the sensitivity to the split difference. If \u03b2 is positive, participants prefer getting more than the opponent; if negative, they prefer equity. But given the task, participants might reject offers where they get less than their fair share (reference) and also dislike inequity. However, the sign of \u03b2 depends on how the term is structured. Since the term is (split_self - split_opp), a positive \u03b2 means that the participant prefers getting more than the opponent, which could be self-interest. But if \u03b2 is negative, it might indicate aversion to inequity (preferring smaller differences). However, this is ambiguous. To allow the model to capture either, set \u03b2's bounds as [-10, 10], generous but finite.\n\nThe variables used are split_self, split_opp, combined_earning, and token_self (for reference). The trial_role is responder, so we filter for trial_role=1.\n\nNow, the mathematical model:\n\n<MODEL>\nU = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\n</MODEL>\n\nVariables:\n\n- lambda: loss aversion parameter, learnable, range [1, 5]\n- beta: inequity sensitivity, learnable, range [-10, 10]\n- split_self, token_self, split_opp, combined_earning: data variables\n\nThe target variable is 'accept', which is binary. The utility U is converted to a probability via logistic function: P(accept) = 1 / (1 + exp(-U)).\n\nNow, specifying the variables in JSON:\n\nFor each variable in the model:\n\n- lambda: learnable, description: \"Loss aversion parameter, scaling the disutility of receiving less than one's contribution\", range [1,5], inclusive.\n- beta: learnable, description: \"Sensitivity to the difference between one's own and the opponent's share\", range [-10,10].\n- split_self: data variable.\n- token_self: data variable.\n- split_opp: data variable.\n- combined_earning: data variable.\n\nThe variables U is calculated, not learnable.\n\nSo the JSON would be:\n\n{\n  \"variables\": {\n    \"lambda\": {\n      \"description\": \"Loss aversion parameter, scaling the disutility of receiving less than one's contribution (3*token_self)\",\n      \"range\": {\n        \"min\": 1,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to the difference between one's own and the opponent's share, normalized by combined earnings\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for the opponent in \u00a3\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money earned in the trial (\u00a3)\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe summary should explain the model concisely.\n\n<SUMMARY>\nA prospect-theoretic model combining loss aversion relative to the participant's contribution (3*token_self) and sensitivity to inequity between shares. Losses (receiving less than contributed) are scaled by lambda, while beta captures reactions to differences in proposed splits, adjusted for total earnings.\n</SUMMARY>\n</think>\n\n<MODEL>\nU = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\n</MODEL>\n\n<target_variable>accept</target_variable>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"lambda\": {\n      \"description\": \"Loss aversion parameter scaling disutility when receiving less than contributed amount\",\n      \"range\": {\n        \"min\": 1,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity sensitivity parameter for split differences normalized by total earnings\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Participant's proposed share in pounds\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\u00a3/token)\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Opponent's proposed share in pounds\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total trial earnings (3\u00a3*(tokens_self + tokens_opp))\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nHybrid prospect theory model evaluating offers relative to contribution-based reference points (3*token_self) with loss aversion (lambda) and normalized inequity sensitivity (beta). Losses below contribution threshold are amplified, while split differences are scaled by total earnings magnitude.\n</SUMMARY>"
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['lambda', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"lambda\": {\n      \"description\": \"Loss aversion parameter scaling disutility when receiving less than contributed amount\",\n      \"range\": {\n        \"min\": 1,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity sensitivity parameter for split differences normalized by total earnings\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Participant's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\\u00a3/token)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Opponent's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total trial earnings (3\\u00a3*(tokens_self + tokens_opp))\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, lambda_param=1, beta_param=0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 1)  # Avoid division by zero\n\n        reference = 3 * token_self\n        gain = max(split_self - reference, 0)\n        loss = max(reference - split_self, 0)\n\n        # Utility calculation\n        utility = (split_self - 3*token_self) - lambda_param * max(3*token_self - split_self, 0) + beta_param * (split_self - split_opp) / combined_earning\n\n        model_predictions.append(utility)\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n```",
      "timestamp": 1744015830.605458
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['lambda', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"lambda\": {\n      \"description\": \"Loss aversion parameter scaling disutility when receiving less than contributed amount\",\n      \"range\": {\n        \"min\": 1,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity sensitivity parameter for split differences normalized by total earnings\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Participant's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\\u00a3/token)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Opponent's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total trial earnings (3\\u00a3*(tokens_self + tokens_opp))\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "```python\nimport json\nimport math\n\ndef simulate_model(trial_data, lambda_val=1.0, beta_val=0.0):\n    \"\"\"\n    Simulates the mathematical model for each trial in the input data.\n\n    Args:\n        trial_data (list): A list of dictionaries, where each dictionary represents a trial\n                            and contains the necessary data (split_self, token_self, split_opp,\n                            combined_earning).\n        lambda_val (float): The loss aversion parameter.\n        beta_val (float): The inequity sensitivity parameter.\n\n    Returns:\n        list: A list of model predictions, one for each trial.\n    \"\"\"\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0)\n        combined_earning = trial.get(\"combined_earning\", 1)  # Avoid division by zero\n\n        reference = 3 * token_self\n        gain = max(split_self - reference, 0)\n        loss = max(reference - split_self, 0)\n\n        utility = (split_self - 3*token_self) - lambda_val * max(3*token_self - split_self, 0) + beta_val * (split_self - split_opp) / combined_earning\n\n        model_predictions.append(utility)\n\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 1, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 4, \"token_self\": 4, \"combined_earning\": 24, \"split_opp\": 12.0, \"split_self\": 12.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 12.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 12.0}, {\"ID\": 2, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 4, \"token_self\": 4, \"combined_earning\": 24, \"split_opp\": 12.0, \"split_self\": 12.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 12.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 12.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n",
      "timestamp": 1744015834.513899
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['lambda', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"lambda\": {\n      \"description\": \"Loss aversion parameter scaling disutility when receiving less than contributed amount\",\n      \"range\": {\n        \"min\": 1,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity sensitivity parameter for split differences normalized by total earnings\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Participant's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\\u00a3/token)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Opponent's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total trial earnings (3\\u00a3*(tokens_self + tokens_opp))\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "```python\nimport json\nimport math\n\ndef simulate_model(trial_data, lambda_param=1.0, beta_param=0.0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0.0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0.0)\n        combined_earning = trial.get(\"combined_earning\", 1.0)\n\n        reference = 3 * token_self\n        \n        utility = (split_self - reference) - lambda_param * max(reference - split_self, 0) + beta_param * (split_self - split_opp) / combined_earning\n\n        model_predictions.append(utility)  # Return utility instead of probability\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 0, \"group\": \"control\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 1, \"token_self\": 1, \"combined_earning\": 6, \"split_opp\": 3.0, \"split_self\": 3.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 3.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 3.0}, {\"ID\": 1, \"group\": \"control\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 1, \"token_self\": 1, \"combined_earning\": 6, \"split_opp\": 4.0, \"split_self\": 2.0, \"splitperc_opp\": 67, \"splitperc_self\": 33, \"accept\": 0.0, \"accepted_amount\": 0.0, \"accepted_perc\": 0.0, \"proposed_perc\": 33.0, \"proposed_amount\": 2.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1744015837.655352
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['lambda', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"lambda\": {\n      \"description\": \"Loss aversion parameter scaling disutility when receiving less than contributed amount\",\n      \"range\": {\n        \"min\": 1,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity sensitivity parameter for split differences normalized by total earnings\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Participant's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\\u00a3/token)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Opponent's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total trial earnings (3\\u00a3*(tokens_self + tokens_opp))\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "",
      "timestamp": 1744015838.133471
    },
    {
      "solver": "simulate_model_solver",
      "input": "\n                Write Python code to simulate the following mathematical model using only Python standard library (no pandas or numpy).\n                The code should take a list of dictionaries as input (each dictionary representing a trial) and return a value for each trial.\n\n                IMPORTANT: Use these exact parameter names for learnable parameters: ['lambda', 'beta']\n\n                Please write a function called `simulate_model` that:\n                1. Takes a list of dictionaries as input (each dictionary contains trial data)\n                2. Has parameter names that matches the variable descriptions exactly\n                3. Implements the mathematical model above using only standard Python math operations\n                4. Returns a list of model_predictions for each trial (please use the exact name \"model_predictions\" as the return value)\n\n                Your code will be implemented within the following code block (please do not include any code outside of this code block, for instance, you should not include data_json = # imported from .json dumps, or data = json.loads(data_json)):\n\n                ```python\nimport json\nimport math\n\n[YOUR SIMULATE_MODEL FUNCTION HERE]\n\n# Data as list of dictionaries\ndata_json = # imported from .json dumps\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n'''\n\nBelow is an example for you to follow:\n<EXAMPLE>\nModel Specification:\nU_i = \u03b2 + \u03b5E + \u03b7N\n\nVariable Descriptions:\nU_i: Utility (probability) of choosing action i \u03b2: [learnable] Base tendency parameter (0-1) representing inherent preference \u03b5: [learnable] Environmental sensitivity parameter (0-1) E: Environmental cue value (-1 to 1) \u03b7: Noise parameter (0-1) N: Random noise drawn from normal distribution N(0,1)\n\nDataset Structure:\nVariables available:\n- trial_id (int64)\n- environmental_cue (float64)\n- beta (float64)\n- epsilon (float64)\n- eta (float64)\n- utility (float64)\n- choice_made (int64)\n- trial_number (int64)\n- participant_id (int64)\n\n\n<YOUR RESPONSE>\ndef simulate_model(trial_data, beta=1, epsilon=1, eta=1):\n    model_predictions = []\n    for trial in trial_data:\n        E = trial.get(\"environmental_cue\", 0)\n        u1, u2 = random.random(), random.random()\n        N = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        model_predictions.append(beta + (epsilon * E) + (eta * N))\n    return model_predictions\n</YOUR RESPONSE>\n\nNow, create a simulate_model function for the following model incorporating the information provided. DO NOT OUTPUT ANYTHING AFTER YOU RETURN THE MODEL PREDICTIONS.\n                Model Specification:\n                U = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\n\n                Variable Descriptions:\n                {\n  \"variables\": {\n    \"lambda\": {\n      \"description\": \"Loss aversion parameter scaling disutility when receiving less than contributed amount\",\n      \"range\": {\n        \"min\": 1,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity sensitivity parameter for split differences normalized by total earnings\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Participant's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\\u00a3/token)\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Opponent's proposed share in pounds\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total trial earnings (3\\u00a3*(tokens_self + tokens_opp))\",\n      \"range\": {\n        \"min\": \"-inf\",\n        \"max\": \"inf\",\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"distribution\": null,\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nDataset Structure:\n\nDataset Structure:\nVariables available:\n- ID (int64)\n- group (object)\n- trial_number (int64)\n- trial_type (int64)\n- trial_role (int64)\n- token_opp (int64)\n- token_self (int64)\n- combined_earning (int64)\n- split_opp (float64)\n- split_self (float64)\n- splitperc_opp (int64)\n- splitperc_self (int64)\n- accept (float64)\n- accepted_amount (float64)\n- accepted_perc (float64)\n- proposed_perc (float64)\n- proposed_amount (float64)\n\nNumber of observations: 3276\n\n                ",
      "output": "\n```python\nimport json\nimport math\n\ndef simulate_model(trial_data, lambda_val=1.0, beta_val=0.0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0.0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0.0)\n        combined_earning = trial.get(\"combined_earning\", 1)  # Avoid division by zero\n\n        reference = 3 * token_self\n        loss = max(reference - split_self, 0)\n\n        utility = (split_self - reference) - lambda_val * loss + beta_val * (split_self - split_opp) / combined_earning\n        model_predictions.append(utility)  # Store utilities\n    return model_predictions\n\n# Data as list of dictionaries\ndata_json = '[{\"ID\": 1, \"group\": \"treatment\", \"trial_number\": 1, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 1, \"token_self\": 1, \"combined_earning\": 6, \"split_opp\": 3.0, \"split_self\": 3.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 3.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 3.0}, {\"ID\": 2, \"group\": \"treatment\", \"trial_number\": 2, \"trial_type\": 1, \"trial_role\": 1, \"token_opp\": 1, \"token_self\": 1, \"combined_earning\": 6, \"split_opp\": 3.0, \"split_self\": 3.0, \"splitperc_opp\": 50, \"splitperc_self\": 50, \"accept\": 1.0, \"accepted_amount\": 3.0, \"accepted_perc\": 50.0, \"proposed_perc\": 50.0, \"proposed_amount\": 3.0}]'\ndata = json.loads(data_json)\n\n# Get results for the data\nresults = simulate_model(data)\nprint(json.dumps({\"results\": results}))  # Output as JSON for reliable parsing\n```",
      "timestamp": 1744015841.252962
    }
  ],
  "learnable_parameters": {
    "lambda": {
      "description": "Loss aversion parameter scaling disutility when receiving less than contributed amount",
      "range": {
        "min": 1,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Inequity sensitivity parameter for split differences normalized by total earnings",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    }
  },
  "model_specification": "U = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning",
  "variable_descriptions": {
    "lambda": {
      "description": "Loss aversion parameter scaling disutility when receiving less than contributed amount",
      "range": {
        "min": 1,
        "max": 5,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "beta": {
      "description": "Inequity sensitivity parameter for split differences normalized by total earnings",
      "range": {
        "min": -10,
        "max": 10,
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": true,
      "source": "learnable"
    },
    "split_self": {
      "description": "Participant's proposed share in pounds",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "token_self": {
      "description": "Participant's token contribution (3\u00a3/token)",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "split_opp": {
      "description": "Opponent's proposed share in pounds",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    },
    "combined_earning": {
      "description": "Total trial earnings (3\u00a3*(tokens_self + tokens_opp))",
      "range": {
        "min": "-inf",
        "max": "inf",
        "inclusive_min": true,
        "inclusive_max": true
      },
      "distribution": null,
      "learnable": false,
      "source": "data"
    }
  },
  "model_summary": "A prospect-theoretic model combining loss aversion relative to the participant's contribution (3*token_self) and sensitivity to inequity between shares. Losses (receiving less than contributed) are scaled by lambda, while beta captures reactions to differences in proposed splits, adjusted for total earnings.",
  "target_variable": "accept",
  "full_reasoning": "<think>\nOkay, let's tackle this step by step. The goal is to design a computational model that predicts whether a participant (as the responder) accepts or rejects a proposal. The model needs to focus on responder behavior, use the given variables appropriately, and ensure parameters can be recovered while maintaining accuracy.\n\nFirst, I need to identify which variables are relevant for the responder's decision. The target variable is \"accept\", which is binary (1 or 0). The model should predict the utility of accepting, which will then be converted to a probability via a logistic function.\n\nKey variables from the dataset that might influence the responder's decision:\n- token_self: The participant's contribution (tokens they found).\n- token_opp: The opponent's contribution.\n- split_self: The proposed share for the participant in \u00a3.\n- split_opp: The proposed share for the opponent.\n- combined_earning: Total money in the pot (each token is \u00a33, so this is 3*(token_self + token_opp)).\n- trial_type: Whether contributions are equal, opponent more, or participant more.\n\nThe model should incorporate fairness considerations and the participant's own contribution. Since the utility needs to predict acceptance, the model should compare the proposed split to what's considered fair based on contributions.\n\nI recall that in ultimatum games, people often reject unfair offers even if it means getting nothing, which suggests that fairness (or inequity aversion) plays a role. So, the model might include terms for both the participant's share relative to their contribution and the difference between the shares.\n\nPossible components of utility:\n1. Utility from the participant's share compared to their contribution.\n2. Disutility from inequity (difference between split_self and split_opp).\n3. Sensitivity parameters for these components.\n\nConsidering the parameters need to be learnable with bounds. Let's think about each component:\n\n1. **Contribution-based Fairness**: The participant's expected fair share might be proportional to their contribution. For example, if they contributed token_self tokens out of total tokens (token_self + token_opp), their fair share would be (token_self / (token_self + token_opp)) * combined_earning. The difference between the proposed split_self and this fair share could be a term, scaled by a parameter.\n\n2. **Inequity Aversion**: The difference between split_opp and split_self could cause disutility. People might dislike when the opponent gets more than them. This could be modeled as (split_opp - split_self) scaled by another parameter.\n\n3. **Non-linear Transformations**: Maybe the utility isn't linear in these differences. For example, people might be more sensitive to inequity when it's against them. So, perhaps a parameter that scales the inequity term only when split_opp > split_self.\n\n4. **Reference Points**: The participant's contribution (token_self) might serve as a reference point. If split_self is less than what they contributed (in \u00a3 terms, 3*token_self), that could be a loss, and treated differently than gains.\n\nPutting this together, the utility function could have terms for gains/losses relative to contribution and inequity aversion. Let's formalize this.\n\nLet\u2019s define:\n- Reference point: 3 * token_self (the amount the participant contributed in \u00a3).\n- Gain/Loss: split_self - reference. If positive, it's a gain; if negative, a loss.\n- Inequity: split_opp - split_self.\n\nProspect Theory elements: People weigh losses more than gains. So, we can have a loss aversion parameter (\u03bb) that scales the loss term.\n\nInequity aversion can be modeled with a parameter (\u03b2) that scales the difference. However, maybe the inequity effect depends on the total combined earning. For example, a \u00a35 difference in a \u00a310 pot is more significant than in a \u00a3100 pot. So, perhaps normalize the inequity by combined_earning.\n\nAlso, trial_type might affect the sensitivity. For example, if the participant contributed more (trial_type=3), they might be more sensitive to inequity.\n\nBut the model should not use trial_type directly unless specified. Wait, the task says the model should use the variables in the dataset. Since trial_type is available, maybe it can be used to modulate parameters. For example, different \u03b2 for different trial types.\n\nBut to keep it simple initially, let's focus on the core variables.\n\nDraft model:\n\nUtility = [\u03b1 * (split_self - 3*token_self)] + [\u03b2 * (split_opp - split_self)/combined_earning] + [\u03bb * (split_self < 3*token_self) * (split_self - 3*token_self)]\n\nWait, but \u03bb would be for loss aversion. So maybe:\n\nUtility = (split_self - reference) + \u03bb * (split_self < reference) * (split_self - reference) + \u03b2 * (split_opp - split_self)/combined_earning\n\nBut this might not capture the non-linear aspects. Alternatively, separate gain and loss terms:\n\nIf split_self >= reference:\n    gain_utility = (split_self - reference)^\u03c3\nElse:\n    loss_utility = -\u03bb * (reference - split_self)^\u03c1\n\nThen add the inequity term.\n\nBut introducing \u03c3 and \u03c1 as parameters might complicate things. However, the task says parameters need bounds. Maybe fix \u03c3 and \u03c1 to 1 for simplicity, leaving \u03bb and \u03b2 as learnable.\n\nAlternatively, allow \u03c3 and \u03c1 to vary but with bounds (e.g., between 0.5 and 2).\n\nBut the model needs to be parsimonious to pass parameter recovery. Let's try:\n\nUtility = (split_self - reference) + \u03bb * max(reference - split_self, 0) + \u03b2 * (split_opp - split_self)/combined_earning\n\nWait, that might not be right. The loss aversion is typically a multiplier on losses. So:\n\nUtility = (split_self - reference) + \u03bb * (reference - split_self) * I(split_self < reference) + \u03b2 * (split_opp - split_self)/combined_earning\n\nWhere I() is an indicator function. But this can be rewritten as:\n\nIf split_self >= reference:\n    utility_gain = (split_self - reference)\nElse:\n    utility_loss = \u03bb * (reference - split_self)\n\nThen add the inequity term.\n\nBut combining these:\n\nUtility = [ (split_self >= reference) * (split_self - reference) ] + [ (split_self < reference) * \u03bb * (split_self - reference) ] + \u03b2 * (split_opp - split_self)/combined_earning\n\nWait, that might not be correct. If split_self < reference, then (split_self - reference) is negative, so multiplying by \u03bb (which is positive) would make it more negative. But loss aversion suggests that losses are felt more strongly, so perhaps:\n\nUtility = (split_self - reference) + \u03bb * max(reference - split_self, 0) + \u03b2 * (split_opp - split_self)/combined_earning\n\nWait, no. Let's think again. In prospect theory, the value function is:\n\nV(x) = x^\u03c3 if x >=0, -\u03bb*(-x)^\u03c1 if x <0.\n\nSo for the difference (split_self - reference), if positive, it's a gain, else a loss.\n\nSo:\n\ngain_loss_utility = if split_self >= reference: (split_self - reference)^\u03c3 else: -\u03bb * (reference - split_self)^\u03c1\n\nThen add the inequity term.\n\nBut this introduces \u03c3, \u03c1, and \u03bb as parameters. To keep it manageable, maybe set \u03c3=1 and \u03c1=1, simplifying to:\n\ngain_loss_utility = (split_self - reference) - \u03bb * (reference - split_self) * I(split_self < reference)\n\nWhich simplifies to:\n\ngain_loss_utility = (split_self - reference) * [1 + (\u03bb -1) * I(split_self < reference)]\n\nBut this might not capture the true loss aversion. Alternatively, separate the terms:\n\ngain_loss_utility = (split_self - reference) + (\u03bb -1) * max(reference - split_self, 0)\n\nBecause when split_self < reference, the loss is (reference - split_self), and the total utility would be (split_self - reference) + (\u03bb -1)(reference - split_self) = (split_self - reference) + (\u03bb -1)(reference - split_self) = (split_self - reference)(1 - (\u03bb -1)) = (split_self - reference)(2 - \u03bb). Hmm, maybe not the best approach.\n\nAlternatively, model the gain and loss separately:\n\ngain = max(split_self - reference, 0)\nloss = max(reference - split_self, 0)\ngain_loss_utility = gain - \u03bb * loss\n\nThen add the inequity term.\n\nSo:\n\nUtility = (gain - \u03bb * loss) + \u03b2 * (split_opp - split_self)/combined_earning\n\nThis seems better. Here, \u03bb >1 would indicate loss aversion.\n\nAdditionally, the inequity term (split_opp - split_self) could be positive or negative. If the opponent is getting more, that's negative for the participant. But maybe the responder dislikes when the opponent gets more, so the term is (split_opp - split_self), scaled by \u03b2. If \u03b2 is positive, then a higher split_opp than split_self would decrease utility, leading to lower acceptance.\n\nBut perhaps the inequity term should be the absolute difference, or only consider cases where the opponent gets more. Alternatively, split_opp - split_self could be positive (opponent gets more) or negative (participant gets more). The parameter \u03b2 would then capture sensitivity to this difference.\n\nBut to normalize, dividing by combined_earning makes sense, as a \u00a31 difference in a \u00a310 pot is more significant than in a \u00a3100 pot.\n\nPutting it all together:\n\nUtility = [max(split_self - reference, 0) - \u03bb * max(reference - split_self, 0)] + \u03b2 * (split_opp - split_self)/combined_earning\n\nWhere reference = 3 * token_self.\n\nThis model has two learnable parameters: \u03bb (loss aversion) and \u03b2 (inequity aversion). Additionally, maybe a parameter for the gain term, but in the current setup, the gain is linear. If we want to allow diminishing sensitivity to gains, we could add a parameter \u03c3 for the gain exponent:\n\nUtility = [ (max(split_self - reference, 0))^\u03c3 - \u03bb * (max(reference - split_self, 0))^\u03c1 ] + \u03b2 * (split_opp - split_self)/combined_earning\n\nBut this adds \u03c3 and \u03c1 as parameters. However, the task says to keep the model applicable and not too complex. Since parameter recovery is a priority, perhaps keep \u03c3 and \u03c1 fixed (e.g., \u03c3=1, \u03c1=1) to reduce the number of parameters. Alternatively, set \u03c3=0.88 and \u03c1=0.88 as per prospect theory findings, but fix them, leaving \u03bb and \u03b2 as learnable.\n\nBut the user's example included parameters with ranges, so let's proceed with the model that has \u03bb and \u03b2 as learnable, with \u03c3 and \u03c1 fixed.\n\nWait, the task says \"For any learnable parameters in your model, you must specify finite numerical bounds\". So if \u03c3 and \u03c1 are fixed (not learnable), they don't need bounds. But if they are learnable, they do.\n\nTo balance complexity and recovery, perhaps fix \u03c3 and \u03c1 to 1, making the gain and loss terms linear. Then the model has two parameters: \u03bb (loss aversion) and \u03b2 (inequity aversion).\n\nAlternatively, allow \u03c3 and \u03c1 to be learnable but with tight bounds. For example, \u03c3 in [0.5, 1.5], \u03c1 in [0.5, 2]. But this adds more parameters, which might complicate recovery.\n\nGiven the dataset has 36 instances per participant, having 2-3 parameters is manageable.\n\nSo, proceeding with:\n\nUtility = (split_self - reference) - \u03bb*(reference - split_self) * I(split_self < reference) + \u03b2*(split_opp - split_self)/combined_earning\n\nBut when split_self < reference, the term becomes (split_self - reference) - \u03bb*(reference - split_self) = (split_self - reference)(1 + \u03bb). For example, if \u03bb=2, then the loss is weighted 2 times more than the gain.\n\nWait, let's re-express:\n\nIf split_self >= reference:\n    gain = split_self - reference\n    loss = 0\nElse:\n    gain = 0\n    loss = reference - split_self\n\nUtility = gain - \u03bb*loss + \u03b2*(split_opp - split_self)/combined_earning\n\nYes, this makes sense. So the utility is the gain minus lambda times the loss, plus beta times the inequity term.\n\nNow, the probability of accepting is logistic(Utility). The model needs to predict the log odds of accepting, which is the utility.\n\nParameters:\n- lambda (\u03bb): loss aversion factor, must be >=1 since losses loom larger. Bounds [1, 5], for example.\n- beta (\u03b2): sensitivity to inequity. If beta is positive, then when split_opp > split_self (opponent gets more), the utility decreases, making acceptance less likely. Bounds could be [-5, 5], but likely negative since (split_opp - split_self) being positive (opponent gets more) would decrease utility, so beta should be negative to penalize that. Wait, no: if beta is negative, then a positive (split_opp - split_self) would multiply by beta, leading to a negative term, reducing utility. So beta's sign determines the direction of the effect.\n\nAlternatively, take absolute value or square, but perhaps better to let beta be learned as a free parameter. So beta could be negative, indicating that higher opponent shares reduce utility.\n\nBut to avoid confusion, perhaps define the inequity term as (split_self - split_opp), so that higher values (participant gets more) increase utility. Then beta would be positive. Let me think.\n\nCurrent term: \u03b2*(split_opp - split_self)/combined_earning\n\nIf split_opp > split_self, this term is positive. If beta is negative, that would decrease utility, making acceptance less likely. If beta is positive, it would increase utility, which doesn't make sense. So beta should be negative to penalize the participant when the opponent gets more.\n\nBut maybe participants care about fairness in terms of the difference relative to the total. Alternatively, the term could be (split_self - fair_share), where fair_share is based on contributions.\n\nWait, another approach: fair_share = (token_self / (token_self + token_opp)) * combined_earning\n\nThen, the difference between split_self and fair_share could be a term. But this introduces another component.\n\nBut the initial model with gain-loss and inequity terms might be sufficient.\n\nSo, parameters are \u03bb (loss aversion) and \u03b2 (inequity aversion). Both need bounds.\n\nFor \u03bb, since it's loss aversion, typically >=1. So bounds [1, 5].\n\nFor \u03b2, it's the sensitivity to the opponent's share relative to self. If the participant dislikes the opponent getting more, \u03b2 should be negative. So bounds could be [-5, 0], but maybe wider [-10, 10] to allow the model to learn.\n\nWait, but the term is (split_opp - split_self)/combined_earning. If split_opp is larger than split_self, this term is positive. If beta is negative, then a positive term multiplied by beta gives a negative contribution to utility, reducing the chance of acceptance. That makes sense. So beta should be negative. But to allow the model to capture different behaviors, maybe not restrict the sign. For example, if beta is positive, it would imply that the participant prefers when the opponent gets more, which is unlikely. So perhaps constrain beta to be <=0. But the task says bounds should be generous. So maybe set beta's range as [-10, 10], but in practice, we expect it to be negative.\n\nAlternatively, use absolute value of the difference, but that might not capture the direction.\n\nAlternatively, the term could be (split_self - split_opp)/combined_earning, so a positive value means the participant gets more. Then beta positive would mean they prefer that, which is sensible. Let's adjust the model:\n\nUtility = gain - \u03bb*loss + \u03b2*(split_self - split_opp)/combined_earning\n\nNow, if split_self > split_opp, the term is positive, and if beta is positive, it increases utility. This makes more intuitive sense. So beta would be positive, and we can set bounds [0, 10], but maybe [-10, 10] to allow for negative values (though unlikely).\n\nBut this changes the interpretation. The original term was (split_opp - split_self), which is negative when the participant gets more. By flipping it to (split_self - split_opp), it's positive when the participant gets more, and beta positive would mean they like that. So this seems better.\n\nSo adjusting the model:\n\nUtility = [gain - \u03bb*loss] + \u03b2*(split_self - split_opp)/combined_earning\n\nWhere gain = max(split_self - reference, 0), loss = max(reference - split_self, 0)\n\nThis way, a positive beta means the participant prefers getting more than the opponent, which aligns with self-interest. However, if beta is negative, it might indicate a preference for equity, but that's less likely. Alternatively, participants might have inequity aversion, disliking both being ahead and behind. But the model as structured only considers the difference in splits, not the fair share based on contributions.\n\nWait, but the first term (gain - \u03bb*loss) already captures the comparison to their own contribution (reference = 3*token_self). The second term captures the difference in splits. So combining both own contribution fairness and split difference.\n\nBut maybe the split difference should be compared to the fair split based on contributions. For example, the fair split would be (token_self / (token_self + token_opp)) * combined_earning. Then, the difference between split_self and the fair share could be a term.\n\nBut that introduces another component. Let's consider that.\n\nFair_share_self = (token_self / (token_self + token_opp)) * combined_earning\n\nThen, split_self - fair_share_self could be a term. If positive, the participant is getting more than fair; if negative, less.\n\nBut this might be redundant with the gain/loss term, which is based on their own contribution (reference = 3*token_self). Because the fair share based on contributions is exactly 3*token_self, since combined_earning is 3*(token_self + token_opp), so:\n\nFair_share_self = 3*token_self\n\nBecause (token_self / (token_self + token_opp)) * 3*(token_self + token_opp) = 3*token_self.\n\nAh, right! So the fair share based on contributions is exactly 3*token_self. Therefore, the gain/loss term (split_self - reference) where reference=3*token_self is exactly the difference between the proposed split and the fair share. So the first term already captures deviation from the contribution-based fair share.\n\nThe second term, (split_self - split_opp)/combined_earning, captures the difference in splits, which might relate to another form of fairness (equality regardless of contributions). But in the task description, it's noted that two equal splits may not be fair if contributions differ. So participants might care about both their share relative to their contribution and the equality of splits.\n\nThus, the model includes both terms: one for contribution-based fairness (gain/loss) and one for split equality (inequity).\n\nSo the final model is:\n\nUtility = (split_self - reference) - \u03bb*max(reference - split_self, 0) + \u03b2*(split_self - split_opp)/combined_earning\n\nBut wait, when split_self >= reference, the first term is (split_self - reference), and the second term is 0. When split_self < reference, the first term is (split_self - reference) - \u03bb*(reference - split_self) = (split_self - reference) * (1 + \u03bb). Which is equivalent to:\n\nIf split_self >= reference:\n    utility_contribution = (split_self - reference)\nElse:\n    utility_contribution = -(1 + \u03bb)*(reference - split_self)\n\nThen add the inequity term.\n\nBut combining it into one expression:\n\nutility_contribution = (split_self - reference) - \u03bb*max(reference - split_self, 0)\n\nWhich simplifies to:\n\nIf split_self >= reference: (split_self - reference)\nElse: (split_self - reference) - \u03bb*(reference - split_self) = (split_self - reference) - \u03bb*(reference - split_self) = -(1 + \u03bb)*(reference - split_self)\n\nSo the contribution term is linear in gains and losses, with loss aversion \u03bb.\n\nThe inequity term is \u03b2*(split_self - split_opp)/combined_earning.\n\nThus, the utility function becomes:\n\nU = (split_self - reference) - \u03bb*max(reference - split_self, 0) + \u03b2*(split_self - split_opp)/combined_earning\n\nNow, the parameters are \u03bb (loss aversion) and \u03b2 (inequity sensitivity). Both need bounds.\n\nFor \u03bb, since it's loss aversion, typically >=1. So set bounds [1, 5], inclusive.\n\nFor \u03b2, it's the sensitivity to the split difference. If \u03b2 is positive, participants prefer getting more than the opponent; if negative, they prefer equity. But given the task, participants might reject offers where they get less than their fair share (reference) and also dislike inequity. However, the sign of \u03b2 depends on how the term is structured. Since the term is (split_self - split_opp), a positive \u03b2 means that the participant prefers getting more than the opponent, which could be self-interest. But if \u03b2 is negative, it might indicate aversion to inequity (preferring smaller differences). However, this is ambiguous. To allow the model to capture either, set \u03b2's bounds as [-10, 10], generous but finite.\n\nThe variables used are split_self, split_opp, combined_earning, and token_self (for reference). The trial_role is responder, so we filter for trial_role=1.\n\nNow, the mathematical model:\n\n<MODEL>\nU = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\n</MODEL>\n\nVariables:\n\n- lambda: loss aversion parameter, learnable, range [1, 5]\n- beta: inequity sensitivity, learnable, range [-10, 10]\n- split_self, token_self, split_opp, combined_earning: data variables\n\nThe target variable is 'accept', which is binary. The utility U is converted to a probability via logistic function: P(accept) = 1 / (1 + exp(-U)).\n\nNow, specifying the variables in JSON:\n\nFor each variable in the model:\n\n- lambda: learnable, description: \"Loss aversion parameter, scaling the disutility of receiving less than one's contribution\", range [1,5], inclusive.\n- beta: learnable, description: \"Sensitivity to the difference between one's own and the opponent's share\", range [-10,10].\n- split_self: data variable.\n- token_self: data variable.\n- split_opp: data variable.\n- combined_earning: data variable.\n\nThe variables U is calculated, not learnable.\n\nSo the JSON would be:\n\n{\n  \"variables\": {\n    \"lambda\": {\n      \"description\": \"Loss aversion parameter, scaling the disutility of receiving less than one's contribution (3*token_self)\",\n      \"range\": {\n        \"min\": 1,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Sensitivity to the difference between one's own and the opponent's share, normalized by combined earnings\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Proposed share for the participant in \u00a3\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Number of tokens found by the participant\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Proposed share for the opponent in \u00a3\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total money earned in the trial (\u00a3)\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n\nThe summary should explain the model concisely.\n\n<SUMMARY>\nA prospect-theoretic model combining loss aversion relative to the participant's contribution (3*token_self) and sensitivity to inequity between shares. Losses (receiving less than contributed) are scaled by lambda, while beta captures reactions to differences in proposed splits, adjusted for total earnings.\n</SUMMARY>\n</think>\n\n<MODEL>\nU = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\n</MODEL>\n\n<target_variable>accept</target_variable>\n\n<VARIABLES>\n{\n  \"variables\": {\n    \"lambda\": {\n      \"description\": \"Loss aversion parameter scaling disutility when receiving less than contributed amount\",\n      \"range\": {\n        \"min\": 1,\n        \"max\": 5,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"beta\": {\n      \"description\": \"Inequity sensitivity parameter for split differences normalized by total earnings\",\n      \"range\": {\n        \"min\": -10,\n        \"max\": 10,\n        \"inclusive_min\": true,\n        \"inclusive_max\": true\n      },\n      \"learnable\": true,\n      \"source\": \"learnable\"\n    },\n    \"split_self\": {\n      \"description\": \"Participant's proposed share in pounds\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"token_self\": {\n      \"description\": \"Participant's token contribution (3\u00a3/token)\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"split_opp\": {\n      \"description\": \"Opponent's proposed share in pounds\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    },\n    \"combined_earning\": {\n      \"description\": \"Total trial earnings (3\u00a3*(tokens_self + tokens_opp))\",\n      \"learnable\": false,\n      \"source\": \"data\"\n    }\n  }\n}\n</VARIABLES>\n\n<SUMMARY>\nHybrid prospect theory model evaluating offers relative to contribution-based reference points (3*token_self) with loss aversion (lambda) and normalized inequity sensitivity (beta). Losses below contribution threshold are amplified, while split differences are scaled by total earnings magnitude.\n</SUMMARY>",
  "prediction_type": "utility",
  "current_model": "Specification: U = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\nSummary: A prospect-theoretic model combining loss aversion relative to the participant's contribution (3*token_self) and sensitivity to inequity between shares. Losses (receiving less than contributed) are scaled by lambda, while beta captures reactions to differences in proposed splits, adjusted for total earnings.\nTarget Variable: accept\nPrediction Type: utility",
  "simulation_errors": [
    {
      "retry_number": 0,
      "error": "  File \"<string>\", line 28\n    results = simulate_model(data, lambda=0.5, beta=0.5)\n                                         ^\nSyntaxError: invalid syntax\n",
      "timestamp": 1744015830.668166
    },
    {
      "retry_number": 1,
      "error": "  File \"<string>\", line 41\n    results = simulate_model(data, lambda=0.5, beta=0.5)\n                                         ^\nSyntaxError: invalid syntax\n",
      "timestamp": 1744015834.5712209
    },
    {
      "retry_number": 2,
      "error": "  File \"<string>\", line 25\n    results = simulate_model(data, lambda=0.5, beta=0.5)\n                                         ^\nSyntaxError: invalid syntax\n",
      "timestamp": 1744015837.696589
    },
    {
      "retry_number": 3,
      "error": "Could not extract simulate_model function from generated code",
      "timestamp": 1744015838.133493
    },
    {
      "retry_number": 4,
      "error": "  File \"<string>\", line 25\n    results = simulate_model(data, lambda=0.5, beta=0.5)\n                                         ^\nSyntaxError: invalid syntax\n",
      "timestamp": 1744015841.299983
    }
  ],
  "current_retry": 4,
  "simulation_error": "Failed after maximum retries",
  "simulation_code": "def simulate_model(trial_data, lambda_val=1.0, beta_val=0.0):\n    model_predictions = []\n    for trial in trial_data:\n        split_self = trial.get(\"split_self\", 0.0)\n        token_self = trial.get(\"token_self\", 0)\n        split_opp = trial.get(\"split_opp\", 0.0)\n        combined_earning = trial.get(\"combined_earning\", 1)  # Avoid division by zero\n\n        reference = 3 * token_self\n        loss = max(reference - split_self, 0)\n\n        utility = (split_self - reference) - lambda_val * loss + beta_val * (split_self - split_opp) / combined_earning\n        model_predictions.append(utility)  # Store utilities\n    return model_predictions",
  "total_retries": 5,
  "fitting_warnings": [
    "Error fitting participant 1001: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2299297781080423, 'beta': -9.355916600912092}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.684437940008432, 'beta': 7.6478473351193585}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.7579486679979097, 'beta': -5.093519894114918}\"]",
    "Error fitting participant 1002: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.211890909220365, 'beta': 6.14176286599729}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.7487495934524553, 'beta': 7.094703080635085}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.898354556948573, 'beta': 4.865383496286174}\"]",
    "Error fitting participant 1003: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.739851464502162, 'beta': 7.784954655942549}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.0858206967445265, 'beta': -3.248696839900946}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.840099337146405, 'beta': 9.667191115691327}\"]",
    "Error fitting participant 1004: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.7681493511898587, 'beta': 7.343183068095655}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.6517145692316806, 'beta': -4.896103817264141}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.002683038592353, 'beta': 1.3952020585892537}\"]",
    "Error fitting participant 1005: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4221521016977028, 'beta': -9.223232072307663}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.525635556911847, 'beta': 9.303888345787733}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2644585302638522, 'beta': 6.684955905836517}\"]",
    "Error fitting participant 1006: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5493408869293646, 'beta': 3.074033854516216}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.715691120907236, 'beta': 2.7134260438012614}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.3230080273560527, 'beta': 6.582473554162565}\"]",
    "Error fitting participant 1007: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.697515248558863, 'beta': 5.232549018747768}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.4712305361249465, 'beta': 6.993089822081558}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.593237976090517, 'beta': 3.42469868666673}\"]",
    "Error fitting participant 1008: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.075487964709868, 'beta': -1.647255333277899}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.5601750292765164, 'beta': -3.677060106123424}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.175409417935877, 'beta': -4.360587384329477}\"]",
    "Error fitting participant 1009: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9035638681286686, 'beta': -0.703764316096473}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.0677991771467275, 'beta': -7.333554110824954}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.752363622267359, 'beta': 8.19322243608833}\"]",
    "Error fitting participant 1010: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2512246864192798, 'beta': -2.205851406353845}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7371633474364327, 'beta': 4.399953079481882}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.0854234187649556, 'beta': 3.735534034868987}\"]",
    "Error fitting participant 1011: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.976616549420736, 'beta': 0.8439122382525373}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.96056142637315, 'beta': -4.089837914975714}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.08305123946251, 'beta': -5.182309920103361}\"]",
    "Error fitting participant 1012: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.6915843297423376, 'beta': -5.48350402630954}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.401592666669952, 'beta': 9.119449250167825}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.6192477461310957, 'beta': -7.243963210420352}\"]",
    "Error fitting participant 1013: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.0031493231286985, 'beta': 2.8693006593147814}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.781959259555229, 'beta': 5.169896430793742}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.663404762722151, 'beta': 7.968027092143526}\"]",
    "Error fitting participant 1015: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.786845328044362, 'beta': 0.22673255548007987}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.363704763175537, 'beta': 0.34523975149204134}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.3864452834834773, 'beta': 3.2618107129644685}\"]",
    "Error fitting participant 1016: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.994623498625967, 'beta': -8.645257349590516}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.446995220767043, 'beta': -8.240825850289193}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.454738651177694, 'beta': 0.8049035291011002}\"]",
    "Error fitting participant 1017: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.611428302793563, 'beta': -1.6581134857282613}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.101679340947394, 'beta': -5.6609846818583875}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.6865857886438023, 'beta': 3.843496411719782}\"]",
    "Error fitting participant 1018: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1019064374387884, 'beta': -3.456205336596783}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.0249998119208685, 'beta': -0.7098381320234441}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.557731130698604, 'beta': 1.4284466780181546}\"]",
    "Error fitting participant 1019: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.00426085167684, 'beta': 9.773604485842853}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.499802431836498, 'beta': -9.661074003176973}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.783721309117107, 'beta': -6.4575772117086245}\"]",
    "Error fitting participant 1020: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.837751720186676, 'beta': 9.504304126029115}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.650191818010183, 'beta': -6.233783281516048}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.913136050879067, 'beta': 1.4725029969827368}\"]",
    "Error fitting participant 1021: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9939364255731302, 'beta': -8.455184887532166}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5443715983775412, 'beta': -2.2547388631401404}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.130522735059632, 'beta': 2.926884051739613}\"]",
    "Error fitting participant 1022: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.066510722135357, 'beta': 2.3855275173902886}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.583150253251013, 'beta': 5.929359948799636}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.6171992027367383, 'beta': -1.6855648890159252}\"]",
    "Error fitting participant 1023: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.1038718560958167, 'beta': -9.878091351256748}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8605997255454452, 'beta': 2.8984612235722107}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.797071698108047, 'beta': -0.7038063670287702}\"]",
    "Error fitting participant 1024: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.6063709170071334, 'beta': -9.208752541877516}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.386862153504181, 'beta': 8.179449417563635}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.096945140364046, 'beta': 4.719343952821941}\"]",
    "Error fitting participant 1025: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.106887911405476, 'beta': -4.74343836866101}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.045478971910411, 'beta': 4.712707498250289}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.404063814042436, 'beta': 3.4476196132732504}\"]",
    "Error fitting participant 1026: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.933262048391054, 'beta': -3.3662137145305904}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.374524080213017, 'beta': -1.7184806905290255}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9097101675368857, 'beta': -8.051930636280094}\"]",
    "Error fitting participant 1027: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8227865483022163, 'beta': 2.9030580980148315}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.526105641627337, 'beta': 9.861421015360051}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1215242469201328, 'beta': 3.227553551284627}\"]",
    "Error fitting participant 1028: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.522815129263237, 'beta': -3.6226966737503545}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2706464439482383, 'beta': 6.973649146701483}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8696076942276507, 'beta': -0.5139032515132751}\"]",
    "Error fitting participant 1029: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.5609764558427295, 'beta': -9.808487799874346}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.811246729287027, 'beta': 8.219732852351594}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1027094189599143, 'beta': -4.750372525526809}\"]",
    "Error fitting participant 1030: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5303564083329597, 'beta': -1.70998278702287}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.020121871658182, 'beta': 1.58516509041905}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5128770039246997, 'beta': 6.943150378205591}\"]",
    "Error fitting participant 1031: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.9738234838305138, 'beta': -2.745714794426439}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2642941976892246, 'beta': -2.95111271540641}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.2902508195819715, 'beta': 5.615252703703073}\"]",
    "Error fitting participant 1032: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.950641706372983, 'beta': -9.142689572269585}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.393822657526259, 'beta': -2.7100706879578285}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.5103563542551015, 'beta': -4.502559139256697}\"]",
    "Error fitting participant 1033: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8517134976310436, 'beta': 6.095679062649165}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.977016567651549, 'beta': -1.0701457911962855}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.043194787346783, 'beta': -5.811245513625054}\"]",
    "Error fitting participant 1034: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8201643080489895, 'beta': -8.72516943384101}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.0867252543151777, 'beta': 8.471544141968302}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.783778812666088, 'beta': -4.432207883740369}\"]",
    "Error fitting participant 1035: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.525623761454729, 'beta': -2.1681424606656963}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.418318328186425, 'beta': -5.954746819302201}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.58874895665709, 'beta': 0.3455109265461189}\"]",
    "Error fitting participant 1036: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.269634496988795, 'beta': 5.623177451999201}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9371852785142427, 'beta': -3.5888122054034577}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.1871678759007462, 'beta': 4.061776207646734}\"]",
    "Error fitting participant 1037: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.779718237508987, 'beta': -8.878157870175123}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4695922557232133, 'beta': 6.010953901014425}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.428167371889281, 'beta': -3.862842740730188}\"]",
    "Error fitting participant 1038: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4159762700250917, 'beta': 4.914385507934515}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.9540595645693117, 'beta': 6.0263131613903695}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.8083474382939055, 'beta': 1.3522894801334822}\"]",
    "Error fitting participant 1039: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.394442155167461, 'beta': -5.164322634746583}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.214281426818531, 'beta': 6.129982066611252}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.6105152555109057, 'beta': 3.649359696500838}\"]",
    "Error fitting participant 1040: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.1602946391008455, 'beta': -4.7467912713073135}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.508460942051213, 'beta': -0.2992357608995295}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.320445608365637, 'beta': 4.451452137137796}\"]",
    "Error fitting participant 1041: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.649789652391296, 'beta': 0.3454498514853359}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.80760537775281, 'beta': 3.5624683774773764}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9663169886772254, 'beta': 2.0793665892594007}\"]",
    "Error fitting participant 1042: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.950104153258568, 'beta': -8.270480509720741}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4119366531291417, 'beta': -0.8849413787837985}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.282742402882012, 'beta': 9.631123856212582}\"]",
    "Error fitting participant 1043: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.7198937331999096, 'beta': 7.766182350605252}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.672377945523414, 'beta': -4.652487700126963}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.512141627853171, 'beta': -3.3588382468457105}\"]",
    "Error fitting participant 1044: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4617839054885944, 'beta': 7.5777172936885115}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.4896814714368327, 'beta': -1.0953742149955659}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9801501459222473, 'beta': 1.0457117281171726}\"]",
    "Error fitting participant 2001: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.058375183644576, 'beta': -6.308775183313364}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.927717403106389, 'beta': 4.0551043265824305}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5544307270803892, 'beta': 0.540339747590707}\"]",
    "Error fitting participant 2002: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.7730681676816094, 'beta': 7.2586121178376075}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.487074315904175, 'beta': -3.4086783570139634}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.406301193572171, 'beta': 3.370995284356386}\"]",
    "Error fitting participant 2003: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.90173620721977, 'beta': -9.970527960553818}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.8981737510502765, 'beta': 6.170582430417998}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.072966479325478, 'beta': -1.192471664010215}\"]",
    "Error fitting participant 2004: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.660203215919557, 'beta': 0.29033273632011714}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.3109330739559524, 'beta': -5.4350048355784075}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.257130549679031, 'beta': -7.525631936906323}\"]",
    "Error fitting participant 2005: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.2732546324638205, 'beta': -5.896038322513444}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.0244425400912318, 'beta': -2.9140871721310617}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.605271199417165, 'beta': 2.6268973774458164}\"]",
    "Error fitting participant 2006: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5493313779295441, 'beta': 7.482046258699061}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2769705969222716, 'beta': -7.215615651427831}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.162243222845479, 'beta': 0.23313000982942356}\"]",
    "Error fitting participant 2007: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.104938164727475, 'beta': -8.026682126746556}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.459839393445154, 'beta': 7.6870033134506315}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.846848741422085, 'beta': 8.477550864124478}\"]",
    "Error fitting participant 2008: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2079153585166997, 'beta': 5.491566852225134}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.23626746295828, 'beta': 7.1199173713342425}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.999973251770794, 'beta': 8.08703996506571}\"]",
    "Error fitting participant 2009: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.9615863377877423, 'beta': -3.5297252034503295}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5865134549155098, 'beta': -1.8584884235273957}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.4344065274592186, 'beta': -2.3890213719224107}\"]",
    "Error fitting participant 2010: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5995652472037762, 'beta': -8.49873341497089}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.486656028419831, 'beta': 1.6200075746305558}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5913752613074497, 'beta': 5.617581760925582}\"]",
    "Error fitting participant 2011: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4138092784859744, 'beta': -0.26534963922403243}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.700482118311862, 'beta': -6.040826140870392}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8281900301271987, 'beta': -0.5868579373218648}\"]",
    "Error fitting participant 2012: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8217558029903054, 'beta': -5.122493320567938}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.920270189460897, 'beta': 7.804644070576298}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.3994553032242996, 'beta': -9.02802037658991}\"]",
    "Error fitting participant 2013: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.227900687667093, 'beta': -1.0976136832779453}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.861492212545237, 'beta': 5.487781910116279}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.570636252447406, 'beta': 2.1665726167887165}\"]",
    "Error fitting participant 2014: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7922186938630915, 'beta': 4.910368987010678}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.0291747605561543, 'beta': -8.404519305442248}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2919197197353474, 'beta': 3.820764949524282}\"]",
    "Error fitting participant 2015: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.946961936171919, 'beta': 4.5259900238806505}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.380124280127446, 'beta': -2.464902922142458}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.523437740584168, 'beta': -6.828496318798036}\"]",
    "Error fitting participant 2016: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.924211424383838, 'beta': 5.507001220940248}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.421275249947909, 'beta': -2.5889564246783703}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.3361494586056168, 'beta': -4.5838787617236605}\"]",
    "Error fitting participant 2017: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.7351646957843907, 'beta': -9.82233363597687}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.56051759346585, 'beta': -0.4145241931598047}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7018866365834455, 'beta': -4.443799030007025}\"]",
    "Error fitting participant 2018: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.677411186089487, 'beta': 7.545414625515562}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.307581208028234, 'beta': 2.7325527165298613}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.807707567128861, 'beta': -7.721063551496135}\"]",
    "Error fitting participant 2019: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1091459988320924, 'beta': 6.121461897279843}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4673606344355812, 'beta': -8.067443239806655}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.9869724921676446, 'beta': 1.7234378877728762}\"]",
    "Error fitting participant 2020: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.03554111475426, 'beta': -2.0129445255219895}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.725338765111404, 'beta': 2.1992037469617074}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.2973642689248845, 'beta': -2.9248176395212173}\"]",
    "Error fitting participant 2021: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.719303832056289, 'beta': -6.849535039104023}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1969371003684777, 'beta': 8.148470636001917}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.338835759759064, 'beta': -7.805648910656724}\"]",
    "Error fitting participant 2022: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1379377325745295, 'beta': 9.422392748012069}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.944027754003823, 'beta': -3.5810360078737595}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8481881682304002, 'beta': -7.608175681332021}\"]",
    "Error fitting participant 2023: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4733220506239704, 'beta': 5.5911043013010175}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.401345161057016, 'beta': 3.3926229488493203}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.839140968811339, 'beta': -1.005909133991425}\"]",
    "Error fitting participant 2024: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8660670660657481, 'beta': 5.655576886418427}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.784437202180785, 'beta': 3.4673449469376543}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1955056285259675, 'beta': 6.3226070108943375}\"]",
    "Error fitting participant 2025: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.301530723785614, 'beta': 3.352886785256487}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.3688854792844114, 'beta': 2.9189631732011385}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.025527902059789, 'beta': 1.198189045585881}\"]",
    "Error fitting participant 2026: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8088336936340745, 'beta': 7.028073983081523}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.8732016438824246, 'beta': -6.042886890744255}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4955770120754601, 'beta': 8.983330032554381}\"]",
    "Error fitting participant 2027: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.752610061607712, 'beta': 0.5522480727117127}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1430228769705515, 'beta': -9.428174134893766}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.038969253617993, 'beta': 6.260467524804511}\"]",
    "Error fitting participant 2028: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.701623421393933, 'beta': -8.023136362503614}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.3610172509646725, 'beta': -4.932375753330898}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5078896513625129, 'beta': 0.8625236153218214}\"]",
    "Error fitting participant 2029: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.383424365785414, 'beta': 0.7457457307607118}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.7369637027196303, 'beta': 4.398539733324348}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.0486002051531456, 'beta': -4.323818040628344}\"]",
    "Error fitting participant 2030: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.894475284402237, 'beta': -1.8971335777960334}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1619359465637698, 'beta': 5.7851092825673405}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7729477527268935, 'beta': 2.389612366741707}\"]",
    "Error fitting participant 2031: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.355238814621386, 'beta': -4.026103315852742}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2986399178214088, 'beta': -7.010548922297808}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.7223485397201115, 'beta': 1.735485652490432}\"]",
    "Error fitting participant 2032: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.6753854902044165, 'beta': 6.634009934691786}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.016406013334732, 'beta': 7.364347576773749}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1137972696370606, 'beta': -4.985649604378284}\"]",
    "Error fitting participant 2033: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4159475180110555, 'beta': -9.286278640092158}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.033645763687451, 'beta': 6.249576245200306}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.260269206637979, 'beta': 6.935558359219513}\"]",
    "Error fitting participant 2034: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.010651910551203, 'beta': -3.596246691622307}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.3506603050750434, 'beta': -9.863430220198676}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5621255870745023, 'beta': -0.7864267261392914}\"]",
    "Error fitting participant 2035: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.772329726566182, 'beta': -1.265368684264157}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.0353797388005095, 'beta': 5.8358625599948954}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.3005363500740614, 'beta': 0.11950411308000675}\"]",
    "Error fitting participant 2036: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.12250803675845, 'beta': 6.932891489405012}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.671277964220771, 'beta': 4.096532566271495}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.283604213078119, 'beta': 7.9925789806105705}\"]",
    "Error fitting participant 2037: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.678737503405138, 'beta': -7.96699864371643}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.712803646318779, 'beta': 3.417672871206566}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.54994015146454, 'beta': -2.658481947422853}\"]",
    "Error fitting participant 2038: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4352651287430258, 'beta': -7.955014781381857}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.495366361629701, 'beta': -4.8999172542724985}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.469658192660445, 'beta': -8.709861365384572}\"]",
    "Error fitting participant 2039: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.915767914670726, 'beta': 2.302086946328753}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5507223628455864, 'beta': -1.4728716051317488}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5087364178444527, 'beta': -2.5330423323746842}\"]",
    "Error fitting participant 2040: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1947265817480845, 'beta': 0.2620942536959632}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.285072049561664, 'beta': 3.7327898449434986}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.639742491028496, 'beta': -9.04112861977409}\"]",
    "Error fitting participant 2041: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.925198221333088, 'beta': -2.94032679314536}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.1613166007746405, 'beta': -7.181943308250416}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4062991029406327, 'beta': 8.585739405019787}\"]",
    "Error fitting participant 2042: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.607443627165635, 'beta': -4.377073997739831}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.33853773570737, 'beta': -0.16071955936422455}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4817349407895613, 'beta': -7.157816595461202}\"]",
    "Error fitting participant 2043: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7671745628073054, 'beta': 3.645977013270649}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.426179130815566, 'beta': 1.0223281148100263}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.927436215274706, 'beta': 2.3307394354700897}\"]",
    "Error fitting participant 2044: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.3280292441379316, 'beta': -0.874574492559482}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.658822136601854, 'beta': 9.621803180626774}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.645220145152618, 'beta': 2.0052717146038663}\"]",
    "Error fitting participant 2045: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8792283036712747, 'beta': 3.3364995991318445}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.225780474547687, 'beta': 1.9111553855529362}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.989080107791697, 'beta': 7.339284728754066}\"]",
    "Error fitting participant 2046: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.029664068871521, 'beta': 7.170112439551005}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2949464997376223, 'beta': -8.450595302118682}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.572811196256197, 'beta': -6.68188245019127}\"]",
    "Error fitting participant 2047: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.1556808356315544, 'beta': 2.1707959407111233}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.6794004055327143, 'beta': -7.929153650383127}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.900164155384626, 'beta': 6.323196687888469}\"]",
    "Error fitting participant 2048: Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2481244697107603, 'beta': 2.266634537889768}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.963998731427611, 'beta': 5.491943516260587}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.51661303954914, 'beta': 6.132192628387358}\"]"
  ],
  "skipped_participants": [
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2299297781080423, 'beta': -9.355916600912092}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.684437940008432, 'beta': 7.6478473351193585}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.7579486679979097, 'beta': -5.093519894114918}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.211890909220365, 'beta': 6.14176286599729}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.7487495934524553, 'beta': 7.094703080635085}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.898354556948573, 'beta': 4.865383496286174}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.739851464502162, 'beta': 7.784954655942549}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.0858206967445265, 'beta': -3.248696839900946}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.840099337146405, 'beta': 9.667191115691327}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.7681493511898587, 'beta': 7.343183068095655}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.6517145692316806, 'beta': -4.896103817264141}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.002683038592353, 'beta': 1.3952020585892537}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4221521016977028, 'beta': -9.223232072307663}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.525635556911847, 'beta': 9.303888345787733}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2644585302638522, 'beta': 6.684955905836517}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5493408869293646, 'beta': 3.074033854516216}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.715691120907236, 'beta': 2.7134260438012614}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.3230080273560527, 'beta': 6.582473554162565}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.697515248558863, 'beta': 5.232549018747768}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.4712305361249465, 'beta': 6.993089822081558}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.593237976090517, 'beta': 3.42469868666673}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.075487964709868, 'beta': -1.647255333277899}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.5601750292765164, 'beta': -3.677060106123424}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.175409417935877, 'beta': -4.360587384329477}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9035638681286686, 'beta': -0.703764316096473}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.0677991771467275, 'beta': -7.333554110824954}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.752363622267359, 'beta': 8.19322243608833}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2512246864192798, 'beta': -2.205851406353845}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7371633474364327, 'beta': 4.399953079481882}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.0854234187649556, 'beta': 3.735534034868987}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.976616549420736, 'beta': 0.8439122382525373}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.96056142637315, 'beta': -4.089837914975714}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.08305123946251, 'beta': -5.182309920103361}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.6915843297423376, 'beta': -5.48350402630954}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.401592666669952, 'beta': 9.119449250167825}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.6192477461310957, 'beta': -7.243963210420352}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.0031493231286985, 'beta': 2.8693006593147814}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.781959259555229, 'beta': 5.169896430793742}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.663404762722151, 'beta': 7.968027092143526}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.786845328044362, 'beta': 0.22673255548007987}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.363704763175537, 'beta': 0.34523975149204134}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.3864452834834773, 'beta': 3.2618107129644685}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.994623498625967, 'beta': -8.645257349590516}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.446995220767043, 'beta': -8.240825850289193}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.454738651177694, 'beta': 0.8049035291011002}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.611428302793563, 'beta': -1.6581134857282613}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.101679340947394, 'beta': -5.6609846818583875}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.6865857886438023, 'beta': 3.843496411719782}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1019064374387884, 'beta': -3.456205336596783}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.0249998119208685, 'beta': -0.7098381320234441}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.557731130698604, 'beta': 1.4284466780181546}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.00426085167684, 'beta': 9.773604485842853}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.499802431836498, 'beta': -9.661074003176973}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.783721309117107, 'beta': -6.4575772117086245}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.837751720186676, 'beta': 9.504304126029115}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.650191818010183, 'beta': -6.233783281516048}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.913136050879067, 'beta': 1.4725029969827368}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9939364255731302, 'beta': -8.455184887532166}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5443715983775412, 'beta': -2.2547388631401404}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.130522735059632, 'beta': 2.926884051739613}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.066510722135357, 'beta': 2.3855275173902886}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.583150253251013, 'beta': 5.929359948799636}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.6171992027367383, 'beta': -1.6855648890159252}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.1038718560958167, 'beta': -9.878091351256748}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8605997255454452, 'beta': 2.8984612235722107}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.797071698108047, 'beta': -0.7038063670287702}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.6063709170071334, 'beta': -9.208752541877516}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.386862153504181, 'beta': 8.179449417563635}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.096945140364046, 'beta': 4.719343952821941}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.106887911405476, 'beta': -4.74343836866101}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.045478971910411, 'beta': 4.712707498250289}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.404063814042436, 'beta': 3.4476196132732504}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.933262048391054, 'beta': -3.3662137145305904}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.374524080213017, 'beta': -1.7184806905290255}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9097101675368857, 'beta': -8.051930636280094}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8227865483022163, 'beta': 2.9030580980148315}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.526105641627337, 'beta': 9.861421015360051}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1215242469201328, 'beta': 3.227553551284627}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.522815129263237, 'beta': -3.6226966737503545}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2706464439482383, 'beta': 6.973649146701483}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8696076942276507, 'beta': -0.5139032515132751}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.5609764558427295, 'beta': -9.808487799874346}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.811246729287027, 'beta': 8.219732852351594}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1027094189599143, 'beta': -4.750372525526809}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5303564083329597, 'beta': -1.70998278702287}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.020121871658182, 'beta': 1.58516509041905}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5128770039246997, 'beta': 6.943150378205591}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.9738234838305138, 'beta': -2.745714794426439}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2642941976892246, 'beta': -2.95111271540641}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.2902508195819715, 'beta': 5.615252703703073}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.950641706372983, 'beta': -9.142689572269585}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.393822657526259, 'beta': -2.7100706879578285}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.5103563542551015, 'beta': -4.502559139256697}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8517134976310436, 'beta': 6.095679062649165}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.977016567651549, 'beta': -1.0701457911962855}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.043194787346783, 'beta': -5.811245513625054}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8201643080489895, 'beta': -8.72516943384101}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.0867252543151777, 'beta': 8.471544141968302}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.783778812666088, 'beta': -4.432207883740369}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.525623761454729, 'beta': -2.1681424606656963}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.418318328186425, 'beta': -5.954746819302201}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.58874895665709, 'beta': 0.3455109265461189}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.269634496988795, 'beta': 5.623177451999201}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9371852785142427, 'beta': -3.5888122054034577}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.1871678759007462, 'beta': 4.061776207646734}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.779718237508987, 'beta': -8.878157870175123}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4695922557232133, 'beta': 6.010953901014425}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.428167371889281, 'beta': -3.862842740730188}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4159762700250917, 'beta': 4.914385507934515}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.9540595645693117, 'beta': 6.0263131613903695}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.8083474382939055, 'beta': 1.3522894801334822}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.394442155167461, 'beta': -5.164322634746583}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.214281426818531, 'beta': 6.129982066611252}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.6105152555109057, 'beta': 3.649359696500838}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.1602946391008455, 'beta': -4.7467912713073135}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.508460942051213, 'beta': -0.2992357608995295}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.320445608365637, 'beta': 4.451452137137796}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.649789652391296, 'beta': 0.3454498514853359}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.80760537775281, 'beta': 3.5624683774773764}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9663169886772254, 'beta': 2.0793665892594007}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.950104153258568, 'beta': -8.270480509720741}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4119366531291417, 'beta': -0.8849413787837985}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.282742402882012, 'beta': 9.631123856212582}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.7198937331999096, 'beta': 7.766182350605252}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.672377945523414, 'beta': -4.652487700126963}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.512141627853171, 'beta': -3.3588382468457105}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4617839054885944, 'beta': 7.5777172936885115}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.4896814714368327, 'beta': -1.0953742149955659}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.9801501459222473, 'beta': 1.0457117281171726}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.058375183644576, 'beta': -6.308775183313364}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.927717403106389, 'beta': 4.0551043265824305}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5544307270803892, 'beta': 0.540339747590707}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.7730681676816094, 'beta': 7.2586121178376075}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.487074315904175, 'beta': -3.4086783570139634}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.406301193572171, 'beta': 3.370995284356386}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.90173620721977, 'beta': -9.970527960553818}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.8981737510502765, 'beta': 6.170582430417998}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.072966479325478, 'beta': -1.192471664010215}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.660203215919557, 'beta': 0.29033273632011714}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.3109330739559524, 'beta': -5.4350048355784075}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.257130549679031, 'beta': -7.525631936906323}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.2732546324638205, 'beta': -5.896038322513444}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.0244425400912318, 'beta': -2.9140871721310617}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.605271199417165, 'beta': 2.6268973774458164}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5493313779295441, 'beta': 7.482046258699061}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2769705969222716, 'beta': -7.215615651427831}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.162243222845479, 'beta': 0.23313000982942356}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.104938164727475, 'beta': -8.026682126746556}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.459839393445154, 'beta': 7.6870033134506315}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.846848741422085, 'beta': 8.477550864124478}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2079153585166997, 'beta': 5.491566852225134}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.23626746295828, 'beta': 7.1199173713342425}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.999973251770794, 'beta': 8.08703996506571}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.9615863377877423, 'beta': -3.5297252034503295}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5865134549155098, 'beta': -1.8584884235273957}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.4344065274592186, 'beta': -2.3890213719224107}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5995652472037762, 'beta': -8.49873341497089}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.486656028419831, 'beta': 1.6200075746305558}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5913752613074497, 'beta': 5.617581760925582}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4138092784859744, 'beta': -0.26534963922403243}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.700482118311862, 'beta': -6.040826140870392}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.8281900301271987, 'beta': -0.5868579373218648}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8217558029903054, 'beta': -5.122493320567938}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.920270189460897, 'beta': 7.804644070576298}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.3994553032242996, 'beta': -9.02802037658991}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.227900687667093, 'beta': -1.0976136832779453}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.861492212545237, 'beta': 5.487781910116279}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.570636252447406, 'beta': 2.1665726167887165}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7922186938630915, 'beta': 4.910368987010678}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.0291747605561543, 'beta': -8.404519305442248}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2919197197353474, 'beta': 3.820764949524282}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.946961936171919, 'beta': 4.5259900238806505}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.380124280127446, 'beta': -2.464902922142458}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.523437740584168, 'beta': -6.828496318798036}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.924211424383838, 'beta': 5.507001220940248}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.421275249947909, 'beta': -2.5889564246783703}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.3361494586056168, 'beta': -4.5838787617236605}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.7351646957843907, 'beta': -9.82233363597687}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.56051759346585, 'beta': -0.4145241931598047}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7018866365834455, 'beta': -4.443799030007025}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.677411186089487, 'beta': 7.545414625515562}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.307581208028234, 'beta': 2.7325527165298613}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.807707567128861, 'beta': -7.721063551496135}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1091459988320924, 'beta': 6.121461897279843}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4673606344355812, 'beta': -8.067443239806655}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.9869724921676446, 'beta': 1.7234378877728762}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.03554111475426, 'beta': -2.0129445255219895}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.725338765111404, 'beta': 2.1992037469617074}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.2973642689248845, 'beta': -2.9248176395212173}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.719303832056289, 'beta': -6.849535039104023}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1969371003684777, 'beta': 8.148470636001917}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.338835759759064, 'beta': -7.805648910656724}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1379377325745295, 'beta': 9.422392748012069}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.944027754003823, 'beta': -3.5810360078737595}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8481881682304002, 'beta': -7.608175681332021}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4733220506239704, 'beta': 5.5911043013010175}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.401345161057016, 'beta': 3.3926229488493203}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.839140968811339, 'beta': -1.005909133991425}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8660670660657481, 'beta': 5.655576886418427}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.784437202180785, 'beta': 3.4673449469376543}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1955056285259675, 'beta': 6.3226070108943375}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.301530723785614, 'beta': 3.352886785256487}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.3688854792844114, 'beta': 2.9189631732011385}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.025527902059789, 'beta': 1.198189045585881}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8088336936340745, 'beta': 7.028073983081523}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.8732016438824246, 'beta': -6.042886890744255}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4955770120754601, 'beta': 8.983330032554381}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.752610061607712, 'beta': 0.5522480727117127}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.1430228769705515, 'beta': -9.428174134893766}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.038969253617993, 'beta': 6.260467524804511}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.701623421393933, 'beta': -8.023136362503614}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.3610172509646725, 'beta': -4.932375753330898}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5078896513625129, 'beta': 0.8625236153218214}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.383424365785414, 'beta': 0.7457457307607118}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.7369637027196303, 'beta': 4.398539733324348}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.0486002051531456, 'beta': -4.323818040628344}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.894475284402237, 'beta': -1.8971335777960334}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1619359465637698, 'beta': 5.7851092825673405}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7729477527268935, 'beta': 2.389612366741707}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.355238814621386, 'beta': -4.026103315852742}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.2986399178214088, 'beta': -7.010548922297808}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.7223485397201115, 'beta': 1.735485652490432}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.6753854902044165, 'beta': 6.634009934691786}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.016406013334732, 'beta': 7.364347576773749}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1137972696370606, 'beta': -4.985649604378284}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4159475180110555, 'beta': -9.286278640092158}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.033645763687451, 'beta': 6.249576245200306}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.260269206637979, 'beta': 6.935558359219513}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.010651910551203, 'beta': -3.596246691622307}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.3506603050750434, 'beta': -9.863430220198676}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5621255870745023, 'beta': -0.7864267261392914}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.772329726566182, 'beta': -1.265368684264157}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.0353797388005095, 'beta': 5.8358625599948954}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.3005363500740614, 'beta': 0.11950411308000675}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.12250803675845, 'beta': 6.932891489405012}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.671277964220771, 'beta': 4.096532566271495}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.283604213078119, 'beta': 7.9925789806105705}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.678737503405138, 'beta': -7.96699864371643}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.712803646318779, 'beta': 3.417672871206566}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.54994015146454, 'beta': -2.658481947422853}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.4352651287430258, 'beta': -7.955014781381857}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.495366361629701, 'beta': -4.8999172542724985}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.469658192660445, 'beta': -8.709861365384572}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.915767914670726, 'beta': 2.302086946328753}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.5507223628455864, 'beta': -1.4728716051317488}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.5087364178444527, 'beta': -2.5330423323746842}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.1947265817480845, 'beta': 0.2620942536959632}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.285072049561664, 'beta': 3.7327898449434986}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.639742491028496, 'beta': -9.04112861977409}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.925198221333088, 'beta': -2.94032679314536}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.1613166007746405, 'beta': -7.181943308250416}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4062991029406327, 'beta': 8.585739405019787}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.607443627165635, 'beta': -4.377073997739831}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.33853773570737, 'beta': -0.16071955936422455}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.4817349407895613, 'beta': -7.157816595461202}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.7671745628073054, 'beta': 3.645977013270649}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.426179130815566, 'beta': 1.0223281148100263}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.927436215274706, 'beta': 2.3307394354700897}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.3280292441379316, 'beta': -0.874574492559482}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.658822136601854, 'beta': 9.621803180626774}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.645220145152618, 'beta': 2.0052717146038663}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.8792283036712747, 'beta': 3.3364995991318445}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.225780474547687, 'beta': 1.9111553855529362}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.989080107791697, 'beta': 7.339284728754066}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.029664068871521, 'beta': 7.170112439551005}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2949464997376223, 'beta': -8.450595302118682}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.572811196256197, 'beta': -6.68188245019127}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.1556808356315544, 'beta': 2.1707959407111233}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 3.6794004055327143, 'beta': -7.929153650383127}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.900164155384626, 'beta': 6.323196687888469}\"]"
    },
    {
      "participant_id": null,
      "error": "Error in parameter fitting: All optimization attempts failed. Errors: [\"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 1.2481244697107603, 'beta': 2.266634537889768}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 4.963998731427611, 'beta': 5.491943516260587}\", \"Error in likelihood calculation: simulate_model() got an unexpected keyword argument 'lambda'\\nParams: {'lambda': 2.51661303954914, 'beta': 6.132192628387358}\"]"
    }
  ],
  "num_skipped_participants": 91,
  "fitting_error": "No successful parameter fits obtained",
  "recovery_error": "No fitting results found",
  "previous_models": [
    "Specification: U = (split_self - 3*token_self) - lambda * max(3*token_self - split_self, 0) + beta * (split_self - split_opp) / combined_earning\nSummary: A prospect-theoretic model combining loss aversion relative to the participant's contribution (3*token_self) and sensitivity to inequity between shares. Losses (receiving less than contributed) are scaled by lambda, while beta captures reactions to differences in proposed splits, adjusted for total earnings.\nTarget Variable: accept\nPrediction Type: utility"
  ]
}