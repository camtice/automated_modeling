run_number,average_bic,model_specification,model_summary,version,alpha_recovery,beta_recovery,gamma_recovery,theta_recovery,lambda_param_recovery,tau_param_recovery,kappa_recovery,mu_recovery,delta_recovery,rho_recovery,weight_self_recovery,penalty_advantage_recovery,eta_recovery,epsilon_recovery,lambda_neg_recovery,lambda_pos_recovery,phi_recovery
16,33.15372342510713,"Let f = (token_self/(token_self + token_opp)) * combined_earning
U_accept = split_self - lambda_param * ln(1 + exp(-tau_param * (split_self - f)))","A reference-dependent responder utility model that penalizes offers falling short of a fairness benchmark based on token shares and combined earnings. The model calculates a fairness reference (f) and subtracts a logarithmically scaled penalty from the offered monetary share, using two learnable parameters (lambda_param and tau_param) to control the magnitude and sensitivity of the penalty.",v1,,,,,0.5119302494829077,0.3769307001152133,,,,,,,,,,,
6,33.32415504379323,"Let f = (token_self / (token_self + token_opp)) × combined_earning  
Let Δ = split_self − f  
U_accept = f + { Δ^rho,                     if Δ ≥ 0  
              − lambda_param × (−Δ)^rho,      if Δ < 0 }",A reference-dependent utility model in which the fairness benchmark is computed from the participant’s token share and combined earnings. The utility for accepting an offer combines the fairness reference with a nonlinear evaluation of deviations from fairness: gains (offers above fairness) are mapped via a power function while losses (offers below fairness) incur an amplified penalty governed by a loss aversion parameter. Two learnable parameters (rho and lambda_param) govern diminishing sensitivity and loss aversion.,v1,,,,,0.8025405906363311,,,,,0.7152626157361615,,,,,,,
11,33.48442388112805,"Let f = (token_self / (token_self + token_opp)) × combined_earning  
Let Δ = split_self − f  
Then,  
U_accept = { split_self + (Δ)^gamma,       if Δ ≥ 0  
             split_self − lambda_param × (−Δ)^gamma,  if Δ < 0 }","This model computes responder utility as the offered monetary share plus an adjustment based on fairness deviation. A fairness benchmark is derived from the participant’s token share and combined earnings. The adjustment is applied using a power function with curvature parameter gamma, and a loss‐aversion parameter lambda_param scales negative deviations, capturing asymmetric sensitivity to fairness.",v1,,,0.5879776932488179,,0.8087990965155603,,,,,,,,,,,,
10,33.6666573659074,"Let f = (token_self / (token_self + token_opp)) × combined_earning  
U_accept = split_self − kappa × f × | ln(split_self / f) |^(epsilon)",A reference-dependent utility model computed by first deriving a fairness benchmark from the token distribution and total earnings. The offered monetary share is penalized according to the absolute logarithmic deviation from this fairness benchmark. The penalty is scaled by the fairness reference and modulated by two learnable parameters: kappa (penalty weight) and epsilon (curvature or sensitivity to deviation). This formulation enables a calibrated assessment of fairness deviations while predicting responder acceptance based solely on the computed utility.,v1,,,,,,,0.6617387320762008,,,,,,,0.7591585140132779,,,
5,33.919640769986295,U_accept = split_self - alpha * | split_self - (token_self/(token_self + token_opp))*combined_earning |^delta,"A utility model for responders that calculates the attractiveness of accepting an offer by subtracting a non-linear penalty from the monetary share offered (split_self). The penalty is determined by the absolute deviation between the offer and a fairness reference derived from the participant’s token share relative to the total and the combined earnings. Two learnable parameters, alpha and delta, control the sensitivity and curvature of the penalty, respectively.",v1,0.8581735838891374,,,,,,,,0.6135866296786503,,,,,,,,
18,34.09741892334743,"U_accept = split_self - lambda_param * [ max( ((token_self/(token_self + token_opp)) * combined_earning / split_self - 1), 0 ) ]^(tau_param)","A reference-dependent responder utility model that computes utility as the offered monetary share (split_self) minus a penalty applied when the offer falls below a fairness benchmark. The fairness benchmark is defined as the participant’s proportional token share multiplied by the combined earnings. The penalty is a nonlinear function of the relative deviation from fairness, with its magnitude and curvature controlled by two learnable parameters, lambda_param and tau_param.",v1,,,,,0.7967592424766149,0.7294598651165385,,,,,,,,,,,
20,34.896593582161806,"Let f = (token_self / (token_self + token_opp)) * combined_earning  
Let Δ = split_self - f  
U_accept = split_self + I(Δ ≥ 0) · [ eta * f * ln(1 + Δ/f) ] - I(Δ < 0) · [ lambda_param * f * ln(1 + (f - split_self)/f) ]","The model computes responder utility as the sum of the offered monetary share and a fairness adjustment. The fairness benchmark is determined by the participant’s proportional token allocation scaled by combined earnings. For offers above the benchmark, a bonus is added via a logarithmic function scaled by eta, whereas offers below the benchmark are penalized via a logarithmic term scaled by lambda_param. This parsimonious two-parameter formulation captures nonlinear, diminishing sensitivity to deviations from a fairness reference.",v1,,,,,0.9059611299917866,,,,,,,,0.23696331592335665,,,,
9,34.89885644853785,"Let f = (token_self / (token_self + token_opp)) * combined_earning  
Let Δ = split_self - f  
Define Δ_positive = max(Δ, 0) and Δ_negative = max(-Δ, 0)  
Then, the utility of accepting an offer is given by:  
U_accept = split_self + f * ( eta * tanh( Δ_positive / f ) - lambda_param * tanh( Δ_negative / f ) )",This model computes the utility for a responder by first deriving a fairness benchmark from the participant’s token share and the total earnings. The offered share (split_self) is then adjusted by a bonus if it exceeds this fairness reference or a penalty if it falls short. These adjustments are scaled by separate learnable parameters (eta for positive deviations and lambda_param for negative deviations) and modulated by a saturating hyperbolic tangent function.,v1,,,,,0.870712197943301,,,,,,,,0.044978970722926365,,,,
17,34.909185480062426,"Let f = (token_self / (token_self + token_opp)) * combined_earning  
Let Δ = split_self - f  
U_accept = I(Δ ≥ 0) · [ split_self + tau_param · ln(1 + Δ) ] + I(Δ < 0) · [ split_self - lambda_param · ln(1 + (f - split_self)) ]",This model computes the utility of accepting an offer by first deriving a fairness reference from token allocation and combined earnings. The utility equals the offered split adjusted by a logarithmic bonus if the offer exceeds fairness or by a penalty if it falls short—with two learnable parameters governing the bonus (tau_param) and penalty (lambda_param) effects.,v1,,,,,0.9401594799390206,0.09259158465637231,,,,,,,,,,,
12,34.96183904340063,"f = (token_self / (token_self + token_opp)) * combined_earning
δ = (split_self - f) / f
U_accept = split_self + lambda_param * arctan( tau_param * δ )",This model computes the utility of accepting an offer by adding the monetary value (split_self) to a fairness adjustment determined by the normalized deviation from a fairness reference. The fairness reference is defined as the participant's token share times the combined earnings. The deviation is transformed via an arctan function—scaled by two learnable parameters (lambda_param and tau_param) that respectively modulate the magnitude and sensitivity of the fairness adjustment. This formulation captures saturating sensitivity to fairness deviations in a parsimonious two-parameter framework.,v1,,,,,0.8713590708981541,0.24699111047946123,,,,,,,,,,,
1,35.00456496133006,"U_accept = split_self - alpha * max( (token_self/(token_self+token_opp))*combined_earning - split_self , 0 ) - beta * max( split_self - (token_self/(token_self+token_opp))*combined_earning , 0 )","The model computes the utility of accepting an offer by comparing the offered monetary share (split_self) to a fairness reference derived from the participant’s token allocation (token_self) relative to total tokens (token_self + token_opp) and the combined earning. Two learnable parameters (alpha and beta) penalize deviations from this fairness reference, reflecting sensitivity to disadvantageous and advantageous inequity respectively.",v1,0.9169567976921029,0.5237196671145925,,,,,,,,,,,,,,,
14,35.2258436327227,"Let f = (token_self / (token_self + token_opp)) * combined_earning  
Let D = ln(split_self / f)  
U_accept = split_self - { [I(split_self < f) × lambda_neg  +  I(split_self ≥ f) × lambda_pos] × |D| }","A reference-dependent utility model in which the fairness benchmark is computed from the participant’s token share and group earnings. The utility for accepting an offer equals the monetary offer (split_self) diminished by a penalty proportional to the absolute logarithmic deviation from fairness. Two learnable parameters—lambda_neg for disadvantageous deviations and lambda_pos for advantageous deviations—modulate the penalty, capturing asymmetric inequity aversion.",v1,,,,,,,,,,,,,,,0.8994954633164022,0.5667184904166462,
13,36.36407228997444,"Let f = (token_self / (token_self + token_opp)) × combined_earning  
Let Δ = |split_self − f| / f  
U_accept = split_self × (1 − lambda_param × (Δ)^rho)",A multiplicative utility model for responders where the offered monetary share (split_self) is adjusted by a fairness penalty based on the normalized deviation of the offer from a fairness benchmark. The fairness benchmark is determined by the participant’s relative token contribution scaled by total earnings. The two learnable parameters (lambda_param and rho) control the magnitude and nonlinearity of the penalty.,v1,,,,,0.7102786544793677,,,,,0.7474451354520553,,,,,,,
19,36.65177142743666,"Let f = (token_self / (token_self + token_opp)) × combined_earning  
U_accept = phi × split_self + (1 - phi) × f - lambda_param × (split_self - f)²","A utility model for responders that integrates self-interest and fairness considerations. The model calculates a fairness reference from the participant’s token share and total earnings, then forms a weighted combination of the offer and this fairness reference. A quadratic penalty for deviations from fairness adjusts the utility, with two learnable parameters controlling the weight on self-interest (phi) and sensitivity to fairness deviations (lambda_param).",v1,,,,,0.8273399111676376,,,,,,,,,,,,0.1845063433365864
2,40.22961830172509,"U_accept = split_self - gamma * [max( fairness_reference - split_self, 0 )]^2 - theta * [max( split_self - fairness_reference, 0 )]^2

where fairness_reference = (token_self / (token_self + token_opp)) * combined_earning",A quadratic inequity aversion model for responders. The utility of accepting an offer is computed as the offered monetary value minus a squared penalty based on deviation from a fairness reference derived from token allocation. Two learnable parameters govern the sensitivity to disadvantageous (gamma) and advantageous (theta) inequity.,v1,,,0.7454669426940942,0.5932763934501908,,,,,,,,,,,,,
4,40.80297091787691,"U_accept = ln(1 + split_self) 
           - kappa * max( ln(1 + (token_self/(token_self+token_opp))*combined_earning) - ln(1 + split_self), 0 )
           - mu    * max( ln(1 + split_self) - ln(1 + (token_self/(token_self+token_opp))*combined_earning), 0 )","This model computes the utility of accepting an offer by first applying a logarithmic transformation to the offered monetary amount to capture diminishing sensitivity. A fairness reference is derived from the participant’s token ratio and combined earnings. Two learnable parameters (kappa and mu) penalize deviations from this fairness reference, with separate weights for disadvantageous and advantageous inequity, respectively.",v1,,,,,,,0.8707503430371827,0.19630269451783458,,,,,,,,,
7,44.27470760696037,"U_accept = split_self - (1 - weight_self) * ( max((token_self/(token_self+token_opp))*combined_earning - split_self, 0) + penalty_advantage * max(split_self - (token_self/(token_self+token_opp))*combined_earning, 0) )",This model computes the utility of accepting an offer as a trade-off between self-interested monetary gain and fairness deviations relative to a fairness benchmark calculated from token allocations and total earnings. Two learnable parameters govern the weight on self-interest and differential sensitivity to over-generous offers.,v1,,,,,,,,,,,0.7267217690629173,0.4491494263698585,,,,,
3,65.2336498294567,U_accept = split_self + lambda_param * exp(- tau_param * abs(split_self - ((token_self/(token_self+token_opp))*combined_earning))),"A utility model for responder behavior that combines the offered monetary share (split_self) with an additional fairness bonus. The bonus is maximal when the offer equals the fairness reference (computed from token_self, token_opp, and combined_earning) and decays exponentially with the absolute deviation from fairness. The model includes two learnable parameters: lambda_param, scaling the bonus magnitude, and tau_param, governing the decay rate.",v1,,,,,0.26811647613755896,0.4996295628820913,,,,,,,,,,,
15,66.73455775365089,"Let f = (token_self/(token_self+token_opp)) × combined_earning

Then, 
 if split_self ≥ f:
  U_accept = f^(1 – eta) × (split_self)^(eta)
 else:
  U_accept = f^(1 – theta) × (split_self)^(theta)","A multiplicative (geometric) utility model for responder behavior that compares the offered share to a fairness benchmark derived from token allocations and combined earnings. When the offer meets or exceeds the fairness reference, reward sensitivity is governed by the parameter eta; when below, penalty sensitivity is governed by theta. This two-parameter formulation captures asymmetric integration of fairness and monetary outcomes.",v1,,,,0.45826998641411,,,,,,,,,0.12087320448798096,,,,
8,,"Let f = (token_self/(token_self+token_opp)) × combined_earning  
Let Δ = ln(split_self/f)  
U_accept = split_self + { eta × f × Δ   if Δ ≥ 0  
                          - lambda × f × |Δ|   if Δ < 0 }","The model computes the utility for accepting an offer by combining the monetary value offered (split_self) with a fairness adjustment based on the logarithmic deviation of the offer from a fairness benchmark (f). This benchmark is derived from the participant’s token share relative to total tokens and the combined earnings. Two learnable parameters, eta and lambda, separately scale the bonus for offers exceeding fairness and the penalty for offers below fairness.",v1,,,,,,,,,,,,,,,,,
