run_number,average_bic,model_specification,model_summary,version,gamma_recovery,alpha_recovery,theta_recovery,kappa_recovery
1,,"U_i = gamma * split_self - alpha * max( (combined_earning * (token_self / (token_self + token_opp))) - split_self , 0 )","This model computes the utility for a responder’s acceptance decision by combining the monetary gain (split_self) with a fairness penalty based on the difference between an expected fair share (derived from token_self, token_opp, and combined_earning) and the actual offer. Two learnable parameters, gamma and alpha, capture sensitivity to monetary reward and aversion to disadvantageous inequality respectively.",v5,0.8562801940856043,0.8398124927400509,,
2,,"U_accept = split_self - theta * ( max( combined_earning * ( token_self / ( token_self + token_opp ) ) - split_self , 0 ) )^(kappa)",A utility model for responder behavior that computes the acceptance utility as the offered share minus a non-linear penalty for offers that fall below the fairness norm. The fairness norm is determined by the proportion of tokens obtained relative to the total tokens. Two learnable parameters—theta and kappa—control the sensitivity and non-linearity of the penalty.,v5,,,0.7122481784025382,0.5651843952757094
3,,U_accept = lambda_param * split_self - beta * (split_self - combined_earning * (token_self / (token_self + token_opp)))^2,A reference-dependent utility model for respondent behavior where the utility of accepting an offer is determined by a weighted monetary gain minus a quadratic penalty for deviation from the fairness norm. The fairness norm is computed as the expected fair share based on tokens contributed. This model employs two learnable parameters: lambda_param scales the gain and beta governs fairness sensitivity.,v5,,,,